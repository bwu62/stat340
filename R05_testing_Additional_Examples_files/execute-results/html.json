{
  "hash": "152a1cbc24914ee13dd6592f78eab0b1",
  "result": {
    "markdown": "---\ntitle: \"Monte Carlo Testing: Additional Examples\"\noutput: \n  html_document:\n    self_contained: yes\n---\n\n::: {.cell}\n\n:::\n\n\n\n\n## Type 1 Errors and the P-Value Distribution\n\nI want to sample from $N(\\mu, 5^2)$\n\nSay I want to test $H_0: \\mu = 10$ vs $H_A: \\mu > 10$\n\nI can use the test statistic simply being $\\bar{X}$. Naturally if $\\bar{X}$ is high then I would consider that evidence against $H_0$. \n\nHere's a function to generate sample data given $\\mu$ and a test statistic function.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-2_3d47fc8e0c95bbf146f2423c39560c84'}\n\n```{.r .cell-code}\ngenerate_data <- function(n, mu){\n  return(rnorm(n, mu, 5))\n}\n\ntest_stat <- function(someData){\n  return(mean(someData))\n}\n```\n:::\n\n\n\n\nSuppose we want to use a significance level of 7%. Why 7%? Why not!?! Imagine here is some data - note that this data is being generated from an alternative model ($\\mu=11$).\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-3_ab20344cc038c0cb3529eff064a0cb30'}\n\n```{.r .cell-code}\nset.seed(1)\nmyData <- rnorm(25, 11, 5)\nmyData\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1]  7.86773095 11.91821662  6.82185694 18.97640401 12.64753886  6.89765808\n [7] 13.43714526 14.69162353 13.87890676  9.47305806 18.55890584 12.94921618\n[13]  7.89379710 -0.07349944 16.62465459 10.77533195 10.91904868 15.71918105\n[19] 15.10610598 13.96950661 15.59488686 14.91068150 11.37282492  1.05324152\n[25] 14.09912874\n```\n:::\n:::\n\n\n\nLet's generate a test statistic distribution under the null hypothesis. What are typical values of $\\bar{X}$ when the null hypothesis is true? If you remember the central limit theorem it will come as no surprise that this distribution is $N(10, 5^2/25)$\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-4_7ee82e49457318e068e002d21a517fa4'}\n\n```{.r .cell-code}\nNMC <- 10000\nt.sim <- replicate(NMC, test_stat(generate_data(25, 10)))\nhist(t.sim)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(t.sim);var(t.sim)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.994043\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9879012\n```\n:::\n:::\n\n\n\nWe want to pick a rejection threshold (a critical value) $t_\\alpha$ designed to achieve a significance level (type 1 error rate) of .07. If $\\bar{X} \\geq t_\\alpha$ we reject $H_0$, and this should occur 7% of the time when $H_0$ is true. Rather than using the theoretical normal distribution for $\\bar{X}$, we'll use the simulated test statistics to approximate the threshold. \n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-5_4cc9c2cd1710a95be8ef09e8f1c866d9'}\n\n```{.r .cell-code}\nt.crit <- quantile(t.sim, .93)\n#verify\nmean(t.sim >= t.crit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.07\n```\n:::\n:::\n\n\n\nLet's just make sure that if the null hypothesis is true that we end up rejecting 7%\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-6_4c5f9e7a540b3457b5bca128c70fb227'}\n\n```{.r .cell-code}\nresult <- FALSE\nfor(i in 1:NMC){\n  nullData <- generate_data(25, 10)\n  result[i] <- test_stat(nullData) >= t.crit\n  # result will be TRUE if the test stat is beyond the critical\n  # value, ie. we reject the null\n  #False otherwise\n}\nmean(result)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0732\n```\n:::\n:::\n\n\n\nI'm curious about the p-values you get from this test. I mean - if the null hypothesis is true, what p-values would you expect to get?\nLet's repeat with the null hypothesis, and just record p-values. \n\nTo put it another way - Because $T(D_0)$ is a random variable, $Pr[T(D) > T(d_0)]$ is a random variable as well; the $p$-value under the null hypothesis can be thought of as a random variable and thus it has a distribution. What is the shape of this distribution? Let's use Monte Carlo to estimate it.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-7_f51f59890a5d352c59dc255f0c588a89'}\n\n```{.r .cell-code}\np.val <- 0\nfor(i in 1:NMC){\n  nullData <- generate_data(25, 10)\n  p.val[i] <- mean(t.sim >= test_stat(nullData))\n}\nhist(p.val)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\nIf the null hypothesis is true, and we get data and calculate a p-value, the p-value will be uniformly distributed according to a uniform(0,1). This turns out to be the case for ANY hypothesis test. It's a little less precise when your test statistic is a discrete r.v., but it is more or less going to be the case.\n\nWhat about the distribution of p-values if the mean  = 11 (i.e. the null is false) - this is the model from which we got our initial data.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-8_692c597c64db846a0c5328b792bb0e6c'}\n\n```{.r .cell-code}\np.val <- 0\nfor(i in 1:NMC){\n  altData <- generate_data(25, 11)\n  p.val[i] <- mean(t.sim >= test_stat(altData))\n}\nhist(p.val)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(p.val <= 0.07)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3277\n```\n:::\n\n```{.r .cell-code}\nmean(p.val >0.07)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6723\n```\n:::\n:::\n\n\n\nSo it seems if the data was drawn from a normal distribution with a mean of 11, we have a 32.77% probability of getting \"strong evidence\" to reject H_0. That's the power of this test.\n\n\n## Achieving exact alpha with a discrete test statistic distribution\n\nGoing back to the example from lecture, suppose we want to achieve an exact type 1 error rate of 2.5%. We're flipping a coin and we want to know what rejection rule we should use for the lower-bound on the number of heads - to conclude that the coin is not fair.\n\nTo remind you - we are flipping a coin 200 times, we have to come up with a rule such that if the flips are producing too low of heads, we reject the null hypothesis (and we would do something similar for the upper tail).\n\nThe goal is to find some rejection rule for low X that will reject 2.5% of the time for a fair coin.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-9_3d18799dcc4a72b8fabdd3f3b3592051'}\n\n```{.r .cell-code}\n#P (X <= 85) for X~Binom(200, .5)\npbinom(85, 200, .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0200186\n```\n:::\n\n```{.r .cell-code}\n#P (X <= 86) for X~Binom(200, .5)\npbinom(86, 200, .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02798287\n```\n:::\n\n```{.r .cell-code}\n#P(X = 86)\ndbinom(86, 200, .5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.007964274\n```\n:::\n:::\n\n\n\n$$P(X \\leq 85) = 0.0200186$$\n\n$$P(X \\leq 86) = 0.02798287$$\n\n$$P(X=86) = .00796$$\n\nThe idea is we will reject always when X \\<= 85, sometimes when X =\n86, and never when X \\> 86. What do we want for the sometimes? We want\n$$P(X \\leq 85) + P(reject \\cap X=86) = 0.025$$ In other words:\n\n$$P(reject \\cap X=86) = 0.025-P(X \\leq 85)$$ Expanding the left hand\nside\n\n$$P(reject | X=86)\\cdot P(X=86) = 0.025-P(X \\leq 85)$$ Divide both sides\nby $P(X=86)$ $$P(reject | X=86) = \\dfrac{0.025-P(X \\leq 85)}{P(X=86)}$$\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-10_7a12f7d1f190f8103e5275e7d338e128'}\n\n```{.r .cell-code}\n#We look at the probability under H0 that we get 86 heads\nP.reject.86 <- (.025 - pbinom(85, 200, .5) )/dbinom(86, 200, .5)\nP.reject.86\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6254687\n```\n:::\n:::\n\n\n\nSo if we get 86 heads, we will reject 62.5% of the time. That's the idea.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-11_a6e209183c4e1685ca0f2e6509024355'}\n\n```{.r .cell-code}\nhack_decision_rule <- function(T){\n  #T is the number of heads\n  if(T <= 85){ return (TRUE)} #always reject if T <= 85\n  else if(T==86) {return(rbinom(1, 1, P.reject.86))}\n  else {return (FALSE)}\n}\n\n#verify that we actually are able to achieve a precise 2.5% type 1 error rate using this \"hack\" rule.\ntrials <- rbinom(100000, 200, .5)\nreject <- FALSE\nfor(i in 1:100000){\n  reject[i] <- hack_decision_rule(trials[i])\n}\nmean(reject)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02499\n```\n:::\n:::\n\n\nThere are two other ways that we could attempt (that come to my mind) to achieve an exact $\\alpha$ with a discrete test statistic distribution. \n\n1. If $X \\leq 86$ reject $p$ of the time \n2. Always reject 2.5% of the time.\n\nBoth of these will attain an $\\alpha$ of 0.025, but they are not as powerful (probably). Let's consider different probabilities from 0 to .5 and compare the powers We're only refining the rejection rule for the lower tail, so we don't need to look at the upper tail right now.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-12_72091f3b90e85ad8739118cb24f676bb'}\n\n```{.r .cell-code}\nset.seed(1)\n\nrule1.p <- 0.025/pbinom(86,200, .5)\n\nhack_decision_rule1 <- function(T){\n  if(T <= 86){\n    return(runif(1) < rule1.p)\n  } else {\n    return(FALSE)\n  }\n}\nhack_decision_rule2 <- function(T){\n  return(runif(1) < 0.025)\n}\n\n\nif(file.exists(\"discrete_exact_alpha.rds\")){\n  #So this knits faster- load the results if they exist.\n  discreteRR <- readRDS(\"discrete_exact_alpha.Rds\")\n} else {\n  pseq <- seq(0,.5,.01)\n  rr_0 <- rep(0,length(pseq))\n  rr_1 <- rr_0\n  rr_2 <- rr_0\n  NMC <- 5000\n  for(j in 1:length(pseq)){\n    p <- pseq[j]\n    for(i in 1:NMC){\n      #generate data\n      nheads <- rbinom(1, 200, p)\n      rr_0[j] <- rr_0[j] + hack_decision_rule(nheads)\n      rr_1[j] <- rr_1[j] + hack_decision_rule1(nheads)\n      rr_2[j] <- rr_2[j] + hack_decision_rule2(nheads)\n    }\n  }\n  rr_0 <- rr_0 / NMC\n  rr_1 <- rr_1 / NMC\n  rr_2 <- rr_2 / NMC\n  discreteRR <- data.frame(pseq, rr_0, rr_1, rr_2)\n  saveRDS(discreteRR, \"discrete_exact_alpha.rds\")\n}\n\nplot(rr_0~pseq, type=\"l\", ylab=\"rejection rate\", xlab=\"Prob of H\", main=\"Power Comparison of Hack Rejection Rules\", data=discreteRR)\nabline(h=0.025, col=\"red\")\nlines(x=discreteRR$pseq, y=discreteRR$rr_1, col=\"blue\")\nlines(x=discreteRR$pseq, y=discreteRR$rr_2, col=\"orange\")\nlegend(x=0, y=.6, legend=c(\"Hack rule\", \"reject sometimes if T<=86\", \"reject sometimes\"), col=c(\"black\",\"blue\",\"orange\"), lwd=1)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nCompare their levels:\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-13_827a67d0dbef2bf3683fbc9ea40f4d65'}\n\n```{.r .cell-code}\nrr_0[51]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0256\n```\n:::\n\n```{.r .cell-code}\nrr_1[51]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0252\n```\n:::\n\n```{.r .cell-code}\nrr_2[51]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0262\n```\n:::\n:::\n\n\n\n\n## Example: Comparing Consistency\n\nStudent A and Student B are in the same class. We are curious to know about their academic performance, in particular their consistency in grade performance. \n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-14_f83abc6615ce65acf20912a9c57226cc'}\n\n```{.r .cell-code}\nA.grades <- c(83.4,91.4,83.0,77.2,81.9,76.8,82.4,89.0,82.0,74.9,77.6,78.7,79.1,77.8,80.3)\nB.grades <- c(75.9,66.4,98.1,83.0,72.4,70.6,67.1,73.0,89.1,87.0,83.9,63.7,88.6,90.0,75.5,86.5,77.5)\nA.grades <- A.grades - mean(A.grades)\nB.grades <- B.grades - mean(B.grades)\nboxplot(A.grades, B.grades, horizontal=TRUE)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n\nThe question is this: are the two students equally consistent in their grades? Let's think about consistency in terms of variance\n\nLet's fill in the following code:\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-15_e49db1a6a95449eb668284aca7d1b806'}\n\n```{.r .cell-code}\n#Takes in two data vectors, compares them in a meaningful way and returns a statistic that is useful to compare consistency.\ntestStat <- function(dataA, dataB){\n  return(var(dataA)-var(dataB))\n}\n\n#This function takes in two vector, combines them, shuffles,\n#And splits them\n#Then calculates a test statistic\npermuteAndCompute <- function(dataA, dataB){\n  combinedData <- c(dataA, dataB)\n  shuffledData<- sample(combinedData)\n  simA <- shuffledData[1:length(dataA)]\n  simB <- shuffledData[-(1:length(dataA))] #I use negative index\n  return(testStat(simA, simB))\n}\n\n#Takes in the two data sets and a number of monte carlo runs. It will perform the MC simulation NMC times and it will return a distribution of test statistics.\nmcDist <- function(dataA, dataB, MNC){\n  results <- 0\n  for(i in 1:NMC){\n    results[i] <- permuteAndCompute(dataA, dataB)\n  }\n  return(results)\n}\n```\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-16_4f2abc4dbfc053da2e96957a8612b5e4'}\n\n```{.r .cell-code}\nobsTestStat <- testStat(A.grades, B.grades)\nmcSamplingDistribution <- mcDist(A.grades, B.grades, 10000)\n\nhist(mcSamplingDistribution)\nabline(v=obsTestStat)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n\n```{.r .cell-code}\nobsTestStat\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -76.26533\n```\n:::\n\n```{.r .cell-code}\nmean(mcSamplingDistribution >= obsTestStat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9992\n```\n:::\n\n```{.r .cell-code}\nmean(mcSamplingDistribution <= obsTestStat)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8e-04\n```\n:::\n:::\n\n\nWe have about .1% in the left-tail, more extreme than our observed test statitic. However the question we were investigating is \"Are they equally consistent or not\"\n\nSo - if the test statisitc was very positive (+75 or so) we would also consider that evidence for the alternative.\n\nThis is a two-sided test; we need to calculate probability from both sides of the distribution. What we want to do then is to follow this model:\n\n1. calculate the probability <= obs. test stat\n2. calculate the probability >= obs. test stat\n3. take the smaller of the two\n4. Double it\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-17_804c658d34f6d7ef548193bb6bd38e05'}\n\n```{.r .cell-code}\n#This is my formula for a 2-tailed p-value\n2*min(mean(mcSamplingDistribution>=obsTestStat),\n      mean(mcSamplingDistribution<=obsTestStat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0016\n```\n:::\n:::\n\n\nIf this was a symmetric distribution, this would be the same as:\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-18_94af8810e35e212e4f226706f58e9554'}\n\n```{.r .cell-code}\nmean(mcSamplingDistribution>= abs(obsTestStat)) + mean(mcSamplingDistribution<= -abs(obsTestStat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.002\n```\n:::\n:::\n\n\n\n\nLet's suppose we used some other measure of \"difference of consistency\"\nFor example, we could take the ratio of the variances.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-19_6505377384515cc6592a963d35b71678'}\n\n```{.r .cell-code}\n#redefine the test statistic\ntestStat <- function(dataA, dataB){\n  return(var(dataA)/var(dataB))\n}\n\nobsTestStat <- testStat(A.grades, B.grades)\nmcSamplingDistribution <- mcDist(A.grades, B.grades, 10000)\n\nhist(mcSamplingDistribution, breaks=50)\nabline(v=obsTestStat)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n\n```{.r .cell-code}\n2*min(mean(mcSamplingDistribution >= obsTestStat),\n      mean(mcSamplingDistribution <= obsTestStat))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.002\n```\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n## Coin Flips\n\nBelow are two sequences of 300 “coin flips” (H for heads, T for tails). One of these is a true sequence of 300 independent flips of a fair coin. The other was generated by a person typing out H’s and T’s and trying to seem random. Which sequence is truly composed of coin flips?\n\nSequence 1:\n\nTTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHH TTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHH TTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHT HTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTT HHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\n\nSequence 2:\n\nHTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTH THTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHH TTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTT THTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTH HHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT\n\nUseful functions:\n`flips <- strsplit(\"HHTHTHHTHTHHHT\",\"\")`\n`with(rle(flips[[1]]), lengths[values==\"H\"])`\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-20_264489cb8306333eb9141de58a592145'}\n\n```{.r .cell-code}\nseq1 =\"TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\"\n\nseq2 = \"HTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTHTHTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHHTTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTTTHTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTHHHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT\"\n\nflips1 <- strsplit(seq1,\"\")\nH.run.lengths1 <- with(rle(flips1[[1]]), lengths[values==\"H\"])\ntable(H.run.lengths1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH.run.lengths1\n 1  2  3  4 \n66 27  8  1 \n```\n:::\n\n```{.r .cell-code}\nflips2 <- strsplit(seq2,\"\")\nH.run.lengths2 <- with(rle(flips2[[1]]), lengths[values==\"H\"])\ntable(H.run.lengths2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nH.run.lengths2\n 1  2  3  4  5  6  7  8 \n45 15  9  5  1  1  1  1 \n```\n:::\n:::\n\n\n\nWhat if we use maximum run length as a test statistic?\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-21_6148985a354a5a7cd660c19008a173d6'}\n\n```{.r .cell-code}\nt.dist <- 0 #empty vector for our simulated distribution of test statistics\nMCN <- 10000\nn <- 300\nfor(i in 1:MCN){\n  flips <- sample(c(\"H\",\"T\"), 300, replace=TRUE)\n  H.run.lengths <- with(rle(flips), lengths[values==\"H\"])\n  t.dist[i] <- max(H.run.lengths)\n}\n\nhist(t.dist)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nWhat are the two test statistics?\nWe would use a 2-tailed p-value. We would consider a sequence of Hs and Ts to be weird if the max run length was too long or too short. \n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-22_1500ff3a63fac1b921583ade127f5733'}\n\n```{.r .cell-code}\n(t1 <- max(H.run.lengths1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4\n```\n:::\n\n```{.r .cell-code}\n2*(min(mean(t.dist <= t1),mean(t.dist >= t1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0108\n```\n:::\n\n```{.r .cell-code}\n(t2 <- max(H.run.lengths2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 8\n```\n:::\n\n```{.r .cell-code}\n2*(min(mean(t.dist <= t2),mean(t.dist >= t2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.8884\n```\n:::\n:::\n\n\nIt probably comes as no surprise that the first sequence of Hs and Ts is suspicious. It does not contain enough long runs of Hs- when flips are done randomly you tend to get at least some much longer runs.\n\nWe could repeat this with the \"average\" run length as well.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-23_8711b5fd0d7e240f203f26db68a726fa'}\n\n```{.r .cell-code}\nt.dist <- 0 #empty vector for our simulated distribution of test statistics\nMCN <- 10000\nn <- 300\nfor(i in 1:MCN){\n  flips <- sample(c(\"H\",\"T\"), 300, replace=TRUE)\n  H.run.lengths <- with(rle(flips), lengths[values==\"H\"])\n  t.dist[i] <- mean(H.run.lengths)\n}\n\nhist(t.dist)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n\n```{.r .cell-code}\n(t1 <- mean(H.run.lengths1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.45098\n```\n:::\n\n```{.r .cell-code}\n2*(min(mean(t.dist <= t1),mean(t.dist >= t1)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0\n```\n:::\n\n```{.r .cell-code}\n(t2 <- mean(H.run.lengths2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.897436\n```\n:::\n\n```{.r .cell-code}\n2*(min(mean(t.dist <= t2),mean(t.dist >= t2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.568\n```\n:::\n:::\n\n\nThe average run length of 1.45 would virtually never happen if a coin was actually flipped 300 times. That's conclusive evidence that the first sequence was made up by a human.\n\nWhat are some other test statistics that could have worked?\n\n## Example: hot hands\n\nFor the next example, let's do something slightly more complicated.\n\nA certain professional basketball player believes he has \"[hot hands](https://en.wikipedia.org/wiki/Hot_hand)\" when shooting 3-point shots (i.e. if he makes a shot, he’s more likely to also make the next shot). His friend doesn’t believe him, so they make a wager and hire you, a statistician, to settle the bet.\n\nAs a sample, you observe the next morning as the player takes the same 3-point shot 200 times in a row (assume he is well rested, in good physical shape, and doesn’t feel significantly more tired after the experiment), so his level of mental focus doesn’t change during the experiment). You obtain the following results, where Y denotes a success and N denotes a miss:\n\n```\nYNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNNNYNNYYYYNNYY\nNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNNYNNNNNNYNNNYNNYNNNNNYNYY\nYNNYYYNYNNNNYNNNNNNNYYNNYYNNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNN\nNYYYYYYNYYNNYN\n```\n\nThe hypotheses being tested are:\n$H_0:$ there is no hot hand effect - shots are independent\n$H_A:$ there is some dependency between shots\n\n\nNote that the existence of a \"hot hands\" effect means the shots are not independent. Also note that there's a third possibility: that the player is more likely to \"[choke](https://en.wikipedia.org/wiki/Choke_(sports))\" and miss the next shot if he scored the previous one (e.g. maybe scoring a shot makes him feel more nervous because he feels like he's under pressure).\n\n### Attempt 1: run length\n\nSince the existence of a hot hands effect tends to increase the run lengths of `Y`s compared to if the shots were independent, we can use the longest run length as a way of comparing independence vs hot hands (note if the player is a choker, they will tend to have shorter runs of `Y`s than if they were independent, so you can simply ignore this case for now and compare hot hands v. independence for simplicity).\n\nNow, how exactly do you compare these two situations and determine which is a better fit for the data?\n\nOne thing that's worth noting is that ***if a sequence of repeated experiments is independent, then it shouldn't matter what order the results are in***. This should be fairly easy to understand and agree with.\n\nLet's ***assume that the throws are totally independent***. Recall we also assume he doesn't get tired so his baseline shot-making ability doesn't change over the course of the experiment. Therefore, we should be able to (under these assumptions) ***arbitrarily reorder his shots without affecting any statistical properties of his shot sequence***. So let's do that!\n\nWe begin by parsing the throws into a vector of `Y` and `N`.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-24_ab2f955282e2ba1c9009bc83611b6783'}\n\n```{.r .cell-code}\n# the sequence of throws is broken up into 4 chunks for readability, then\n# paste0 is used to merge them into a single sequence, then\n# strplit(\"YN...N\",split=\"\") is used to split the string at every \"\", so\n# we get a vector of each character, and finally\n# [[1]] is used to get the vector itself (strsplit actually outputs a list\n# with the vector as the first element; [[1]] removes the list wrapper)\n# \n# for more info about the strsplit function, see\n# https://www.journaldev.com/43001/strsplit-function-in-r\n\nthrows = strsplit(\n   paste0(\"YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNN\",\n          \"NYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNN\",\n          \"YNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYY\",\n          \"NNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN\"), split=\"\")[[1]]\n\nthrows\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  [1] \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"N\"\n [19] \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\"\n [37] \"N\" \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\"\n [55] \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\"\n [73] \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"N\"\n [91] \"N\" \"N\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\"\n[109] \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"Y\" \"Y\" \"Y\" \"N\"\n[127] \"N\" \"Y\" \"Y\" \"Y\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\"\n[145] \"Y\" \"Y\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\"\n[163] \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\"\n[181] \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\"\n[199] \"Y\" \"N\"\n```\n:::\n:::\n\n\n\n\nNext, we write a function to get the longest run of `Y`s in the throw sequence. Here we use a convenient function called `rle( )` which is short for [run length encoding](https://en.wikipedia.org/wiki/Run-length_encoding), which turns our sequence of throws into sequences of runs (e.g. YNNNNYYNNNY becomes something like \"1 `Y`, 4 `N`s, 2 `Y`s, 3 `N`s, and 1 `Y`\"). We can then simply take the longest of the `Y` runs.\n\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-25_45b31bc744dca7eeae6597ff54a11328'}\n\n```{.r .cell-code}\nlongestRun = function(x, target = 'Y'){\n    max(0, with(rle(x), lengths[values==target]))\n}\n\nlongestRun(throws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 6\n```\n:::\n:::\n\n\n\nNow, we randomly shuffle the sequence of throws many times and see what the longest `Y` runs look like for these shuffled sequences.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-26_7366bfb09ff25a1ec497ec381c86c209'}\n\n```{.r .cell-code}\n# set number of reps to use\nMCN = 10000\n\n# create vector to save results in\nmc.runs = rep(0,MCN)\n\n# for each rep, randomize sequence and find longest run of Y\nfor(i in 1:MCN){\n    mc.runs[i] = longestRun(sample(throws))\n}\n```\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-27_cfb17992789e0b063ddf5f9eae289195'}\n\n```{.r .cell-code}\noptions(max.print=50)\nmc.runs\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 4 4 6 5 6 3 4 4 5 6 4 4 3 3 4 3 5 5 4 3 4 6 5 4 3 5 4 4 4 3 4 4 4 4 6 4 5 5\n[39] 5 6 4 5 3 6 6 8 4 4 5 6\n [ reached getOption(\"max.print\") -- omitted 9950 entries ]\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-28_b3b0474d02e005964bd8559264cf67c1'}\n\n```{.r .cell-code}\nbarplot(table(mc.runs))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\nFor a two tailed test - if you would reject the null hypothesis if the test statistic lived in either the upper or the lower tail of the distribution, a general rule to use is to take the proportion >= observed and <= observed, whichever one is smaller, double that.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-29_50ee513ebdb190f32c97e68b98161261'}\n\n```{.r .cell-code}\nmean(mc.runs >= 6) * 2 #because it's a 2 tailed test\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2128\n```\n:::\n:::\n\n\n\nCompared to other shuffled sequences, our run length doesn't seem that unlikely. Therefore, this method seems inconclusive.\n\nCan we find an even better \"statistic\" to use?\n\n### Attempt 2: running odds ratio\n\nConsider **every pair of consecutive throws** and make a table of the outcomes. For example, the first 8 throws in the sequence are YNNNNYYN. Breaking this into consecutive pairs, we have YN, NN, NN, NN, NY, YY, YN. This gives the table:\n\n<center>\n<div style=\"width:100px;\">\n\n| NN | NY | YN | YY |\n|:--:|:--:|:--:|:--:|\n| 3  | 1  | 2  | 1  |\n\n</div>\n</center>\n\nSuppose we do this for the entire sequence of 200 throws (note this gives you 199 pairs). If we **divide the number of NY by the number of NN**, we get an estimate for **how much _more_ likely he is to make the next shot _assuming he missed his last shot_**.\n\nSimilarly, we can **divide the number of YY by the number of YN** to get an estimate for **how much _more_ likely he is to make the next shot _assuming he scored his last shot_**.\n\nNow, note that **if the \"hot hands\" effect really exists** in the data, then **YY/YN should be larger than NY/NN** in a large enough sample. We use this fact to define the following quantity:\n\n$$R=\\frac{(\\text{# of YY})/(\\text{# of YN})}{(\\text{# of NY})/(\\text{# of NN})}$$\n\nThe ratio $R$ represents, in some sense, **how much more likely** the player is to **make the next shot** if he **made the previous shot _vs_ if he didn't make the previous shot** (note the **_vs_**). This is exactly what we're trying to investigate!\n\nIf there is a \"hot hands\" effect, the numerator should be greater than the denominator and we should have $R>1$. If the throws are independent and do not affect each other then in theory we should have $R=1$. If the player is actually a choker (i.e. he is more likely to miss after a successful shot), then we should have $R<1$. (Side note: this is basically an [odds ratio](https://journalfeed.org/article-a-day/2018/idiots-guide-to-odds-ratios)).\n\nNow, we can use the same general method as the first attempt. If we assume his throws are independent and his shot probability doesn't change significantly during the experiment, then we can randomly shuffle his throws and no properties should change. So let's do that!\n\nFirst, I wrote a function to split the sequence of throws into consecutive pairs and then tabulates them.\n\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-30_27c18b13b41100551d82f656d66e1e8b'}\n\n```{.r .cell-code}\n# define function for tabulating consecutive pairs\ntableOfPairs = function(x) {n=length(x);Rfast::Table(paste(x[1:(n-1)],x[2:n],sep=\"\"))}\n\n# test function for correct output\ntableOfPairs(strsplit(\"YNNNNYYN\",split=\"\")[[1]])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nNN NY YN YY \n 3  1  2  1 \n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-31_fb97b993c9dcadb781cf2d9c1bb84242'}\n\n```{.r .cell-code}\n# run function on original sequence of throws\ntableOfPairs(throws)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n NN  NY  YN  YY \n102  34  35  28 \n```\n:::\n:::\n\n\n\nNext, I wrote a function that takes the above table as an input and returns the ratio R as defined above.\n\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-32_5054e8c7b3a0b4934b49d65e5ad187bb'}\n\n```{.r .cell-code}\nratioFromTable = function(tb) setNames((tb[\"YY\"]/tb[\"YN\"])/(tb[\"NY\"]/tb[\"NN\"]),\"R\")\n\n# run on our data\nratioFromTable(tableOfPairs(throws))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  R \n2.4 \n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-33_4532bd66732378e4c8926d3944cc99f3'}\n\n```{.r .cell-code}\n# we can check this is correct by manually computing it\n(28/35)/(34/102)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.4\n```\n:::\n:::\n\n\n\n\nNow we just need to shuffle the sequence and see what this ratio looks like for other sequences.\n\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-34_0f295543a9938a66e4bfb1903090be1d'}\n\n```{.r .cell-code}\n# set number of reps to use\nN = 10000\n\n# create another vector to save results in\nmc.ratios = rep(NA,N)\n\n# for each rep, randomize sequence and find ratio R\n#for(i in 1:N){\n#    mc.ratios[i] = ratioFromTable(tableOfPairs(sample(throws)))\n#}\n\n# alternatively, use replicate\nmc.ratios = replicate(N,ratioFromTable(tableOfPairs(sample(throws))))\n```\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-35_fa495a1e92e95c98722d00f1d39f2396'}\n\n```{.r .cell-code}\noptions(max.print=50)\nround(mc.ratios,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n0.84 0.84 2.16 0.64 0.90 0.67 1.59 1.43 0.81 1.88 0.84 1.43 0.90 0.93 1.43 2.31 \n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n1.53 0.75 0.93 1.59 1.24 1.43 1.29 1.29 1.95 1.01 0.81 1.20 1.29 0.51 0.39 0.97 \n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n0.75 1.29 0.57 0.93 0.75 1.12 1.24 0.64 0.93 1.04 1.95 0.93 1.53 0.57 0.67 0.48 \n   R    R \n0.97 1.70 \n [ reached getOption(\"max.print\") -- omitted 9950 entries ]\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-36_40ad2927d0ed4a05539420b0f80383be'}\n\n```{.r .cell-code}\nhist(mc.ratios, breaks=20)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-37_262bf369cb51e37a8e82c529ba3dbf1e'}\n\n```{.r .cell-code}\nmean(mc.ratios>=ratioFromTable(tableOfPairs(throws)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0023\n```\n:::\n:::\n\n\n\nNow we can see our original ratio of $R=2.4$ seems extremely unlikely! In particular, most of the shuffled statistics are centered around 1 (which is what we expect, since we established $R=1$ for independent sequences).\n\nThis method (which is a little more refined than the simpler run length method) appears to show that our original sequence isn't well explained by the throws being independent. Since $R=2.4\\gg1$ and this result appears unlikely to happen under independence, we may conclude **the player does actually have hot hands**.\n\n\n\n\n## Permutation test: Drug Trials (Extended)\n\nApologies- this part of the document is a little messy, I have to remove duplicate work!\n\nThe results from a medical trial; larger numbers are \"better\"\nWe want to know if there is evidence that the drug (treatment group) performs on average better than control. \n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-38_10951e9603c407edce8e20fcea8aecd8'}\n\n```{.r .cell-code}\ncontrol <- c(-0.10, -0.55, 1.24, -0.97, -0.76,  0.21,-0.27, -1.02, 0.58, 1.67, -1.07, 0.17, 1.45, 0.34, 1.15, 0.18, -0.97, 0.43, -1.39, -2.76 );\n\ntreatment <- c( 0.54, 0.36, 0.59, -0.57, 0.53, -0.78, -0.44, -0.98, 1.31, 0.50, 0.57, 0.49, -0.96, 2.41, 0.85, 1.93, 0.95, 1.45, 1.61, -1.16 );\n\nboxplot(treatment, control)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n:::\n\n\n\nThink about the logic of how a monte carlo test would work.\nWe need to come up with a null model corresponding with the null hypothesis - but we really should be clear about the hypothesis that we're testing.\n\nClaim (alt hypothesis) - the drug works. \nNull hypothesis - the drug does not work.\n\nThe null model implies - these 40 values just happened to be partitioned randomly into control group and treatment group.\n\nSo in a Monte Carlo simulation, we can take these 40 values, shuffle them around, and re-partition them into \"control\" and \"Treatment\"\n\nThe second important decision we have to make: on each simulation of the data we need to calculate some statistic or number - some function of the data - that captures the... unusualness of it somehow. Something that would lend evidence towards the alt. hypothesis. \n\nThe alt hypothesis (\"the drug works\") - evidence in support of this would be: the mean for the treatment group > the mean for the control group. \n\nOne idea: Our test statistic could be \"TRUE/FALSE is the mean_Trt > mean_Cntrl\"\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-39_80cf82baf3d8072cbe36be3de96e896e'}\n\n```{.r .cell-code}\n#Create a test_statistic function\ntestStat <- function(dataA, dataB){\n\n  #true if mean dataA > mean dataB\n  #return(mean(dataA) > mean(dataB)) \n  #Oops that doesn't work well, because it's only TRUE or FALSE\n  \n  return(mean(dataA)-mean(dataB))\n}\n```\n:::\n\n\n\nNow let's get the test statistic for the observed data:\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-40_d99006a4dafc9ea862d04c0744dbe127'}\n\n```{.r .cell-code}\ntestStat(treatment, control)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.582\n```\n:::\n\n```{.r .cell-code}\n#MC simulation\nNMC <- 10000\nresults <- FALSE\nfor(i in 1:NMC){\n  #We need to re-partition the data into two groups.\n  combinedData <- c(treatment, control)\n  #we next get 'simulated' treatment and control data\n  shuffledData<- sample(combinedData)\n  treatmentSim <- shuffledData[1:length(treatment)]\n  controlSim <- shuffledData[-(1:length(treatment))]\n  results[i] <- testStat(treatmentSim, controlSim)\n}\nhist(results)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-40-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#Recall the alternative hypothesis:\n# The drug works\n# We only would consider evidence of this if the meanT-meanC >0\n# So we only are going to calculate a p-value from the upper-tail\n# of the distribution.\n\nmean(results >= testStat(treatment, control))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0445\n```\n:::\n:::\n\n\n\nIt might be interesting to check what a parametric T test would give us.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-41_0d29847e7e7cf3972db01f2c4e9d800b'}\n\n```{.r .cell-code}\nt.test(treatment, control, var.equal=TRUE, alternative=\"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tTwo Sample t-test\n\ndata:  treatment and control\nt = 1.7437, df = 38, p-value = 0.04465\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.01928295        Inf\nsample estimates:\nmean of x mean of y \n    0.460    -0.122 \n```\n:::\n:::\n\n\n\n\nSome other test statistics we could possibly use:\n\n* difference of the medians\n* difference of the max values\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-42_9138c643db7fc317d6b7a0c6af126997'}\n\n```{.r .cell-code}\n#Create a test_statistic function\ntestStat <- function(dataA, dataB){\n  return(max(dataA)-max(dataB))\n}\n\npermute_and_compute <- function( treatment, control ){\n    combinedData <- c(treatment, control)\n    shuffledData<- sample(combinedData)\n    treatmentSim <- shuffledData[1:length(treatment)]\n    controlSim <- shuffledData[-(1:length(treatment))]\n    return(testStat(treatmentSim, controlSim))\n}\n\nNMC <- 10000\nresults <- FALSE\nfor(i in 1:NMC){\n  results[i] <- permute_and_compute(treatment, control)\n}\n\nhist(results, breaks=50)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-42-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(results >= testStat(treatment, control))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2461\n```\n:::\n:::\n\n\n\n\n\nWhat if we use a different test statistic?\nLet's compare the medians.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-43_4b2f12619431b2ef449e5ba033dc50e7'}\n\n```{.r .cell-code}\ntestStat <- function(A,B){\n  return(median(A)-median(B))\n}\n\n#rerun comparing the sample medians\ntest_statistics <- 0\n\n(Tobsd <- testStat(treatment, control))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.5\n```\n:::\n\n```{.r .cell-code}\nfor(i in 1:NMC ) {\n  test_statistics[i] <- permute_and_compute( treatment, control );\n}\nhist( test_statistics, breaks=60 )\nabline( v=Tobsd, lw=3, col='red' )\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-43-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsum(test_statistics >= Tobsd)/NMC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0912\n```\n:::\n:::\n\n\n\n\nLet's use KS test statistics - the max deviance between their empirical distributions!=\n\nwhat is that?\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-44_7fdf9f54009bedf88e9485b78309fb35'}\n\n```{.r .cell-code}\ncompare <- function(x, y) {\n  n <- length(x); m <- length(y)\n  w <- c(x, y)\n  o <- order(w)\n  z <- cumsum(ifelse(o <= n, m, -n))\n  i <- which.max(abs(z))\n  w[o[i]]\n}\nu<-compare(treatment, control) #this is where the max difference occurs\nabs(mean(treatment < u) - mean(control < u))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.35\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-45_5ef3fdfec0e94716ea2846b55685a52d'}\n\n```{.r .cell-code}\necdf1 <- ecdf(treatment)\necdf2 <- ecdf(control)\nplot(ecdf1, verticals=TRUE, do.points=FALSE, col=\"blue\", main=\"Compare ecdfs\")\nplot(ecdf2, verticals=TRUE, do.points=FALSE, add=TRUE, col='red')\nlines(c(u,u), c(ecdf1(u), ecdf2(u)), lwd=2)\nlegend(x=min(control,treatment), y=1, legend=c(\"treatment\",\"control\"), col=c(\"blue\",\"red\"), lty=1, xjust=-1)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n\nWe can pull this out from the built-in `ks.test` function. The test statistic is the max deviance between the two empirical CDFs\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-46_e25f57ff449f46ff50cde1fca8b8717c'}\n\n```{.r .cell-code}\ntestStat <- function(A,B){\n  return(as.numeric(ks.test(A,B)[1]))\n}\ntestStat(treatment, control)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-47_75e6e1828f58b285708567ca90edf8c0'}\n\n```{.r .cell-code}\n#rerun\n\n(Tobsd <- testStat(treatment, control))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4\n```\n:::\n\n```{.r .cell-code}\ntest_statistics <- rep( 0, NMC ); # Vector to store our \"fake\" test statistics\nfor(i in 1:NMC ) {\n  test_statistics[i] <- permute_and_compute( treatment, control );\n}\nhist( test_statistics )\nabline( v=Tobsd, lw=3, col='red' )\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-47-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsum(test_statistics >= Tobsd)/NMC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.0839\n```\n:::\n:::\n\n\n\nWe can spend all our time picking different test statistics, and maybe we find one that gives us a low p-value for this data set. But we have to be careful that it would generalize well to other datasets. Furthermore, we want to make sure that it would not be giving us false positives if we were comparing two populations that really did have the same distribution\n\nLet's demonstrate this comparing our 3 test statistics:\n\n* difference of means\n* difference of maximums\n* difference of medians\n* max ecdf deviation\n\nAnd we can compare them over datasets that are generated from increasingly different populations.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-48_152a59514cbd890e3eace95b8bcabb26'}\n\n```{.r .cell-code}\ngenerate_experiment <- function(nA, nB, effectSize){\n  #we will assume that the values in our data are drawn from a distribution - \n  #here we'll use a normal distribution. We could use permutation methods but \n  #that doesn't really change the point we're trying to make.\n  return(list(rnorm(nA, mean=effectSize, sd=1),\n              rnorm(nB, mean=0, sd=1)))\n}\n#for example\ngenerate_experiment(20,20,.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[[1]]\n [1]  0.01778166  1.65018162  1.18477818 -1.57641934 -0.64161057  0.08577461\n [7]  1.17349869 -0.47823398 -0.26032160 -0.93443191  2.05777102 -0.84094665\n[13]  2.32726497  0.03211071  1.11730385 -0.27515971  0.35877537 -0.59002849\n[19]  0.68524808  0.39028353\n\n[[2]]\n [1] -0.78766069  0.47220868  1.80632883 -0.50308343  1.82997227 -1.65046780\n [7]  0.18774042  1.89357611 -0.85911026  1.78447769 -0.06779853 -0.42485180\n[13] -1.36963966  0.95244591  0.30041995 -0.57886123 -0.35174480 -1.13007872\n[19]  0.49605029  1.07811514\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-49_e30703393ebf6cb12bc0b39c361f8d6d'}\n\n```{.r .cell-code}\nnDatasets <- 200\nNMC <- 200\nset.seed(1)\neffects <- seq(0,1,.1)\nrr.mean <- 0; rr.max <- 0; rr.median <-0; rr.ks <- 0\nfor(k in 1:length(effects)){\n  effect <- effects[k]\n  rej.mean <- rep(FALSE,nDatasets)\n  rej.max <- rep(FALSE,nDatasets)\n  rej.median <- rep(FALSE,nDatasets)\n  rej.ks <- rep(FALSE,nDatasets)\n  #generate 100 different datasets\n  for(j in 1:nDatasets){\n    exp.data <- generate_experiment(20,20,effect)\n    \n    obs.mean <- mean(exp.data[[1]])-mean(exp.data[[2]])\n    obs.median <- median(exp.data[[1]])-median(exp.data[[2]])\n    obs.max <- max(exp.data[[1]])-max(exp.data[[2]])\n    obs.ks <- as.numeric(ks.test(exp.data[[1]],exp.data[[2]])[1])\n    \n    #permute and compute 1000 times\n    t.means <- 0\n    t.medians <- 0\n    t.maxes <- 0\n    t.kss <- 0\n    for(i in 1:NMC){\n      shuffledData <- sample(c(exp.data[[1]],exp.data[[2]]))\n      simA <- shuffledData[1:length(exp.data[[1]])]\n      simB <- shuffledData[-(1:length(exp.data[[1]]))]\n      \n      t.means[i] <- mean(simA)-mean(simB)\n      t.medians[i] <- median(simA)-median(simB)\n      t.maxes[i] <- max(simA)-max(simB)\n      t.kss[i] <- as.numeric(ks.test(simA,simB)[1])\n    }    \n    rej.mean[j] <- mean(t.means >= obs.mean) <=0.05\n    rej.median[j] <- mean(t.medians >= obs.median) <=0.05\n    rej.max[j] <- mean(t.maxes >= obs.max) <=0.05\n    rej.ks[j] <- mean(t.kss >= obs.ks) <=0.05\n  }\n  rr.mean[k] <- mean(rej.mean)\n  rr.max[k] <- mean(rej.max)\n  rr.median[k] <- mean(rej.median)\n  rr.ks[k] <- mean(rej.ks)\n}\n\nplot(x=effects, y=rr.mean, type=\"l\", ylim=c(0,1), main=\"Rejection rate of different test statistics\", xlab=\"effect size\", ylab=\"rejection rate\")\nlines(x=effects, y=rr.max, col=\"red\")\nlines(x=effects, y=rr.median, col=\"blue\")\nlines(x=effects, y=rr.ks, col=\"darkgreen\")\nabline(h=.05)\nlegend(x=0, y=1, legend=c(\"means\",\"medians\",\"deviance\",\"max\"), col=c(\"black\",\"blue\",\"darkgreen\",\"red\"), lty=1)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-49-1.png){width=672}\n:::\n:::\n\n\n\n\n## Random number string\n\nI want is the numbers 1 through 20 shuffled up. I ask both a computer and a person to provide me with this number string. Suppose I get the string, but I don't know whether it came from the computer or the person.\n\nfrom 2/20/2025 afternoon lecture\n4 7 5 18 11 12 14 2 3 17 10 1 9 16 20 8 13 19 6 15\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-50_d2aa2d8c16f8a1380bfe28728dcd3630'}\n\n```{.r .cell-code}\nobs.Seq <- c(4,7,5,18,11,12,14,2,3,17,10,1,9,16,20,8,13,19,6,15)\n```\n:::\n\n\n\n$H_0:$ data generated truly randomly\n$H_A:$ data generated by humans, not randomly, but attempted\n\nLet's use The number of values that are not in their original place\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-51_3ff438fa28b2f99fe9b40d7185da65db'}\n\n```{.r .cell-code}\ncount_moved <- function(x) {\n  n <- length(x)\n  sum((1:n) == x)\n}\n\nt.sim <- replicate(10000, count_moved(sample(20)))\nhist(t.sim)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-51-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmean(t.sim >= count_moved(obs.Seq))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8\n\n(generated 9/26: 1 6 18 12 15 19 20 5 2 17 11 3 9 16 4 7 8 10 13 14)\n\nHow can I use a permutation test to determine the likely source of this string of numbers???? Said more precisely, is there evidence that sequence came from a human (not a true randomizer). The idea here is that humans are actually not great at mentally randomizing. We put patterns into things subconsciously.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-52_2dd288e165f232f0e455dbe2b1af4cb7'}\n\n```{.r .cell-code}\nnumbers.obs <- c(19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8)\n#We need a way to generate a random shuffling of the numbers\n\ngenerate_numbers <- function(n.numbers){\n  return(sample(n.numbers))\n}\n```\n:::\n\n\n\nWe also need some statistic or measurement of our data which indicates the weirdness of it. Let's think about what wouldn't be random.\n\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20\n\nAn increasing sequence of integers is very much not random. We can quantify this by counting how many differences are positive. In a random shuffling we would have some decreases and some increases. So let's start with that statistic: The number of positive differences.\n\n\n### Mean Absolute deviation\nWe can look at the distance of each digit from where it \"started\" from. Rather than just averaging all of their deviations (which would have positive deviations cancelling out with negative deviations) we can look at absolute deviations (absolute values). \n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-53_9beac096be7efdef203d45e22f6ae35e'}\n\n```{.r .cell-code}\nmean_abs_dev <- function(x){\n  n <- length(x)\n  return (mean(abs(x - 1:n)))\n}\n\nt.obs <- mean_abs_dev(obs.Seq)\n\nset.seed(1)\nmean.abs.dev.sim <- replicate(10000, mean_abs_dev(generate_numbers(20)))\nhist(mean.abs.dev.sim)\nabline(v=t.obs)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-53-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#2 tail p-value\n2 * min(mean(mean.abs.dev.sim <= t.obs), mean(mean.abs.dev.sim >= t.obs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4122\n```\n:::\n:::\n\n\nA two-tailed $p$-value of .354 is unconvincing; we have no evidence that this sequence of numbers wasn't truly \"randomly generated.\"\n\n### Number of positive differences\n\nIn an ordered sequence the difference between every pair of consecitive digits is +1 or -1. We could count how many (out of n-1) of the differences are positive. If the number of positive differences is too high OR too low we would suspect non-randomness; this is a two-tailed test.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-54_f6a43bdb64223b59d4987703fa1c6e56'}\n\n```{.r .cell-code}\ncount_pos_diff <- function(numbers){\n return(sum(diff(numbers)>0 ))\n}\n\ncount_pos_diff(numbers.obs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 7\n```\n:::\n\n```{.r .cell-code}\nNMC <- 10000\nset.seed(1)\nt.posdiff <- 0\nfor(i in 1:NMC){\n  t.posdiff[i] <- count_pos_diff(generate_numbers(20))\n}\nhist(t.posdiff)\nabline(v=count_pos_diff(numbers.obs))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n\n```{.r .cell-code}\n(p.val <- 2*min(\n  mean(t.posdiff <= count_pos_diff(numbers.obs)),\n  mean(t.posdiff >= count_pos_diff(numbers.obs))\n))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.1278\n```\n:::\n:::\n\n\nDrawback of this distribution we see is that it is discrete. I'm not a big fan of discrete distributions because they can be sensitive to just a single data value, and the p-value you get increases in big jumps as you move through the distribution.\n\nIf looking only at number of positive differences we don't see an unusually large (or unusually small) number of positive difference. No evidence of randomness. However look at this dataset:\n\n1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-55_7db82f57c2df595adf900845f5672625'}\n\n```{.r .cell-code}\ncount_pos_diff(c(1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10\n```\n:::\n\n```{.r .cell-code}\nplot(c(1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n\n\nIt's very close to the middle of the distribution, but it's a clear pattern. So the SIZE of the differences is clearly meaningful. We would want the differences to have the right amount of variation. If we take the differences of each consecutive pair of numbers, we can look at the *standard deviation* of those differences. That might be a good statistic to use. \n\n### Standard Deviation of Differences\n\nAgain note that this is a two-tailed test, because if the variation is too high or too low that would be evidence of non-randomness.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-56_440c1cbbe2836b523adbca42e9b8a06a'}\n\n```{.r .cell-code}\ncalc_sd_diff <- function(numbers){\n return(sd(diff(numbers) ))\n}\n\nNMC <- 10000\nset.seed(1)\nt.sddiff <- 0\nfor(i in 1:NMC){\n  t.sddiff[i] <- calc_sd_diff(generate_numbers(20))\n}\nhist(t.sddiff)\nabline(v=calc_sd_diff(numbers.obs))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n\n```{.r .cell-code}\n2*min(mean(t.sddiff <= calc_sd_diff(numbers.obs)),\n      mean(t.sddiff >= calc_sd_diff(numbers.obs)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2702\n```\n:::\n:::\n\n\n### Mean absolute difference\n\nAnother statistic we could look at is related to the *absolute values* of the differences - the average absolute difference. Again- this is a two-tailed test. Too high or too low would be evidence of nonrandomness.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-57_abd19bd097d1add4be9af8c17c61b0d1'}\n\n```{.r .cell-code}\ncalc_mean_abs_diff <- function(numbers){\n return(mean(abs(diff(numbers))))\n}\n\nNMC <- 10000\nt.meanabsdiff <- 0\nfor(i in 1:NMC){\n  t.meanabsdiff[i] <- calc_mean_abs_diff(generate_numbers(20))\n}\nhist(t.meanabsdiff, breaks = 50)\nabline(v=calc_mean_abs_diff(numbers.obs))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-57-1.png){width=672}\n:::\n\n```{.r .cell-code}\n2*min(mean(t.meanabsdiff <= calc_mean_abs_diff(obs.Seq)),\n      mean(t.meanabsdiff >= calc_mean_abs_diff(obs.Seq)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9274\n```\n:::\n:::\n\n\nNo evidence of nonrandomness! We could be pretty satisfied that the string of integers was properly shuffled by a computer.\n\nWhat about this one?\n\n2  9  3 13 18  4 17 10  1 15  7  5 12 20  6 19  8 16 14 11\n\nDo you think this was generated by computer or by human?\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-58_a18a3ca1679475348bce910c15876074'}\n\n```{.r .cell-code}\nnew.obs.data<- c(2,9,3,13,18,4,17,10,1,15,7,5,12,20,6,19,8,16,14,11)\n\n#Mean absolute deviation\n2*min(mean(mean.abs.dev.sim >= mean_abs_dev(new.obs.data)),\n    mean(mean.abs.dev.sim <= mean_abs_dev(new.obs.data))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3034\n```\n:::\n\n```{.r .cell-code}\n#Positive Differences\n2*min(mean(t.posdiff >= count_pos_diff(new.obs.data)),\n    mean(t.posdiff <= count_pos_diff(new.obs.data))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9904\n```\n:::\n\n```{.r .cell-code}\n#SD Differences\n2*min(mean(t.sddiff >= calc_sd_diff(new.obs.data)),\n    mean(t.sddiff <= calc_sd_diff(new.obs.data))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.2834\n```\n:::\n\n```{.r .cell-code}\n#Mean Abs Differences\n2*min(mean(t.meanabsdiff >= calc_mean_abs_diff(new.obs.data)),\n    mean(t.meanabsdiff <= calc_mean_abs_diff(new.obs.data))) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.104\n```\n:::\n:::\n\n\n\n### Comparing The Test Statistics\n\nI want to compare the power of the test statistics as the randomness of an array increases. In order to do this we would have to have a sense of an \"effect size\" and look at rejection rates as effect size increases - as the vector gets more and more shuffled.\n\nA function that randomly chooses one value in an array and moves it to the first index can be used to incrementaly shuffle an array, so we can have some that are very unrandom, and very random. There are other ways to create gradually more \"randomized\" lists, but this is the one I'll use.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-59_2fccfdda34e77fc76903a8a80b89a5f3'}\n\n```{.r .cell-code}\npopOnce <- function(x){\n  #pops a random element to the top of the list\n  ridx <- sample(length(x),1)\n  return(c(x[-ridx],x[ridx]))\n}\n```\n:::\n\n\n\nThen the `shuffleN` function takes the numbers in a vector `x` and calls the `popOnce` function `n` times. \n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-60_cdc585d31f5e699c0e959ac1a8bbc201'}\n\n```{.r .cell-code}\nshuffleN <- function(x,n=1){\n  for(i in 1:n){\n    x <- popOnce(x)\n  }\n  return(x)\n}\n\nmaxPops <- 40\npops <- 1:maxPops\nrr.madev <- rep(0, maxPops)\nrr.pd <- rep(0,maxPops)\nrr.sdd <- rep(0,maxPops)\nrr.mad <- rep(0,maxPops)\nNMC <- 5000\n\nfor(k in pops){\n  for(i in 1:NMC){\n     shuffledData <- shuffleN(1:20,k)\n        #Absolute Deviation\n        rr.madev[k] <- rr.madev[k] + (2*min(mean(mean.abs.dev.sim >= mean_abs_dev(shuffledData)),\n            mean(mean.abs.dev.sim <= mean_abs_dev(shuffledData))) <=0.05 )\n        #Positive Differences\n        rr.pd[k] <- rr.pd[k] + (2*min(mean(t.posdiff >= count_pos_diff(shuffledData)),\n            mean(t.posdiff <= count_pos_diff(shuffledData))) <=0.05 )\n        #SD Differences\n        rr.sdd[k] <- rr.sdd[k] + (2*min(mean(t.sddiff >= calc_sd_diff(shuffledData)),\n            mean(t.sddiff <= calc_sd_diff(shuffledData))) <= 0.05)\n        #Mean Abs Differences\n        rr.mad[k] <- rr.mad[k] + (2*min(mean(t.meanabsdiff >= calc_mean_abs_diff(shuffledData)),\n            mean(t.meanabsdiff <= calc_mean_abs_diff(shuffledData))) <=0.05) \n  }\n}\nplot(pops, rr.pd/NMC, type=\"l\", col=\"darkgreen\")\nlines(pops, rr.sdd/NMC, col=\"blue\")\nlines(pops, rr.mad/NMC, col=\"red\")\nlines(pops, rr.madev/NMC, col=\"orange\")\nlegend(col=c(\"darkgreen\",\"blue\",\"red\", \"orange\"), x=maxPops*.65, y=1, legend=c(\"#posDiff\",\"sdDiff\",\"meanAbsDiff\", \"meanAbsDev\"), lty=1)\nabline(h=0.05, lty=2)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-60-1.png){width=672}\n:::\n:::\n\n\nFrom this plot it seems that the number of positive differences in the sequence is the most powerful test statistic to detect a non-randomized list. BUt of course, this is only if \"non-randomized\" is measured in the way we've defined in terms of the \"effect size\".\n\n\n## Random Visual Noise detection\n\nSuppose we see an image on a monitor and want to determine if it is just random noise or if there is a pattern. Maybe this is data from a transmission from deep space. Is it just random noise, or is it possibly a message from an intelligent alien species? \n\nFor simplicity let's say it is black/white pixels in a 16x16 grid:\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-61_a6d5a39159ba549b40c400f7ef220d18'}\n\n```{.r .cell-code}\nobsData <- matrix(\n           c(0,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,\n             1,1,0,1,0,1,0,0,0,0,1,1,0,1,0,1,\n             0,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,\n             0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,1,\n             1,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,\n             1,0,0,0,0,1,1,1,1,1,0,0,1,1,1,0,\n             1,0,1,0,1,0,0,0,1,1,0,0,0,1,0,1,\n             0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,\n             0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,\n             0,0,1,1,0,0,0,1,0,1,1,0,1,1,1,1,\n             0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,\n             1,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,\n             0,1,0,1,1,1,1,1,0,1,1,0,1,1,0,0,\n             1,0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,\n             1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,\n             0,1,1,1,0,1,1,0,1,1,0,1,0,1,0,1),\n           nrow=16,byrow=TRUE)\npar(pty=\"s\")\nrequire(raster)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: raster\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: sp\n```\n:::\n\n```{.r .cell-code}\nimage(obsData, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-61-1.png){width=672}\n:::\n:::\n\n\nWe want to devise a hypothesis test that will test for randomness. What can we use? Let's look at some summary stats.\n\n### Test Stat 1: variability in number of 1s per region\n\nIdea: Slice it into 4x4 chunks, and count the number of 1s, then calculate the standard deviation. If we have random noise data then we should have some variability in each of these 16 sub-squares.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-62_e87cebbe4cfe043efb4b20aed1399862'}\n\n```{.r .cell-code}\nsd_chunk_1s <- function(matrix){\n  #I will assume that the data is 16x16. This function is not going to\n  #generalize well to other sized inputs.\n  results <- numeric(0)\n  for(i in seq(1,15, 4)){\n    for(j in seq(1,15, 4)){\n      chunk <- matrix[i:(i+3), j:(j+3)]\n      results <- rbind(results, sum(chunk))\n    }\n  }\n  return(sd(results))\n}\nsd_chunk_1s(obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1.181454\n```\n:::\n:::\n\n\n\nWe can perform our permutation test, randomizing the grid and calculating the test statistic on each run.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-63_94f467b7ca744ba459d7157dda40a960'}\n\n```{.r .cell-code}\n#Permutation test\ntest.stat.sim <- 0\nfor(i in 1:1000){\n  test.stat.sim[i] <- sd_chunk_1s(matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE))\n}\nhist(test.stat.sim, breaks=25)\nabline(v=sd_chunk_1s(obsData))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n\n```{.r .cell-code}\n#two tailed test, ie if sd too low or too high that is unusual\n2*min(mean(test.stat.sim <= sd_chunk_1s(obsData)), \n      mean(test.stat.sim >= sd_chunk_1s(obsData)))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.02\n```\n:::\n:::\n\n\n\n\n### Test Stat 2: Horizontal Symmetry\n\nWe can look at what proportion of pixels are the same as their mirror pixel. A fast way to calculate that is to compare the image with it's flip, do a conditional comparison of values, and divide by 2.\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-64_953347fe7696752968196a1cee68a711'}\n\n```{.r .cell-code}\ncalculate_horizontal_symmetry <- function(matrix){\n  nCols <- dim(matrix)[2]\n  matrix.flip <- matrix[,nCols:1]\n  return(sum(matrix==matrix.flip)/2)\n}\n\nNMC <- 10000\nresults <- 0\nfor(i in 1:NMC){\n  shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n  results[i] <- calculate_horizontal_symmetry(shuffledMatrix)\n}\nhist(results)\nabline(v=calculate_horizontal_symmetry(obsData))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-64-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncalculate_horizontal_symmetry(obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 63\n```\n:::\n:::\n\n\n\n\n\n### Test stat 3: Neighbor edges\nLook at vertical neighbors (and horizontal neighbors)\n\nA 00 edge will sum to 0\nA 01 or 10 edge will sum to 1\nA 11 edge will sum to 2\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-65_b567ff23751faf5f5ea664d165a8756a'}\n\n```{.r .cell-code}\nvEdges <- obsData[-1,]+obsData[-16,]\nhEdges <- obsData[,-1]+obsData[,-16]\ntable(vEdges)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvEdges\n  0   1   2 \n 46 121  73 \n```\n:::\n\n```{.r .cell-code}\ntable(hEdges)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nhEdges\n  0   1   2 \n 43 126  71 \n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-66_877a7248d22f05d6720655284dc05de9'}\n\n```{.r .cell-code}\n#Add two tables together!\ntable(vEdges)+table(hEdges)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nvEdges\n  0   1   2 \n 89 247 144 \n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-67_72908a43401c8a0f989bf1d45669f89d'}\n\n```{.r .cell-code}\n#Let's let the BW edges be the test statistic to see how well it works:\ncount_bw_edges <- function(matrix){\n  dims <- dim(matrix)\n  vEdges <- matrix[-1,]+matrix[-dim(matrix)[1],]\n  hEdges <- matrix[,-1]+matrix[,-dim(matrix)[2]]\n  bwEdges <- as.numeric((table(factor(vEdges, levels=0:2))+\n                           table(factor(hEdges, levels=0:2)))[2])\n  return(bwEdges)\n}\n\ncount_bw_edges(obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 247\n```\n:::\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-68_51e80391818068ff9cb5f3887012dbd4'}\n\n```{.r .cell-code}\nNMC <- 5000\nresults <- 0\nfor(i in 1:NMC){\n  shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n  results[i] <- count_bw_edges(shuffledMatrix)\n}\nhist(results)\nabline(v=count_bw_edges(obsData))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-68-1.png){width=672}\n:::\n:::\n\n\nThis test statistic doesn't detect all kinds of non-randomness. \nLet's see how well this test statistic does against some image that is definitely not random:\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-69_550fb6617a733e194c901f790c225544'}\n\n```{.r .cell-code}\nobsData2 <- matrix(rep(c(0,1),8*16),nrow=16, byrow=TRUE)\nimage(obsData2, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-69-1.png){width=672}\n:::\n\n```{.r .cell-code}\npar(pty=\"s\")\ncount_bw_edges(obsData2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 240\n```\n:::\n:::\n\n\nOr this one\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-70_cbc3bda7be628c76c14493aef0fea221'}\n\n```{.r .cell-code}\nobsData3 <- matrix(rep(c(rep(c(0,0,1,1),8),rep(c(1,1,0,0),8)),4), ncol=16, byrow=TRUE)\npar(pty=\"s\")\nimage(obsData3, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-70-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncount_bw_edges(obsData3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 224\n```\n:::\n:::\n\n\n\nLet's just create a function to automate the MC simulation\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-71_7d0aa1c196cbd419344cb685ebbd5912'}\n\n```{.r .cell-code}\ngenerateMCsim <- function(data, NMC){\n  results <- 0\n  for(i in 1:NMC){\n    shuffledMatrix <- matrix(data=sample(data), nrow=dim(data)[1], byrow=TRUE)\n    results[i] <- count_bw_edges(shuffledMatrix)\n  }\n  return(results)  \n}\n```\n:::\n\n\nAnd now the Monte Carlo part.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-72_f30e4e5bfdf793a3157bcb0cf9bbaa52'}\n\n```{.r .cell-code}\nmcDist <- generateMCsim(obsData2,5000)\nhist(mcDist)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-72-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncount_bw_edges(obsData2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 240\n```\n:::\n:::\n\n\n\n### Test Stat 4: Moran's I\n\nWe define neighbors to be cardinally adjacent. Moran's index is a statistic that measures how correlated neighboring cells are\n\n$$I = \\frac{N}{W} \\frac{\\sum_{i=1}^N \\sum_{j=1}^N w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\sum_{i=1}^N(x_i-\\bar{x}^2)}$$\n\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-73_89ec00d438be92e73e59f07a680e52af'}\n\n```{.r .cell-code}\nmoranI <- function(matrix){\n  N <- prod(dim(matrix))\n  #The number of adjacencies is the product of the dimensions -1\n  W <- prod(dim(matrix)-1)\n  xbar <- mean(matrix)\n  nRow <- dim(matrix)[1]\n  nCol <- dim(matrix)[2]\n  I <- 2*(sum((matrix-xbar)[-1,]*(matrix-xbar)[-nRow,]) +\n    sum((matrix-xbar)[,-1]*(matrix-xbar)[,-nCol]) )/\n    sum((matrix-xbar)^2) * N/W\n  return(I)\n}\n\nt.sim <- replicate(NMC, moranI(matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)))\nt.obs <- moranI(obsData2)\nhist(t.sim); abline(v=t.obs, col=\"red\");\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-73-1.png){width=672}\n:::\n\n```{.r .cell-code}\n2*min( mean(t.sim>=t.obs), mean(t.sim<=t.obs))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.9392\n```\n:::\n:::\n\n\n\n\n### Test Statistic 5: Number of clumps\n\nLet's count the number of contiguous clumps\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-74_2201ca60b8e881f9287f6cfad504558c'}\n\n```{.r .cell-code}\nlibrary(igraph)\nlibrary(raster)\n\nnClumps <- function(matrix){\n   return(max(as.matrix(clump(raster(matrix), directions=4)), na.rm=TRUE))\n}\n\nnClumps(obsData) + nClumps(1-obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 49\n```\n:::\n:::\n\n\n\nDistribution of clump counts on data example 2\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-75_ac19fae77816eaeef6726dd4f58ef9ec'}\n\n```{.r .cell-code}\nNMC <- 1000\nresults <- 0\n  for(i in 1:NMC){\n    shuffledMatrix <- matrix(data=sample(obsData2), nrow=dim(obsData2)[1], byrow=TRUE)\n    results[i] <- nClumps(shuffledMatrix) + nClumps(1-shuffledMatrix)\n  }\nhist(results)\nabline(v=nClumps(obsData2)+nClumps(1-obsData2))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-75-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnClumps(obsData2)+nClumps(1-obsData2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 16\n```\n:::\n:::\n\n\n\nHow does counting clumps work on the first dataset?\n\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-76_bd718760369ccc49de6d7e56105c5ab3'}\n\n```{.r .cell-code}\nNMC <- 1000\nresults <- 0\n  for(i in 1:NMC){\n    shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n    results[i] <- nClumps(shuffledMatrix)+nClumps(1-shuffledMatrix)\n  }\nhist(results)\nabline(v=nClumps(obsData)+nClumps(1-obsData))\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-76-1.png){width=672}\n:::\n\n```{.r .cell-code}\nnClumps(obsData)+nClumps(1-obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 49\n```\n:::\n:::\n\n\n\n\n### Test Statistic 6: Variation in clump sizes\nPerhaps the standard deviation of the clump sizes might be useful.\n\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-77_a27ed613df6cef51cb645b390d5e6fa1'}\n\n```{.r .cell-code}\nsdClumpSize <- function(matrix){\n  clumps1 <- as.vector(table(as.matrix(clump(raster(matrix), directions=4))))\n  clumps0 <- as.vector(table(as.matrix(clump(raster(1-matrix), directions=4))))\n  return(sd(c(clumps0,clumps1)))\n}\n```\n:::\n\n::: {.cell hash='R05_testing_Additional_Examples_cache/html/unnamed-chunk-78_2413d960e3e288c97c806f66f6d4a7a2'}\n\n```{.r .cell-code}\nNMC <- 1000\nresults <- 0\n  for(i in 1:NMC){\n    shuffledMatrix <- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n    results[i] <- sdClumpSize(shuffledMatrix)\n  }\nhist(results)\n```\n\n::: {.cell-output-display}\n![](R05_testing_Additional_Examples_files/figure-html/unnamed-chunk-78-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsdClumpSize(obsData)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.27186\n```\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}