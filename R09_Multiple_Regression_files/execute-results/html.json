{
  "hash": "b46e68b13e383706d656ffa8fc9597f5",
  "result": {
    "markdown": "---\ntitle: \"R10 multiple regression further concepts\"\nauthor: \"Brian Powers\"\ndate: \"2025-03-31\"\noutput: html_document\n---\n\n\n\n\n\n## R Squared and Correlation\n\nConsider a simple linear model on mtcars:\n\n$$ hp = \\beta_0 + \\beta_1 wt  + \\epsilon$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt.fit <- lm(hp ~ 1+wt, data=mtcars)\nsummary(mt.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = hp ~ 1 + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-83.430 -33.596 -13.587   7.913 172.030 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -1.821     32.325  -0.056    0.955    \nwt            46.160      9.625   4.796 4.15e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.44 on 30 degrees of freedom\nMultiple R-squared:  0.4339,\tAdjusted R-squared:  0.4151 \nF-statistic:    23 on 1 and 30 DF,  p-value: 4.146e-05\n```\n:::\n:::\n\n\n$R^2 = 0.4339$\n\nWhat is the sample correlation between wt and hp? We use \"r\" to refer to this statistic. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(mtcars$wt, mtcars$hp)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6587479\n```\n:::\n:::\n\n\nAnd if we square that\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(mtcars$wt, mtcars$hp)^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4339488\n```\n:::\n:::\n\n\n\nIn general, $R^2$ is the correlation between $\\hat{y}_i$ and $y_i$. So for a multiple regression model:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmt.fit2 <- lm(qsec ~ 1+disp + wt + hp, data=mtcars)\nsummary(mt.fit2)$r.sq\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6808292\n```\n:::\n\n```{.r .cell-code}\nplot(mtcars$qsec, predict(mt.fit2))\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncor(mtcars$qsec, predict(mt.fit2))^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6808292\n```\n:::\n:::\n\n\n\n## Interaction plot for mtcars - looking for an interaction\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(interactions)\n#In the mtcars dataset, am and vs are simply numerical variable\n# I want to treat them as categorical variables - aka \"factor variables\"\nmtcars$am <- factor(mtcars$am)\nmtcars$vs <- factor(mtcars$vs)\n\nmt.fit.cat <- lm(qsec ~ 1+ am * vs , data=mtcars)\ncat_plot(mt.fit.cat, pred=\"vs\", modx=\"am\", geom=\"line\", plot.points=TRUE, data=mtcars, interval=F)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\nThe fact that these are parallel indicates that there is no measurable interaction - the effects of engine shape and transmission type seem to be additive. We also see that when we look at the model output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(mt.fit.cat)$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               Estimate Std. Error    t value     Pr(>|t|)\n(Intercept) 17.14250000  0.3049732 56.2098626 2.443930e-30\nam1         -1.34583333  0.5282290 -2.5478216 1.661442e-02\nvs1          2.82464286  0.5024460  5.6217837 5.087630e-06\nam1:vs1      0.07869048  0.7732481  0.1017661 9.196676e-01\n```\n:::\n:::\n\n\nThe $p$-value on the am-vs interaction term is really high: that's .917 indicating that we have no statistical evidence of an interaction. In other words, the inclusion of an interaction term in the model is not statistically justified.\n\n\n\n\n## Multiple categorical levels : the iris dataset\n\nThe iris dataset is somewhat famous - we have 50 samples from each of three iris species: Setosa, Versicolor and Virginica. Four characteristics are measured: Sepal length & width, and Petal length & width. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(iris)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n```\n:::\n\n```{.r .cell-code}\n#fix the factor leveling just in case\niris$Species <- factor(iris$Species, levels = c(\"setosa\", \"versicolor\", \"virginica\"))\n```\n:::\n\n\nSpecies is a categorical (factor) variable with three values. In a case like this we still need a numerical encoding of the categorical variable. But using 0, 1 and 2 would be problematic for two reasons. First it implies a progression - an ordering - across the three categorical levels. Secondly, it implies that the effect moving from level 0 to 1 is the same effect of moving from level 1 to level 2. \n\nThe proper way to model such a situation is to identify one of the categorical levels (values) to be a \"baseline\". R will do this for you alphabetically. In this case setosa is the first alphabetically. Then we create a dummy (indicator) variable for each of the remaining levels. Again - this is automatically done for us by R.\n\nIf we were to predict sepal length based only on species, the model would look like this:\n$$Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\epsilon$$\nThe three cases are handled by the indicator variables as follows:\n```{}\nSpecies       X_1     X2   \n-------------------------\nsetosa        0       0\nversicolor    1       0\nvirginica     0       1\n```\n\nThe model fit is easy in R.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsepal.fit <- lm(Sepal.Length ~ 1 + Species, data=iris)\nsummary(sepal.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)         5.0060     0.0728  68.762  < 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,\tAdjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n$$\\hat{Y} = 5.0060 + .93 X_{vers} + 1.582 X_{vir}$$\n- For a setosa variety, $X_1=0, X_2=0$ so the predicted sepal length is just 5.006\n- For versicolor, $X_1=1, X_2=0$ so the predicted length is 5.006+.93=5.936\n- For viginica, $X_1=0, X_2=1$ so the predicted length is 5.006 + 1.582 = 6.588\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naggregate(Sepal.Length ~ Species, data=iris, FUN=mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     Species Sepal.Length\n1     setosa        5.006\n2 versicolor        5.936\n3  virginica        6.588\n```\n:::\n:::\n\n\n\nIn other words, the intercept is the mean response for the baseline, the $\\beta_1, \\beta_2$ coefficients are the mean difference between each other case and the baseline.\n\nIf you wanted versicolor to be the baseline instead you can refactor your variable\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris$Species <- factor(iris$Species, \n                       levels = c(\"versicolor\",\"setosa\",\"virginica\"))\nsepal.fit <- lm(Sepal.Length ~ 1+Species, data=iris)\nsummary(sepal.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        5.9360     0.0728  81.536  < 2e-16 ***\nSpeciessetosa     -0.9300     0.1030  -9.033 8.77e-16 ***\nSpeciesvirginica   0.6520     0.1030   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,\tAdjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIf you are curious about the dummy variable encoding that R is doing, you can look at the \"model matrix\"\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel.matrix(sepal.fit)\n```\n:::\n\n\nBut I don't want to refactor the variable, so I'm going to revert the dataset to its default factoring.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrm(iris)\n```\n:::\n\n\n\n\n## A Model with categorical and quantitative variables\n\nIf we were attempting to predict Sepal Length from Petal Length and Species, a model might look like this:\n$$Sepal.L = \\beta_0 + \\beta_1 Species_{Vers} + \\beta_2 Species_{Vir} + \\beta_3 Petal.L + \\epsilon$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(Sepal.Length ~ Petal.Length, data=iris, col=iris$Species, pch=16)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\nA fitted model (without an interaction) would look like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsepal.fit1 <- lm(Sepal.Length ~ 1+Species+Petal.Length, data=iris)\nsummary(sepal.fit1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species + Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        3.68353    0.10610  34.719  < 2e-16 ***\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\nPetal.Length       0.90456    0.06479  13.962  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.338 on 146 degrees of freedom\nMultiple R-squared:  0.8367,\tAdjusted R-squared:  0.8334 \nF-statistic: 249.4 on 3 and 146 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nLet's just interpret the coefficients for a moment:\n\nFirst off, setosa is the baseline case. The coefficient of Petal length is 0.90456; this means that controlling for species (i.e. holding species constant) a 1 cm increase in Petal Length is associated with a .90456 cm average increase in Sepal Length. \n$$\\hat{sepal.L}_{set} = 3.68 + .906 petal.L$$\n\n\nFor the versicolor coefficient, -1.60097 is interpreted as the average difference in sepal length between a versicolor compared to a setosa iris, controlling for petal length. \n$$\\hat{sepal.L}_{vers} = (3.68 -1.6) + .906 petal.L$$\n\nI'm not going to check the model assumptions at the moment because that's not my primary focus. I want to plot the model with the quantitative and categorical predictor:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract_plot( lm(Sepal.Length ~ Species+Petal.Length, data=iris), pred=Petal.Length, modx=Species, plot.points=TRUE)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\n\nYou can see clearly that the model fit encodes three parallel lines. We can get the linear equations by writing out the cases. Take setosa first:\n$$\\hat{Sepal.L}_{Set} = 3.68 + 0.90456 Petal.L$$\nFor versicolor we have\n$$\\hat{Sepal.L}_{Vers} = 3.68 - 1.6 + 0.90456 Petal.L = 2.08 + .90456 Petal.L$$\n\nFor virginica\n$$\\hat{Sepal.L}_{Vers} = 3.68 - 2.12 + 0.90456 Petal.L = 1.56 + .90456 Petal.L$$\n\n\n## Interactions between categorical and quantitative variables\n\nAdding an interaction between the categorical predictor and the quantitative predictor allows more model flexibility. It allows for different intercepts *and* different slopes. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\niris.fit<-lm(Sepal.Length ~ 1+ Petal.Length + Species + Petal.Length:Species, data=iris)\n# or I can use 1 + Petal.Length * Species\n\ninteract_plot(iris.fit, pred=Petal.Length, modx=Species, plot.points=TRUE)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n\n\nLooking at the model summary tells us which of the interactions are statistically supported. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(iris.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Petal.Length + Species + Petal.Length:Species, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73479 -0.22785 -0.03132  0.24375  0.93608 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                      4.2132     0.4074  10.341  < 2e-16 ***\nPetal.Length                     0.5423     0.2768   1.959  0.05200 .  \nSpeciesversicolor               -1.8056     0.5984  -3.017  0.00302 ** \nSpeciesvirginica                -3.1535     0.6341  -4.973 1.85e-06 ***\nPetal.Length:Speciesversicolor   0.2860     0.2951   0.969  0.33405    \nPetal.Length:Speciesvirginica    0.4534     0.2901   1.563  0.12029    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3365 on 144 degrees of freedom\nMultiple R-squared:  0.8405,\tAdjusted R-squared:  0.8349 \nF-statistic: 151.7 on 5 and 144 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nIn this case neither of them. \n\nThe interpretation of these interaction coefficients gives us an adjustment to the slope on `Petal.Length`. Here's the full model with coefficient estimates (rounded):\n\n$$\\hat{Sepal.L} = 4.2 + .54Petal.L + -1.8 Species_{Ver} - \\\\ 3.2 Species_{Vir}+0.29Petal.L\\cdot Species_{Ver} + .45 Petal.L \\cdot Species_{Vir}$$\n\nThe indicator variables $Species_{Ver}$ and $Species_{Vir}$ are (0,0) for setosa, (1,0) for versicolor and (0,1) for virginica.\n\nFor instance, the model for Setosa would be:\n\n$$\\hat{Sepal.L}_{Vers} = 4.2 + .54Petal.L$$\nFor the versicolor two more coefficients will play a part:\n$$\\hat{Sepal.L}_{Ver} = 4.2 + .54Petal.L + -1.8(1) +0.29Petal.L(1)$$\nWhich simplifies to\n$$\\hat{Sepal.L}_{Ver} = 2.4 + .83 Petal.L$$\nThis is the linear equation for the orange dotted line above. We could do the same for virginica but the idea is the same.\n\nSo the coefficients of the interaction terms can be interpreted as the difference in slope between the base case (versicolor) and the other cases (setosa or virginica). \n\nThat is how we use interactions between categorical variables and quantitative variables.\n\nYou can actually calculate these coefficients by subsetting the data into three species and fitting three linear models!\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"setosa\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) Petal.Length \n   4.2131682    0.5422926 \n```\n:::\n\n```{.r .cell-code}\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"versicolor\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) Petal.Length \n    2.407523     0.828281 \n```\n:::\n\n```{.r .cell-code}\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"virginica\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n (Intercept) Petal.Length \n   1.0596591    0.9957386 \n```\n:::\n:::\n\n\nThe coefficient estimate for the interaction term Petal.Length:Speciessetosa is \n\n\n::: {.cell}\n\n```{.r .cell-code}\n0.5422926 - 0.828281\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -0.2859884\n```\n:::\n:::\n\n\n\n## More on Interactions - two quantitative predictors\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(interactions)\n```\n:::\n\n\n\nAn interaction of two quantitative variables $X1$ and $X2$ would allow for a model that looks something like this:\n\n$$ Y = 10 + 5X1 + 3X2 - 2 X1 \\times X2 + \\epsilon$$\n\nThis describes a hyperbolic paraboloid\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](https://mathcurve.com/surfaces.gb/paraboloidhyperbolic/ph-droites.gif)\n:::\n:::\n\n\n\nAn interaction between two quantitative variables is one form of a non-additive relationship with the response variable. \n\nHere is a dataset where the linear model includes an interaction between two continuous variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstates <- as.data.frame(state.x77)\nsummary(states)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   Population        Income       Illiteracy       Life Exp    \n Min.   :  365   Min.   :3098   Min.   :0.500   Min.   :67.96  \n 1st Qu.: 1080   1st Qu.:3993   1st Qu.:0.625   1st Qu.:70.12  \n Median : 2838   Median :4519   Median :0.950   Median :70.67  \n Mean   : 4246   Mean   :4436   Mean   :1.170   Mean   :70.88  \n 3rd Qu.: 4968   3rd Qu.:4814   3rd Qu.:1.575   3rd Qu.:71.89  \n Max.   :21198   Max.   :6315   Max.   :2.800   Max.   :73.60  \n     Murder          HS Grad          Frost             Area       \n Min.   : 1.400   Min.   :37.80   Min.   :  0.00   Min.   :  1049  \n 1st Qu.: 4.350   1st Qu.:48.05   1st Qu.: 66.25   1st Qu.: 36985  \n Median : 6.850   Median :53.25   Median :114.50   Median : 54277  \n Mean   : 7.378   Mean   :53.11   Mean   :104.46   Mean   : 70736  \n 3rd Qu.:10.675   3rd Qu.:59.15   3rd Qu.:139.75   3rd Qu.: 81163  \n Max.   :15.100   Max.   :67.30   Max.   :188.00   Max.   :566432  \n```\n:::\n:::\n\n\n\nFirst I want to fit a model that does not include an interaction term.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit.noi <- lm(Income ~ Illiteracy + Murder + `HS Grad`, data = states)\nsummary(fit.noi)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Income ~ Illiteracy + Murder + `HS Grad`, data = states)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1068.21  -274.66   -40.47   182.69  1203.82 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  2017.96     750.10   2.690 0.009917 ** \nIlliteracy   -176.84     187.25  -0.944 0.349919    \nMurder         30.48      26.70   1.141 0.259591    \n`HS Grad`      45.19      11.51   3.925 0.000288 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 490.1 on 46 degrees of freedom\nMultiple R-squared:  0.4028,\tAdjusted R-squared:  0.3638 \nF-statistic: 10.34 on 3 and 46 DF,  p-value: 2.551e-05\n```\n:::\n:::\n\n\nThe interaction term can be added to the formula explicitly by including an addition `Illiteracy:Murder` term, or just use the asterisk to include the interaction and both single terms as well. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiti <- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)\nsummary(fiti)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Income ~ Illiteracy * Murder + `HS Grad`, data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-916.27 -244.42   28.42  228.14 1221.16 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        1414.46     737.84   1.917  0.06160 .  \nIlliteracy          753.07     385.90   1.951  0.05724 .  \nMurder              130.60      44.67   2.923  0.00540 ** \n`HS Grad`            40.76      10.92   3.733  0.00053 ***\nIlliteracy:Murder   -97.04      35.86  -2.706  0.00958 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 459.5 on 45 degrees of freedom\nMultiple R-squared:  0.4864,\tAdjusted R-squared:  0.4407 \nF-statistic: 10.65 on 4 and 45 DF,  p-value: 3.689e-06\n```\n:::\n:::\n\n\n\nThe interactions between variables can be explored using interaction plots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract_plot(fiti, pred = Illiteracy, modx = Murder)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n\nThe interaction plot takes the other quantitative variable (HS Grad) and sets its value to be the mean (53.11). Three lines are given here indicating how the line changes as the Murder variable may vary from its average mean value. \n\n$$\\hat{Income} = 1414.46+753 Ill + 130.60 Mur + 40.76(53.11) -97.04 Ill \\times Mur$$\nYou can see that as the murder rate increases the effect of illiteracy on income changes. This is reflected in the negative value for the interaction. In this case the Mur variable is continuous rather than categorical. The interpretation of the coefficient on the interaction term is a little more tricky to put into words. That's beyond this course. \n\nThe important take-away here is that the interaction term is statistically significant, so we have strong evidence from the data that there is a real interaction in the model. The average effect of Illiteracy and Murder rate on per capita income is not purely additive.\n\n## MT Cars polynomial model\n\nLet's briefly compare a linear model with a second order model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(mpg~hp, data=mtcars)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n\n```{.r .cell-code}\nmtcars.pow1 <- lm(mpg ~ 1 + hp, data=mtcars)\nsummary(mtcars.pow1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ 1 + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 30.09886    1.63392  18.421  < 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,\tAdjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n```\n:::\n\n```{.r .cell-code}\nplot(mtcars.pow1, which=1)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-26-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars.pow2 <- lm(mpg ~ 1 + hp + I(hp^2), data=mtcars)\nsummary(mtcars.pow2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ 1 + hp + I(hp^2), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5512 -1.6027 -0.6977  1.5509  8.7213 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.041e+01  2.741e+00  14.744 5.23e-15 ***\nhp          -2.133e-01  3.488e-02  -6.115 1.16e-06 ***\nI(hp^2)      4.208e-04  9.844e-05   4.275 0.000189 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.077 on 29 degrees of freedom\nMultiple R-squared:  0.7561,\tAdjusted R-squared:  0.7393 \nF-statistic: 44.95 on 2 and 29 DF,  p-value: 1.301e-09\n```\n:::\n\n```{.r .cell-code}\nplot(mtcars.pow2, which=1)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nFor the moment let's consider a third order polynomial just to see if the linearity is better satisfied.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmtcars.pow3 <- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3), data=mtcars)\nsummary(mtcars.pow3)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = mpg ~ 1 + hp + I(hp^2) + I(hp^3), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8605 -1.3972 -0.5736  1.6461  9.0738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.422e+01  5.961e+00   7.419 4.43e-08 ***\nhp          -2.945e-01  1.178e-01  -2.500   0.0185 *  \nI(hp^2)      9.115e-04  6.863e-04   1.328   0.1949    \nI(hp^3)     -8.701e-07  1.204e-06  -0.722   0.4760    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.103 on 28 degrees of freedom\nMultiple R-squared:  0.7606,\tAdjusted R-squared:  0.7349 \nF-statistic: 29.65 on 3 and 28 DF,  p-value: 7.769e-09\n```\n:::\n\n```{.r .cell-code}\nplot(mtcars.pow3, which=1)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-28-1.png){width=672}\n:::\n:::\n\n\n\n\n## Variable Transformations\n\nVariable transformations can be effective at remedying violated regression assumptions. Briefly here are the assumptions:\n\n- The error term has a normal distribution\n- The response variable has a linear relationship with the predictors. In other words, the errors have zero mean\n- The error term has constant variance\n- The errors are independent.\n\nThe fourth assumption is hard to check, but the first three can be checked with diagnostic plots of the residuals.\n\nTransformation of either a predictor variable or the response variable may be appropriate, often with theoretical justification. But first let's explore the effect of variable transformation.\n\nHere is some left-skewed data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nleft.skew.data <- rbeta(60, 2, 1)*10\nhist(left.skew.data)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\nAnd here are histograms of the data taken to different powers\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nhist(log(left.skew.data), main=\"Log transform\")\nhist((left.skew.data)^.5, main=\"square root transform\")\nhist((left.skew.data)^2, main=\"squaring\")\nhist((left.skew.data)^3, main=\"cubing\")\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-30-1.png){width=672}\n:::\n:::\n\n\n\nYou can see that squaring the values give us a more symmetric distribution. Generally speaking, if you have left-skewed data, raising your values to a power $>1$ can do this for you.\n\nOn the other hand, here is some right-skewed data - an exponential population\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nright.skew.data <- rexp(60, 2)\nhist(right.skew.data, breaks=25)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-31-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npar(mfrow=c(2,2))\nhist(log(right.skew.data), main=\"log\")\nhist((right.skew.data)^.5, main=\"sqrt\")\nhist((right.skew.data)^2, main=\"Square\", breaks=20)\nhist((right.skew.data)^3, main=\"cube\", breaks=20)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\nA right skew is amplified by taking your data to a higher power, but square roots and log transformations can make your data more symmetric and sometimes more normally distributed.\n\nVariable transformations can often be effective means to remedy violated regression assumptions.\n\n### Real Estate Air Conditioning\n\nThe real estate data is in the `realestate.txt` file. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal.estate <- read.table(\"data/realestate.txt\", header=T)\nreal.estate$Air <- factor(real.estate$Air)\n```\n:::\n\n\n\nWhile there are many variables, we'll focus only on three: \n- Y = sale price of the home (in 1000s of dollars)\n- X1 = square footage of the home (in 1000s of sqft)\n- X2 = whether or not the home has air conditioning\n\nThe interaction model is\n$$Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\beta_3 X_{i,1}X_{i,2} + \\epsilon_i$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal.fit <- lm(SalePrice ~ 1+SqFeet*Air, data=real.estate)\nlibrary(interactions)\ninteract_plot(real.fit, pred=SqFeet, modx=Air, plot.points = TRUE)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n\n```{.r .cell-code}\nsummary(real.fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = SalePrice ~ 1 + SqFeet * Air, data = real.estate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-248.01  -37.13   -7.80   22.25  381.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   -3.218     30.085  -0.107 0.914871    \nSqFeet       104.902     15.748   6.661 6.96e-11 ***\nAir1         -78.868     32.663  -2.415 0.016100 *  \nSqFeet:Air1   55.888     16.580   3.371 0.000805 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 77.01 on 517 degrees of freedom\nMultiple R-squared:  0.6887,\tAdjusted R-squared:  0.6869 \nF-statistic: 381.2 on 3 and 517 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nplot(real.fit, which=1:2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-34-2.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-34-3.png){width=672}\n:::\n:::\n\n\n\nThere is strong evidence supporting the interaction in the model, and both preditors are strong predictors of sale price. However there are two big problems. One is the residuals spread is not constant, and the residuals do not seem to be normally distributed. The non constant spread can be seen in the plot\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract_plot(real.fit, pred=SqFeet, modx=Air, plot.points = TRUE)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(resid(real.fit))\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-35-2.png){width=672}\n:::\n:::\n\n\nthe residuals are unimodal but the right tail is too long - this is shown in the QQ plot but more obviously emphasized in the histogram above.\n\nWhen large values of $y$ have higher variance than low values of $y$ a log transformation of the response variable will often fix the problem. This is because error is not in absolute term but rather relative to the magnitude of $y$. Logs fix this mathematically.\n\nWe'll transform Sale Price in the model:\n$$\\ln Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\beta_3 X_{i,1}X_{i,2} + \\epsilon_i$$\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal.fit.log <- lm(log(SalePrice) ~ 1+SqFeet*Air, data=real.estate)\nsummary(real.fit.log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(SalePrice) ~ 1 + SqFeet * Air, data = real.estate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73816 -0.13955 -0.02114  0.11740  0.82106 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  4.32846    0.08813  49.113   <2e-16 ***\nSqFeet       0.46808    0.04613  10.146   <2e-16 ***\nAir1         0.11522    0.09569   1.204    0.229    \nSqFeet:Air1  0.02202    0.04857   0.453    0.650    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2256 on 517 degrees of freedom\nMultiple R-squared:  0.7274,\tAdjusted R-squared:  0.7259 \nF-statistic: 459.9 on 3 and 517 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nplot(real.fit.log, which=1:2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-36-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-36-2.png){width=672}\n:::\n\n```{.r .cell-code}\ninteract_plot(real.fit.log, pred=SqFeet, modx=Air, plot.points = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing data real.estate from global environment. This could cause incorrect\nresults if real.estate has been altered since the model was fit. You can\nmanually provide the data to the \"data =\" argument.\n```\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-36-3.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(resid(real.fit.log))\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-36-4.png){width=672}\n:::\n:::\n\n\nThe predictor variable SqFeet also exhibits a right skew\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(real.estate$SqFeet)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n\n\nIn a case like this a linear model may possibly fit better if the predictor is log transformed too. This is worth checking\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nreal.estate$logSqFeet <- log(real.estate$SqFeet)\nreal.fit.log <- lm(log(SalePrice) ~ 1+logSqFeet*Air, data=real.estate)\nsummary(real.fit.log)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(SalePrice) ~ 1 + logSqFeet * Air, data = real.estate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58375 -0.13872 -0.01541  0.11524  0.76531 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     4.62997    0.05729  80.811  < 2e-16 ***\nlogSqFeet       0.97321    0.09119  10.673  < 2e-16 ***\nAir1           -0.04867    0.06549  -0.743  0.45770    \nlogSqFeet:Air1  0.27514    0.09838   2.797  0.00535 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2183 on 517 degrees of freedom\nMultiple R-squared:  0.7448,\tAdjusted R-squared:  0.7434 \nF-statistic: 503.1 on 3 and 517 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nplot(real.fit.log, which=1:2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-38-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-38-2.png){width=672}\n:::\n\n```{.r .cell-code}\ninteract_plot(real.fit.log, pred=logSqFeet, modx=Air, plot.points = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nUsing data real.estate from global environment. This could cause incorrect\nresults if real.estate has been altered since the model was fit. You can\nmanually provide the data to the \"data =\" argument.\n```\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-38-3.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(resid(real.fit.log))\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-38-4.png){width=672}\n:::\n:::\n\n\nThe interaction of air conditioning and square feet is brought out more clearly when this transformation is done, as shown in the significance of the predictor. This model also has the strongest $R^2$ of all three models considered.\n\nLet's consider how this model would estimate / predict the sale price of a 1800 sqft house with air conditioning.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npredict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n5.315058 \n```\n:::\n\n```{.r .cell-code}\n#to conver this to our units, we take e^<this>\nexp(predict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"))\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       1 \n203.3764 \n```\n:::\n\n```{.r .cell-code}\n#to convert a 95% confidence interval back to original units, do the same\n#this is my 95% prediction interval for ONE particular house that is 1800 sqft\nexp(predict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"), interval=\"prediction\")\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       fit      lwr      upr\n1 203.3764 132.3514 312.5161\n```\n:::\n:::\n\n\n\n\n## Multicollinearity\n\nWhen predictor variables exhibit high correlation we call this multicollinearity. This can cause a number of problems which we'll discuss. First let's consider some synthetic data where the predictors are perfectly correlated - this is an extreme case.\n\n$$ X_2 = 2.5 X_1-1$$\nIf the true model looks like this:\n$$Y = 10 + 2X_1 +.3 X_2 + \\epsilon, \\quad \\epsilon \\sim N(0, 4^2)$$\nWe could actually write $X_2$ in terms of $X_1$ or vice versa. For instance, we could substitute in the expression $2.5X_1-1$ and get\n$$Y = 10 + 2X_1 + .3(2.5X_1-1) + \\epsilon = 9.7 + 2.75X_1 + \\epsilon$$\nOr we could substitute $X_1 = \\frac{X_2+1}{2.5}$ and get\n$$ Y = 10 + 2\\frac{X_2+1}{2.5}+.3X_2+\\epsilon = 10.8 + 1.1 X_2+ \\epsilon$$\nBoth of these models are perfectly true. The fact is that both would fit the data just as well.\n\nIn fact, if your predictors exhibit perfect correlation then R will be unable to properly fit a linear model to the data.\n\nI'll simulate a sample of size 30.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1 <- runif(30, 10,20) \nX2 <- 2.5 * X1 -1\nY <- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nsummary(lm(Y~1+X1+X2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ 1 + X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3413 -2.1553 -0.0532  2.5223  5.1597 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  11.3450     3.3053   3.432  0.00188 ** \nX1            2.6857     0.2188  12.273 8.74e-13 ***\nX2                NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.195 on 28 degrees of freedom\nMultiple R-squared:  0.8433,\tAdjusted R-squared:  0.8377 \nF-statistic: 150.6 on 1 and 28 DF,  p-value: 8.738e-13\n```\n:::\n:::\n\n\n\nThe case of perfect correlation is rare but a strong correlation could certainly arise. I'll add a little bit of random spice in to the calculation of $X_2$ and we'll look at the correlation plots.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nX1 <- runif(30, 10,20) \nX2 <- 2.5 * X1 -1 + rnorm(30,0,1)\nY <- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nplot(data.frame(Y,X1,X2))\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-41-1.png){width=672}\n:::\n:::\n\n\nWe can numerically measure the correlation using the `cor` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(data.frame(Y,X1,X2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           Y        X1        X2\nY  1.0000000 0.8710865 0.8966127\nX1 0.8710865 1.0000000 0.9944797\nX2 0.8966127 0.9944797 1.0000000\n```\n:::\n:::\n\n\n\nIf we attempt to fit a linear model this is what we get:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Y~1+X1+X2))$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate Std. Error   t value     Pr(>|t|)\n(Intercept) 16.339897  3.4972494  4.672214 7.349847e-05\nX1          -5.446292  2.1220827 -2.566484 1.613689e-02\nX2           3.200272  0.8458235  3.783617 7.822802e-04\n```\n:::\n:::\n\n\nIn the true model the coefficients of $X_1$ and $X_2$ are $2$ and $.3$. These quite far off! The intercept is way off as well.\n\nI'll repeat again with newly generated data to see how it differs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX1 <- runif(30, 10,20) \nX2 <- 2.5 * X1 -1 + rnorm(30,0,1)\nY <- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nsummary(lm(Y~X1+X2))$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error    t value    Pr(>|t|)\n(Intercept) 11.9418373  4.2513025  2.8089832 0.009126691\nX1           3.0977225  1.9737924  1.5694267 0.128195112\nX2          -0.2370767  0.7921835 -0.2992699 0.767025684\n```\n:::\n:::\n\n\nNotice that the coefficient of $X_1$ is now a lot lower (and negative) and the coefficient of $X_2$ is positive now! Furthermore, the $p$-values for the coefficients are all $>0.05$. This is common when predictors are highly correlated. Typical problems that you will see when you have multicollinearity are:\n\n- Parameters of the model become indeterminate\n- Standard errors of the estimates become large\n- addition or removal of a predictor can drastically change the coefficients - even flip their signs\n- it becomes difficult or impossible to interpret the coefficients\n- predictions risk extrapolation\n\nThe first two problems are understandable if we think about the extreme case. We actually had three models that were equally valid when we had perfect correlation (ignoring $\\epsilon$ for simplicity):\n\n$$Y = 10 + 2X_1 +.3 X_2, \\quad Y=9.7 + 2.75X_1, \\quad Y=10.8 + 1.1 X_2$$\nIn fact, there are an infinite number of models you could fit that would fit equally well. This means we have no certainty of either of the coefficient values. In fact, the uncertainty of $\\beta_1$ and $\\beta_2$ affects our uncertainty of $\\beta_0$ as well! This causes the standard errors to be large.\n\nWhat happens when we remove a predictor? I'll refit the model with only $X_1$ and only $X_2$ to see\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Y~X1+X2))$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error    t value    Pr(>|t|)\n(Intercept) 11.9418373  4.2513025  2.8089832 0.009126691\nX1           3.0977225  1.9737924  1.5694267 0.128195112\nX2          -0.2370767  0.7921835 -0.2992699 0.767025684\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(Y~X1))$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 12.034186  4.1705844 2.885491 7.441841e-03\nX1           2.512754  0.2697257 9.315962 4.491075e-10\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(Y~X2))$coefficients\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              Estimate Std. Error  t value     Pr(>|t|)\n(Intercept) 13.3338353  4.2649971 3.126341 4.099863e-03\nX2           0.9941401  0.1128976 8.805676 1.474748e-09\n```\n:::\n:::\n\n\nNotice that in the models with only 1 predictor the $p$-value shoots down to close to zero. The standard error shrinks as well. When we have a model with only one of these correlated predictors we can focus on the predictive power of that one predictor and it is not muddied by the influence of the other one. \n\nThis leads to the fourth problem. Interpretation of coefficients in a multiple regression model requires us to say something like \"holding other variables constant\". But if $X_1$ and $X_2$ exhibit strong correlation in the real world, then it's not reasonable to hold one constant while changing the other.\n\nWhat about the fifth problem about extrapolations? Let's look at the data summaries:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(data.frame(X1,X2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       X1              X2       \n Min.   :10.64   Min.   :25.31  \n 1st Qu.:13.36   1st Qu.:33.30  \n Median :15.15   Median :37.10  \n Mean   :15.26   Mean   :37.26  \n 3rd Qu.:17.37   3rd Qu.:42.42  \n Max.   :19.03   Max.   :46.84  \n```\n:::\n:::\n\n\nOur dataset includes $X1$ over the range 10 to 20 roughly, and $X2$ over the range 23 to 50. So one might think that $Y$ could be predicted for any pair of $X1,X2$ values in the range $[10,20]\\times[25,50]$. For example, can we predict $\\hat{Y}|X_1=11, X_2=48$ ? \n\n\n::: {.cell}\n\n```{.r .cell-code}\nlmfit <- lm(Y~X1+X2)\npredict(lmfit, newdata=data.frame(X1=11, X2=48))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      1 \n34.6371 \n```\n:::\n:::\n\n\nNo problem there. But let's look again at the data range as a scatterplot\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(X1,X2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-48-1.png){width=672}\n:::\n:::\n\n\nThe only data we've observed is over thin diagonal strip. Our model is not supported outside of this range, so predicting $Y$ for $X1=11, X2=48$ has a tremendous amount of uncertainty.\n\n### Detecting Multicollinearity\n\nA simple statistic that measures multicollinearity is the Variance Inflation Factor or VIF. The idea is that we take each predictor and fit a linear model with this predictor as the response and the other predictors as predictors. We measure how strong the coefficient of determination ($R^2$) is, and calculate\n$$VIF_j = \\frac{1}{1-R^2_j}$$\nFor instance, in this case we have\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrsq = summary(lm(X1~X2))$r.square\n1/(1-rsq)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 51.80865\n```\n:::\n:::\n\n\nThis is very high - a VIF more than 10 is extremely problematic.\n\nThere's a function in the `car` package to calculate VIF for all predictors in a model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: carData\n```\n:::\n\n```{.r .cell-code}\nvif(lmfit)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      X1       X2 \n51.80865 51.80865 \n```\n:::\n:::\n\n\nNote that in this case both predictors have the same VIF. This is because when we regress $X1$ on $X2$ the correlation is the same as if we regress $X2$ on $X1$. \n\n### How to handle Multicollinearity\n\nThere are some more advanced methods but probably the simplest method is to remove one of the problematic predictors. In this case we could choose arbitrarily to keep $X1$ or $X2$ in the model and drop the other one. Either choice would be fine. \n\nNote that in the case of multicollinearity this dropping of a redundant variable will not cause $R^2$ to suffer much at all. Observe:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lm(Y~X1+X2))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4838 -2.3684 -0.4808  1.6046  9.7260 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  11.9418     4.2513   2.809  0.00913 **\nX1            3.0977     1.9738   1.569  0.12820   \nX2           -0.2371     0.7922  -0.299  0.76703   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.747 on 27 degrees of freedom\nMultiple R-squared:  0.7569,\tAdjusted R-squared:  0.7389 \nF-statistic: 42.03 on 2 and 27 DF,  p-value: 5.113e-09\n```\n:::\n\n```{.r .cell-code}\nsummary(lm(Y~X1))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = Y ~ X1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3143 -2.0682 -0.4356  1.2547  9.8196 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  12.0342     4.1706   2.885  0.00744 ** \nX1            2.5128     0.2697   9.316 4.49e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.686 on 28 degrees of freedom\nMultiple R-squared:  0.7561,\tAdjusted R-squared:  0.7474 \nF-statistic: 86.79 on 1 and 28 DF,  p-value: 4.491e-10\n```\n:::\n:::\n\n\n\n### A Data Example\n\nThe datafile (`wages85.txt`) is downloaded from http://lib.stat.cmu.edu/datasets/. It contains 534 observations on 11 variables sampled from the Current Population Survey of 1985. The Current Population Survey (CPS) is used to supplement census information between census years. These data consist of a random sample of 534 persons from the CPS, with information on wages and other characteristics of the workers, including sex, number of years of education, years of work experience, occupational status, region of residence and union membership. We wish to determine whether wages are related to these characteristics.\nIn particular, we are seeking for the following model:\n\n$wage = \\beta_0 + \\beta_1 south + \\beta_2 sex + \\beta_3 exp + \\beta_4 union + \\beta_5 age + \\beta_6 race_2 + \\beta_7 race_3 + \\cdots$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwages <- read.table(\"data/wages85.txt\", header=TRUE)\n```\n:::\n\n\nBut we should understand how the data is encoded. In particular some of the variables are categorical variables: SOUTH, SEX, RACE, SECTOR, MARR, OCCUPATION, UNION. We should convert these to categorical `factor` type variables rather than numeric.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwages$SOUTH <- factor(wages$SOUTH)\nwages$SEX <- factor(wages$SEX)\nwages$RACE <- factor(wages$RACE)\nwages$SECTOR <- factor(wages$SECTOR)\nwages$MARR <- factor(wages$MARR)\nwages$OCCUPATION <- factor(wages$OCCUPATION)\nwages$UNION <- factor(wages$UNION)\n```\n:::\n\n\n\nNow we can fit the model. Note that with all of these categorical variables we will have a lot of model coefficients. With a sample size of 500+ that's not a huge concern right now. Anyway, the point is going to be about multicollinearity.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# the \".\" indicates I want to use ALL predictors in the model. No interactions though.\nwage.lm <- lm(WAGE ~ . , data=wages)\nsummary(wage.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = WAGE ~ ., data = wages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.409  -2.486  -0.631   1.872  35.021 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   2.2781     6.6976   0.340  0.73390    \nEDUCATION     0.8128     1.0869   0.748  0.45491    \nSOUTH1       -0.5627     0.4198  -1.340  0.18070    \nSEX1         -1.9425     0.4194  -4.631 4.60e-06 ***\nEXPERIENCE    0.2448     1.0818   0.226  0.82103    \nUNION1        1.6017     0.5127   3.124  0.00188 ** \nAGE          -0.1580     1.0809  -0.146  0.88382    \nRACE2         0.2314     0.9915   0.233  0.81559    \nRACE3         0.8379     0.5745   1.458  0.14532    \nOCCUPATION2  -4.0638     0.9159  -4.437 1.12e-05 ***\nOCCUPATION3  -3.2682     0.7626  -4.286 2.17e-05 ***\nOCCUPATION4  -3.9754     0.8108  -4.903 1.26e-06 ***\nOCCUPATION5  -1.3336     0.7289  -1.829  0.06791 .  \nOCCUPATION6  -3.2905     0.8005  -4.111 4.59e-05 ***\nSECTOR1       1.0409     0.5492   1.895  0.05863 .  \nSECTOR2       0.4774     0.9661   0.494  0.62141    \nMARR1         0.3005     0.4112   0.731  0.46523    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.282 on 517 degrees of freedom\nMultiple R-squared:  0.3265,\tAdjusted R-squared:  0.3056 \nF-statistic: 15.66 on 16 and 517 DF,  p-value: < 2.2e-16\n```\n:::\n\n```{.r .cell-code}\nplot(wage.lm, which=1:2)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: not plotting observations with leverage one:\n  444\n```\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-54-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-54-2.png){width=672}\n:::\n:::\n\n\n\nWithout going into the summary, it's clear that variance increases with predicted wage; this is a good indication that a log transformation or square root transformation of the response variable could be useful. In fact that could even fix the non normality of residuals\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(resid(wage.lm), breaks=50)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-55-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwage.log.lm <- lm(log(WAGE) ~ ., data=wages)\nplot(wage.log.lm, which=1:2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-56-2.png){width=672}\n:::\n\n```{.r .cell-code}\nhist(resid(wage.log.lm), breaks=50)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-56-3.png){width=672}\n:::\n:::\n\n\nThe log transform fixed both problems. So instead of predicting wage, we'll predict log(wage). But let's look at the summary output.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(wage.log.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(WAGE) ~ ., data = wages)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36167 -0.27926  0.00049  0.27957  1.79838 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.54415    0.66955   2.306 0.021491 *  \nEDUCATION    0.12525    0.10865   1.153 0.249530    \nSOUTH1      -0.09290    0.04197  -2.214 0.027291 *  \nSEX1        -0.21812    0.04193  -5.202 2.85e-07 ***\nEXPERIENCE   0.06799    0.10814   0.629 0.529813    \nUNION1       0.21178    0.05126   4.132 4.20e-05 ***\nAGE         -0.05858    0.10806  -0.542 0.587963    \nRACE2       -0.03345    0.09912  -0.338 0.735876    \nRACE3        0.07973    0.05743   1.388 0.165636    \nOCCUPATION2 -0.36440    0.09156  -3.980 7.88e-05 ***\nOCCUPATION3 -0.20964    0.07624  -2.750 0.006171 ** \nOCCUPATION4 -0.38345    0.08105  -4.731 2.89e-06 ***\nOCCUPATION5 -0.05278    0.07287  -0.724 0.469223    \nOCCUPATION6 -0.26555    0.08002  -3.318 0.000969 ***\nSECTOR1      0.11532    0.05491   2.100 0.036186 *  \nSECTOR2      0.09296    0.09658   0.962 0.336262    \nMARR1        0.06335    0.04111   1.541 0.123899    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4281 on 517 degrees of freedom\nMultiple R-squared:  0.3617,\tAdjusted R-squared:  0.342 \nF-statistic: 18.31 on 16 and 517 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\n\nBy looking at the model summary, the R-squared value of 0.362 is not bad for a cross sectional data of 534 observations. The F-value is highly significant implying that all the explanatory variables together significantly explain the log of wages. However, coming to the individual regression coefficients, it is seen that as many as five variables (race, education, experience, age, marriage) are not statistically significant.\n\nFor further diagnosis of the problem, let us first look at the pair-wise correlation among the numerical variables.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nX<-wages[,c(\"EDUCATION\",\"EXPERIENCE\",\"WAGE\",\"AGE\")]\nlibrary(GGally)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: package 'GGally' was built under R version 4.2.3\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nLoading required package: ggplot2\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n```\n:::\n\n```{.r .cell-code}\nggpairs(X)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-58-1.png){width=672}\n:::\n:::\n\n\n\nThe high correlation between age and experience might be the root cause of multicollinearity.\nAgain by looking at the partial correlation coefficient matrix among the variables, it is also clear that the partial correlation between experience  education, age  education and age  experience are quite high.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(X)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            EDUCATION  EXPERIENCE       WAGE        AGE\nEDUCATION   1.0000000 -0.35267645 0.38192207 -0.1500195\nEXPERIENCE -0.3526764  1.00000000 0.08705953  0.9779612\nWAGE        0.3819221  0.08705953 1.00000000  0.1769669\nAGE        -0.1500195  0.97796125 0.17696688  1.0000000\n```\n:::\n:::\n\n\n\nIf we look at the VIF for the model we can get a sense of how bad it really is.\nNote that VIF is defined for quantitative variables, so if I use the vif function I will get a \"generalized\" vif since I have so many categorical predictors. That's ok. It has the same general interpretation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(wage.log.lm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                  GVIF Df GVIF^(1/(2*Df))\nEDUCATION   234.855660  1       15.325001\nSOUTH         1.061343  1        1.030215\nSEX           1.272000  1        1.127830\nEXPERIENCE 5212.811724  1       72.199804\nUNION         1.128802  1        1.062451\nAGE        4669.808671  1       68.335998\nRACE          1.094538  2        1.022840\nOCCUPATION    3.020968  5        1.116901\nSECTOR        1.444859  2        1.096368\nMARR          1.111966  1        1.054498\n```\n:::\n:::\n\n\nEducation, age and experience all have extremely high VIF measures. This is not surprising. \n\nThe simplest remedy for the problem is to remove one of these correlated predictors from the model. Since experience has the highest VIF, let's remove EXPERIENCE. This can be done by adding `-EXPERIENCE` to the model. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwage.log.fit2 <- lm(log(WAGE) ~ . - EXPERIENCE, data=wages)\nsummary(wage.log.fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = log(WAGE) ~ . - EXPERIENCE, data = wages)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36094 -0.28111  0.00376  0.27757  1.79529 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  1.139876   0.186526   6.111 1.95e-09 ***\nEDUCATION    0.057206   0.009541   5.996 3.80e-09 ***\nSOUTH1      -0.093477   0.041934  -2.229 0.026233 *  \nSEX1        -0.216721   0.041846  -5.179 3.20e-07 ***\nUNION1       0.211528   0.051224   4.129 4.24e-05 ***\nAGE          0.009347   0.001724   5.421 9.10e-08 ***\nRACE2       -0.034072   0.099060  -0.344 0.731019    \nRACE3        0.079864   0.057397   1.391 0.164692    \nOCCUPATION2 -0.364504   0.091508  -3.983 7.77e-05 ***\nOCCUPATION3 -0.210472   0.076181  -2.763 0.005935 ** \nOCCUPATION4 -0.384026   0.080997  -4.741 2.75e-06 ***\nOCCUPATION5 -0.050349   0.072726  -0.692 0.489054    \nOCCUPATION6 -0.265467   0.079977  -3.319 0.000966 ***\nSECTOR1      0.114832   0.054868   2.093 0.036846 *  \nSECTOR2      0.093247   0.096523   0.966 0.334462    \nMARR1        0.062131   0.041037   1.514 0.130629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 518 degrees of freedom\nMultiple R-squared:  0.3613,\tAdjusted R-squared:  0.3428 \nF-statistic: 19.53 on 15 and 518 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nNotice that now age and education are statistically significant. Also the $R^2$ is still about the same - it's gone down from .362 to .361. Plus now we can be much more confident in the coefficient estimates.\n\nLet's check the VIF values to see if that problem has been fixed\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(wage.log.fit2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n               GVIF Df GVIF^(1/(2*Df))\nEDUCATION  1.813186  1        1.346546\nSOUTH      1.060840  1        1.029971\nSEX        1.268426  1        1.126244\nUNION      1.128733  1        1.062418\nAGE        1.190469  1        1.091086\nRACE       1.094327  2        1.022791\nOCCUPATION 2.999691  5        1.116112\nSECTOR     1.444471  2        1.096294\nMARR       1.109491  1        1.053324\n```\n:::\n:::\n\n\nYes. We'd really only be concerned if a single VIF value was greater than 10, or if the average was much bigger than 2.5 or so. This is nothing to be concerned with.\n\nChecking other model diagnostics - to make sure our linear regression assumptions are still met\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wage.log.fit2, which=1:2)\n```\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-63-1.png){width=672}\n:::\n\n::: {.cell-output-display}\n![](R09_Multiple_Regression_files/figure-html/unnamed-chunk-63-2.png){width=672}\n:::\n:::\n",
    "supporting": [
      "R09_Multiple_Regression_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}