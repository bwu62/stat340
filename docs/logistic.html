<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 340: Data Science II - 31&nbsp; Logistic Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./R10_LogisticReg_Extended.html" rel="next">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./prediction1.html">Prediction</a></li><li class="breadcrumb-item"><a href="./logistic.html"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Logistic Regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 340: Data Science II</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">STAT 340 Index</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Sampling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability_rv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R01_Prob_RVs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probability and Random Variable R Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rv_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probability and Random Variables Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Random Variable Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R02_Distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Variable Distributions R Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cov_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Independence and Conditional Probability Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R03_MonteCarloExamples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Monte Carlo Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mc_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Monte Carlo Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_MonteCarlo_Battleship.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Monte Carlo Battleship</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Statistical Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R04_Monte_Carlo_Testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Monte Carlo Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing1_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Monte Carlo Testing Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Statistical Testing, Continued</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R05_testing_Additional_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Monte Carlo Testing: Additional Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing2_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Testing and Power Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Estimation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Estimation Part 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation1_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Point Estimation Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R06_More_Estimation_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Estimation Examples</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Estimation Part 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation2_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Interval Estimation Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R07_More_Estimation_and_CI_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interval Estimation Examples</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Prediction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Prediction (Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./slr_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Simple Linear Regression Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R08_prediction_examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Simple Linear Regression - Examples</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Prediction (Multiple Linear Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R09_Multiple_Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">R10 multiple regression further concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlr_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Multiple Linear Regression Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R10_LogisticReg_Extended.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Logistic Regression - Extended Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Logistic Regression Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Model Selection and Cross Validation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_cv-MSEcomparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">cv-extra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Bias_Variance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Model_Selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">R11 Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Ridge_LASSO.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">R11: Ridge and Lasso</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cv_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Cross Validation Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Bootstrapping</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bootstrapping</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R12_Bootstrap_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">R13_Bootstrap</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bootstrap_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Bootstrap Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RV_summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Random Variable Summary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">31.1</span> Learning objectives</a></li>
  <li><a href="#logistic-regression-motivation" id="toc-logistic-regression-motivation" class="nav-link" data-scroll-target="#logistic-regression-motivation"><span class="header-section-number">31.2</span> Logistic regression: motivation</a></li>
  <li><a href="#formulating-a-model" id="toc-formulating-a-model" class="nav-link" data-scroll-target="#formulating-a-model"><span class="header-section-number">31.3</span> Formulating a model</a>
  <ul class="collapse">
  <li><a href="#example-linear-regression-on-the-pima-data-set" id="toc-example-linear-regression-on-the-pima-data-set" class="nav-link" data-scroll-target="#example-linear-regression-on-the-pima-data-set"><span class="header-section-number">31.3.1</span> Example: linear regression on the <code>Pima</code> data set</a></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">31.3.2</span> Logistic regression</a></li>
  </ul></li>
  <li><a href="#logistic-regression-in-r" id="toc-logistic-regression-in-r" class="nav-link" data-scroll-target="#logistic-regression-in-r"><span class="header-section-number">31.4</span> Logistic regression in R</a></li>
  <li><a href="#interpreting-model-coefficients" id="toc-interpreting-model-coefficients" class="nav-link" data-scroll-target="#interpreting-model-coefficients"><span class="header-section-number">31.5</span> Interpreting model coefficients</a></li>
  <li><a href="#fitting-the-model" id="toc-fitting-the-model" class="nav-link" data-scroll-target="#fitting-the-model"><span class="header-section-number">31.6</span> Fitting the model</a>
  <ul class="collapse">
  <li><a href="#a-bit-of-philosophy-maximum-likelihood" id="toc-a-bit-of-philosophy-maximum-likelihood" class="nav-link" data-scroll-target="#a-bit-of-philosophy-maximum-likelihood"><span class="header-section-number">31.6.1</span> A bit of philosophy: maximum likelihood</a></li>
  <li><a href="#example-mean-of-a-normal" id="toc-example-mean-of-a-normal" class="nav-link" data-scroll-target="#example-mean-of-a-normal"><span class="header-section-number">31.6.2</span> Example: mean of a normal</a></li>
  </ul></li>
  <li><a href="#prediction-versus-feature-selection-the-two-cultures" id="toc-prediction-versus-feature-selection-the-two-cultures" class="nav-link" data-scroll-target="#prediction-versus-feature-selection-the-two-cultures"><span class="header-section-number">31.7</span> Prediction versus feature selection: the two cultures</a></li>
  <li><a href="#review" id="toc-review" class="nav-link" data-scroll-target="#review"><span class="header-section-number">31.8</span> Review</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Logistic Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>These notes concern the problem of <em>logistic regression</em>. This is a regression method that allows us to handle the situation where we have predictors and responses, but the predictors are binary rather than continuous. Fitting a linear regression model in this situation would be a bit silly, for reasons illustrated below. Logistic regression lets us keep all the “nice” things about linear regression (i.e., that we like linear functions), while being better suited to this different data setting.</p>
<section id="learning-objectives" class="level2" data-number="31.1">
<h2 data-number="31.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">31.1</span> Learning objectives</h2>
<p>After this lecture, you will be able to</p>
<ul>
<li>Explain the motivation for logistic regression</li>
<li>Fit a logistic regression model to data with one or more predictors and a binary response</li>
<li>Interpret estimated coefficients in a logistic regression model</li>
</ul>
</section>
<section id="logistic-regression-motivation" class="level2" data-number="31.2">
<h2 data-number="31.2" class="anchored" data-anchor-id="logistic-regression-motivation"><span class="header-section-number">31.2</span> Logistic regression: motivation</h2>
<p>Consider the following data, which you’ll play with more in discussion section: the Pima Indian data set.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: package 'MASS' was built under R version 4.2.3</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Pima.te)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  npreg glu bp skin  bmi   ped age type
1     6 148 72   35 33.6 0.627  50  Yes
2     1  85 66   29 26.6 0.351  31   No
3     1  89 66   23 28.1 0.167  21   No
4     3  78 50   32 31.0 0.248  26  Yes
5     2 197 70   45 30.5 0.158  53  Yes
6     5 166 72   19 25.8 0.587  51  Yes</code></pre>
</div>
</div>
<p>This data was collection by the US National Institute of Diabetes and Digestive and Kidney Diseases. Each row of this data set corresponds to a woman of Pima Indian descent living near Phoenix Arizona. The columns include a number of biomarkers (e.g., glucose levels, blood pressure and age), as well as a column indicating whether or not each subject had diabetes (as measured according to measures laid out by the World Health Organization). See <code>?Pima.te</code> for more information.</p>
<p>Can we predict whether or not a given person has diabetes based on their biomarkers? It is natural to cast this as a prediction problem just like the linear regression problems we have discussed in recent weeks: we are given pairs <span class="math inline">\((X_i, Y_i)\)</span>, where <span class="math inline">\(X_i\)</span> is a vector of predictors and <span class="math inline">\(Y_i\)</span> is a response. The difference is that now, <span class="math inline">\(Y_i\)</span> is more naturally thought of as a binary label (in this case, indicating whether or not a subject has diabetes), instead of a continuous number.</p>
<p><strong>Example: image classification</strong> Suppose that we have a collection of images, and our goal is to detect whether or not a given image has a cat in it. Our data takes the form of a collection of pairs <span class="math inline">\((X_i, Y_i)\)</span>, <span class="math inline">\(i=1,2,\dots,n\)</span> where <span class="math inline">\(X_i\)</span> is an image (e.g., a bunch of a pixels) and <span class="math display">\[
Y_i = \begin{cases} 1 &amp;\mbox{ if image } i \text{ contains a cat }\\
                    0 &amp;\mbox{ if image } i \text{ does not contain a cat. }
                    \end{cases}
\]</span> <strong>Example: fraud detection</strong> An online banking or credit card service might like to be able to detect whether or not a given transaction is fraudulent. Predictors <span class="math inline">\(X_i\)</span> might take the form of things like transaction amount, location and time, and the binary label <span class="math inline">\(Y_i\)</span> corresponds to whether or not the transaction is fraudulent.</p>
<p><strong>Example: brain imaging</strong> In own work in neuroscience, a common task is to detect whether or not a subject has a disease or disorder (e.g., Parkinson’s or schizophrenia) based on brain imaging data obtained via, for example, functional magnetic resonance imaging (fMRI). Our labels <span class="math inline">\(Y_i\)</span> are given by <span class="math display">\[
Y_i = \begin{cases} 1 &amp;\mbox{ if subject } i \text{ has the disease/disorder }\\
                    0 &amp;\mbox{ if subject } i \text{ does not have the disease/disorder. }
                    \end{cases}
\]</span> Our predictors <span class="math inline">\(X_i\)</span> for subject <span class="math inline">\(i \in \{1,2,\dots,n\}\)</span> might consist of a collection of features derived from the brain imagining (e.g., average pixel values or correlations between different locations in the brain). In my own research, <span class="math inline">\(X_i\)</span> is a network that describes brain structure, constructed from the imaging data. In other studies, <span class="math inline">\(X_i\)</span> might even be the brain imaging data itself. Our goal is to predict <span class="math inline">\(Y_i\)</span> based on the observed imaging data (or image-derived features) <span class="math inline">\(X_i\)</span>.</p>
</section>
<section id="formulating-a-model" class="level2" data-number="31.3">
<h2 data-number="31.3" class="anchored" data-anchor-id="formulating-a-model"><span class="header-section-number">31.3</span> Formulating a model</h2>
<p>A natural approach to this problem would be to just take our existing knowledge of linear regression and apply it here. If our predictor <span class="math inline">\(X_i\)</span> has <span class="math inline">\(p\)</span> dimensions, <span class="math display">\[
X_i = (X_{i,1}, X_{i,2}, X_{i,3}, \dots, X_{i,p} )^T \in \mathbb{R}^p,
\]</span> then we might try to fit a model of the form <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_{i,1} + \beta_2 X_{i,2} + \cdots + \beta_p X_{i,p}
\]</span></p>
<p>The obvious problem with this is that the right-hand side of this equation is an arbitrary real number, whereas the left-hand side (i.e., <span class="math inline">\(Y_i\)</span>) is <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>.</p>
<section id="example-linear-regression-on-the-pima-data-set" class="level3" data-number="31.3.1">
<h3 data-number="31.3.1" class="anchored" data-anchor-id="example-linear-regression-on-the-pima-data-set"><span class="header-section-number">31.3.1</span> Example: linear regression on the <code>Pima</code> data set</h3>
<p>For easier coding later, let’s add a column to the <code>Pima.te</code> data set. The <code>type</code> column is <code>Yes</code> or <code>No</code> according to whether or not each subject has diabetes. Let’s add a column with a more straightforward name and which directly encodes this diabetes status as <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>Pima.te<span class="sc">$</span>diabetes <span class="ot">&lt;-</span> <span class="fu">ifelse</span>( Pima.te<span class="sc">$</span>type<span class="sc">==</span><span class="st">'Yes'</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(Pima.te)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  npreg glu bp skin  bmi   ped age type diabetes
1     6 148 72   35 33.6 0.627  50  Yes        1
2     1  85 66   29 26.6 0.351  31   No        0
3     1  89 66   23 28.1 0.167  21   No        0
4     3  78 50   32 31.0 0.248  26  Yes        1
5     2 197 70   45 30.5 0.158  53  Yes        1
6     5 166 72   19 25.8 0.587  51  Yes        1</code></pre>
</div>
</div>
<p>Now, let’s pick a predictor and fit a simple linear regression model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>pima_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>( diabetes <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> glu, <span class="at">data=</span>Pima.te );</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pima_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = diabetes ~ 1 + glu, data = Pima.te)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.9516 -0.2701 -0.1138  0.2408  1.0025 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.6278036  0.0892443  -7.035 1.16e-11 ***
glu          0.0080171  0.0007251  11.057  &lt; 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.4023 on 330 degrees of freedom
Multiple R-squared:  0.2703,    Adjusted R-squared:  0.2681 
F-statistic: 122.3 on 1 and 330 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>Looks like we managed to capture some signal there, at least according to the F-statistic! Let’s look at a plot.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(<span class="at">data=</span>Pima.te, <span class="fu">aes</span>(<span class="at">x=</span>glu, <span class="at">y=</span>diabetes)) <span class="sc">+</span> <span class="fu">geom_point</span>()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">'lm'</span>, <span class="at">se=</span><span class="cn">FALSE</span>, <span class="at">formula=</span><span class="st">'y ~ 1 + x'</span>)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="logistic_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Hmm… so our fitted model takes in a number (i.e., <code>glu</code>) and outputs a number that is our prediction for <span class="math inline">\(Y_i\)</span>. The trouble is that our responses are binary, but our outputs can be anything. If our goal is to predict <span class="math inline">\(y\)</span>, and we predict <span class="math inline">\(y\)</span> to be <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 x\)</span>, then we can (potentially) predict arbitrary large or small values for <span class="math inline">\(y\)</span>, if <span class="math inline">\(x\)</span> is suitably large or small. That’s a bit of an awkward fact– after all, we want our prediction to be <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. This isn’t the end of the world. We could do something like “clipping” our output <span class="math inline">\(\hat{\beta}_0 + \hat{\beta}_1 x\)</span> to whichever of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> is closest or something like that. But this feels clumsy, at best. Is there a more appropriate approach?</p>
<p>Well, there are many, but an especially simple one is <em>logistic regression</em>, which seeks to keep the “linear combination of predictors” idea that we like so much from linear regression, but modify how we make predictions to be better-suited to the “binary response” setting.</p>
</section>
<section id="logistic-regression" class="level3" data-number="31.3.2">
<h3 data-number="31.3.2" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">31.3.2</span> Logistic regression</h3>
<p>The core problem with applying linear regression to a binary response is that the output of linear regression can be any number, while we are really only interested in outputs in <span class="math inline">\(\{0,1\}\)</span>. How might we transform our <em>linear</em> prediction, <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span>, into something more sensible?</p>
<p>Logistic regression solves this problem by taking our linear prediction <span class="math inline">\(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\)</span> and turning it into a <em>probability</em>, i.e., a number between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>. We do that using the <em>logistic function</em>, <span class="math display">\[
\sigma( z ) = \frac{ 1 }{1 + e^{-z}} = \frac{ e^{z} }{ 1 + e^{z} }.
\]</span></p>
<p>Here is a plot of the function:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>z<span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">4</span>,<span class="dv">4</span>,<span class="fl">0.01</span>);</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( z, <span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="fu">exp</span>(<span class="sc">-</span>z)), <span class="at">col=</span><span class="st">'blue'</span> );</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>); <span class="fu">abline</span>(<span class="at">h=</span><span class="dv">1</span>);</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="logistic_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The logistic function is an example of a <a href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid function</a> (fancy Greek for “S-shaped”).</p>
<p>Using the logistic function, we can modify the output of our model by passing our linear regression model’s prediction, which is a real number, as the input of the logistic function so that it outputs a number between zero and one. <span class="math display">\[
\sigma\left( \hat{y} \right)
=
\sigma\left( \hat{\beta}_0 + \hat{\beta}_1 x \right)
=
\frac{ 1 }{1 + \exp\{ -(\hat{\beta}_0 + \hat{\beta}_1 x) \} }
=
\frac{ \exp\{ \hat{\beta}_0 + \hat{\beta}_1 x \} }{1 + \exp\{ \hat{\beta}_0 + \hat{\beta}_1 x \} }
\]</span></p>
<p>The especially nice thing about this is that since this number is between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, we can interpret this as a probability: <span class="math display">\[
\Pr[ Y_i = 1 ; X_i=x, \beta_0, \beta_1 ]
=
\sigma\left( \beta_0 + \beta_1 x \right)
=
\frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \} }
\]</span></p>
<p>The semi-colon notation there is just to stress that we are treating <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> as parameters and the data <span class="math inline">\(x\)</span> as an <em>input</em>, but <em>not</em> as things that we are conditioning on.</p>
<p><strong>Aside:</strong> there is a whole branch of statistics called <em>Bayesian statistics</em> that seeks to treat parameters like our coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> as variables that we can condition on. It’s a very cool area, especially useful in machine learning, but it will have to wait for later courses!</p>
<p>Now, when it comes time to make a prediction on input <span class="math inline">\(x\)</span>, we can pass <span class="math inline">\(\beta_0 + \beta_1 x\)</span> into the sigmoid function, and predict <span class="math display">\[
\hat{y} = \begin{cases} 1 &amp;\mbox{ if } \sigma( \beta_0 + \beta_1 x ) &gt; \frac{1}{2} \\
                        0 &amp;\mbox{ if } \sigma( \beta_0 + \beta_1 x ) \le \frac{1}{2}. \end{cases}
\]</span></p>
<p>In the multivariate setting, where our <span class="math inline">\(i\)</span>-th predictor <span class="math inline">\(X_i\)</span> takes the form <span class="math display">\[
X_i = \left( X_{i,1}, X_{i,2}, \dots, X_{i,p} \right),
\]</span></p>
<p><em>multiple</em> logistic regression is the straight-forward extension of this idea. Given (estimated) coefficients <span class="math inline">\(\beta_0, \beta_1, \beta_2, \dots, \beta_p\)</span>, we output a probability <span class="math display">\[
\begin{aligned}
\Pr[ Y_i = 1 &amp;; X_i=(x_1,x_2,\dots,x_p), \beta_0, \beta_1, \dots, \beta_p ] \\
&amp;=
\sigma\left( \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p \right) \\
&amp;=
\frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_p x_p) \} }
\end{aligned}
\]</span></p>
</section>
</section>
<section id="logistic-regression-in-r" class="level2" data-number="31.4">
<h2 data-number="31.4" class="anchored" data-anchor-id="logistic-regression-in-r"><span class="header-section-number">31.4</span> Logistic regression in R</h2>
<p>Let’s ignore, for now, the question of <em>how</em> we fit this model (spoiler alert: least squares isn’t going to make a lot of sense anymore; we’ll need something more clever), let’s ask R to fit a logistic regression model to our problem of predicting diabetes from the <code>glu</code> variable in the Pima data set.</p>
<p>The function that we use in R is <code>glm</code>. “GLM” stands for <em>generalized</em> linear model. That is, we are generalizing linear regression. In particular, we are generalizing linear regression by doing linear regression, but then passing the linear regression prediction <span class="math inline">\(\beta_0 + \beta_1 x\)</span> through another function.</p>
<p>To perform <em>logistic</em> regression, we need to specify to R that we are using a “binomial” family of responses– our response <span class="math inline">\(Y\)</span> is binary, so it can be thought of as a Binomial random variable, with <span class="math inline">\(\Pr[ Y = 1] = \sigma( \beta_0 + \beta_1 X)\)</span>.</p>
<p>Other than that, fitting a model with <code>glm</code> is basically the same as using plain old <code>lm</code>. Even the model summary output looks about the same:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>pima_logistic <span class="ot">&lt;-</span> <span class="fu">glm</span>( diabetes <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> glu, <span class="at">data=</span>Pima.te, <span class="at">family=</span>binomial );</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(pima_logistic)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = diabetes ~ 1 + glu, family = binomial, data = Pima.te)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.2343  -0.7270  -0.4985   0.6663   2.3268  

Coefficients:
             Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept) -5.946808   0.659839  -9.013   &lt;2e-16 ***
glu          0.042421   0.005165   8.213   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 420.30  on 331  degrees of freedom
Residual deviance: 325.99  on 330  degrees of freedom
AIC: 329.99

Number of Fisher Scoring iterations: 4</code></pre>
</div>
</div>
<p>But notice now that our model’s outputs are always between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>( Pima.te, <span class="fu">aes</span>(<span class="at">x=</span>glu, <span class="at">y=</span>diabetes) ) <span class="sc">+</span> <span class="fu">geom_point</span>();</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">formula=</span><span class="st">'y ~ 1+x'</span>, <span class="at">se=</span><span class="cn">FALSE</span>,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">method=</span><span class="st">'glm'</span>,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>                       <span class="at">method.args=</span><span class="fu">list</span>(<span class="at">family =</span> <span class="st">"binomial"</span>) );</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that geom_smooth needs an extra list() of arguments to specify the</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="co"># extra arguments that we passed to the glm() function above.</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="co"># In particular, we need to tell the GLM to use a binomial response.</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="logistic_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>If we interpret our model’s output as a probability, namely the probability that we observe label <span class="math inline">\(Y\)</span> for predictors <span class="math inline">\(X\)</span>, then it is clear that as <code>glu</code> increases, our model thinks it is more likely that a person has diabetes.</p>
<p>Just as with linear regression, our logistic model object will make predictions for new, previously unseen predictors if we use the <code>predict</code> function.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Reminder: we pass a data frame into the predict function, and our model</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="co"># will produce an output for ease row of that data frame.</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># So this is making a prediction for a subject with `glu=200`.</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Looking at our scatter plot above, it's clear that `glu=200` is at the</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co"># far upper end of our data distribution, so we should expect our model to</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co"># produce an output close to 1.</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(pima_logistic, <span class="at">type=</span><span class="st">'response'</span>, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">glu=</span><span class="dv">200</span>) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1 
0.9267217 </code></pre>
</div>
</div>
<p>Generally speaking, as <code>glu</code> increases, our model is more confident that the subject has diabetes.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>glu_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">50</span>,<span class="dv">200</span>,<span class="dv">10</span>);</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>logistic_outputs <span class="ot">&lt;-</span> <span class="fu">predict</span>(pima_logistic, <span class="at">type=</span><span class="st">'response'</span>,</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>                            <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">glu=</span>glu_vals) )</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>( glu_vals, logistic_outputs)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="logistic_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Again, these are our model’s predicted outputs for given input values for glucose levels. As <code>glu</code> increases, our model assigns higher and higher probabilities to the possibility that a subject has diabetes.</p>
<p>You’ll play around with logistic regression and the Pima data set more in discussion section, where you’ll find that if we choose our predictors more carefully (and use more predictors), we can build a pretty good model!</p>
</section>
<section id="interpreting-model-coefficients" class="level2" data-number="31.5">
<h2 data-number="31.5" class="anchored" data-anchor-id="interpreting-model-coefficients"><span class="header-section-number">31.5</span> Interpreting model coefficients</h2>
<p>In linear regression, our model coefficients had a simple interpretation: <span class="math inline">\(\beta_1\)</span> was the change in response associated with a unit change in the corresponding predictor. Is there an analogous interpretation for logistic regression?</p>
<p>Well, there is, but it requires a bit of extra work to define some terms so that we can talk sensibly about <em>what</em> changes in response to a change in a predictor.</p>
<p>The <em>odds</em> of an event <span class="math inline">\(E\)</span> with probability <span class="math inline">\(p\)</span> are given by <span class="math display">\[
\operatorname{Odds}(E) = \frac{ p }{ 1-p }.
\]</span></p>
<p>That is, the odds of an event is the ratio of the probability of the event happening to the probability that it doesn’t happen. So, for example, if we have an event that occurs with probability <span class="math inline">\(1/2\)</span>, the odds of that event are <span class="math inline">\((1/2)/(1/2) = 1\)</span>, which we usually say as “one to one odds”. Some of this kind of vocabulary may be familiar to you from sports betting, if you’ve been to Las Vegas or watched the Kentucky Derby.</p>
<p>So, let’s suppose that we have a logistic regression model with a single predictor, that takes predictor <span class="math inline">\(x\)</span> and outputs a probability <span class="math display">\[
p(x) = \frac{ 1 }{ 1 + \exp\{ -(\beta_0 + \beta_1 x) \} }.
\]</span> and note that <span class="math display">\[
1-p(x) = \frac{ \exp\{ -(\beta_0 + \beta_1 x) \} }
{1 + \exp\{ -(\beta_0 + \beta_1 x) \}}
\]</span> The <em>odds</em> associated with this probability are <span class="math display">\[
\operatorname{Odds}(x)
= \frac{ p(x) }{1-p(x)}
= \frac{1 }{ \exp\{ -(\beta_0 + \beta_1 x) \} }
= \exp\{ \beta_0 + \beta_1 x \}.
\]</span></p>
<p>Since the odds can get very large or very small, especially once we start working with small probabilities, it is often much easier to work with the <em>logarithm</em> of the odds, usually called the “log-odds” for short. This quantity is especially important in statistics associated with biology (e.g., drug trials and studies of risks associated with diseases). If we look at the <em>log</em> odds associated with our logistic regression model, <span class="math display">\[
\operatorname{Log-Odds}(x)
= \log \operatorname{Odds}(x)
= \log \exp\{ \beta_0 + \beta_1 x \}
= \beta_0 + \beta_1 x.
\]</span></p>
<p>In other words, under our logistic regression model, the “slope” <span class="math inline">\(\beta_1\)</span> is the increase in the log-odds of the response being <span class="math inline">\(1\)</span> associated with a unit increase in <span class="math inline">\(x\)</span>.</p>
<p>Said yet another way, logistic regression is what happens if we use a linear regression model to predict the log-odds of the event that the response is <span class="math inline">\(1\)</span>, i.e., the log-odds of the event <span class="math inline">\(Y_i = 1\)</span>.</p>
</section>
<section id="fitting-the-model" class="level2" data-number="31.6">
<h2 data-number="31.6" class="anchored" data-anchor-id="fitting-the-model"><span class="header-section-number">31.6</span> Fitting the model</h2>
<p>We said previously that we were going to ignore, for the time being, the matter of <em>how</em> we fit out logistic regression model. Well, let’s come back to that question.</p>
<p>Let’s start by recalling <em>ordinary least squares</em> regression, where we had the following <em>loss function</em>:</p>
<p><span class="math display">\[
\ell( \beta_0 ,\beta_1 )
= \sum_{i=1}^n \left(y_i - (\beta_0 + \beta_1 x_i) \right)^2
= \sum_{i=1}^n (\hat{y}_i - y_i)^2,
\]</span></p>
<p>where <span class="math inline">\(\hat{y_i}=\beta_0+\beta_1 x_i\)</span> is the predicted <span class="math inline">\(y_i\)</span> based on the coefficients <span class="math inline">\(\beta_0,\beta_1\)</span>. This function measures how “wrong” our model is if we use coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, and we choose these coefficients in such a way as to make thie loss function as small as possible. We used the <em>square</em> of the loss because it makes a lot of the math easier.</p>
<p>Now that we’re working with logistic regression instead of linear, how do we decide what makes a good choice of coefficient? We <em>could</em> try and force a way to use least squares, but things would get a bit complicated.</p>
<p>Let’s try something different. Our logistic regression model takes a predictor and outputs a probability that the response is <span class="math inline">\(1\)</span>. That is, for our <span class="math inline">\(i\)</span>-th predictor, our model outputs (we’re sticking with the case of one predictor for simplicity, but the idea extends to multiple predictors in the natural way): <span class="math display">\[
\Pr[ Y_i=1; X_i=x, \beta_0, \beta_1 ]
= \frac{1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}}.
\]</span></p>
<p>In other words, this is the probability that our model gives to the event that the <span class="math inline">\(i\)</span>-th response is <span class="math inline">\(1\)</span>. On the other hand, our model assigns probability to the event that the <span class="math inline">\(i\)</span>-th response is <span class="math inline">\(0\)</span> given by <span class="math display">\[
\begin{aligned}
\Pr[ Y_i=0; X_i=x, \beta_0, \beta_1 ]
&amp;=
1-\Pr[ Y_i=1; X_i=x, \beta_0, \beta_1 ] \\
&amp;= \frac{ \exp\{ -(\beta_0 + \beta_1 x) \} }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \\
&amp;= \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}}
.
\end{aligned}
\]</span></p>
<p>Now, if the <span class="math inline">\(i\)</span>-th response really is <span class="math inline">\(0\)</span>, then we want this number to be big. And if the <span class="math inline">\(i\)</span>-th response really is <span class="math inline">\(1\)</span>, we want <span class="math inline">\(1/(1+\exp\{ -(\beta_0 + \beta_1 x )\})\)</span> to be big.</p>
<p>Further, we want this pattern to hold for all of our data. That is, we want our model to give a “high probability” to all of our data. If our <span class="math inline">\(i\)</span>-th observation takes the form <span class="math inline">\((x_i,y_i)\)</span>, then the probability of our data under the model, assuming the observations are independent, is <span class="math display">\[
\prod_{i=1}^n \Pr[ Y_i=y_i; X_i=x_i, \beta_0, \beta_1 ].
\]</span></p>
<p>For one of our data points, using the fact that <span class="math inline">\(y_i\)</span> is either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, we can write <span class="math display">\[
\Pr[ Y_i=y_i; X_i=x_i, \beta_0, \beta_1 ]
=
\left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i}
\left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i}.
\]</span> So the probability of our whole data set is <span class="math display">\[
\prod_{i=1}^n
\left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i}
\left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i}.
\]</span> So this is the probability of all of our data under our model, for a particular choice of parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. We call this the <em>likelihood</em> of the data. One way to pick our model coefficients is to choose them so that this quantity is large– if this model is large, that means our data agrees with the model.</p>
<p>So we want to choose <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to make this probability, the likelihood, large. How do we do that? Well, our likelihood is a product of a bunch of things, and products are hard to work with. Let’s take a logarithm. Remember, logs turn products into sums, and sums are easy to work with. <span class="math display">\[
\begin{aligned}
\log
&amp; \prod_{i=1}^n
\left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i}
\left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i} \\
&amp;= \sum_{i=1}^n \log \left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i} \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i}
\end{aligned}
\]</span></p>
<p>Now, let’s notice that an individual term in this sum is a log of a product, and that <span class="math inline">\(\log a^b = b \log a\)</span>, so that: <span class="math display">\[
\begin{aligned}
\log &amp;\left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i} \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i} \\
&amp;= \log \left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i}
+ \log \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i} \\
&amp;= y_i \log \left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)
+ (1-y_i) \log \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)
\end{aligned}
\]</span></p>
<p>So the <em>logarithm</em> of our likelihood is <span class="math display">\[
\begin{aligned}
\log
&amp; \prod_{i=1}^n
\left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)^{y_i}
\left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)^{1-y_i} \\
&amp;= \sum_{i=1}^n
y_i \log \left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)
+ (1-y_i) \log \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right).
\end{aligned}
\]</span> Notice that since the logarithm function is <em>monotone</em>, making the likelihood large is the same as making the likelihood large and vice versa.</p>
<p>Logistic regression chooses the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to make this log-likelihood large. Equivalently, we <em>minimize</em> the <em>negative</em> log likelihood, <span class="math display">\[
\ell(\beta_0, \beta_1)
=
-\sum_{i=1}^n \left[
y_i \log \left( \frac{ 1 }{1 + \exp\{ -(\beta_0 + \beta_1 x) \}} \right)
+ (1-y_i) \log \left( \frac{ 1 }{1 + \exp\{ \beta_0 + \beta_1 x \}} \right)
\right].
\]</span></p>
<p>That is, while our <em>loss function</em> for linear regression is the sum of squared errors, our loss function for logistic regression is the <em>negative log likelihood</em>.</p>
<p>Unlike in the least squares case, these optimized coefficients do not have nice closed form solutions. Still, when we conduct logistic regression in R, this minimization problem is solved for us using tools from optimization.</p>
<section id="a-bit-of-philosophy-maximum-likelihood" class="level3" data-number="31.6.1">
<h3 data-number="31.6.1" class="anchored" data-anchor-id="a-bit-of-philosophy-maximum-likelihood"><span class="header-section-number">31.6.1</span> A bit of philosophy: maximum likelihood</h3>
<p>It turns out that this idea of choosing our parameters to maximize the probability of the data is so popular in statistics that it has a name: <em>maximum likelihood estimation</em>. When we need to estimate the value of a parameter, we choose the one that maximizes the likelihood of the data.</p>
<p>Interestingly, in many situations, the <em>least squares</em> estimate and the <em>maximum likelihood</em> estimate of a parameter are, in fact, the same. In many situations, the sample mean <em>is</em> the least squares estimate of our parameter, and it is <em>also</em> the estimate that we would obtain if we maximize the likelihood. Examples of this kind of situation include: linear regression, estimating the mean of a normal, and estimating the rate parameter of the Poisson.</p>
<p>You’ll see many more connections between these ideas, and establish some of the interesting properties of maximum likelihood estimation in your more advanced statistics classes. For now, let’s very quickly establish that the sample mean of the normal is <em>both</em> the least squares estimator <em>and</em> the maximum likelihood estimator.</p>
</section>
<section id="example-mean-of-a-normal" class="level3" data-number="31.6.2">
<h3 data-number="31.6.2" class="anchored" data-anchor-id="example-mean-of-a-normal"><span class="header-section-number">31.6.2</span> Example: mean of a normal</h3>
<p>Consider the setting in which we observe data <span class="math inline">\(X_1,X_2,\dots,X_n\)</span> drawn independently and identically distributed according to a normal distribution with <em>unknown</em> mean <span class="math inline">\(\mu\)</span> and <em>known</em> variance <span class="math inline">\(\sigma^2\)</span> (assuming the variance is known is just for the sake of making some things simpler– a similar story is true if we have to estimate the variance, as well).</p>
<p>So let’s suppose that we observe <span class="math inline">\(X_1=x_1, X_2=x_2, \dots, X_n=x_n\)</span>. Let’s consider two different ways to estimate the mean <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Least squares estimation.</strong> In least squares, we want to choose the number that minimizes the sum of squares between our estimate and the data: <span class="math display">\[
\min_{m} \sum_{i=1}^n ( x_i - m )^2.
\]</span></p>
<p>Let’s dust off our calculus and solve this. We’ll take the derivative and set it equal to zero. First, let’s find the derivative: <span class="math display">\[
\frac{d}{d m} \sum_{i=1}^n ( x_i - m )^2
= \sum_{i=1}^n \frac{d}{d m} ( x_i - m )^2
= 2 \sum_{i=1}^n (x_i - m),
\]</span></p>
<p>where we used the fact that the derivative is linear (so the derivative of a sum is the sum of the derivatives) to get the first equality.</p>
<p>Now, let’s set that derivative equal to zero and solve for <span class="math inline">\(m\)</span>. <span class="math display">\[
\begin{aligned}
0 &amp;= 2 \sum_{i=1}^n (x_i - m) \\
0 &amp;= \sum_{i=1}^n x_i - nm \\
n m &amp;= \sum_{i=1}^n x_i \\
m &amp;= \frac{1}{n} \sum_{i=1}^n x_i
\end{aligned}
\]</span></p>
<p>So the least squares estimate of <span class="math inline">\(\mu\)</span> is just the sample mean, <span class="math display">\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n x_i.
\]</span></p>
<p><strong>Maximum likelihood estimation.</strong> Okay, now let’s try a maximum likelihood approach. Remember, maximum likelihood says that we should choose our estimate to be the number that makes our data as “likely” as possible– that is, the number that makes the probability of the data large.</p>
<p>Under the normal, the “probability” (it’s actually a density, not a probability, but that’s okay) of our data is <span class="math display">\[
\begin{aligned}
\Pr[ X_1=x_1, X_2=x_2, \dots, X_n=x_n; \mu, \sigma^2 ]
&amp;= \prod_{i=1}^n \Pr[ X_i = x_i; \mu, \sigma^2 ] \\
&amp;= \prod_{i=1}^n \frac{1}{\sqrt{ 2 \pi \sigma^2 } }
  \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\},
\end{aligned}
\]</span> where we used the fact that our data is independent to write the “probability” as a product.</p>
<p>Okay, now how are we going to choose <span class="math inline">\(\mu\)</span> to minimize this? Well, we have a product of things, and products are annoying. Let’s take the log of this, instead, to make it a sum: <span class="math display">\[
\begin{aligned}
\log \Pr[ X_1=x_1, X_2=x_2, \dots, X_n=x_n; \mu, \sigma^2 ]
&amp;= \log \prod_{i=1}^n \frac{1}{\sqrt{ 2 \pi \sigma^2 } }
  \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\} \\
&amp;= \sum_{i=1}^n \log  \frac{1}{\sqrt{ 2 \pi \sigma^2 } }
            \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\} \\
&amp;= \sum_{i=1}^n \log  \frac{1}{\sqrt{ 2 \pi \sigma^2 } }
  + \sum_{i=1}^n \log \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\},
\end{aligned}
\]</span></p>
<p>where we made repeated use of the fact that the logarithm of a product is the sum of the logarithms.</p>
<p>Taking the log to simplify the likelihood is so common that we have a name for this quantity: the <em>log likelihood</em>. Notice that because the logarithm is a monotone function, maximizing the likelihood and maximizing the log-likelihood are equivalent. The maximizer of one will be the maximizer of the other, so there’s no problem here.</p>
<p>So we have <span class="math display">\[
\log \Pr[ X_1=x_1, X_2=x_2, \dots, X_n=x_n; \mu, \sigma^2 ]
= \sum_{i=1}^n \log  \frac{1}{\sqrt{ 2 \pi \sigma^2 } }
  + \sum_{i=1}^n \log \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\}.
\]</span></p>
<p>We’re almost there!</p>
<p>Notice that the first of these two sums <em>doesn’t depend</em> on <span class="math inline">\(\mu\)</span>. So for the purposes of finding which value of <span class="math inline">\(\mu\)</span> makes the log-likelihood large, we can ignore it, and just concentrate on maximizing <span class="math display">\[
\sum_{i=1}^n \log \exp\left\{ \frac{ -(x_i - \mu)^2}{ 2\sigma^2 } \right\}.
\]</span></p>
<p>Since <span class="math inline">\(\log \exp x = x\)</span>, this simplifies to <span class="math display">\[
\sum_{i=1}^n \frac{ -(x_i - \mu)^2}{ 2\sigma^2 }
= \frac{-1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2
\]</span></p>
<p>Now, <em>maximizing</em> <span class="math inline">\(-f(\mu)\)</span> is the same as <em>minimizing</em> <span class="math inline">\(f(\mu)\)</span>. In other words, the maximum likelihood estimate for the mean of our normal distribution is the value of <span class="math inline">\(\mu\)</span> that <em>minimizes</em> <span class="math display">\[
\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2.
\]</span></p>
<p>If we take derivatives and set the result equal to zero, we’re going to get the very same least squares problem we had before, and we know that the minimizer is obtained by taking <span class="math display">\[
\mu = \frac{1}{n} \sum_{i=1}^n x_i,
\]</span></p>
<p>the sample mean!</p>
<p>So, to recap, two different ways to measure what is a “good” estimate give us the same answer: least squares and maximum likelihood <em>both</em> lead us to estimate the mean of the normal as the sample mean!</p>
<p>This “specialness” of the sample mean is actually very common in the “nice” distributions that we see a lot in our early statistics courses, but it’s worth bearing in mind that this isn’t generally the case: in more “interesting” settings, least squares and maximum likelihood estimators can be very different! Stay tuned in your later courses to learn more.</p>
</section>
</section>
<section id="prediction-versus-feature-selection-the-two-cultures" class="level2" data-number="31.7">
<h2 data-number="31.7" class="anchored" data-anchor-id="prediction-versus-feature-selection-the-two-cultures"><span class="header-section-number">31.7</span> Prediction versus feature selection: the two cultures</h2>
<p>We have motivated most of our discussion of regression the last few weeks by talking about <em>prediction</em>. We see data <span class="math inline">\(X_i\)</span>, and our goal is to predict (i.e., guess) an associated response or label <span class="math inline">\(Y_i\)</span>. A “good” model is one that does well at predicting these labels or responses.</p>
<p><strong>Example: image recognition</strong> Consider once again the problem of detecting whether or not an image contains a cat. We observe <span class="math inline">\((X_i, Y_i)\)</span> pairs in which each image <span class="math inline">\(X_i\)</span> has an associated label <span class="math display">\[
Y_i = \begin{cases} 1 &amp;\mbox{ if image } i \text{ contains a cat }\\
                    0 &amp;\mbox{ if image } i \text{ does not contain a cat. }
                    \end{cases}
\]</span></p>
<p>Our goal is then to build a model that takes in an image <span class="math inline">\(x\)</span> and produces a prediction (i.e., a best guess) <span class="math inline">\(\hat{y}\)</span> based on <span class="math inline">\(X\)</span> as to the true label <span class="math inline">\(y\)</span>. A good model is one that correctly guesses <span class="math inline">\(y\)</span> most of the time. Something like <span class="math display">\[
\Pr[ \hat{y} = y ] \text{ is close to } 1
\]</span></p>
<p><strong>Example: housing prices</strong> Consider once again the problem of predicting how much a house will sell for. We get predictors in the form of a vector <span class="math inline">\(x\)</span> containing the house’s age, square footage, proximity to parks, quality of the local school system, and so on, and our goal is to predict the price <span class="math inline">\(y\)</span> of that house. A “good” model is one whose prediction <span class="math inline">\(\hat{y}\)</span> is “close” to the true value <span class="math inline">\(y\)</span> “on average”. Something like <span class="math display">\[
\mathbb{E} \left( \hat{y} - y \right)^2 \text{ is small. }
\]</span></p>
<p>Now, if our goal is <em>only</em> to detect whether or not a picture contains a cat, or <em>only</em> to predict how much a house will sell for, then that’s kind of the end of the story.</p>
<p>But suppose that we have a really good predictor for housing price, and someone asks us “what is it that makes a house expensive?”. Just because we have a good <em>predictive</em> model doesn’t mean that we can answer this question easily.</p>
<p>For example, large complicated neural nets are very good at image detection, but it’s very hard to take a trained neural net and figure out what it is about a particular image that causes the neural net to “believe” that there is or isn’t a cat in the image. There is no easy way to determine what the “meaningful” or “predictive” features of the image are. Indeed, this problem is a major area of research currently in machine learning and statistics. See <a href="https://en.wikipedia.org/wiki/Explainable_artificial_intelligence">here</a> if you’re curious to learn more.</p>
<p>One of the good things about linear and logistic regression is that we can very easily determine which features were “useful” or “meaningful” for prediction– the estimated coefficients and associated p-values tell us quite a lot about which predictors (i.e., “features” in the language of machine learning) are (probably) useful.</p>
<p>Indeed, sometimes the determination of which predictors are “useful” or “meaningful” is the important part of the statistical question.</p>
<p><strong>Example: fMRI data and schizophrenia.</strong> In my own research, I work a lot with functional magnetic resonance imagine (fMRI) data obtained from studies in which the subjects are either healthy controls or are diagnosed with schizophrenia. It’s tempting to want to build a model that can predict, given an fMRI scan of a person’s brain, whether or not this person has schizophrenia.</p>
<p>That’s all fine and good, but doctors already have a very good way of telling whether or not a person has schizophrenia, and it’s way cheaper than doing an MRI: just talk to them for five minutes! If you doubt this, search around for videos of interviews with schizophrenics.</p>
<p>So being able to predict whether or not someone has schizophrenia based on an fMRI scan is interesting and perhaps impressive, but it isn’t at all useful scientifically or clinically: we have a cheaper simpler way of telling if someone is schizophrenic.</p>
<p>What <em>is</em> important to my colleagues in neuroscience and neurology is a more subtle question: <em>what is it</em> that is different between the brains of schizophrenic and healthy controls?</p>
<p>In other words, oftentimes we don’t just want a model that can perform prediction well, but we want a model that <em>explains</em> what it is in our data that generally explains the differences we see in responses or labels. Further, we usually want to be able to use that model to make inferences about the world– to estimate things like how levels of coffee consumption correlate with health outcomes.</p>
<p>This tension, between prediction and modeling, was discussed extensively in a <a href="https://projecteuclid.org/journals/statistical-science/volume-16/issue-3/Statistical-Modeling--The-Two-Cultures-with-comments-and-a/10.1214/ss/1009213726.full">famous paper</a> from 2001 by Leo Breiman, titled <em>Statistical Modeling: The Two Cultures</em>.</p>
<p>Now, it’s not as though there’s some huge fight between “prediction people” and “modeling people”. We work together all the time! But it’s useful to understand the distinction between these two different kinds of problems and to be able to distinguish when one or the other is the more appropriate “hat” to be wearing. Keep an eye out for these kinds of things as you progress in your data science career!</p>
</section>
<section id="review" class="level2" data-number="31.8">
<h2 data-number="31.8" class="anchored" data-anchor-id="review"><span class="header-section-number">31.8</span> Review</h2>
<ul>
<li>Relationship between probability, odds and log-odds</li>
<li>Sigmoid function <span class="math inline">\(\sigma(z)=\frac{1}{1+e^{-z}}\)</span></li>
<li>Simple logistic model <span class="math inline">\(\text{log-odds}(Y=1)=\beta_0 + \beta_1 X_1\)</span></li>
<li>Predicting <span class="math inline">\(y\)</span> from a logistic model</li>
<li>interpreting the coefficients of a logistic model</li>
<li>Z tests of significance of logistic coefficients</li>
<li>fitting a logistic model (maximize log-likelihood / minimize residual deviance)</li>
<li>likelihood / log likelihood / residual deviance</li>
<li>confusion matrix for a logistic model, prediction statistics</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
  </div>
  <div class="nav-page nav-page-next">
      <a href="./R10_LogisticReg_Extended.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Logistic Regression - Extended Examples</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>