<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.353">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>STAT 340: Data Science II - 25&nbsp; Prediction (Simple Linear Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./R08_prediction_examples.html" rel="next">
<link href="./estimation2_practice.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./prediction1.html">Prediction</a></li><li class="breadcrumb-item"><a href="./prediction1.html"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Prediction (Simple Linear Regression</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">STAT 340: Data Science II</a> 
        <div class="sidebar-tools-main">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">STAT 340</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false">
 <span class="menu-text">Sampling</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./probability_rv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Probability and Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R01_Prob_RVs.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Probability and Random Variable R Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./rv_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Probability and Random Variables Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Random Variable Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R02_Distributions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Random Variable Distributions R Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./distr_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variable Distribution Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Monte Carlo</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R03_MonteCarloExamples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Monte Carlo Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mc_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Monte Carlo Practice</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R_MonteCarlo_Battleship.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Monte Carlo Battleship</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false">
 <span class="menu-text">Testing</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Introduction to Statistical Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R04_Monte_Carlo_Testing.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Monte Carlo Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing1_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Monte Carlo Testing Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Statistical Testing, Continued</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R05_testing_Additional_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Monte Carlo Testing: Additional Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./testing2_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Testing and Power Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false">
 <span class="menu-text">Estimation</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Estimation Part 1</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R06_More_Estimation_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Estimation Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation1_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Point Estimation Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Estimation Part 2</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R07_More_Estimation_and_CI_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Interval Estimation Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./estimation2_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interval Estimation Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true">
 <span class="menu-text">Prediction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction1.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Prediction (Simple Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R08_prediction_examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Simple Linear Regression - Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./slr_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Simple Linear Regression Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prediction2.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">28</span>&nbsp; <span class="chapter-title">Prediction (Multiple Linear Regression)</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R09_Multiple_Regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">29</span>&nbsp; <span class="chapter-title">R10 multiple regression further concepts</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mlr_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">30</span>&nbsp; <span class="chapter-title">Multiple Linear Regression Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">31</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R10_LogisticReg_Extended.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">32</span>&nbsp; <span class="chapter-title">Logistic Regression - Extended Examples</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./logistic_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">33</span>&nbsp; <span class="chapter-title">Logistic Regression Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">34</span>&nbsp; <span class="chapter-title">Model Selection and Cross Validation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_cv-MSEcomparison.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">35</span>&nbsp; <span class="chapter-title">cv-extra</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Bias_Variance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">36</span>&nbsp; <span class="chapter-title">Bias Variance Tradeoff</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Model_Selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">37</span>&nbsp; <span class="chapter-title">R11 Model Selection</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R11_Ridge_LASSO.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">38</span>&nbsp; <span class="chapter-title">R11: Ridge and Lasso</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./cv_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">39</span>&nbsp; <span class="chapter-title">Cross Validation Practice</span></span></a>
  </div>
</li>
          <li class="px-0"><hr class="sidebar-divider hi "></li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false">
 <span class="menu-text">Bootstrapping</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bootstrap.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">40</span>&nbsp; <span class="chapter-title">Bootstrapping</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./R12_Bootstrap_Examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">41</span>&nbsp; <span class="chapter-title">R13_Bootstrap</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bootstrap_practice.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">42</span>&nbsp; <span class="chapter-title">Bootstrap Practice</span></span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start collapsed" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" aria-expanded="false" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 ">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./RV_summary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Random Variable Summary</span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#learning-objectives" id="toc-learning-objectives" class="nav-link active" data-scroll-target="#learning-objectives"><span class="header-section-number">25.1</span> Learning objectives</a></li>
  <li><a href="#prediction-an-overview" id="toc-prediction-an-overview" class="nav-link" data-scroll-target="#prediction-an-overview"><span class="header-section-number">25.2</span> Prediction: an overview</a></li>
  <li><a href="#writing-down-the-model" id="toc-writing-down-the-model" class="nav-link" data-scroll-target="#writing-down-the-model"><span class="header-section-number">25.3</span> Writing down the model</a></li>
  <li><a href="#interpreting-simple-linear-regression" id="toc-interpreting-simple-linear-regression" class="nav-link" data-scroll-target="#interpreting-simple-linear-regression"><span class="header-section-number">25.4</span> Interpreting simple linear regression</a>
  <ul class="collapse">
  <li><a href="#caution-causality" id="toc-caution-causality" class="nav-link" data-scroll-target="#caution-causality"><span class="header-section-number">25.4.1</span> Caution: causality</a></li>
  </ul></li>
  <li><a href="#fitting-the-model" id="toc-fitting-the-model" class="nav-link" data-scroll-target="#fitting-the-model"><span class="header-section-number">25.5</span> Fitting the model</a>
  <ul class="collapse">
  <li><a href="#choosing-a-loss-sum-of-squares" id="toc-choosing-a-loss-sum-of-squares" class="nav-link" data-scroll-target="#choosing-a-loss-sum-of-squares"><span class="header-section-number">25.5.1</span> Choosing a loss: sum of squares</a></li>
  <li><a href="#minimizing-the-loss" id="toc-minimizing-the-loss" class="nav-link" data-scroll-target="#minimizing-the-loss"><span class="header-section-number">25.5.2</span> Minimizing the loss</a></li>
  </ul></li>
  <li><a href="#interpreting-the-estimates" id="toc-interpreting-the-estimates" class="nav-link" data-scroll-target="#interpreting-the-estimates"><span class="header-section-number">25.6</span> Interpreting the estimates</a>
  <ul class="collapse">
  <li><a href="#interpreting-the-estimated-slope" id="toc-interpreting-the-estimated-slope" class="nav-link" data-scroll-target="#interpreting-the-estimated-slope"><span class="header-section-number">25.6.1</span> Interpreting the estimated slope</a></li>
  <li><a href="#interpreting-the-intercept-term-hatbeta_0" id="toc-interpreting-the-intercept-term-hatbeta_0" class="nav-link" data-scroll-target="#interpreting-the-intercept-term-hatbeta_0"><span class="header-section-number">25.6.2</span> Interpreting the intercept term <span class="math inline">\(\hat{\beta}_0\)</span></a></li>
  </ul></li>
  <li><a href="#variance-of-estimates" id="toc-variance-of-estimates" class="nav-link" data-scroll-target="#variance-of-estimates"><span class="header-section-number">25.7</span> Variance of estimates</a></li>
  <li><a href="#running-simple-linear-regression" id="toc-running-simple-linear-regression" class="nav-link" data-scroll-target="#running-simple-linear-regression"><span class="header-section-number">25.8</span> Running simple linear regression</a></li>
  <li><a href="#working-with-lm-output-diagnostics" id="toc-working-with-lm-output-diagnostics" class="nav-link" data-scroll-target="#working-with-lm-output-diagnostics"><span class="header-section-number">25.9</span> Working with <code>lm()</code> output: diagnostics</a>
  <ul class="collapse">
  <li><a href="#homoscedasticity" id="toc-homoscedasticity" class="nav-link" data-scroll-target="#homoscedasticity"><span class="header-section-number">25.9.1</span> Homoscedasticity</a></li>
  <li><a href="#assessing-normality-of-the-residuals" id="toc-assessing-normality-of-the-residuals" class="nav-link" data-scroll-target="#assessing-normality-of-the-residuals"><span class="header-section-number">25.9.2</span> Assessing normality of the residuals</a></li>
  </ul></li>
  <li><a href="#testing-and-confidence-intervals-for-coefficients" id="toc-testing-and-confidence-intervals-for-coefficients" class="nav-link" data-scroll-target="#testing-and-confidence-intervals-for-coefficients"><span class="header-section-number">25.10</span> Testing and confidence intervals for coefficients</a></li>
  <li><a href="#making-predictions" id="toc-making-predictions" class="nav-link" data-scroll-target="#making-predictions"><span class="header-section-number">25.11</span> Making predictions</a></li>
  <li><a href="#assessing-model-fit" id="toc-assessing-model-fit" class="nav-link" data-scroll-target="#assessing-model-fit"><span class="header-section-number">25.12</span> Assessing model fit</a></li>
  <li><a href="#looking-ahead-model-selection" id="toc-looking-ahead-model-selection" class="nav-link" data-scroll-target="#looking-ahead-model-selection"><span class="header-section-number">25.13</span> Looking ahead: model selection</a></li>
  <li><a href="#review" id="toc-review" class="nav-link" data-scroll-target="#review"><span class="header-section-number">25.14</span> Review</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Prediction (Simple Linear Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<p>In these notes, we will introduce the task of prediction and begin talking about some of the fundamental tools for performing prediction, focusing on linear regression. The task of prediction will be a theme for much of the remainder of the course, and it is not an exaggeration to say that prediction is the fundamental task that lies at the heart of machine learning. Tasks like image classification (e.g., “does this image contain a cat or not?”) are very naturally cast as prediction problems, as are many of the most basic problems in machine learning (e.g., predict how likely a person is to engage with the next piece of content in their feed).</p>
<section id="learning-objectives" class="level2" data-number="25.1">
<h2 data-number="25.1" class="anchored" data-anchor-id="learning-objectives"><span class="header-section-number">25.1</span> Learning objectives</h2>
<p>After this lesson, you will be able to</p>
<ul>
<li>Explain both simple and multiple linear regression</li>
<li>Use R to run linear regression on a given data set and interpret the resulting coefficient estimates</li>
<li>Explain what it means to associate a p-value to an estimated coefficient</li>
</ul>
</section>
<section id="prediction-an-overview" class="level2" data-number="25.2">
<h2 data-number="25.2" class="anchored" data-anchor-id="prediction-an-overview"><span class="header-section-number">25.2</span> Prediction: an overview</h2>
<p>In a prediction problem, we are given data pairs <span class="math inline">\((X_1, Y_1), (X_2, Y_2), \dots, (X_n, Y_n)\)</span> and we want to use <span class="math inline">\(X_i\)</span> to predict <span class="math inline">\(Y_i\)</span>.</p>
<p>We call the <span class="math inline">\(X_i\)</span> values the <em>predictors</em> (also called the <em>independent variables</em>), and we call our <span class="math inline">\(Y_i\)</span> values the <em>responses</em> (or the <em>dependent variables</em> or the <em>outcomes</em>).</p>
<p>Let’s look at an example that we discussed in our very first lecture.</p>
<p><img src="images/edu_income.png" style="width:70%;"></p>
<p>Here, our <span class="math inline">\((X_i,Y_i)\)</span> pairs correspond to years of education (<span class="math inline">\(X_i\)</span>) and income (<span class="math inline">\(Y_i\)</span>). That is, our predictors are years of education, and our responses are income.</p>
<p>Our goal is to use this data to learn a function that maps years of education to income. That is, we want a function that takes years of education as input and outputs a prediction as to how much income we predict for a person with that income.</p>
<p>Now, the very first problem we run into is what we mean by learning a function– there are lots of functions out there!</p>
<p><strong>Linear regression</strong> makes a particular choice: we will learn a linear function that maps our predictors to (predicted) responses.</p>
<p>You discussed this problem at some length in STAT240, in the setting where our <span class="math inline">\(X_i\)</span> and <span class="math inline">\(Y_i\)</span> variables were all real-valued. That is, you learned about <em>simple linear regression</em>, in which we model our data as <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i,
\]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> is the <em>intercept term</em>, <span class="math inline">\(\beta_1\)</span> is the coefficient associated with our predictors, and <span class="math inline">\(\epsilon_i\)</span> is an <em>error term</em>.</p>
<p>One way to think of this is that we imagine that given a value for the predictor <span class="math inline">\(X_i\)</span>, say <span class="math inline">\(X_i = x_i\)</span>, the “true” value of <span class="math inline">\(Y_i\)</span> would be <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>. However, we don’t see this quantity exactly. Instead, due either to uncertainty in our data collection or to random factors (or both), we see a noisy version of this quantity, namely <span class="math inline">\(Y_i = \beta_0 + \beta_1 x_i + \epsilon_i\)</span>.</p>
<p><strong>Example: income and education</strong></p>
<p>We might predict that a person with <span class="math inline">\(X_i\)</span> years of education makes <span class="math display">\[
Y_i = 20 + 4 X_i
\]</span> thousands of dollars in salary. But there are lots of other factors in addition to years of education that might influence how much money someone makes. Including the error <span class="math inline">\(\epsilon_i\)</span> is a way to account for these unmeasured variables.</p>
<p><strong>Example: astronomy</strong></p>
<p>In the early days of astronomy, scientists like <a href="https://en.wikipedia.org/wiki/Johannes_Kepler">Johannes Kepler</a> and <a href="https://en.wikipedia.org/wiki/Tycho_Brahe">Tycho Brahe</a> were trying to fit the trajectories of planets in the sky to curves. When an astronomer takes a measurement of where a planet is, there are many sources of noise that cause them to make the measurement imprecisely. These sources of noise include both human error (e.g., imprecision in positioning the telescope, misreading a sight) and external sources (e.g., variations in humidity in the atmosphere change the way light bends as it reaches the observer’s eye). If we model the position of a planet at time <span class="math inline">\(X_i\)</span> as <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i
\]</span> for some choice of coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, we might account for these sources of <em>measurement error</em> via our error term <span class="math inline">\(\epsilon_i\)</span>. Of course, this linear model is incorrect– the planets follow quite complicated trajectories in the night sky, not simple straight lines. We’ll come back to the matter of when linear models are or are not appropriate.</p>
</section>
<section id="writing-down-the-model" class="level2" data-number="25.3">
<h2 data-number="25.3" class="anchored" data-anchor-id="writing-down-the-model"><span class="header-section-number">25.3</span> Writing down the model</h2>
<p>To recap, a simple linear model has an outcome <span class="math inline">\(y\)</span> and single predictor <span class="math inline">\(x\)</span>. It is defined by the following equation:</p>
<p><span class="math display">\[
\begin{aligned}
y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i \\
    &amp;= ~~~~~~~\hat{y}_i ~~~~~~ + \epsilon_i
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(i = 1,2, \dots, n\)</span>, and the error terms <span class="math inline">\(\epsilon_i \sim N(0,\sigma^2)\)</span> are independent over <span class="math inline">\(i=1,2,\dots,n\)</span>. The <span class="math inline">\(\hat{y}_i\)</span> notation is to stress that once we have chosen values for the coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, our prediction of the response of the <span class="math inline">\(i\)</span>-th data point is <span class="math inline">\(\hat{y}_i = \beta_0 + \beta_1 x_i\)</span>.</p>
<p>This equation represents our <em>model</em>, not the truth! We want to choose <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> so that this model describes our observed data as well as possible, but we have to bear in mind that this linearity assumption, that <span class="math inline">\(y_i\)</span> is (exactly or approximately) expressible as <span class="math inline">\(\beta_0 + \beta_1 x_i\)</span>, is an <em>assumption</em>. Our model will be good at predicting outcomes only in so far as this model agrees with reality.</p>
<p>The subscript <span class="math inline">\(i\)</span> in our regression equation indexes the <span class="math inline">\(n\)</span> observations in the dataset. Think of <span class="math inline">\(i\)</span> as a row number. So another way to think about our model <span class="math display">\[
y_i = \beta_0 + \beta_1 x_i + \epsilon_i,
\]</span></p>
<p>is as a system of <span class="math inline">\(n\)</span> equations, <span class="math display">\[
\begin{aligned}
y_1 &amp;= \beta_0 + \beta_1 x_1 + \epsilon_1 \\
y_2 &amp;= \beta_0 + \beta_1 x_2 + \epsilon_2 \\
    &amp;\vdots \\
y_i &amp;= \beta_0 + \beta_1 x_i + \epsilon_i \\
&amp;\vdots \\
y_{n-1} &amp;= \beta_0 + \beta_1 x_{n-1} + \epsilon_{n-1} \\
y_{n} &amp;= \beta_0 + \beta_1 x_{n} + \epsilon_{n}.
\end{aligned}
\]</span></p>
<p>The error terms <span class="math inline">\(\epsilon_i\)</span> in a linear model correspond, essentially, to the part of the variation in the data that remains unexplained by the deterministic portion of the model (encoded in the linear function <span class="math inline">\(\beta_0 + \beta_1 x\)</span>).</p>
<p>One of the key assumptions of a linear model is that the residuals are independent and have mean zero, <span class="math inline">\(\mathbb{E} \epsilon_i = 0\)</span>. Most typically, we further assume that they are normally distributed with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We’ll do that here in these notes, but this choice can sometimes be “relaxed”, in the sense that we may not need to assume that the errors are normal for linear regression to work, depending on what we want to do downstream. Later in your studies, when you take your mathematical statistics course, you’ll put that statement on firmer ground; for now, we’ll have to leave it vague.)</p>
</section>
<section id="interpreting-simple-linear-regression" class="level2" data-number="25.4">
<h2 data-number="25.4" class="anchored" data-anchor-id="interpreting-simple-linear-regression"><span class="header-section-number">25.4</span> Interpreting simple linear regression</h2>
<p>So let’s suppose that we’re using the linear model <span class="math display">\[
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i.
\]</span></p>
<p>To simplify things, let’s just ignore the error term for a moment. After all, <span class="math inline">\(\epsilon_i\)</span> just captures uncertainty in our measurements. In the ideal world of no measurement error, our model predicts that for a particular choice of predictor <span class="math inline">\(x\)</span>, we will measure the response <span class="math display">\[
y = \beta_0 + \beta_1 x.
\]</span></p>
<p>Now, let’s first consider what happens when <span class="math inline">\(x=0\)</span>. Then <span class="math inline">\(y=\beta_0\)</span>. Said another way, if we plotted the line <span class="math inline">\(y = \beta_0 + \beta_1 x\)</span>, <span class="math inline">\(\beta_0\)</span> would be the intercept of our model.</p>
<p>For example, here’s the function with <span class="math inline">\(\beta_0 = -2\)</span> and <span class="math inline">\(\beta_1 = 1.\)</span></p>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">2</span>,<span class="dv">8</span>,<span class="fl">0.1</span>)</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span>;</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span>;</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> x;</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(x,y)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>);</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Equivalently, since we know that this function forms a line:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Pass NULL to plot to create an empty plot with axes.</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="cn">NULL</span>, <span class="at">xlab=</span><span class="st">""</span>, <span class="at">ylab=</span><span class="st">""</span>, <span class="at">xlim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">2</span>, <span class="dv">8</span>), <span class="at">ylim=</span><span class="fu">c</span>(<span class="sc">-</span><span class="dv">4</span>, <span class="dv">6</span>))</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">a=</span><span class="sc">-</span><span class="dv">2</span>, <span class="at">b=</span><span class="dv">1</span>, <span class="at">col=</span><span class="st">'red'</span>, <span class="at">lw=</span><span class="dv">3</span>);</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">h=</span><span class="dv">0</span>);</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v=</span><span class="dv">0</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Looking at those two plots, it’s clear that <span class="math inline">\(\beta_0 = -2\)</span> is indeed the intercept of our function. But to reiterate, the “typical” interpretation of the parameter <span class="math inline">\(\beta_0\)</span> is as describing what would happen if we observed a data point for which our predictor <span class="math inline">\(x\)</span> were equal to zero.</p>
<p>Now, it’s pretty obvious that <span class="math inline">\(\beta_1\)</span> is the slope of our function. But how do we interpret it? Well, let’s suppose that we take one measurement with predictor <span class="math inline">\(x\)</span>. Our model says that (again, ignoring the error term for now) we will see a response <span class="math display">\[
y = \beta_0 + \beta_1 x.
\]</span></p>
<p>Now, let’s suppose we take another measurement, this time at predictor value <span class="math inline">\(x+1\)</span>. Our model predicts that we will measure the response <span class="math display">\[
y' = \beta_0 + \beta_1 (x+1).
\]</span></p>
<p>If we subtract one from the other, we have <span class="math display">\[
y' - y = \beta_0 + \beta_1 (x+1) - (\beta_0 + \beta_1 x)
= \beta_1.
\]</span></p>
<p>In other words, <span class="math inline">\(\beta_1\)</span> is the change in response that our model predicts if we increase the value of our predictor by one unit.</p>
<p><strong>Example: income and education</strong></p>
<p>Let’s come back to our model predicting income (in tens of thousands of dollars) from education, and suppose that we have fit a model of the form <span class="math display">\[
y = 20 + 4 x.
\]</span></p>
<p>So our coefficients are <span class="math inline">\(\beta_0 = 20\)</span> and <span class="math inline">\(\beta_1 = 4\)</span>. Thus, our model predicts that an increase in education by <span class="math inline">\(1\)</span> year is associated with in an increase of $40K in salary (4 times our unit of measurement, $10K/year).</p>
<p>Similarly, since <span class="math inline">\(\beta_0=20\)</span>, our model “predicts” that a person with zero years of education will receive a salary of $20K per year.</p>
<p><strong>Example: a cautionary tale</strong></p>
<p>Interpreting the intercept as describing the response at <span class="math inline">\(x=0\)</span> can get a little bit weird if we push the idea too far. Let’s consider a similar problem, this time of predicting income from height. Suppose that we fit a model that predicts income (in thousands of dollars) from height (in centimeters), <span class="math display">\[
y = 10 + 0.4 x,
\]</span></p>
<p>where <span class="math inline">\(x\)</span> is height in centimeters (note that the units on this example don’t really make sense– don’t let that bother you; it’s not the point).</p>
<p>The intercept of this model is <span class="math inline">\(\beta_0 = 10\)</span>. So our model “predicts” that a person with height <span class="math inline">\(x=0\)</span> would make a salary of $10,000 per year. Now, that’s all fine and good, except that I, for one, have never encountered a person with height 0 cm.</p>
<p>So our model makes a prediction, but it is making a prediction on an input that we don’t really every expect to encounter in the real word.</p>
<p>The high-level point is that often our linear regression model really only makes sense over a certain range of values, and we should be careful when using our model to extrapolate to “strange” values of <span class="math inline">\(x\)</span>.</p>
<p>Even though we might be able to associate a response with any particular input <span class="math inline">\(x\)</span>, that doesn’t mean that every such input is realistic. These matters will mostly have to wait for later courses on modeling, but it’s a point we’ll come back to a couple of times over the next few weeks, and it’s a common pitfall in interpreting linear regression models, so it’s worth bringing it to your attention now.</p>
<section id="caution-causality" class="level3" data-number="25.4.1">
<h3 data-number="25.4.1" class="anchored" data-anchor-id="caution-causality"><span class="header-section-number">25.4.1</span> Caution: causality</h3>
<p>It is always tempting in talking about models like this to say, having fit a model, that “increasing the value of the predictor by one unit causes an increase in the response by one unit” or that “increasing the predicor by one unit results in an increase of such and such amount”. Indeed, many statisticians will say something like this when speaking informally. Still, we should be careful to avoid giving a causal interpretation to our findings.</p>
<p>For example, suppose that we fit a linear regression model to predict the number of cancer cases per year in Wisconsin based on pollution levels (measured in, say, <a href="https://www.epa.gov/air-trends/particulate-matter-pm25-trends">PM2.5</a>), and we estimate <span class="math inline">\(\beta_1 = 10.2\)</span>. We might be tempted to say that a unit increase of PM2.5 <em>causes</em>, on average*, an additional 10.2 cancer cases.</p>
<p>For better or worse, this is a stronger statement than what we can conclude from a linear model fitted in this way. We can only say that a unit increase in PM2.5 is <em>associated</em> with an increase of 10.2 cancer cases. This is the old “correlation is not causation” saying, wearing a slightly different hat.</p>
<p>There is a whole area in statistics called <em>causal inference</em> that attempts to use statistics to make causal statements, but it is, unfortunately, outside the scope of the course.</p>
</section>
</section>
<section id="fitting-the-model" class="level2" data-number="25.5">
<h2 data-number="25.5" class="anchored" data-anchor-id="fitting-the-model"><span class="header-section-number">25.5</span> Fitting the model</h2>
<p>Suppose that we have chosen values of our coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> in our regression model. How do we decide how “good” or “bad” this choice of coefficients is? We need a function that takes a particular choice of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> and outputs a number that measures how well or poorly the resulting model describes our data.</p>
<p>In the setting where larger values of this function correspond to worse model fit, we call this kind of a function a <em>loss function</em>: it takes a choice of model parameters (i.e., coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, in our case), and outputs a number that measures how poorly our model fits the data.</p>
<section id="choosing-a-loss-sum-of-squares" class="level3" data-number="25.5.1">
<h3 data-number="25.5.1" class="anchored" data-anchor-id="choosing-a-loss-sum-of-squares"><span class="header-section-number">25.5.1</span> Choosing a loss: sum of squares</h3>
<p>There are lots of functions we could choose to use as our loss function, but by far the most common choice is the <em>residual sum of squares</em> (RSS), sometimes called the <em>sum of squared errors</em> (SSE): <span class="math display">\[
\ell( \beta_0, \beta_1 )
= \sum_{i=1}^n (y_i - \hat{y}_i)^2
= \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2.
\]</span></p>
<p>The terms <span class="math inline">\(y_i - \hat{y}_i = y_i - (\beta_0 + \beta_1x_i)\)</span> are called the <em>residuals</em>. The word <em>residual</em> comes from the word <em>residue</em> (cue flashback to chem lab?), which refers to something that is left over. The residuals are what is left over after we try to predict the responses from the predictors <span class="math inline">\(x_1,x_2,\dots,x_n\)</span>.</p>
<p>Our goal is then to choose our coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to minimize the sum of squared residuals loss in the equation above. We call this <em>ordinary least squares</em> (OLS) regression. “Least squares” because, well, we’re minimizing the sum of squares. “Ordinary” because there are other sums of squares we could look at that would be a little less ordinary (see <a href="https://en.wikipedia.org/wiki/Linear_least_squares">here</a> for details, if you’re curious).</p>
<p>Let’s note that the sum of squared errors is not the only possible loss we could choose. For example, we might try to minimize the sum of absolute deviations, <span class="math display">\[
\sum_{i=1}^n |y_i - \hat{y}_i|
= \sum_{i=1}^n \left|y_i - (\beta_0 + \beta_1x_i) \right|.
\]</span></p>
<p>As we’ve seen in recent lectures, though, trying to minimize this loss with respect to our coefficients <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> can be challenging.</p>
</section>
<section id="minimizing-the-loss" class="level3" data-number="25.5.2">
<h3 data-number="25.5.2" class="anchored" data-anchor-id="minimizing-the-loss"><span class="header-section-number">25.5.2</span> Minimizing the loss</h3>
<p>So we have a loss function <span class="math display">\[
\ell(\beta_0,\beta_1) = \sum_{i=1}^n (y_i - (\beta_0 + \beta_1x_i))^2,
\]</span> and we want to choose <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> to minimize this quantity.</p>
<p>To do that, we are going to dust off our calculus textbooks, take derivatives, set those derivatives equal to zero, and solve for <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>. That is, we want to solve <span class="math display">\[
\frac{ \partial \ell( \beta_0, \beta_1 )}{ \partial \beta_0 } = 0
~~~\text{ and } ~~~
\frac{ \partial \ell( \beta_0, \beta_1 ) }{ \partial \beta_1 }
= 0.
\]</span></p>
<p>I’ll spare you the mathematical details; if you’re curious, you can find a derivation of the solution in any introductory regression book, or <a href="https://pages.stat.wisc.edu/~kdlevin/teaching/Fall2022/STAT340/lecs/OLS_derivation.html">here</a></p>
<p>The important point is that we find that our estimates should be <span class="math display">\[
\begin{aligned}
\hat{\beta}_0 &amp;= \bar{y} - \hat{\beta}_1 \bar{x} \\
\hat{\beta}_1 &amp;= \frac{ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
                      { \sum_{i=1}^n (x_i - \bar{x})^2 },
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> are the means of the predictors and responses, respectively: <span class="math display">\[
\bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
~~~\text{ and }~~~
\bar{y} = \frac{1}{n} \sum_{i=1}^n y_i.
\]</span></p>
</section>
</section>
<section id="interpreting-the-estimates" class="level2" data-number="25.6">
<h2 data-number="25.6" class="anchored" data-anchor-id="interpreting-the-estimates"><span class="header-section-number">25.6</span> Interpreting the estimates</h2>
<p>Let’s pause and try to interpret what these two estimates <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> actually mean. Let’s start with <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<section id="interpreting-the-estimated-slope" class="level3" data-number="25.6.1">
<h3 data-number="25.6.1" class="anchored" data-anchor-id="interpreting-the-estimated-slope"><span class="header-section-number">25.6.1</span> Interpreting the estimated slope</h3>
<p>By our definition of <span class="math inline">\(\hat{\beta}_1\)</span>, we have</p>
<p><span class="math display">\[
\hat{\beta}_1
=
\frac{ \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
                      { \sum_{i=1}^n (x_i - \bar{x})^2 }
=
\frac{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
                      { \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2 },
\]</span> where we multiplied the numerator and denominator by <span class="math inline">\(1/n\)</span>.</p>
<p>Now, let’s notice that the denominator is just the (uncorrected) sample variance of the predictors: <span class="math display">\[
s_x^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2.
\]</span> If we define the analogous quantity for the predictors, <span class="math display">\[
s_y^2 = \frac{1}{n} \sum_{i=1}^n (y_i - \bar{y})^2,
\]</span> we have <span class="math display">\[
\hat{\beta}_1
= \frac{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
{s_x^2}
=
\frac{ s_y }{ s_x }
\frac{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
{ s_x s_x }.
\]</span></p>
<p>Now, let’s look at the other sum, <span class="math display">\[
\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}),
\]</span> and notice that it is the sample covariance of the predictors and responses. Recalling our definition of the correlation as <span class="math display">\[
\rho_{x,y}
= \frac{ \mathbb{E} (X - \mathbb{E} X)( Y - \mathbb{E}Y)}
{\sqrt{ (\operatorname{Var} X)( \operatorname{Var} Y) }},
\]</span></p>
<p>we notice that <span class="math display">\[
\hat{\rho}_{x,y}
=
\frac{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
{ s_x s_x }
=
\frac{ \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y}) }
{ \sqrt{ s_x^2 s_y^2 }}
\]</span></p>
<p>is the <em>sample correlation</em> between our predictors and responses– we plugged in the sample versions of the covariance and the variances.</p>
<p>So, our estimated coefficient <span class="math inline">\(\hat{\beta}_1\)</span> can be expressed as <span class="math display">\[
\hat{\beta}_1
=
\frac{ s_y }{ s_x } \hat{\rho}_{x,y}.
\]</span></p>
<p>In other words, the slope of our model is the ratio of the standard deviations, scaled by the correlation between our predictors and responses.</p>
<p>An interesting case to think about is when the predictors and responses are perfectly correlated (i.e., the predictors and responses form a perfect line, with no “jitter”). Then our estimated slope is <span class="math inline">\(\hat{\beta}_1 = \sqrt{ s_y^2/s_x^2 } = s_y/s_x\)</span>. In other words, the slope of our model is just the ratio of the standard deviation of the responses to that of the predictors. Think of this as like a “change of units” from predictors to responses. If our predictors are measured in, say, years of education, and our responses are measured in dollars per year, then the ratio of the standard deviations has units <span class="math display">\[
\frac{ \text{dollars per year} }{ \text{years of education}},
\]</span></p>
<p>and multiplying this by our predictor, which is measured in “years of education”, we get <span class="math display">\[
\text{ response }
=
\frac{ \text{dollars per year} }{ \text{years of education}}
\cdot
\text{ years of education }
=
\text{ dollars per year },
\]</span> which is what we expect.</p>
<p>This kind of “dimension analysis”, which you may have seen before in physics, can be a useful way to make sure that you’re performing calculations correctly!</p>
</section>
<section id="interpreting-the-intercept-term-hatbeta_0" class="level3" data-number="25.6.2">
<h3 data-number="25.6.2" class="anchored" data-anchor-id="interpreting-the-intercept-term-hatbeta_0"><span class="header-section-number">25.6.2</span> Interpreting the intercept term <span class="math inline">\(\hat{\beta}_0\)</span></h3>
<p>Turning our attention to <span class="math inline">\(\hat{\beta}_0\)</span>, we have</p>
<p><span class="math display">\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]</span> Why does this choice make sense?</p>
<p>Well, let’s suppose that we decided to make our lives even harder by restricting our choice of prediction function to be a constant. That is, suppose we wanted to choose a prediction function that returns the same output, say, <span class="math inline">\(\hat{y}\)</span>, no matter the input <span class="math inline">\(x\)</span>.</p>
<p>If we wanted to choose this output using the same least-squares approach that we used above, we would want to choose <span class="math inline">\(\hat{y}\)</span> so that it minimizes <span class="math display">\[
\sum_{i=1}^n (y_i - y)^2.
\]</span> A little bit of calculus (seriously, this one’s easy– try it!) shows that the way to minimize this is to choose the output <span class="math display">\[
\hat{y}= \frac{1}{n} \sum_{i=1}^n y_i = \bar{y}.
\]</span> Now, thankfully, we are not actually trying to predict our data with a constant function. We are allowed to choose a slope!</p>
<p>Having chosen our slope <span class="math inline">\(\hat{\beta}_1\)</span>, our model <em>without</em> an intercept term predicts that the <span class="math inline">\(i\)</span>-th observation should have response <span class="math inline">\(\hat{\beta}_1 x_i\)</span>. If we add an intercept term to the model, sticking with our least squares loss, we would like to choose <span class="math inline">\(\beta_0\)</span> so as to minimize <span class="math display">\[
\sum_{i=1}^n \left( y_i - \hat{\beta}_1 x_i - \beta_0 \right)^2.
\]</span></p>
<p>The exact same kind of calculus argument (taking a derivative with respect to <span class="math inline">\(\beta_0\)</span>, this time– again, give it a try!), gets us <span class="math display">\[
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.
\]</span></p>
</section>
</section>
<section id="variance-of-estimates" class="level2" data-number="25.7">
<h2 data-number="25.7" class="anchored" data-anchor-id="variance-of-estimates"><span class="header-section-number">25.7</span> Variance of estimates</h2>
<p>After fitting, we can find our predicted <span class="math inline">\(\hat{y_i}\)</span>, i.e.&nbsp;the <span class="math inline">\(y\)</span> values on the line.</p>
<p><span class="math display">\[
\hat{y_i}=\hat{\beta_0}+\hat{\beta_1}x_i
\]</span></p>
<p>as well as our model residuals <span class="math inline">\(\hat{\epsilon}_i\)</span></p>
<p><span class="math display">\[
\hat{\epsilon_i}=y_i-\hat{y_i}
\]</span></p>
<p>From this, we also get for free an estimate of the variance of the residuals <span class="math inline">\(\sigma^2\)</span>, which happens to be very useful in computing other statistics. The reason is that <strong>the larger the residuals’ variance, the less precisely we can estimate our regression coefficients</strong>, which should make a lot of sense.</p>
<p><span class="math display">\[
\hat{\sigma}^2=\text{mean squared error}=\frac{SSE}{n-2}=\frac1{n-2}\sum_i(y_i-\hat{y_i})^2
\]</span></p>
<p>We can also easily derive the variance of the slope. First, observe that</p>
<p><span class="math display">\[\begin{align}
\sum_i (x_i - \bar{x})\bar{y}
&amp;= \bar{y}\sum_i (x_i - \bar{x})\\
&amp;= \bar{y}\left(\left(\sum_i x_i\right) - n\bar{x}\right)\\
&amp;= \bar{y}\left(n\bar{x} - n\bar{x}\right)\\
&amp;= 0
\end{align}\]</span></p>
<p>This means that</p>
<p><span class="math display">\[\begin{align}
\sum_i (x_i - \bar{x})(y_i - \bar{y})
&amp;= \sum_i (x_i - \bar{x})y_i - \sum_i (x_i - \bar{x})\bar{y}\\
&amp;= \sum_i (x_i - \bar{x})y_i\\
&amp;= \sum_i (x_i - \bar{x})(\beta_0 + \beta_1x_i + \epsilon_i )\\
\end{align}\]</span></p>
<p>Using this, we can easily derive <span class="math inline">\(\text{Var}(\hat{\beta_1})\)</span> as follows:</p>
<p><span class="math display">\[\begin{align}
\text{Var}(\hat{\beta_1})
&amp; = \text{Var} \left(\frac{\sum_i (x_i - \bar{x})(y_i - \bar{y})}{\sum_i (x_i - \bar{x})^2} \right) \\
&amp;= \text{Var} \left(\frac{\sum_i (x_i - \bar{x})(\beta_0 + \beta_1x_i + \epsilon_i )}{\sum_i (x_i - \bar{x})^2} \right), \;\;\;\text{substituting in the above} \\
&amp;= \text{Var} \left(\frac{\sum_i (x_i - \bar{x})\epsilon_i}{\sum_i (x_i - \bar{x})^2} \right), \;\;\;\text{noting only $\epsilon_i$ is a random variable} \\
&amp;=  \frac{\sum_i (x_i - \bar{x})^2\text{Var}(\epsilon_i)}{\left(\sum_i (x_i - \bar{x})^2\right)^2} , \;\;\;\text{independence of } \epsilon_i \text{ and, Var}(cX)=c^2\text{Var}(X) \\
&amp;= \frac{\sigma^2}{\sum_i (x_i - \bar{x})^2} \\
\end{align}\]</span></p>
</section>
<section id="running-simple-linear-regression" class="level2" data-number="25.8">
<h2 data-number="25.8" class="anchored" data-anchor-id="running-simple-linear-regression"><span class="header-section-number">25.8</span> Running simple linear regression</h2>
<p>Okay, that’s enough abstraction. Let’s apply this to some real data and see how things go.</p>
<p>In the 1920s, <a href="https://en.wikipedia.org/wiki/Thomas_Midgley_Jr">Thomas Midgley Jr.</a> discovered that adding <a href="https://en.wikipedia.org/wiki/Tetraethyllead">tetraethyllead</a> to gasoline decreased engine knocking (i.e., when fuel doesn’t fully ignite in an engine cylinder, which may damage the engine). He won the 1923 <a href="https://en.wikipedia.org/wiki/William_H._Nichols_Medal">Nichols medal</a>, a prestigious prize in chemistry, for his discovery.</p>
<p>The result of burning tetraethyllead in gasoline resulted in high levels of lead in the atmostphere. By the 1950s to 70s, researchers started to suspect that increased lead levels in the atmosphere was causing widespread lead poisoning, with symptoms ranging from depression, loss of appetite, and amnesia to anemia, insomnia, slurred speech, and cognitive impairment. Starting in the 1980s, the use of tetraethyllead in gasoline started to be phased out. At most gas stations in the United States, you’ll notice that gasoline is still marked as being “unleaded”, just in case you were worried!</p>
<p>In more recent years, a more controversial theory has emerged, suggesting that exposure to lead (be it in the atmosphere or in paint in older buildings) correlates with incidents of violent crime later in life <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. This study was first conducted in the US, but it was soon replicated in other countries and the similar results have been found elsewhere in the world.</p>
<p><img src="lead1.png" style="width:70%;"></p>
<p><img src="lead2.png" style="width:70%;"></p>
<p><img src="lead3.png" style="width:70%;"></p>
<p>Let’s look at a <a href="https://doi.org/10.1016/j.envint.2012.03.005">dataset</a> that contains atmospheric lead content levels and aggravated assault rates for several cities in the US and see if we can build a simple linear regression model to explain the trend and make predictions.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>lead <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">'data/lead.csv'</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># First things first: let's look at the data.</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(lead)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     city air.pb.metric.tons aggr.assault.per.million
1 Atlanta                421                     1029
2 Atlanta                429                      937
3 Atlanta                444                      887
4 Atlanta                457                      533
5 Atlanta                461                     1012
6 Atlanta                454                      848</code></pre>
</div>
</div>
<p>The variables we are interested in are lead levels in the atmosphere (measured in metric tons of lead emitted) and the aggravated assault rate per million 22 years later. We want to predict the assault rate from lead levels, so our predictor (or explanatory variable or independent variable, if you prefer) is lead levels, and our response (or dependent variable) is the assault rate. This data is available for a number of cities:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">levels</span>( <span class="fu">as.factor</span>(lead<span class="sc">$</span>city) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Atlanta"      "Chicago"      "Indianapolis" "Minneapolis"  "New Orleans" 
[6] "San Diego"   </code></pre>
</div>
</div>
<p>For simplicity, let’s focus on the city of Atlanta.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>atlanta_lead <span class="ot">&lt;-</span> lead[ lead<span class="sc">$</span>city<span class="sc">==</span><span class="st">'Atlanta'</span>, ];</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="do">## Alternative approach, using filter in dplyr:</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co"># library(dplyr)</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co"># atlanta_lead  lead %&gt;% filter(city == "Atlanta")</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the data to get a look at what's going on.</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>( atlanta_lead,</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>          <span class="fu">aes</span>(<span class="at">x=</span>air.pb.metric.tons, <span class="at">y=</span>aggr.assault.per.million));</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">geom_point</span>();</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">x=</span><span class="st">"Lead levels (metric tons)"</span>,</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y=</span><span class="st">"Agg'd assaults (per million)"</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>                 <span class="at">title=</span><span class="st">"Violent crime and atmospheric lead (22 year lag)"</span> )</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Visually, it’s quite clear that assaults are well-predicted as a linear function of lead levels. One thing that is already perhaps of concern is that the data appears to be a bit more “spread out” in the vertical direction for lower lead levels. This is a bit concerning in light of our assumption that the error terms were all distributed according to a normal with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We’ll come back to this point below. For now, let’s press on.</p>
<p>To fit a linear model in R, we use the <code>lm()</code> function (<code>lm</code> for “linear model”). The syntax is as simple as <code>lm(y ~ 1 + x, data=dframe)</code>, where <code>dframe</code> is the data frame containing our data, and <code>y ~ 1 + x</code> means to regress the variable <code>y</code> (i.e., the column <code>y</code> in the dataframe <code>dframe</code>) against the variable <code>x</code> and an intercept term (that’s the <code>1</code> in the model formula): <span class="math display">\[
y = \beta_1 x + \beta_0
\]</span></p>
<p>Note that the <code>1</code> in the model formula <code>y ~ 1 + x</code> is completely optional– R will include an intercept term automatically. Still, I recommend including it for the sake of clarity.</p>
<p>The function <code>lm</code> returns an object of the class <code>lm</code>. This is an object that contains a bunch of information about our fitted model. We’ll see some of that information below.</p>
<p>So in our case, we want to regress <code>aggr.assault.per.million</code> against <code>air.pb.metric.tons</code>. That is, we want a model like <span class="math display">\[
\text{agg.assault} = \beta_0 + \beta_1 \cdot \text{air.pb}
\]</span></p>
<p>So we’ll write that as <code>aggr.assault.per.million ~ 1+ air.pb.metric.tons</code>. Let’s try fitting the model and then we’ll ask R to summarize our model. Running <code>summary()</code> on the model object gives us a variety of useful summary statistics and other information about the fitted model.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>atlanta_lead_lm <span class="ot">&lt;-</span> <span class="fu">lm</span>(aggr.assault.per.million <span class="sc">~</span> <span class="dv">1</span> <span class="sc">+</span> air.pb.metric.tons,</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>                      <span class="at">data=</span>atlanta_lead)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(atlanta_lead_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, 
    data = atlanta_lead)

Residuals:
    Min      1Q  Median      3Q     Max 
-356.36  -84.55    6.89  122.93  382.88 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        107.94276   80.46409   1.342    0.189    
air.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 180.6 on 34 degrees of freedom
Multiple R-squared:  0.898, Adjusted R-squared:  0.895 
F-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>We’ll come back a little later to talk about some of these numbers in more detail. For now, let’s notice in particular that we have estimated coefficients, accessible in the <code>coefficients</code> attribute of our model object:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>atlanta_lead_lm<span class="sc">$</span>coefficients</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       (Intercept) air.pb.metric.tons 
        107.942757           1.403746 </code></pre>
</div>
</div>
<p>So our model predicts that in Atlanta, an increase of one metric ton of lead in the atmosphere is associated with an increase of about 1.4 aggravated assaults per million people. Similarly, the intercept indicates that our model predicts that in the absence of any lead in the atmosphere, there would be about 108 aggravated assaults per million people.</p>
</section>
<section id="working-with-lm-output-diagnostics" class="level2" data-number="25.9">
<h2 data-number="25.9" class="anchored" data-anchor-id="working-with-lm-output-diagnostics"><span class="header-section-number">25.9</span> Working with <code>lm()</code> output: diagnostics</h2>
<p>Let’s look at how our fitted model tracks the data by overlaying the fitted line on our scatterplot above. One way to do this would be to use <code>abline</code> with the entries of <code>atlanta_lead_lm$coefficients</code> to specify the slope and intercept, but <code>lm</code> is so common that <code>ggplot2</code> has this same basic functionality built-in, in the form of <code>geom_smooth()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>( atlanta_lead,</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x=</span>air.pb.metric.tons,<span class="at">y=</span>aggr.assault.per.million));</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The argument `se` specifies whether or not to include a</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence interval around the plotted line.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll talk about that later.</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For now we'll just suppress the CI with se=FALSE</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span><span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">"lm"</span>,</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">formula=</span><span class="st">"y~x"</span>,</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">se=</span><span class="cn">FALSE</span>);</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">x=</span><span class="st">"Lead levels (metric tons)"</span>,</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y=</span><span class="st">"Agg'd assaults (per million)"</span>,</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">title=</span><span class="st">"Violent crime and atmospheric lead (22 year lag)"</span> )</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Looks like a pretty good fit! Let’s look at some of the other information included in the output of <code>lm()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># gets fitted y-values (i.e., points on line of best fit)</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">fitted</span>(atlanta_lead_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        1         2         3         4         5         6         7         8 
 698.9200  710.1499  731.2061  749.4548  755.0698  745.2436  787.3560  836.4871 
        9        10        11        12        13        14        15        16 
 849.1208  990.8992 1038.6266 1096.1802 1118.6401 1169.1750 1183.2125 1187.4237 
       17        18        19        20        21        22        23        24 
1194.4424 1197.2499 1302.5309 1351.6620 1414.8306 1529.9378 1597.3176 1689.9649 
       25        26        27        28        29        30        31        32 
1757.3447 1782.6122 1852.7995 1879.4707 1991.7704 2000.1929 2101.2626 2185.4874 
       33        34        35        36 
2209.3511 2231.8110 2217.7735 2236.0222 </code></pre>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># residuals( model ) gets residuals (the difference between the</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="co"># observed response y and the response predicted by our model)</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">residuals</span>(atlanta_lead_lm)  <span class="co"># We can also use resid()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          1           2           3           4           5           6 
 330.080025  226.850054  155.793859 -216.454844  256.930171  102.756395 
          7           8           9          10          11          12 
-356.355996 -149.487118  382.879164 -273.899218 -268.626594 -280.180195 
         13          14          15          16          17          18 
 175.359863 -294.175006   23.787530  109.576291  -97.442440  -80.249933 
         19          20          21          22          23          24 
  -8.530910   45.337967  -43.830619  116.062179  -55.317646  -70.964906 
         25          26          27          28          29          30 
-129.344731   14.387834  -58.799484  143.529335  148.229626    9.807148 
         31          32          33          34          35          36 
 199.737410  -64.487372   16.648940   14.188998  -27.773538    3.977759 </code></pre>
</div>
</div>
<p>These residuals reflect the error between our model’s prediction and the true observations, and they are often quite informative.</p>
<p>Recall that our model assumes that the observation errors <span class="math inline">\(\epsilon_i\)</span> are normally distributed about zero, with a shared variance <span class="math inline">\(\sigma^2\)</span>. To check that this assumption is (approximately) true, we can plot the residuals:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>resids <span class="ot">&lt;-</span> <span class="fu">residuals</span>(atlanta_lead_lm)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(resids)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>That looks… okay, at any rate. The residuals are (approximately) symmetric about zero, and the histogram looks normal-ish to me. We’ll come back to this point, and later in your studies (e.g., if you take our department’s regression course) you’ll learn lots of ways for assessing model fit (e.g., checking if the normal errors assumption is correct), but for the time being, we’ll be satisfied with the “ocular inspection” method.</p>
<section id="homoscedasticity" class="level3" data-number="25.9.1">
<h3 data-number="25.9.1" class="anchored" data-anchor-id="homoscedasticity"><span class="header-section-number">25.9.1</span> Homoscedasticity</h3>
<p>Another important point, far more important that the normality assumption, is that the variance of the errors <span class="math inline">\(\epsilon_i\)</span> does not depend on the predictor <span class="math inline">\(X_i\)</span>. This is referred to as “homogeneity of variance”, more commonly called <a href="https://en.wikipedia.org/wiki/Homoscedasticity_and_heteroscedasticity">homoscedasticity</a>. Its absence, <em>heteroscedasticity</em>, wherein the variance of the error terms varies with <span class="math inline">\(X_i\)</span>, can be a big problem for linear regression.</p>
<p>So let’s check for it, just visually for now. We want to plot the residuals as a function of their predictor values. If our errors are homoscedastic, we should observe the variance of the residuals about the horizontal line <span class="math inline">\(y=0\)</span> to be more or less constant along the x-axis. R will do this for us automatically if we call <code>plot</code> on our model object. In fact, R will make several plots for us automatically, and return those plots in a list-like object. The residuals as a function of the <code>x</code> values is the first of these, and we can access it with the <code>which</code> keyword to <code>plot()</code>.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(atlanta_lead_lm, <span class="at">which=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>The red line is fitted by R; if our residuals are reasonably well-behaved, this line should be horizontal. Inspecting this plot, it looks as we suspected– the residuals for smaller lead atmospheric levels have slightly higher variance, and tend to be biased toward positive values. Still (and this is an intuition that you’ll develop as you perform more analyses), this doesn’t look especially extreme.</p>
</section>
<section id="assessing-normality-of-the-residuals" class="level3" data-number="25.9.2">
<h3 data-number="25.9.2" class="anchored" data-anchor-id="assessing-normality-of-the-residuals"><span class="header-section-number">25.9.2</span> Assessing normality of the residuals</h3>
<p>The stronger assumption, not required by linear regression per se, but a good assumption to check for use in downstream testing procedures (we’ll talk about that soon!), is that the errors are normal with mean zero. We saw that their histogram above looked pretty reasonable. A better check for fit is to construct a Q-Q plot (still no relation to the Chinese restaurant on University Ave, sadly).</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(atlanta_lead_lm, <span class="at">which=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Let’s recall that a <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">Q-Q-plot</a> displays the quantiles (i.e., the percentiles) of our observed data against the quantiles of the normal distribution. If our data were perfectly normally distributed, then the Q-Q plot would look like a straight line with slope 1 (up to randomness in the data, of course). If our data is not normal, the Q-Q plot will look far different.</p>
<p>Just to see an example of this, let’s generate some data from a t-distribution (which <em>looks</em> normal, but has “heavier tails”), and look at the Q-Q plot.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">rt</span>(<span class="at">n=</span><span class="dv">36</span>, <span class="at">df=</span><span class="dv">3</span>, <span class="at">ncp=</span><span class="dv">0</span>);</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="fu">hist</span>(data)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<div class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qqnorm</span>(data);</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Add a line to the plot to indicate the behavior we would</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="co"># expect to see if the data were normal.</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a><span class="fu">qqline</span>(data, <span class="at">col=</span><span class="st">'red'</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>So this is an example of the kind of behavior we would expect to see if our data were <em>not</em> well-described by a normal. Here’s our lead level data again.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We could also call qqnorm(atlanta_lead_lm$residuals)</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(atlanta_lead_lm, <span class="at">which=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>There are some slightly concerning spots there, especially in the bottom-left, but it’s not too extreme (in my opinion, anyway). Once again, later on you’ll learn more rigorous ways of checking model assumptions like this. We’re just trying to get an intuition for now.</p>
</section>
</section>
<section id="testing-and-confidence-intervals-for-coefficients" class="level2" data-number="25.10">
<h2 data-number="25.10" class="anchored" data-anchor-id="testing-and-confidence-intervals-for-coefficients"><span class="header-section-number">25.10</span> Testing and confidence intervals for coefficients</h2>
<p>So we’ve established that our model is a reasonably good fit for the lead data, at least in the sense that the trend in the data follows our plotted line and such.</p>
<p>Can we conclude from this that the association between lead and aggravated assault rate is “real”? It’s possible, after all, that the observed association is merely due to chance.</p>
<p>Well, in our discussions of hypothesis testing we saw a number of tools for checking if observations were merely due to chance or not. Linear regression has its own set of tools for checking whether observed coefficient estimates are “merely due to chance”.</p>
<p>Let’s look back at our model summary and let’s pay particular attention to the “coefficients” part of the output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(atlanta_lead_lm)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, 
    data = atlanta_lead)

Residuals:
    Min      1Q  Median      3Q     Max 
-356.36  -84.55    6.89  122.93  382.88 

Coefficients:
                    Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)        107.94276   80.46409   1.342    0.189    
air.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 180.6 on 34 degrees of freedom
Multiple R-squared:  0.898, Adjusted R-squared:  0.895 
F-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
<p>The <code>Coefficients</code> table includes our estimates for the coefficients, standard errors for those estimates, t-values (i.e., a test statistic) for those statistics and, at the far right of the table, a column headed <code>Pr[&gt;|t|]</code>. Hey… that’s a p-value!</p>
<p>Notice that in this case, the p-value associated to our estimate of the coefficient of lead levels is quite small. That indicates that an estimate this extreme (or more extreme) is highly unlikely to have arisen entirely by chance.</p>
<p>Now, it is in dealing with these p-values that we need to be a bit careful about our model assumptions. If the assumption of normal, homoscedastic errors is violated, things can go wrong with these p-values. But since our Q-Q plot indicated that our residuals were reasonably normal-looking, we can be somewhat confident that this is reflecting a real effect (well, and it’s pretty clear just from the plot that there’s a linear relationship…).</p>
<p>This p-value arises, in essence, from a <span class="math inline">\(t\)</span>-test. This t-test is designed to test the null hypothesis <span class="math display">\[
H_0 : \beta_1 = 0.
\]</span></p>
<p>In this case, our p-value associated with <span class="math inline">\(\beta_1\)</span> is quite small, indicating a correlation between lead levels and aggravated assault levels. It does <strong>not</strong> imply causation, though, as you well know by now. Nonetheless, this seems to be fairly convincing evidence that there is an association.</p>
<p>Thinking back to our brief discussion of the connection between confidence intervals and testing, you won’t be surprised to learn that we can also compute confidence intervals for the true value of the coefficients.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(atlanta_lead_lm, <span class="at">level=</span><span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        2.5 %     97.5 %
(Intercept)        -55.579958 271.465472
air.pb.metric.tons   1.238891   1.568602</code></pre>
</div>
</div>
<p>Of course, we also tested the hypothesis <span class="math inline">\(H_0 : \beta_0 = 0\)</span>, and the p-value is not especially small (also reflected in the fact that the confidence interval includes <span class="math inline">\(0\)</span>). This indicates that our intercept term was not statistically significantly different from zero.</p>
<p>Note, however, that just because our p-value associated to the intercept term isn’t especially small, that doesn’t mean that the intercept isn’t useful for prediction. Let’s turn our attention to that matter.</p>
</section>
<section id="making-predictions" class="level2" data-number="25.11">
<h2 data-number="25.11" class="anchored" data-anchor-id="making-predictions"><span class="header-section-number">25.11</span> Making predictions</h2>
<p>Suppose that tomorrow a chemical company near Atlanta has an incident, and lead is released into the atmosphere. Suppose that the new atmospheric levels of lead are found to be 1300 metric tons. What would you predict the approximate aggravated assault rate to be 22 years later?</p>
<p>Well, let’s start by just looking at a plot of our model again.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>( atlanta_lead,</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x=</span>air.pb.metric.tons,<span class="at">y=</span>aggr.assault.per.million));</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The argument `se` specifies whether or not to include a</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence interval around the plotted line.</span></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll talk about that later.</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For now we'll just suppress the CI with se=FALSE</span></span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span><span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">"lm"</span>,</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">formula=</span><span class="st">"y~x"</span>,</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">se=</span><span class="cn">FALSE</span>);</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">x=</span><span class="st">"Lead levels (metric tons)"</span>,</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y=</span><span class="st">"Agg'd assaults (per million)"</span>,</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">title=</span><span class="st">"Violent crime and atmospheric lead (22 year lag)"</span> )</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>Just looking at the plot, we see that at x-value 1300, our fitted line is just about exactly at 2000 aggravated assaults per million people.</p>
<p>Hopefully you can imagine how annoying it would be to perform this exercise by hand every time we need a new prediction. Luckily, R model objects (including linear regression) support a function called <code>predict</code>, which does exactly what it sounds like. We pass our model, and some data (i.e., x-values), and <code>predict()</code> outputs our model’s predicted responses at those values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(atlanta_lead_lm, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">air.pb.metric.tons=</span><span class="dv">1300</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>       1 
1932.813 </code></pre>
</div>
</div>
<p>Suppose the company continues to release more lead into the atmosphere, and next year, the levels are measured to be 2000 metric tons. Can we use our model to predict what aggravated assault rates might look like 22 years later?</p>
<p>Well, looking at the plot again, 2000 metric tons is rather far outside the range of our observed predictor values.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> <span class="fu">ggplot</span>( atlanta_lead,</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>              <span class="fu">aes</span>(<span class="at">x=</span>air.pb.metric.tons,<span class="at">y=</span>aggr.assault.per.million));</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="co"># The argument `se` specifies whether or not to include a</span></span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a><span class="co"># confidence interval around the plotted line.</span></span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a><span class="co"># We'll talk about that later.</span></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a><span class="co"># For now we'll just suppress the CI with se=FALSE</span></span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span><span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">geom_smooth</span>(<span class="at">method=</span><span class="st">"lm"</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">formula=</span><span class="st">"y~x"</span>,</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>                                     <span class="at">se=</span><span class="cn">FALSE</span>);</span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>pp <span class="ot">&lt;-</span> pp <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">x=</span><span class="st">"Lead levels (metric tons)"</span>,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>                 <span class="at">y=</span><span class="st">"Agg'd assaults (per million)"</span>,</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>                 <span class="at">title=</span><span class="st">"Violent crime and atmospheric lead (22 year lag)"</span> )</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>pp</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>As we’ve alluded to earlier in these lecture notes, predictions made far outside the range of observed predictors have to be treated carefully They may be reliable, but they also may not. The reliability of a prediction usually decreases the further away it is from your data.</p>
<p>For example, perhaps once lead levels reach a certain point, there just isn’t much more damage they can do to human development. Then we would see the linear trend flatten out at higher lead levels. Our function would cease to be linear, and naively applying our linear models to those values would result in poor prediction performance.</p>
<p>Just to drive the point home, here’s a cautionary tale:</p>
<center>
<a href="https://xkcd.com/1007/"><img id="comic" src="https://imgs.xkcd.com/comics/sustainable.png" title="Though 100 years is longer than a lot of our resources." width="70%"></a>
</center>
<p>|t|)<br>
(Intercept) 17.965050 0.849663 21.144 &lt; 2e-16 <strong><em> disp -0.006622 0.004166 -1.590 0.12317<br>
hp -0.022953 0.004603 -4.986 2.88e-05 </em></strong> wt 1.485283 0.429172 3.461 0.00175 ** — Signif. codes: 0 ‘<em><strong>’ 0.001 ’</strong>’ 0.01 ’</em>’ 0.05 ‘.’ 0.1 ’ ’ 1</p>
<p>Residual standard error: 1.062 on 28 degrees of freedom Multiple R-squared: 0.6808, Adjusted R-squared: 0.6466 F-statistic: 19.91 on 3 and 28 DF, p-value: 4.134e-07</p>
<pre><code>:::
:::


Once again, before we get too eager about interpreting the model, we should check that our residuals are reasonable.


::: {.cell}

```{.r .cell-code}
hist(mtc_model$residuals)</code></pre>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid" width="672"></p>
</div>
<p>:::</p>
<p>Hmm… that isn’t amazing, but on the other hand, there aren’t very many observations to begin with, so we shouldn’t expect a particularly normal-looking histogram.</p>
<p>Checking heteroscedasticity isn’t so easy now, but we can still do things like compare the residuals with a normal via a Q-Q plot:</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(mtc_model, <span class="at">which=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<p><img src="prediction1_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p>In my opinion, this Q-Q plot would likely lead me to question the assumption of normal errors. That doesn’t mean that we can’t proceed with using our linear model, but it will mean that we should be a bit careful with how much credence we give to any quantities that depend on our normality assumption (e.g., our p-values).</p>
<p>Let’s press on regardless, for now, mostly for the sake of demonstration of what we <em>would</em> do, if we were reasonably happy with our model assumptions. Still, we should bear in the back of our minds that perhaps our normality assumptions perhaps aren’t exactly true.</p>
<p>Let’s return to our model output.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mtc_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8121 -0.3125 -0.0245  0.3544  3.3693 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***
disp        -0.006622   0.004166  -1.590  0.12317    
hp          -0.022953   0.004603  -4.986 2.88e-05 ***
wt           1.485283   0.429172   3.461  0.00175 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.062 on 28 degrees of freedom
Multiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 
F-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07</code></pre>
</div>
</div>
<p>Let’s start at the bottom. The residual standard error (RSE) is listed as being 1.062. This is related to the sum of squared errors we discussed earlier, and it’s exactly what it sounds like– the standard error of the model’s residuals. Ideally, we want this number to be small– after all, it measures the error in our model. We’ll come back to this matter later in the semester.</p>
<p>The output lists this as being the residual standard error on 28 degrees of freedom. Remember that as a rule of thumb, the degrees of freedom will be the number of data points less the number of parameters we estimate. In this case, there are 32 data points</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(mtcars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 32</code></pre>
</div>
</div>
<p>and our model has four parameters: the intercept and our three predictors’ coefficients, so 28 degrees of freedom checks out!</p>
<p>Much more on that whole matter soon, but let’s keep looking at that model summary.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mtc_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8121 -0.3125 -0.0245  0.3544  3.3693 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***
disp        -0.006622   0.004166  -1.590  0.12317    
hp          -0.022953   0.004603  -4.986 2.88e-05 ***
wt           1.485283   0.429172   3.461  0.00175 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.062 on 28 degrees of freedom
Multiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 
F-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07</code></pre>
</div>
</div>
<p>The F-statistic associated with our residuals has a very small p-value associated to it, indicating, in essence, that our model fit is much better than would be expected by chance. You’ll learn a lot more about the F-distribution when you learn about analysis of variance (ANOVA) in a later class.</p>
<p>Scanning our way up the model summary, let’s look at the table of coefficient estimates. We see that our intercept term and the coefficients for horsepower (<code>hp</code>) and weight (<code>wt</code>) are flagged as being significant. Thus, briefly putting on our testing hats, we would reject the null hypotheses <span class="math inline">\(H_0 : \beta_0=0\)</span>, <span class="math inline">\(H_0: \beta_{\text{hp}} = 0\)</span> and <span class="math inline">\(H_0: \beta_{\text{wt}} = 0\)</span>. On the other hand, there is insufficient evidence to reject the null <span class="math inline">\(H_0 : \beta_{\text{wt}} = 0\)</span>.</p>
<p>In other words, it appears that horsepower and weight are associated with changes in quarter-mile time, but displacement is not.</p>
</section>
<section id="assessing-model-fit" class="level2" data-number="25.12">
<h2 data-number="25.12" class="anchored" data-anchor-id="assessing-model-fit"><span class="header-section-number">25.12</span> Assessing model fit</h2>
<p>Once we’ve fit a model to the data, how do we tell if our model is good or not? We started talking about this above, and it is a trickier question that it might seem at first. We’ll have lots more to say about the problem in coming weeks. For now, though, let’s consider the most obvious answer to this question.</p>
<p>We fit our model to data by minimizing the sum of squares (we’re sticking with simple linear regression here for simplicity– this idea extends to multiple linear regression in the obvious way), <span class="math display">\[
\ell( \beta_0, \beta_1 )
= \sum_{i=1}^n (y_i - \hat{y}_i)^2
= \sum_{i=1}^n \left( y_i - (\beta_0 + \beta_1 x_i) \right)^2.
\]</span></p>
<p>So what better way to measure how good our model is than using precisely this quantity? We define the residual sum of squares (RSS; also called the sum of squared errors, SSE) to be the sum of squared errors of our model. That is, letting <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> be our estimates of the coefficients, <span class="math display">\[
\operatorname{RSS}
=
\operatorname{SSE}
= \sum_{i=1}^n \epsilon_i^2 = \sum_{i=1}^n (y_i - \hat{y}_i)^2 = \sum_{i=1}^n \left(y_i - (\hat{beta}_0 + \hat{\beta}_1 x_i) \right)^2
\]</span></p>
<p>The number of degrees of freedom will have bearing on the distribution of this error term- Uunder the model where the errors are indeed normally distributed, the residual sum of squares (RSS) will have an <a href="https://en.wikipedia.org/wiki/F-distribution">F-distribution</a> with degrees of freedom given by the number of observations minus the number of parameters (like the t-distribution, the F-distribution has the degrees of freedom as one of its parameters; the other, the 3 in the summary above, comes from the number of coefficients less one). Knowing this fact lets us build a rejection region for using the RSS as a test statistic, and that’s exactly where the overall p-value at the bottom of the summary comes from.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mtc_model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8121 -0.3125 -0.0245  0.3544  3.3693 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***
disp        -0.006622   0.004166  -1.590  0.12317    
hp          -0.022953   0.004603  -4.986 2.88e-05 ***
wt           1.485283   0.429172   3.461  0.00175 ** 
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.062 on 28 degrees of freedom
Multiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 
F-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07</code></pre>
</div>
</div>
<p>Another useful quantity in describing how well our model describes the data is the <a href="https://en.wikipedia.org/wiki/Coefficient_of_determination"><strong>Coefficient of Determination</strong></a>, or <span class="math inline">\(R\)</span>-squared, which can be interpreted as measuring the proportion (between 0 and 1) of the variation in <span class="math inline">\(Y\)</span> that is explained by the variation in <span class="math inline">\(X\)</span>.</p>
<p><span class="math display">\[
R^{2}=\frac{\text{TSS}-\text{RSS}}{\text{TSS}}=1-\frac{\text{RSS}}{\text{TSS}},
\]</span></p>
<p>where <span class="math display">\[
\operatorname{TSS}=\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}
\]</span></p>
<p>is the <strong>total sum of squares</strong>.</p>
<p>In the case of simple linear regression, things simplify so that <span class="math inline">\(R^{2}=r^{2}\)</span>, where <span class="math inline">\(r\)</span> is the correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>,</p>
<p><span class="math display">\[
r
=\frac{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)\left(y_{i}-\bar{y}\right)}
{\sqrt{\sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}} \sqrt{\sum_{i=1}^{n}\left(y_{i}-\bar{y}\right)^{2}}}
\]</span></p>
<p>When this quantity is close to 1, we can be confident that our linear model is accurately capturing a trend in the data.</p>
</section>
<section id="looking-ahead-model-selection" class="level2" data-number="25.13">
<h2 data-number="25.13" class="anchored" data-anchor-id="looking-ahead-model-selection"><span class="header-section-number">25.13</span> Looking ahead: model selection</h2>
<p>One important point that we’ve ignored in our discussion above is how we go about choosing what predictors to include in our model. For example, the <code>mtcars</code> data set has columns</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a><span class="fu">names</span>(mtcars)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] "mpg"  "cyl"  "disp" "hp"   "drat" "wt"   "qsec" "vs"   "am"   "gear"
[11] "carb"</code></pre>
</div>
</div>
<p>In our example above, we just chose a few of these to use as predictors. But suppose that we didn’t know ahead of time which predictors to use. How do we choose which ones to include in our model? Are there downsides to just including all of them?</p>
</section>
<section id="review" class="level2" data-number="25.14">
<h2 data-number="25.14" class="anchored" data-anchor-id="review"><span class="header-section-number">25.14</span> Review</h2>
<ul>
<li>The simple linear regression model</li>
<li>predictor / response variables</li>
<li>assumptions of SLR model</li>
<li>interpretation of SLR components</li>
<li>correlation is not causation</li>
<li>residuals</li>
<li>loss function of OLS: Sum of squared residuals</li>
<li>slope estimate formula and interpretation</li>
<li>intercept estimate formula and interpretation</li>
<li>units of <span class="math inline">\(\beta\)</span> estimates</li>
<li>mean squared error, estimate of <span class="math inline">\(\sigma^2_\epsilon\)</span></li>
<li>variance of <span class="math inline">\(\beta\)</span> estimates</li>
<li>OLS in R, and summary output</li>
<li>OLS diagnostics (residual plot, residual QQ plot)</li>
<li>inference and confidence intervals for coefficients</li>
<li>predictions &amp; prediction intervals</li>
<li><span class="math inline">\(R^2\)</span> and <span class="math inline">\(r\)</span></li>
</ul>


</section>
<section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes">
<hr>
<ol>
<li id="fn1"><p>https://ir.lawnet.fordham.edu/ulj/vol20/iss3/1<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>https://doi.org/10.1016/j.envres.2007.02.008<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./estimation2_practice.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Interval Estimation Practice</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./R08_prediction_examples.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Simple Linear Regression - Examples</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>