[
  {
    "objectID": "index.html#collection-of-shiny-apps",
    "href": "index.html#collection-of-shiny-apps",
    "title": "STAT 340: Data Science II",
    "section": "",
    "text": "Monte Carlo: Normal PDF CDF Inverse CDF \nTesting Part 2: Binomial One/Two Tail Test Power\nLogistic Regression: ROC Plot\nModel Selection: Visualization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>STAT 340</span>"
    ]
  },
  {
    "objectID": "rv_practice.html",
    "href": "rv_practice.html",
    "title": "5  Probability and Random Variables Practice",
    "section": "",
    "text": "6 Practice Problems",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#phone-repair",
    "href": "rv_practice.html#phone-repair",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.1 Phone Repair",
    "text": "6.1 Phone Repair\n(From Introduction to Probability and Statistics for Data Science) The repair of a broken cell phone is either completed on time or it is late and the repair is either done satisfactorily or unsatisfactorily. What is the sample space for a cell phone repair?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample space consists of two outcomes: {satisfactory, unsatisfactory}",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-i",
    "href": "rv_practice.html#dice-probabilities-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.2 Dice Probabilities I",
    "text": "6.2 Dice Probabilities I\nWhat is the probability that at least one of the following events occurs on a single throw of two fair six-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability that at least one of the following occurs requires us to look at the union of the events. “The dice total is 5, 6 or 7”.\nOn a single throw of two fair dice, the dice total can be a single number, so events A, B and C are mutually exclusive. We can calculate the probabilities individually and add them up.\nA 5 can occur in 4 ways: \\((1,4), (2,3), (3,2), (4,1)\\) are the four ways (we are distinguishinng the two dice).\nA 6 can occur in 5 ways: \\((1,5), (2,4), (3,3), (4,2), (5,1)\\).\nA 7 can occur in 6 ways: \\((1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\\)\nIn the end, there are \\(4+5+6 = 15\\) ways in which one of these events can occur. Since there are \\(6\\times 6=36\\) two-dice outcomes, the probability is \\[\\frac{15}{36}\\]",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-ii",
    "href": "rv_practice.html#dice-probabilities-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.3 Dice Probabilities II",
    "text": "6.3 Dice Probabilities II\nWhat is the probability that at least one of the following events occurs on a single throw of two fair four-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA 5 can occur in 4 ways: \\((1,4), (2,3), (3,2), (4,1)\\) are the four ways (we are distinguishing the two dice).\nA 6 can occur in 3 ways: \\((2,4), (3,3), (4,2)\\).\nA 7 can occur in 2 ways: \\((3,4), (4,3)\\)\nIn the end, there are \\(4+3+2 = 9\\) ways in which one of these events can occur. Since there are \\(4\\times 4=16\\) two-dice outcomes, the probability is \\[\\frac{9}{16}\\]",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#using-wikipedia-in-college",
    "href": "rv_practice.html#using-wikipedia-in-college",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 Using Wikipedia in college",
    "text": "6.4 Using Wikipedia in college\nA recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \\(X\\) equal the number of students in a random sample of size \\(n = 31\\) who have used Wikipedia as a source.\n\nHow is \\(X\\) distributed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming independence in sampling, and a representative sample, we can use a Binomial distribution with \\(n=31\\) and \\(p=0.447\\).\n\n\n\n\nSketch the probability mass function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(dbinom(0:31, 31,.447), names=0:31, ylab=\"probability\", main=\"PMF of Binomial(31,.447)\")\n\n\n\n\n\n\n\n\nSketch the cumulative distribution function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(pbinom(0:31, 31, .447), type=\"s\", ylab=\"cumulative prob.\", main=\"CDF of Binomial(31, .447)\")\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\) is equal to 17.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(17, 31, .447)\n\n[1] 0.07532248\n\n\n\n\n\n\nFind the probability that \\(X\\) is at most 13.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npbinom(13, 31, .447)\n\n[1] 0.451357\n\n\n\n\n\n\nFind the probability that \\(X\\) is bigger than 11.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(12:31, 31, .447))\n\n[1] 0.8020339\n\n#or\npbinom(11, 31, .447, lower.tail=FALSE)\n\n[1] 0.8020339\n\n#or\n1-pbinom(11, 31, .447)\n\n[1] 0.8020339\n\n\n\n\n\n\nFind the probability that \\(X\\) is at least 15.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X at least 15)\nsum(dbinom(15:31,31,.447))\n\n[1] 0.406024\n\n\n\n\n\n\nFind the probability that \\(X\\) is between 16 and 19, inclusive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(16:19, 31, .447))\n\n[1] 0.2544758\n\n\n\n\n\n\nGive the mean of \\(X\\), denoted \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(X)=n*p\n31*.447\n\n[1] 13.857\n\n#or you can also do this (but it's too much work)\nsum( (0:31) * dbinom(0:31, 31, .447))\n\n[1] 13.857\n\n\n\n\n\n\nGive the variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Var(X) = n * p * (1-p)\n31 * .447 * (1-.447)\n\n[1] 7.662921\n\n#or - if you want (but why would you want to?)\nsum((0:31 - 31*.447)^2 * dbinom(0:31, 31, .447))\n\n[1] 7.662921\n\n\n\n\n\n\nGive the standard deviation of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#SD(X) = sqrt(n*p*(1-p))\nsqrt(31*.447*(1-.447))\n\n[1] 2.768198\n\n\n\n\n\n\nFind \\(\\mathbb{E}(4X+51.324)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(4X+51.324) = 4*E(X)+51.324\n4*(31*.447) + 51.324\n\n[1] 106.752"
  },
  {
    "objectID": "rv_practice.html#choose-the-distribution",
    "href": "rv_practice.html#choose-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 Choose the distribution",
    "text": "6.5 Choose the distribution\n\nFor the following situations, decide what the distribution of \\(X\\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)\n\n\nWe shoot basketballs at a basketball hoop, and count the number of shots until we make a basket. Let X denote the number of missed shots. On a normal day we would typically make about 37% of the shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of missed shots before the first basket, assuming independence, can be modeled by a Geometric random variable with parameter \\(p=.37\\).\n\n\n\n\nIn a local lottery in which a three digit number is selected randomly, let X be the number selected.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming that all 3 digit numbers are equally likely (A reasonable assumption) the number selected can be modeled by a discrete uniform distribution with minimum 100 and maximum 999.\n\n\n\n\nWe drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let X be the number of times the cup lands perfectly right side up.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we drop the cup 20 times, and the result each time is independent with a constant probability of landing right side up, the number of times it does can be modeled by a Binomial random variable with parameters \\(n=20\\) and \\(p\\) (unknown).\n\n\n\n\nWe toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let X denote the number of missed shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGeometric random variable (unknown parameter value for \\(p\\)).\n\n\n\n\nWorking for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let X count the number of containers that illegally contain contraband.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTechnically we should use a hypergeometric random variable for this situation (since it is a small population size of 557), but since we do not cover the hypergeometric the closest random variable we have is the binomial.\n\n\n\n\nAt the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let X denote the number of birds in the bush.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a discrete random variable, but without other information it’s hard to say. The distribution is likely unimodal and bell-curved. You could probably model this using a normal distribution rounded off to the nearest integer.\n\n\n\n\nWe count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe observation window is the circular area, and the 10 minutes during observation. Assuming the rate of rainfall is constant, the number of raindrops in the circle can be modeled using a Poisson random variable.\n\n\n\n\nWe count the number of moth eggs on our window screen.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCounting indicates a discrete random variable. A binomial or a rounded normal distribution may be appropriate, but we lack enough details to be sure.\n\n\n\n\nWe count the number of blades of grass in a one square foot patch of land.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe location of the sprouting grass could be modeled well by a Poisson random variable - the \\(\\lambda\\) parameter would likely be very large, in the range of 1000 or 10000, and as such the distribution would look very much like a normal distribution.\n\n\n\n\nWe count the number of pats on a baby’s back until (s)he burps.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we define a geometric random variable, we let \\(X\\) be the number of failures before the first success. The last pat (that causes the burp) is the success in this context. So we could use a geometric random variable, but we would have to add 1 to it in order to count all burps (the failures + 1 success)."
  },
  {
    "objectID": "rv_practice.html#valid-pmf",
    "href": "rv_practice.html#valid-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.19 Valid PMF",
    "text": "6.19 Valid PMF\n(From Introduction to Probability and Statistics for Data Science) Let \\(X\\) have pmf \\[p(x)=\\begin{cases}0.2,&x=1\\\\0.3,&x=2\\\\0.5,&x=4\\\\0,&\\text{otherwise}\\end{cases}\\] Show that this is a valid pmf.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-dice-pmf",
    "href": "rv_practice.html#two-dice-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.20 Two Dice PMF",
    "text": "6.20 Two Dice PMF\nLet \\(X\\) denote the sum of two fair, six-sided dice. Specify the domain that \\(X\\) can take on and then write out the equation for the PMF and show that it is valid.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#a-uniform-pmf",
    "href": "rv_practice.html#a-uniform-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 A Uniform PMF",
    "text": "6.8 A Uniform PMF\nLet \\(X\\) have discrete uniform PMF on the values \\(x\\in\\left\\{-1,0,1\\right\\}\\).\n\nWrite the equation for its PMF.\nFind \\(Pr[X&lt;-1]\\) and \\(Pr[X\\leq -1]\\)\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X \\geq 0]\\)\nCalculate the CDF from the PDF. Write out an expression for \\(F(x)\\) and plot the PMF and CDF. ## PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that."
  },
  {
    "objectID": "rv_practice.html#dice-difference",
    "href": "rv_practice.html#dice-difference",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.22 Dice Difference",
    "text": "6.22 Dice Difference\nTwo dice are tossed. Let \\(X\\) be the absolute difference in the number of dots facing up.\n\n\nFind and plot the pmf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#It may be simplest to calculate all possible values of X. \nx &lt;- vector(\"numeric\")\nfor(i in 1:6){\n  for(j in 1:6){\nx[length(x)+1] = abs(i-j)\n  }\n}\n#now that we have all equally likely values, we can just calculate the pmf in a prop.table\npX &lt;- prop.table(table(x))\n#And create a barplot.\nbarplot(pX)\n\n\n\n\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\leq 2\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The probability that X &lt;= 2 is easy to find using the pmf\n#The columns are named with strings, so we can convert 0 1 and 2 to strings to pull out the proper probabilities.\nsum(pX[as.character(0:2)])\n\n[1] 0.6666667\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\) and \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The expected value and variance can be calculated from the pmf.\n\n(EX &lt;- sum(0:5 * pX))\n\n[1] 1.944444\n\n(VarX &lt;- sum((0:5 - EX)^2 * pX))\n\n[1] 2.052469\n\n#or by taking the mean and population variance of the x values themselves\nmean(x)\n\n[1] 1.944444\n\nmean((x-mean(x))^2)\n\n[1] 2.052469",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#voltage-random-variable",
    "href": "rv_practice.html#voltage-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.23 Voltage random variable",
    "text": "6.23 Voltage random variable\nA modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term that is drawn from the set \\(\\{0,-1,-2,-3\\}\\) with respective probabilities \\(\\{4/10, 3/10, 2/10, 1/10\\}\\).\n\n\nFind the pmf of the output \\(Y\\) of the channel.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Let X be the noise\nk &lt;- seq(0,-3)\npk &lt;- c(4/10, 3/10, 2/10, 1/10)\n\ny &lt;- sort(2+k)\npy &lt;- pk[order(2+k)]\n\nbarplot(height=py, names=y, main=\"pmf of Y\")\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability that the channel’s output is equal to the input of the channel?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis happens when there’s no noise, with probability 4/10.\n\n\n\n\nWhat is the probability that the channel’s output is positive?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Interpreting 'positive' to be strictly positive, not zero:\nsum(py[y&gt;0])\n\n[1] 0.7\n\n\n\n\n\n\nFind the expected value and variance of \\(Y\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n(EY &lt;- sum(y*py))\n\n[1] 1\n\n(VarY &lt;- sum((y-EY)^2*py))\n\n[1] 1",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#golf-score",
    "href": "rv_practice.html#golf-score",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.24 Golf Score",
    "text": "6.24 Golf Score\nOn a given day, your golf score takes values from the numbers 1 through 10 with equal probability of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let \\(X_1, X_2\\) and \\(X_3\\) be the scores that you get, and let \\(X\\) be the minimum of these three scores.\n\n\nShow that for any discrete random variable \\(X\\) \\(p_X(k)=\\mathbb{P}(X &gt; k-1) - \\mathbb{P}(X&gt;k)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k-1) = P(X \\geq k) =P(X = k)+P(X &gt; k)\\), thus \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)\\).\n\n\n\n\nWhat is the probability that \\(\\mathbb{P}(X_1&gt;k)\\) for \\(k=1,\\ldots,10\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X_1&gt;k)=\\frac{10-k}{10}\\)\n\n\n\n\nUse (a) to determine the pmf \\(p_X(k)\\) for \\(k=1,\\ldots,10\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k) = P(X_1,X_2,X_3 &gt; k) = P(X_1 &gt; k)P(X_2&gt;k)P(X_3&gt;k)\\)\nThis means \\(P(X&gt;k) =\\frac{(10-k)^3}{10^3}\\), and \\(P(X&gt;k-1)=\\frac{(11-k)^3}{10^3}\\). From the previous result, \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)=\\frac{(11-k)^3-(10-k)^3}{10^3}\\)\n\n\n\n\nWhat is the average score improvement if you play just for one day compared with playing for three days and taking the minimum?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is asking to take the difference of the two expected values. It’s obvious that \\(E(X_1)=5.5\\); We need to find the expected value of \\(X\\).\n\nx &lt;- 1:10\npx &lt;- ((11-x)^3-(10-x)^3)/10^3\n#double check\nsum(px)\n\n[1] 1\n\n(EX &lt;- sum(x*px))\n\n[1] 3.025\n\n5.5-EX\n\n[1] 2.475\n\n\nThe average (expected) point improvement when going from a 1 day point to a minimum of 3 days is 2.475.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#functions-of-a-random-variable",
    "href": "rv_practice.html#functions-of-a-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.25 Functions of a random variable",
    "text": "6.25 Functions of a random variable\nLet \\(g(X) = \\begin{cases}1 & \\text{if }X&gt;10\\\\0 & \\text{otherwise}\\end{cases}\\) and \\(h(X) = \\begin{cases}X-10 & \\text{if }X-10&gt;0\\\\0 & \\text{otherwise}\\end{cases}\\)\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_k = p_1/k\\), find \\(\\mathbb{E}\\left[g(X)\\right]\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nk &lt;- 1:15\np1 &lt;- 1/(sum(1/k))\npk &lt;- p1/k\n\ng &lt;- function(x){\n  return(as.numeric(x&gt;10))\n}\n\nsum(g(k)*pk)\n\n[1] 0.1173098\n\n\n\n\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_{k+1} = p_k/2\\) (for \\(k&gt;1\\)), find \\(\\mathbb{E}\\left[h(X)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nh &lt;- function(x){\n  return(max(0, x-10))\n}\n\np1 &lt;- 1/(sum(1/2^(k-1)))\npk &lt;- p1*(1/2^(k-1))\n\nsum(h(k)*pk)\n\n[1] 5",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#voltage-ii",
    "href": "rv_practice.html#voltage-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.26 Voltage II",
    "text": "6.26 Voltage II\nA voltage \\(X\\) is uniformly distributed on the set \\(\\{-3,\\ldots,3,4\\}\\).\n\n\nFind the mean and variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- -3:4\npx &lt;- 1/length(x)\n\n(EX &lt;- sum(x*px))\n\n[1] 0.5\n\n(VarX &lt;- sum((x-EX)^2*px))\n\n[1] 5.25\n\n\n\n\n\n\nFind the mean and variance of \\(Y=-2X^2+3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny &lt;- -2*x^2+3\n\n(EY &lt;- sum(y*px))\n\n[1] -8\n\n(VarY &lt;- sum((y-EY)^2*px))\n\n[1] 105\n\n\n\n\n\n\nFind the mean and variance of \\(W=\\text{cos}(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- cos(pi*x/8)\n(EW &lt;- sum(w*px))\n\n[1] 0.6284174\n\n(VarW &lt;- sum((w-EW)^2*px))\n\n[1] 0.1050915\n\n\n\n\n\n\nFind the mean and variance of \\(Z=\\text{cos}^2(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz &lt;- w^2\n(EZ &lt;- sum(z*px))\n\n[1] 0.5\n\n(VarZ &lt;- sum((z-EZ)^2*px))\n\n[1] 0.125",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#discrete-random-variable-problems",
    "href": "rv_practice.html#discrete-random-variable-problems",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 Discrete Random Variable Problems",
    "text": "6.14 Discrete Random Variable Problems\n\n\nIf \\(X\\) is \\(\\text{Poisson}(\\lambda)\\), compute \\(\\mathbb{E}\\left[1/(X+1)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be handled mathematically. The formula for \\(E(1/(X+1))\\) is\n\\(E(1/(X+1))=\\sum_{x=0}^{\\infty}\\frac{1}{x+1}\\frac{\\lambda^{x}}{x!}e^{-\\lambda}=\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x}}{(x+1)!}e^{-\\lambda}\\)\nThe trick is to get get the summation to equal 1 and simplify. We multiply by \\(\\lambda/\\lambda\\)\n\\(E(1/(X+1))=\\frac{1}{\\lambda}\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x+1}}{(x+1)!}e^{-\\lambda}\\)\nNow we can make a change of variables: \\(y=x+1\\) and thus \\(x=0\\) becomes \\(y=1\\)\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}\\sum_{y=1}^{\\infty}\\frac{\\lambda^{y}}{y!}e^{-\\lambda}\\)\nThe only thing missing is that the summation starts at \\(y=1\\) instead of \\(y=0\\), But for \\(Y \\sim Poisson(\\lambda)\\), \\(P(Y=0)=e^{-\\lambda}\\) so this summation is \\(1-e^{-\\lambda}\\).\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}(1-e^{-\\lambda})\\)\n\n\n\n\nIf \\(X\\) is \\(\\text{Bernoulli}(p)\\) and \\(Y\\) is \\(\\text{Bernoulli}(q)\\), computer \\(\\mathbb{E}\\left[(X+Y)^3\\right]\\) assuming \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\((X+Y)^3 = X^3+3X^2Y+3XY^2+Y^3\\) so \\(E[(X+Y)^3]=E(X^3)+3E(X^2)E(Y)+3E(X)E(Y^2)+E(Y^2)\\)\nthis is due to independence. Since \\(X\\) an \\(Y\\) are independent, so are \\(X^2\\) and \\(Y\\), and \\(X\\) and \\(Y^2\\). \\(E(X)=E(X^2)=E(X^3)=p\\) and \\(E(Y)=E(Y^2)=E(Y^3)=q\\). Thus \\(E[(X+Y)^3]=p+6pq+q\\)\n\n\n\n\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\Delta(\\theta)=\\mathbb{E}\\left[(X-\\theta)^2\\right]\\). Find \\(\\theta\\) that minimizes the error \\(\\Delta(\\theta)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can expand the expected value and attempt to find the minimum with respect to \\(\\theta\\). \\(E[(X-\\theta)^2]=E[X^2-2\\theta X+\\theta^2]=E(X^2)-2\\theta\\mu+\\theta^2\\). Recall that \\(Var(X)=E(X^2)-\\mu^2\\) so \\(E(X^2)=\\sigma^2+\\mu^2\\) So we can write \\(\\Delta(\\theta)=\\sigma^2 + \\mu^2-2\\theta\\mu + \\theta^2\\) We want to find what value of \\(\\theta\\) minimizes this function - derivative! \\(\\Delta'(\\theta)=-2\\mu+2\\theta=0\\) thus \\(\\theta=\\mu\\) minimizes this.\n\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) are independent uniform random variables in \\(\\{0,1,\\ldots,100\\}\\). Evaluate \\(\\mathbb{P}\\left[\\text{min}(X_1,\\ldots, X_n) &gt; l\\right]\\) for any \\(l \\in \\{0,1,\\ldots,100\\}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(Y=\\min(X_1, \\ldots, X_n)\\) If \\(P(Y &gt;l)\\), that means the minimum exceeds \\(l\\), so all of the values \\(&gt;l\\). \\(P(X_1 &gt; l)=(100-l)/101\\) - you can check: \\(P(X_1&gt;0)=100/101\\). This is the same calculation for each \\(i\\). So \\(P(Y&gt;l)=\\dfrac{(100-l)^n}{101^n}\\).\n\n\n\n\nConsider a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\). \\(p_X(k)={n \\choose k} p^k(1-p)^{n-k}\\). Show that the mean is \\(\\mathbb{E}X= np\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(X)=\\sum_{k=0}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nThe first term is zero so we could write\n\\(\\sum_{k=1}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nNow the following is a fact that is needed but perhaps not well known. It’s the equivalence of \\(k{n \\choose k}=n{n-1 \\choose k-1}\\). We make this subsitution\n\\(\\sum_{k=1}^n n{n-1 \\choose k-1} p^k(1-p)^{n-k}=np\\sum_{k=1}^n {n-1\\choose k-1}p^{k-1}(1-p)^{n-k}\\)\nWe could write \\(n-k=(n-1)-(k-1)\\) and we’ll be making some substitutions: \\(m=n-1\\) and \\(j=k-1\\). This lets us write\n\\(np\\sum_{j=0}^m {m \\choose j}p^j(1-p)^{m-j}=np\\) because the summation =1, as it is just the sum of the pmf of a binomial.\n\n\n\n\n(not for 340) Consider a geometric random variable \\(X\\) with parameter \\(p\\). \\(p_X(k)=p(1-p)^k\\) for \\(k=0,1,\\ldots\\). Show that its mean is \\(\\mathbb{E}X=(1-p)/p\\).\n(not for 340) Consider a Poisson random variable \\(X\\) with parameter \\(\\lambda\\). \\(p_X(k)=\\dfrac{\\lambda^k}{k!}e^{-\\lambda}\\). Show that \\(\\text{Var}X=\\lambda\\).\n(not for 340) Consider the uniform random variable \\(X\\) over values \\(1,2,\\ldots, L\\). Show that \\(\\text{Var}X=\\dfrac{L^2-1}{12}\\). Hint: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\) and \\(\\sum_{i=1}^n i^2=\\frac{n^3}{3}+\\frac{n^2}{2}+\\frac{n}{6}\\)"
  },
  {
    "objectID": "rv_practice.html#hard-drive-failures",
    "href": "rv_practice.html#hard-drive-failures",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 Hard Drive Failures",
    "text": "6.15 Hard Drive Failures\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. The initial cost of building the player is $50, and the manufacturer offers a 1-year warranty. Within one year, the customer can ask for a free repair up to 12 times.\n\n\nLet \\(X\\) be the number of months when the player fails. What is the PMF of \\(X\\)? Hint: \\(\\mathbb{P}(X = 1)\\) may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying \\(X = 1\\) is equivalent to saying that there is only one failure in the entire 12-month period.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of failures should follow a binomial distribution with \\(n=12, p=1/12\\). Thus \\(P(X=k)={n \\choose k}(\\frac{1}{12})^k(\\frac{11}{12})^{n-k}\\)\n\n\n\n\nWhat is the average cost per player?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe cost is \\(50+20X\\) So \\(E(50+20X)=50+20E(X)=50+20\\cdot 12(\\frac{1}{12})=70\\)"
  },
  {
    "objectID": "rv_practice.html#bit-errors",
    "href": "rv_practice.html#bit-errors",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 Bit Errors",
    "text": "6.16 Bit Errors\n\nA binary communication channel has a probability of bit error of \\(p = 10^{-6}\\). Suppose that transmission occurs in blocks of 10,000 bits. Let \\(N\\) be the number of errors introduced by the channel in a transmission block.\n\n\n\nWhat is the PMF of \\(N\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(N\\) follows a binomial distribution with \\(n=10000\\) and \\(p=.000001\\)\n\n\n\n\nFind \\(\\mathbb{P}(N = 0)\\) and $(N b $ 3)$.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(0, 10000, .000001)\n\n[1] 0.9900498\n\npbinom(3, 10000, .000001)\n\n[1] 1\n\n\n\n\n\n\nFor what value of \\(p\\) will the probability of 1 or more errors in a block be 99%?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved directly. \\(P(N \\geq 1)=1-P(X=0)=1-(1-p)^{10000}\\). If we set this to .99 we can solve for \\(p\\) : \\(.99=1-(1-p)^{10000}\\) so \\(.01 = (1-p)^{10000}\\) so \\(p=1-.01^{1/10000}\\)\n\n1-.01^(1/10000)\n\n[1] 0.000460411"
  },
  {
    "objectID": "rv_practice.html#processing-orders",
    "href": "rv_practice.html#processing-orders",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 Processing Orders",
    "text": "6.17 Processing Orders\nThe number of orders waiting to be processed is given by a Poisson random variable with parameter \\(\\alpha = \\frac{\\lambda}{n\\mu}\\), where \\(\\lambda\\) is the average number of orders that arrive in a day, \\(\\mu\\) is the number of orders that an employee can process per day, and n is the number of employees. Let \\(N; = 5\\) and \\(N&lt; = 1\\). Find the number of employees required so the probability that more than four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few \\(n\\)’s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlambda=5\nmu=1\nppois(4, lambda/(1:10 * mu), lower.tail=FALSE)\n\n [1] 0.5595067149 0.1088219811 0.0275432568 0.0091242792 0.0036598468\n [6] 0.0016844329 0.0008589296 0.0004739871 0.0002784618 0.0001721156\n\n#With 3 employees P(X&gt;4) is less than 10%."
  },
  {
    "objectID": "rv_practice.html#normal-random-variable",
    "href": "rv_practice.html#normal-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.18 Normal Random Variable",
    "text": "6.18 Normal Random Variable\nIf \\(Z\\sim \\text{Normal}(\\mu=0, \\sigma^2=1^2)\\) find\n\n\n\\(\\mathbb{P}(Z &gt; 2.64)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(2.64, 0, 1, lower.tail=FALSE)\n\n[1] 0.004145301\n\n\n\n\n\n\n\\(\\mathbb{P}(0 \\leq Z &lt; 0.87)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(.87)-pnorm(0)\n\n[1] 0.3078498\n\n\n\n\n\n\n\\(\\mathbb{P}(|Z| &gt; 1.39)\\) (Hint: draw a picture)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(1.39, lower.tail=FALSE)*2\n\n[1] 0.1645289"
  },
  {
    "objectID": "rv_practice.html#identify-the-distribution",
    "href": "rv_practice.html#identify-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.19 Identify the Distribution",
    "text": "6.19 Identify the Distribution\nFor the following random experiments, decide what the distribution of X should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).\n\n\nWe throw a dart at a dart board. Let X denote the squared linear distance from the bullseye to the where the dart landed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume the dart lands somewhere on the board, and any point is equally likely (not a good assumption for a skilled dart thrower). The probability density would be proportional to the distance to the center squared - Suppose the dart board has radius \\(R\\). Let \\(X\\) be the distance to the dart from the bullseye. Then \\(P(X&lt;r)=\\pi r^2 / (\\pi R^2)=(r/R)^2\\) . The question then is what is \\(P(X^2&lt;r)\\)? Well, take a square root of both sides. \\(=P(X &lt; \\sqrt{r})=\\frac{r}{R^2}\\). This is a uniform distribution’s CDF.\n\n\n\n\nWe randomly choose a textbook from the shelf at the bookstore and let P denote the proportion of the total pages of the book devoted to exercises.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA random proportion you might want to use uniform(0,1) however this is assuming that each proportion is equally likely. This is actually a great example for a beta distribution. Beta distributions are continuous distributions that can be parameterized to model a random proportion and the distribution can can be made to be skewed in different ways.\n\n\n\n\nWe measure the time it takes for the water to completely drain out of the kitchen sink.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s assume the sink is filled to the maximum. We drain the sink and start our timer. In this case, it’s reasonable to model the length of time to drain as a normal distribution.\n\n\n\n\nWe randomly sample strangers at the grocery store and ask them how long it will take them to drive home.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe time it takes to go home could be modeled by a gamma distribution since it is a continuous distribution capped below at 0 and it is a useful way to model the length of time a random process takes to complete."
  },
  {
    "objectID": "rv_practice.html#normal-random-variable-ii",
    "href": "rv_practice.html#normal-random-variable-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.20 Normal Random Variable II",
    "text": "6.20 Normal Random Variable II\nLet \\(X\\) be a Gaussian random variable with \\(\\mu=5\\) and \\(\\sigma^2=16\\).\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(2\\leq X \\leq 7)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X&gt;4)\npnorm(4, mean=5, sd=4, lower.tail=FALSE)\n\n[1] 0.5987063\n\n#P(2 &lt;= X &lt;= 7)\npnorm(7, 5, 4)-pnorm(4,5,4)\n\n[1] 0.2901688\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X &lt; a)=0.8869\\), find \\(a\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.88695, 4)\n\n[1] 5.210466\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X&gt;b)=0.1131\\), find \\(b\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.1131, 5, 4, lower.tail=FALSE)\n\n[1] 9.840823\n\n\n\n\n\n\nIf \\(\\mathbb{P}(13 &lt; X \\leq c)=0.0011\\), find \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First find the probability less than 13\np13 &lt;- pnorm(13, 5, 4)\n#now we can find the quantile for p13+.0011\nqnorm(p13+.0011, 5, 4)\n\n[1] 13.08321\n\n#double check\npnorm(13.08321,5,4)-pnorm(13,5,4)\n\n[1] 0.001100025"
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf",
    "href": "rv_practice.html#a-continuous-cdf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.27 A Continuous CDF",
    "text": "6.27 A Continuous CDF\nConsider a cdf\n\\(F_X(x)-\\begin{cases}0,&\\text{if }x &lt; -1\\\\ 0.5 & \\text{if }-1 \\leq x &lt; 0\\\\(1+x)/2 & \\text{if }0 \\leq x &lt; 1\\\\1&\\text{otherwise}\\end{cases}\\)\nFind \\(\\mathbb{P}(X &lt; -1)\\), \\(\\mathbb{P}(-0.5 &lt; X &lt; 0.5)\\), and \\(\\mathbb{P}(X&gt;0.5)\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X &lt; -1) = 0 because F(x) only goes up to .5 at x=-1, not when x &lt; -1\n\n#P(-.5 &lt; X &lt; .5) = F(.5) - F(-.5)\n(1+.5)/2 - .5\n\n[1] 0.25\n\n#P(X &gt; 0.5) = 1-P(X&lt;.5) = 1-F(.5) \n1- (1+.5)/2\n\n[1] 0.25",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf-ii",
    "href": "rv_practice.html#a-continuous-cdf-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.28 A Continuous CDF II",
    "text": "6.28 A Continuous CDF II\nLet \\(X\\) have PDF \\[f(x) = \\begin{cases} 1-x/2,&0\\leq x \\leq 2\\\\0,&\\text{otherwise}\\end{cases}\\]\n\nSketch the PDF and show geometrically that this is a valid PDF.\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X\\geq 0]\\)\nFind \\(Pr[X&gt;1]\\) and \\(Pr[X\\geq 1]\\)\nUse calculus or software to calculate the CDF from the PDF. Write the expression for \\(F(x)\\) and plot the PDF and CDF.\nUse calculus or software to calculate the expected value of \\(X\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-random-variables",
    "href": "rv_practice.html#two-random-variables",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.29 Two Random Variables",
    "text": "6.29 Two Random Variables\nSuppose that \\(X\\) and \\(Y\\) are random variables with \\(\\mathbb{E}(X)=12\\) and $(Y)=8.\n\nFind \\(\\mathbb{E}(X-Y)\\)\nFind \\(\\mathbb{E}(5X-6Y)\\)\nIs it possible to determine \\(\\mathbb{E}(X^2)\\) with the given information? Explain.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-formula",
    "href": "rv_practice.html#pmf-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.1 PMF Formula",
    "text": "7.1 PMF Formula\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=1,2,\\ldots\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis was done above; because \\(\\sum_{i=1}^\\infty 1/2^k = 1\\), the value of \\(c\\) must be \\(1\\).\n\n\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(6\\leq X \\leq 8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X&gt;4) = 1-P(X \\leq 3)=1-(\\frac12 + \\frac14 + \\frac18)\\)\n\n1-(1/2+1/4+1/8)\n\n[1] 0.125\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe expected value can be calculated by taking the sum \\(\\sum k p_k = \\sum_{k=1}^\\infty \\frac{k}{2^k}\\) which we can show using facts from calculus equals 2. Why? Well, as long as \\(|p|&lt;1, \\sum_{k=1}^{\\infty}p^k=\\frac{p}{1-p}\\) (this is a geometric series). If we take a derivative of both sides we get \\(\\sum_{k=1}^\\infty kp^{k-1}=\\frac{1}{(1-p)^2}\\). Multiply both sides by \\(p\\) to get \\(\\sum_{k=1}^\\infty kp^{k}=\\frac{p}{(1-p)^2}\\). In our case, \\(p=\\frac12\\). Plugging this in we get \\(\\frac{.5}{.5^2}=2\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-formula-ii",
    "href": "rv_practice.html#pmf-formula-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.2 PMF Formula II",
    "text": "7.2 PMF Formula II\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=-1,0,1,2,3,4,5\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sum of the probabilities are \\(c(2 + 1 + \\frac{1}{2}+\\frac{1}{4}+\\frac{1}8+\\frac{1}{16}+\\frac{1}{32})=c\\frac{127}{32}\\) so \\(c=\\frac{32}{127}\\).\n\n\n\n\nFind \\(\\mathbb{P}(1\\leq X &lt; 3)\\) and \\(\\mathbb{P}(1 &lt; X \\leq 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc=32/127\nk = seq(-1,5)\npk=c/2^k\nnames(pk) &lt;- k\nsum(pk[as.character(2)])\n\n[1] 0.06299213\n\nsum(pk[as.character(2:5)])\n\n[1] 0.1181102\n\n\n\n\n\n\nFind \\(\\mathbb{P}(X^3 &lt; 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(X^3 &lt; 5\\) that means \\(X^3 \\leq 4\\) and thus \\(X \\leq 4^{1/3}\\approx 1.587\\)\n\nsum(pk[k&lt;=4^(1/3)])\n\n[1] 0.8818898\n\nsum(pk[k^3&lt;5])\n\n[1] 0.8818898\n\n\n\n\n\n\nFind the pmf and the cdf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(height=pk, names=k, main='pmf of X')\n\n\n\n\n\n\n\nbarplot(height=cumsum(pk), names=k, main='cdf of X')",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-i",
    "href": "rv_practice.html#properties-of-expectation-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.3 Properties of Expectation I",
    "text": "7.3 Properties of Expectation I\nFor a continuous random variable \\(X\\) and constant \\(c\\), prove that \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-ii",
    "href": "rv_practice.html#properties-of-expectation-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.4 Properties of Expectation II",
    "text": "7.4 Properties of Expectation II\nFor a continuous random variable \\(X\\) and constants \\(a,b\\), prove that \\(\\mathbb{E}(aX+b)=a\\mathbb{E}(X)+b\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-fair-coins",
    "href": "rv_practice.html#two-fair-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 Two Fair Coins",
    "text": "6.4 Two Fair Coins\nAlex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets.\n\n\nFind and sketch the joint PMF \\(p_{X,Y} (x, y)\\).\nFind the marginal PMF \\(p_X(x)\\) and \\(p_Y (y)\\).\nFind the conditional PMF \\(P_{X|Y} (x | y)\\). Does \\(P_{X|Y} (x | y) = P_X(x)\\)? Why or why not?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- 0:1\npx &lt;- c(.25, .75) \n\ny&lt;- 0:1\npy &lt;- c(.75, .25)\n\npmf &lt;- px %*% t(py)\nrownames(pmf) &lt;- x\ncolnames(pmf) &lt;- y \naddmargins(pmf)\n\n         0      1  Sum\n0   0.1875 0.0625 0.25\n1   0.5625 0.1875 0.75\nSum 0.7500 0.2500 1.00\n\n\n\nprop.table(pmf, 2)\n\n     0    1\n0 0.25 0.25\n1 0.75 0.75\n\n\nEach column gives P(X=x|Y=y) for the two values of y; you can see that they are the same; the reason is because the value of X and Y are independent.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-fair-dice",
    "href": "rv_practice.html#two-fair-dice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 Two Fair Dice",
    "text": "6.5 Two Fair Dice\nTwo fair dice are rolled. Find the joint PMF of \\(X\\) and \\(Y\\) when\n\n\n\\(X\\) is the larger value rolled, and \\(Y\\) is the sum of the two values.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmax(die1, die2)\noutcomes$y &lt;- die1+die2\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             2          3          4          5          6          7\n  1 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.05555556 0.02777778 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.05555556 0.05555556 0.02777778 0.00000000\n  4 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556\n   \n             8          9         10         11         12\n  1 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  4 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000\n  5 0.05555556 0.05555556 0.02777778 0.00000000 0.00000000\n  6 0.05555556 0.05555556 0.05555556 0.05555556 0.02777778\n\n\n\n\n\n\n\\(X\\) is the smaller, and \\(Y\\) is the larger value rolled.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmin(die1, die2)\noutcomes$y &lt;- pmax(die1,die2)\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             1          2          3          4          5          6\n  1 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n  2 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556\n  3 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556\n  4 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#signal-and-noise",
    "href": "rv_practice.html#signal-and-noise",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.6 Signal and Noise",
    "text": "6.6 Signal and Noise\nLet \\(Y = X+N\\), where \\(X\\) is the input, \\(N\\) is the noise, and \\(Y\\) is the output of a system. Assume that \\(X\\) and \\(N\\) are independent random variables. It is given that \\(E[X] = 0\\), \\(Var[X] = \\sigma^2_X\\), \\(E[N] = 0\\), and \\(Var[N] = \\sigma^2_N\\).\n\n\nFind the correlation coefficient \\(\\rho\\) between the input \\(X\\) and the output \\(Y\\) .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\rho(X,Y)= \\dfrac{Cov(X,Y)}{SD(X)SD(Y)}=\\dfrac{E(XY)-E(X)E(Y)}{\\sigma_X \\cdot \\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(E(XY)=E(X^2+XN)=E(X^2)+E(X)E(N)\\). Because \\(E(X)=E(N)=0\\) this simplifies to\n\\(\\rho(X,y)=\\dfrac{E(X^2)}{\\sigma_x\\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(Var(X)=E(X^2)-E(X)^2 = E(X^2)\\) so we can replace the numerator with \\(\\sigma^2_X\\). So\n\\(\\rho(X,Y) = \\sqrt{\\dfrac{\\sigma^2_X}{\\sigma^2_X+\\sigma^2_N}}\\)\n\n#example: sigma_X = 5, sigma_N=2\nX &lt;- rnorm(10000, 0, 5)\nN &lt;- rnorm(10000, 0, 2)\ncor(X, X+N)\n\n[1] 0.9285301\n\nsqrt(5^2/(5^2+2^2))\n\n[1] 0.9284767\n\n\n\n\n\n\nSuppose we estimate the input \\(X\\) by a linear function \\(g(Y ) = aY\\) . Find the value of a that minimizes the mean squared error \\(E[(X − aY )^2]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E[(X-aY)^2]=E[X^2-2aXY+a^2Y^2]=E[X^2]-2aE[XY]+a^2E[Y^2]\\)\nBecause \\(E(Y)=0\\), \\(E[Y^2]=Var(Y)=\\sigma_X^2+\\sigma_N^2\\). We already have that \\(E(X^2)=\\sigma_X^2\\) and \\(E(XY)=\\sigma_X^2\\). Thus\n$E[(X-aY)^2]=(_X^2+_N2)a2- 2_X^2a+_X^2 $\nThis is a quadratic function in \\(a\\), and the vertex can be found at \\(-\\frac{B}{2A}\\) or in this case \\(a^*=\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\)\n\n\n\n\nExpress the resulting mean squared error in terms of \\(\\eta = \\sigma^2_X/\\sigma^2_N\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPlugging this in for \\(a\\) we get\n\\(E[(X-aY)^2]=(\\sigma_X^2+\\sigma_N^2)\\dfrac{\\sigma_X^4}{(\\sigma_X^2+\\sigma_N^2)^2}-2\\dfrac{\\sigma_X^4}{\\sigma_X^2+\\sigma_N^2}+\\sigma_X^2=\\sigma_X^2\\left(1-\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\right)\\)\n\\(=\\sigma_X^2\\left(\\dfrac{\\sigma_N^2}{\\sigma_X^2+\\sigma_N^2}\\right)=\\dfrac{\\sigma_X^2}{\\eta+1}\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#cat-genetics",
    "href": "rv_practice.html#cat-genetics",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.7 Cat Genetics",
    "text": "6.7 Cat Genetics\nThe gene that controls white coat color in cats, KIT , is known to be responsible for multiple phenotypes such as deafness and blue eye color. A dominant allele W at one location in the gene has complete penetrance for white coat color; all cats with the W allele have white coats. There is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. However, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors.\n\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes.\nAbout 73% of white cats with two blue eyes are deaf\n40% of white cats with one blue eye are deaf.\nOnly 19% of white cats with other eye colors are deaf.\n\n\nCalculate the prevalence of deafness among white cats.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn.blue &lt;- c(0,1,2)\np.n.blue &lt;- c(.60, .30, .10)\n\np.deaf.given.b &lt;- c(.19, .40, .73) #for 0, 1 ,2 blue eyes\n\n#P(Deafness) = P(0b)*P(deaf|0b) + P(1b)*P(deaf|1b) + P(2b)*P(deaf|2b)\n(p.deaf &lt;- sum(p.n.blue * p.deaf.given.b))\n\n[1] 0.307\n\n#Check\nnCats &lt;- 10000\nnblue &lt;- sample(n.blue, size=nCats, replace=TRUE, prob=p.n.blue)\nisdeaf &lt;- FALSE\nfor(i in 1:nCats){\n  isdeaf[i] &lt;- runif(1) &lt; p.deaf.given.b[nblue[i]+1]\n}\nmean(isdeaf)\n\n[1] 0.305\n\n\n\n\n\n\nGiven that a white cat is deaf, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | deaf) = P(2b)*P(deaf|2b) / p(deaf)\np.n.blue[3] * p.deaf.given.b[3] / p.deaf\n\n[1] 0.237785\n\n#check\nmean(nblue[isdeaf]==2)\n\n[1] 0.2403279\n\n\n\n\n\n\nSuppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.\nWhat is the prevalence of blindness among deaf, white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\np.blind.given.nodeaf &lt;- 0.10\np.blind.given.deaf.and.nblue &lt;- c(0.20, 0.4, 0.2) #for 0, 1, 2 blue eyes\n\n#P(blind & deaf) = P(0b)*P(deaf|0b)*P(blind|deaf&0b)+\n#  P(1b)*P(deaf|1b)*P(blind|deaf&1b)+\n#  P(2b)*P(deaf|2b)*P(blind|deaf&2b)+\np.blind.and.deaf &lt;- sum(p.n.blue * p.deaf.given.b * p.blind.given.deaf.and.nblue)\n\n#P(blind | deaf ) = P(blind & deaf) / P(deaf)\n(p.blind.given.deaf = p.blind.and.deaf / p.deaf)\n\n[1] 0.2781759\n\n#check\nisBlind &lt;- FALSE\nfor(i in 1:nCats){\n  if(!isdeaf[i]){\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.nodeaf\n  } else {\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.deaf.and.nblue[nblue[i]+1]\n  }\n}\n#check\nmean(isBlind[isdeaf])\n\n[1] 0.2737705\n\n\n\n\n\n\nWhat is the prevalence of blindness among white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(blind) = P(deaf & blind) + P(nondeaf & blind)\n\n#P(nondeaf & blind ) = P(nondeaf) * P(blind | nondeaf)\np.blind.and.nondeaf &lt;- (1-p.deaf)*p.blind.given.nodeaf\n\n(p.blind &lt;- p.blind.and.deaf + p.blind.and.nondeaf)\n\n[1] 0.1547\n\n#check\nmean(isBlind)\n\n[1] 0.1508\n\n\n\n\n\n\nGiven that a cat is white and blind, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | blind) = P(2b & blind) / p(blind)\n#numerator: P(2b & blind) = P(2b) * [P(deaf|2b) * p(blind|deaf & 2b) + P(nondeaf|2b) * p(blind|nondeaf & 2b)]\np.blind.and.2b &lt;- p.n.blue[3] * (p.deaf.given.b[3]*p.blind.given.deaf.and.nblue[3] + \n (1-p.deaf.given.b[3]) * p.blind.given.nodeaf)\n\n(p.2b.given.blind = p.blind.and.2b / p.blind)\n\n[1] 0.1118293\n\n#check\nmean(nblue[isBlind]==2)\n\n[1] 0.1094164",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-i",
    "href": "rv_practice.html#gss-survey-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 GSS Survey I",
    "text": "6.8 GSS Survey I\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable?\n\n\nLinda is a banker.\nLinda is a banker and considers herself a liberal Democrat.\n\nTo answer this question we will use data from the GSS survey found at https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv.\nThe code for “Banking and related activities” in the indus10 variable is 6870. The values of the column sex are encoded like this:\n1: Male, 2: Female\nThe values of polviews are on a seven-point scale:\n1 Extremely liberal\n2 Liberal\n3 Slightly liberal\n4 Moderate\n5 Slightly conservative\n6 Conservative\n7 Extremely conservative\nDefine “liberal” as anyone whose political views are 3 or below. The values of partyid are encoded:\n0 Strong democrat\n1 Not strong democrat\n2 Independent, near democrat\n3 Independent\n4 Independent, near republican\n5 Not strong republican\n6 Strong republican\n7 Other party\nYou need to compute:\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngss &lt;- read.csv(\"https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv\")\n#banker : indus10==6870\n#female : sex == 2\n#liberal: polviews &lt;= 3\n#democrat: partyid &lt;= 2\n\n#In my reading, I am interpreteing that 'Linda is female' is given from the context;\n\n#P(banker | female) \nmean(gss[gss$sex==2,]$indus10==6870)\n\n[1] 0.02116103\n\n#P(liberal banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3,]$indus10==6870)\n\n[1] 0.01723195\n\n#P(liberal Dem banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3 & gss$partyid &lt;= 1,]$indus10==6870)\n\n[1] 0.01507289",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-ii",
    "href": "rv_practice.html#gss-survey-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.9 GSS Survey II",
    "text": "6.9 GSS Survey II\nCompute the following probabilities:\n\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$partyid&lt;=1,]$polviews&lt;=3)\n\n[1] 0.389132\n\n\n\n\n\n\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews&lt;=3,]$partyid&lt;=1)\n\n[1] 0.5206403",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-iii",
    "href": "rv_practice.html#gss-survey-iii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.10 GSS Survey III",
    "text": "6.10 GSS Survey III\nThere’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65. For these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\nCompute the following probabilities. For each statement, think about whether it is expressing a conjunction, a conditional probability, or both. For the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &lt;30 & gss$polviews &lt;=3)\n\n[1] 0.06579428\n\n\n\n\n\n\nWhat is the probability that a young person is liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$age &lt;30,]$polviews &lt;=3)\n\n[1] 0.3385177\n\n\n\n\n\n\nWhat fraction of respondents are old conservatives?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &gt;65 & gss$polviews &gt;= 5)\n\n[1] 0.06226415\n\n\n\n\n\n\nWhat fraction of conservatives are old?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews &gt;=5,]$age &gt;65)\n\n[1] 0.1820933",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-child-paradox",
    "href": "rv_practice.html#two-child-paradox",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.11 Two Child Paradox",
    "text": "6.11 Two Child Paradox\nSuppose you meet someone and learn that they have two children. You ask if either child is a girl and they say yes. What is the probability that both children are girls? (Hint: Start with four equally likely hypotheses.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBefore we know anything about their two kids, the number of girls \\(X\\) they have could be 0, 1 or 2. Simplifying the scenario with the ‘equally likely hypothesis’ means that we assume each kid has a 50% chance of being a girl, independently. Thus the probabilities are 0.25, 0.5 and 0.25 respectively.\nIf we learn that at least one of the kids is a girl, that tells us that the first possibility, that \\(x=0\\) is not possible. Thus \\(P(X=2 | X&gt;0) = .\\dfrac{25}{.5+.25}= \\frac{1}{3}\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#monty-hall",
    "href": "rv_practice.html#monty-hall",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.12 Monty Hall",
    "text": "6.12 Monty Hall\nThere are many variations of the Monty Hall problem. For example, suppose Monty always chooses Door 2 if he can, and only chooses Door 3 if he has to (because the car is behind Door 2).\n\n\nIf you choose Door 1 and Monty opens Door 2, what is the probability the car is behind Door 3?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#3 equally likely possibilities:\n#C1  -&gt; Monty chooses Door 2\n#C2  -&gt; Monty cannot choose Door 2, chooses Door 3\n#C3  -&gt; Monty chooses Door 2\n\n#So if Monty chooses Door 2, either C1 or C3 must be the case, each equally likely. \n#Thus P(C3 | Monty chooses 2) = .50\n\n\n\n\n\nIf you choose Door 1 and Monty opens Door 3, what is the probability the car is behind Door 2?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#If he chose Door 3, that means that it must be the case that the car is behind \n#door 2; he would *ONLY* choose door 3 in that case.\n\n#Thus P(C2 | Monty chooses 3) = 1.0",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#mms",
    "href": "rv_practice.html#mms",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.13 M&Ms",
    "text": "6.13 M&Ms\nM&M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M’s, changes the mixture of colors from time to time. In 1995, they introduced blue M&M’s.\n\nIn 1994, the color mix in a bag of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan.\nIn 1996, it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n\nSuppose a friend of mine has two bags of M&M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n(Hint: The trick to this question is to define the hypotheses and the data carefully.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(G|94) = .10, P(G|96) = .20\\) \\(P(Y|94) = .20, P(Y|96) = .14\\) \\(P(94)=P(96)=.50\\) assuming equally likely. It’s important to realize only one of two situations could have occurred:\n\nSituation 1: A G was chosen from the 94 bag and a Y was chosen from the 96 bag\nSituation 2: A Y was chosen from the 94 bag and a G was chosen from the 96 bag.\n\nThe corresponding probabilities are \\((.10)(.14)=.014\\) and \\((.20)(.20)=.04)\\). The question could be stated: What is the probability that Sit.1 occurred given that either Sit1 or Sit2 occured.\nThe likelihood is \\(.014 / (.014+.04) = .2592593\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-coins",
    "href": "rv_practice.html#two-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 Two Coins",
    "text": "6.14 Two Coins\nSuppose you have two coins in a box. One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides. You choose a coin at random and see that one of the sides is heads. What is the probability that you chose the trick coin?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is actually similar to the “family with two girls” problem. The equally likely coins are “HT” and “HH”.\n\\(P(Heads | HH) = 1\\) \\(P(Heads | TH) = .5\\)\nBayes Theorem tells us that \\(P(HH | Heads) = \\dfrac{P(HH)*P(Heads|HH)}{P(HH)P(Heads|HH)+P(TH)P(Heads|TH)}=\\dfrac{.5}{.5+.25}=\\frac23\\)\nSeems counter intuitive! If I pick a random coin from the two, you already know that one of the sides is a head without looking at it. Somehow then, when you look at just one side of the coin, seeing a H makes you 67% sure that it is the trick coin. The reason is that seeing the head side is like flipping it once and getting a H. That is less likely to occur with the fair coin, hence this outcome lends evidence to the “trick coin” hypothesis.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#online-sales-i",
    "href": "rv_practice.html#online-sales-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 Online Sales I",
    "text": "6.15 Online Sales I\n(From Introduction to Probability and Statistics for Data Science) Customers can buy one of four produts, each having its own web page with a ``buy’’ link. When they click on the link, they are redirected to a common page containing a registration and payment orm. Once there the customer eithe rbuys the desired product (labeled 1,2,3 or 4) or they fail to complete and a sale is lost. Let event \\(A_i\\) be that a customer is on produt \\(i\\)’s web page and let event \\(B\\) be the event that the customer buys the product. For the purposes of this problem, assume that each potential customer visits at most one product page and so he or she buys at most one produt. For the probabilities shown in the table below, find the probability that a customer buys a product.\n\n\n\nProduct \\((i)\\)\n\\(Pr[B|A_i]\\)\n$Pr[A_i]\n\n\n\n\n1\n\\(0.72\\)\n\\(0.14\\)\n\n\n2\n\\(0.90\\)\n\\(0.04\\)\n\n\n3\n\\(0.51\\)\n\\(0.11\\)\n\n\n4\n\\(0.97\\)\n\\(0.02\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#online-sales-ii",
    "href": "rv_practice.html#online-sales-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 Online Sales II",
    "text": "6.16 Online Sales II\nContinuing from the previous problem, if a random purchase is selected, find the probability that it was item 1 that was purchased. Do the same for items 2, 3 and 4.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#uav",
    "href": "rv_practice.html#uav",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 UAV",
    "text": "6.17 UAV\nConsider a new type of commercial unmanned aerial vehicle (UAV) that has been outfitted with a transponder so that if it crashes it can easily be found and reused. Other older UAVs do not have transponders. Eighty percent of al lUAVs are recovered and, of those recovered, 75% have a transponder. Further, of those not recovered, 90% do not have a transponder. Denote recovery as \\(R+\\) and failure to recover as \\(R-\\). Denote having a transponder as \\(T+\\) and not having a transponder as \\(T-\\). Find\n\n\\(Pr[T+]\\)\nThe probability of not recovering a UAV given that it has a transponder.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#disease-screening",
    "href": "rv_practice.html#disease-screening",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.18 Disease Screening",
    "text": "6.18 Disease Screening\nSuppose that the test for a disease has sensitivity 0.90 and specificity 0.999. The base rate for the disease is 0.002. Find\n\nThe probability that someone selected at random from the population tests positive.\nThe probability that the person has the diseases given a positive test result (the positive predictive value).\nThe probability that the person does not have the disease given a negative test result (the negative predictive value).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-practice",
    "href": "rv_practice.html#pmf-practice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.21 PMF practice",
    "text": "6.21 PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "href": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.5 Variance of a RV from the pmf",
    "text": "7.5 Variance of a RV from the pmf\nLet \\(X\\) be a random variable with pmf \\(p_k = 1/2^k\\) for \\(k=1,2,\\ldots\\).\n\nFind \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance is \\(E(X^2)-(EX)^2=E(X^2)-4\\). The expected value of \\(X^2\\) can be derived, though it’s not so fun…\nStart by taking the equation \\(\\sum_k kp^k = \\frac{p}{(1-p)^2}\\) and take a derivative again. We get \\(\\sum_k k^2 p^{k-1} = \\frac{(1-p)^2+2p(1-p)}{(1-p)^4}\\). Multiply through by \\(p\\) to get \\(\\sum_k k^2 p^k = \\frac{p(1-p)^2+2p^2(1-p)}{(1-p)^4}\\). If we let \\(p=\\frac12\\) we have found \\(E(X^2)=\\sum_{k=1}^\\infty k^2(\\frac12)^k=\\dfrac{\\frac18-\\frac{2}{8}}{\\frac{1}{16}}=6\\). Thus \\(Var(X)=E(X^2)-E(X)^2 = 6-4=2\\). It should be noted that this random variable is actually a geometric random variable (well, according to the “number of trials until and including the first success definition). If we define \\(Y\\sim Geom(.5)\\) using our definition of “number of failures before the first success” then We can let \\(X=Y+1\\). \\(E(X)=E(Y+1)=\\frac{1-.5}{.5}+1=2\\) and \\(Var(X)=Var(Y)=\\frac{1-p}{p2}=\\frac{.5}{.25^2}=2\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#variance-formula",
    "href": "rv_practice.html#variance-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.6 Variance Formula",
    "text": "7.6 Variance Formula\nShow that \\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2-\\mu^2\\). Hint: expand the quantity \\((X-\\mu)^2\\) and distribute the expectation over the resulting terms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe proof goes like this: We first FOIL \\((X-\\mu)^2\\):\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}(X^2 - 2\\mu X + \\mu^2)\\)\nWe next split the expected value into 3 expected values using the fact that \\(\\mathbb{E}\\) is a linear operator.\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mathbb{E}X + \\mathbb{E}\\mu^2\\)\nWe next observe that \\(\\mu^2\\) is constant and \\(\\mathbb{X}=\\mu\\)\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mu + \\mu^2\\)\nWe can simplify the expression and we’re done!",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "distr_practice.html",
    "href": "distr_practice.html",
    "title": "8  Random Variable Distribution Practice",
    "section": "",
    "text": "9 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "distr_practice.html#using-wikipedia-in-college",
    "href": "distr_practice.html#using-wikipedia-in-college",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.1 Using Wikipedia in college",
    "text": "9.1 Using Wikipedia in college\nA recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \\(X\\) equal the number of students in a random sample of size \\(n = 31\\) who have used Wikipedia as a source.\n\nHow is \\(X\\) distributed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming independence in sampling, and a representative sample, we can use a Binomial distribution with \\(n=31\\) and \\(p=0.447\\).\n\n\n\n\nSketch the probability mass function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(dbinom(0:31, 31,.447), names=0:31, ylab=\"probability\", main=\"PMF of Binomial(31,.447)\")\n\n\n\n\n\n\n\n\nSketch the cumulative distribution function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(pbinom(0:31, 31, .447), type=\"s\", ylab=\"cumulative prob.\", main=\"CDF of Binomial(31, .447)\")\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\) is equal to 17.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(17, 31, .447)\n\n[1] 0.07532248\n\n\n\n\n\n\nFind the probability that \\(X\\) is at most 13.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npbinom(13, 31, .447)\n\n[1] 0.451357\n\n\n\n\n\n\nFind the probability that \\(X\\) is bigger than 11.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(12:31, 31, .447))\n\n[1] 0.8020339\n\n#or\npbinom(11, 31, .447, lower.tail=FALSE)\n\n[1] 0.8020339\n\n#or\n1-pbinom(11, 31, .447)\n\n[1] 0.8020339\n\n\n\n\n\n\nFind the probability that \\(X\\) is at least 15.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X at least 15)\nsum(dbinom(15:31,31,.447))\n\n[1] 0.406024\n\n\n\n\n\n\nFind the probability that \\(X\\) is between 16 and 19, inclusive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(16:19, 31, .447))\n\n[1] 0.2544758\n\n\n\n\n\n\nGive the mean of \\(X\\), denoted \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(X)=n*p\n31*.447\n\n[1] 13.857\n\n#or you can also do this (but it's too much work)\nsum( (0:31) * dbinom(0:31, 31, .447))\n\n[1] 13.857\n\n\n\n\n\n\nGive the variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Var(X) = n * p * (1-p)\n31 * .447 * (1-.447)\n\n[1] 7.662921\n\n#or - if you want (but why would you want to?)\nsum((0:31 - 31*.447)^2 * dbinom(0:31, 31, .447))\n\n[1] 7.662921\n\n\n\n\n\n\nGive the standard deviation of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#SD(X) = sqrt(n*p*(1-p))\nsqrt(31*.447*(1-.447))\n\n[1] 2.768198\n\n\n\n\n\n\nFind \\(\\mathbb{E}(4X+51.324)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(4X+51.324) = 4*E(X)+51.324\n4*(31*.447) + 51.324\n\n[1] 106.752"
  },
  {
    "objectID": "distr_practice.html#a-uniform-pmf",
    "href": "distr_practice.html#a-uniform-pmf",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.2 A Uniform PMF",
    "text": "9.2 A Uniform PMF\nLet \\(X\\) have discrete uniform PMF on the values \\(x\\in\\left\\{-1,0,1\\right\\}\\).\n\nWrite the equation for its PMF.\nFind \\(Pr[X&lt;-1]\\) and \\(Pr[X\\leq -1]\\)\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X \\geq 0]\\)\nCalculate the CDF from the PDF. Write out an expression for \\(F(x)\\) and plot the PMF and CDF."
  },
  {
    "objectID": "distr_practice.html#discrete-random-variable-problems",
    "href": "distr_practice.html#discrete-random-variable-problems",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.3 Discrete Random Variable Problems",
    "text": "9.3 Discrete Random Variable Problems\n\n\nIf \\(X\\) is \\(\\text{Poisson}(\\lambda)\\), compute \\(\\mathbb{E}\\left[1/(X+1)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be handled mathematically. The formula for \\(E(1/(X+1))\\) is\n\\(E(1/(X+1))=\\sum_{x=0}^{\\infty}\\frac{1}{x+1}\\frac{\\lambda^{x}}{x!}e^{-\\lambda}=\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x}}{(x+1)!}e^{-\\lambda}\\)\nThe trick is to get get the summation to equal 1 and simplify. We multiply by \\(\\lambda/\\lambda\\)\n\\(E(1/(X+1))=\\frac{1}{\\lambda}\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x+1}}{(x+1)!}e^{-\\lambda}\\)\nNow we can make a change of variables: \\(y=x+1\\) and thus \\(x=0\\) becomes \\(y=1\\)\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}\\sum_{y=1}^{\\infty}\\frac{\\lambda^{y}}{y!}e^{-\\lambda}\\)\nThe only thing missing is that the summation starts at \\(y=1\\) instead of \\(y=0\\), But for \\(Y \\sim Poisson(\\lambda)\\), \\(P(Y=0)=e^{-\\lambda}\\) so this summation is \\(1-e^{-\\lambda}\\).\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}(1-e^{-\\lambda})\\)\n\n\n\n\nIf \\(X\\) is \\(\\text{Bernoulli}(p)\\) and \\(Y\\) is \\(\\text{Bernoulli}(q)\\), computer \\(\\mathbb{E}\\left[(X+Y)^3\\right]\\) assuming \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\((X+Y)^3 = X^3+3X^2Y+3XY^2+Y^3\\) so \\(E[(X+Y)^3]=E(X^3)+3E(X^2)E(Y)+3E(X)E(Y^2)+E(Y^2)\\)\nthis is due to independence. Since \\(X\\) an \\(Y\\) are independent, so are \\(X^2\\) and \\(Y\\), and \\(X\\) and \\(Y^2\\). \\(E(X)=E(X^2)=E(X^3)=p\\) and \\(E(Y)=E(Y^2)=E(Y^3)=q\\). Thus \\(E[(X+Y)^3]=p+6pq+q\\)\n\n\n\n\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\Delta(\\theta)=\\mathbb{E}\\left[(X-\\theta)^2\\right]\\). Find \\(\\theta\\) that minimizes the error \\(\\Delta(\\theta)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can expand the expected value and attempt to find the minimum with respect to \\(\\theta\\). \\(E[(X-\\theta)^2]=E[X^2-2\\theta X+\\theta^2]=E(X^2)-2\\theta\\mu+\\theta^2\\). Recall that \\(Var(X)=E(X^2)-\\mu^2\\) so \\(E(X^2)=\\sigma^2+\\mu^2\\) So we can write \\(\\Delta(\\theta)=\\sigma^2 + \\mu^2-2\\theta\\mu + \\theta^2\\) We want to find what value of \\(\\theta\\) minimizes this function - derivative! \\(\\Delta'(\\theta)=-2\\mu+2\\theta=0\\) thus \\(\\theta=\\mu\\) minimizes this.\n\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) are independent uniform random variables in \\(\\{0,1,\\ldots,100\\}\\). Evaluate \\(\\mathbb{P}\\left[\\text{min}(X_1,\\ldots, X_n) &gt; l\\right]\\) for any \\(l \\in \\{0,1,\\ldots,100\\}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(Y=\\min(X_1, \\ldots, X_n)\\) If \\(P(Y &gt;l)\\), that means the minimum exceeds \\(l\\), so all of the values \\(&gt;l\\). \\(P(X_1 &gt; l)=(100-l)/101\\) - you can check: \\(P(X_1&gt;0)=100/101\\). This is the same calculation for each \\(i\\). So \\(P(Y&gt;l)=\\dfrac{(100-l)^n}{101^n}\\).\n\n\n\n\nConsider a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\). \\(p_X(k)={n \\choose k} p^k(1-p)^{n-k}\\). Show that the mean is \\(\\mathbb{E}X= np\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(X)=\\sum_{k=0}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nThe first term is zero so we could write\n\\(\\sum_{k=1}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nNow the following is a fact that is needed but perhaps not well known. It’s the equivalence of \\(k{n \\choose k}=n{n-1 \\choose k-1}\\). We make this subsitution\n\\(\\sum_{k=1}^n n{n-1 \\choose k-1} p^k(1-p)^{n-k}=np\\sum_{k=1}^n {n-1\\choose k-1}p^{k-1}(1-p)^{n-k}\\)\nWe could write \\(n-k=(n-1)-(k-1)\\) and we’ll be making some substitutions: \\(m=n-1\\) and \\(j=k-1\\). This lets us write\n\\(np\\sum_{j=0}^m {m \\choose j}p^j(1-p)^{m-j}=np\\) because the summation =1, as it is just the sum of the pmf of a binomial.\n\n\n\n\n(not for 340) Consider a geometric random variable \\(X\\) with parameter \\(p\\). \\(p_X(k)=p(1-p)^k\\) for \\(k=0,1,\\ldots\\). Show that its mean is \\(\\mathbb{E}X=(1-p)/p\\).\n(not for 340) Consider a Poisson random variable \\(X\\) with parameter \\(\\lambda\\). \\(p_X(k)=\\dfrac{\\lambda^k}{k!}e^{-\\lambda}\\). Show that \\(\\text{Var}X=\\lambda\\).\n(not for 340) Consider the uniform random variable \\(X\\) over values \\(1,2,\\ldots, L\\). Show that \\(\\text{Var}X=\\dfrac{L^2-1}{12}\\). Hint: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\) and \\(\\sum_{i=1}^n i^2=\\frac{n^3}{3}+\\frac{n^2}{2}+\\frac{n}{6}\\)"
  },
  {
    "objectID": "distr_practice.html#hard-drive-failures",
    "href": "distr_practice.html#hard-drive-failures",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.4 Hard Drive Failures",
    "text": "9.4 Hard Drive Failures\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. The initial cost of building the player is $50, and the manufacturer offers a 1-year warranty. Within one year, the customer can ask for a free repair up to 12 times.\n\n\nLet \\(X\\) be the number of months when the player fails. What is the PMF of \\(X\\)? Hint: \\(\\mathbb{P}(X = 1)\\) may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying \\(X = 1\\) is equivalent to saying that there is only one failure in the entire 12-month period.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of failures should follow a binomial distribution with \\(n=12, p=1/12\\). Thus \\(P(X=k)={n \\choose k}(\\frac{1}{12})^k(\\frac{11}{12})^{n-k}\\)\n\n\n\n\nWhat is the average cost per player?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe cost is \\(50+20X\\) So \\(E(50+20X)=50+20E(X)=50+20\\cdot 12(\\frac{1}{12})=70\\)"
  },
  {
    "objectID": "distr_practice.html#bit-errors",
    "href": "distr_practice.html#bit-errors",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.5 Bit Errors",
    "text": "9.5 Bit Errors\n\nA binary communication channel has a probability of bit error of \\(p = 10^{-6}\\). Suppose that transmission occurs in blocks of 10,000 bits. Let \\(N\\) be the number of errors introduced by the channel in a transmission block.\n\n\n\nWhat is the PMF of \\(N\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(N\\) follows a binomial distribution with \\(n=10000\\) and \\(p=.000001\\)\n\n\n\n\nFind \\(\\mathbb{P}(N = 0)\\) and $(N b $ 3)$.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(0, 10000, .000001)\n\n[1] 0.9900498\n\npbinom(3, 10000, .000001)\n\n[1] 1\n\n\n\n\n\n\nFor what value of \\(p\\) will the probability of 1 or more errors in a block be 99%?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved directly. \\(P(N \\geq 1)=1-P(X=0)=1-(1-p)^{10000}\\). If we set this to .99 we can solve for \\(p\\) : \\(.99=1-(1-p)^{10000}\\) so \\(.01 = (1-p)^{10000}\\) so \\(p=1-.01^{1/10000}\\)\n\n1-.01^(1/10000)\n\n[1] 0.000460411"
  },
  {
    "objectID": "distr_practice.html#processing-orders",
    "href": "distr_practice.html#processing-orders",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.6 Processing Orders",
    "text": "9.6 Processing Orders\nThe number of orders waiting to be processed is given by a Poisson random variable with parameter \\(\\alpha = \\frac{\\lambda}{n\\mu}\\), where \\(\\lambda\\) is the average number of orders that arrive in a day, \\(\\mu\\) is the number of orders that an employee can process per day, and n is the number of employees. Let \\(N; = 5\\) and \\(N&lt; = 1\\). Find the number of employees required so the probability that more than four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few \\(n\\)’s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlambda=5\nmu=1\nppois(4, lambda/(1:10 * mu), lower.tail=FALSE)\n\n [1] 0.5595067149 0.1088219811 0.0275432568 0.0091242792 0.0036598468\n [6] 0.0016844329 0.0008589296 0.0004739871 0.0002784618 0.0001721156\n\n#With 3 employees P(X&gt;4) is less than 10%."
  },
  {
    "objectID": "distr_practice.html#normal-random-variable",
    "href": "distr_practice.html#normal-random-variable",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.7 Normal Random Variable",
    "text": "9.7 Normal Random Variable\nIf \\(Z\\sim \\text{Normal}(\\mu=0, \\sigma^2=1^2)\\) find\n\n\n\\(\\mathbb{P}(Z &gt; 2.64)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(2.64, 0, 1, lower.tail=FALSE)\n\n[1] 0.004145301\n\n\n\n\n\n\n\\(\\mathbb{P}(0 \\leq Z &lt; 0.87)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(.87)-pnorm(0)\n\n[1] 0.3078498\n\n\n\n\n\n\n\\(\\mathbb{P}(|Z| &gt; 1.39)\\) (Hint: draw a picture)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(1.39, lower.tail=FALSE)*2\n\n[1] 0.1645289"
  },
  {
    "objectID": "distr_practice.html#identify-the-distribution",
    "href": "distr_practice.html#identify-the-distribution",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.8 Identify the Distribution",
    "text": "9.8 Identify the Distribution\nFor the following random experiments, decide what the distribution of X should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).\n\n\nWe throw a dart at a dart board. Let X denote the squared linear distance from the bullseye to the where the dart landed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume the dart lands somewhere on the board, and any point is equally likely (not a good assumption for a skilled dart thrower). The probability density would be proportional to the distance to the center squared - Suppose the dart board has radius \\(R\\). Let \\(X\\) be the distance to the dart from the bullseye. Then \\(P(X&lt;r)=\\pi r^2 / (\\pi R^2)=(r/R)^2\\) . The question then is what is \\(P(X^2&lt;r)\\)? Well, take a square root of both sides. \\(=P(X &lt; \\sqrt{r})=\\frac{r}{R^2}\\). This is a uniform distribution’s CDF.\n\n\n\n\nWe randomly choose a textbook from the shelf at the bookstore and let P denote the proportion of the total pages of the book devoted to exercises.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA random proportion you might want to use uniform(0,1) however this is assuming that each proportion is equally likely. This is actually a great example for a beta distribution. Beta distributions are continuous distributions that can be parameterized to model a random proportion and the distribution can can be made to be skewed in different ways.\n\n\n\n\nWe measure the time it takes for the water to completely drain out of the kitchen sink.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s assume the sink is filled to the maximum. We drain the sink and start our timer. In this case, it’s reasonable to model the length of time to drain as a normal distribution.\n\n\n\n\nWe randomly sample strangers at the grocery store and ask them how long it will take them to drive home.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe time it takes to go home could be modeled by a gamma distribution since it is a continuous distribution capped below at 0 and it is a useful way to model the length of time a random process takes to complete."
  },
  {
    "objectID": "distr_practice.html#normal-random-variable-ii",
    "href": "distr_practice.html#normal-random-variable-ii",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.9 Normal Random Variable II",
    "text": "9.9 Normal Random Variable II\nLet \\(X\\) be a Gaussian random variable with \\(\\mu=5\\) and \\(\\sigma^2=16\\).\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(2\\leq X \\leq 7)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X&gt;4)\npnorm(4, mean=5, sd=4, lower.tail=FALSE)\n\n[1] 0.5987063\n\n#P(2 &lt;= X &lt;= 7)\npnorm(7, 5, 4)-pnorm(4,5,4)\n\n[1] 0.2901688\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X &lt; a)=0.8869\\), find \\(a\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.88695, 4)\n\n[1] 5.210466\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X&gt;b)=0.1131\\), find \\(b\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.1131, 5, 4, lower.tail=FALSE)\n\n[1] 9.840823\n\n\n\n\n\n\nIf \\(\\mathbb{P}(13 &lt; X \\leq c)=0.0011\\), find \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First find the probability less than 13\np13 &lt;- pnorm(13, 5, 4)\n#now we can find the quantile for p13+.0011\nqnorm(p13+.0011, 5, 4)\n\n[1] 13.08321\n\n#double check\npnorm(13.08321,5,4)-pnorm(13,5,4)\n\n[1] 0.001100025"
  },
  {
    "objectID": "distr_practice.html#choose-the-distribution",
    "href": "distr_practice.html#choose-the-distribution",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.10 Choose the distribution",
    "text": "9.10 Choose the distribution\n\nFor the following situations, decide what the distribution of \\(X\\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)\n\n\nWe shoot basketballs at a basketball hoop, and count the number of shots until we make a basket. Let X denote the number of missed shots. On a normal day we would typically make about 37% of the shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of missed shots before the first basket, assuming independence, can be modeled by a Geometric random variable with parameter \\(p=.37\\).\n\n\n\n\nIn a local lottery in which a three digit number is selected randomly, let X be the number selected.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming that all 3 digit numbers are equally likely (A reasonable assumption) the number selected can be modeled by a discrete uniform distribution with minimum 100 and maximum 999.\n\n\n\n\nWe drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let X be the number of times the cup lands perfectly right side up.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we drop the cup 20 times, and the result each time is independent with a constant probability of landing right side up, the number of times it does can be modeled by a Binomial random variable with parameters \\(n=20\\) and \\(p\\) (unknown).\n\n\n\n\nWe toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let X denote the number of missed shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGeometric random variable (unknown parameter value for \\(p\\)).\n\n\n\n\nWorking for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let X count the number of containers that illegally contain contraband.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTechnically we should use a hypergeometric random variable for this situation (since it is a small population size of 557), but since we do not cover the hypergeometric the closest random variable we have is the binomial.\n\n\n\n\nAt the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let X denote the number of birds in the bush.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a discrete random variable, but without other information it’s hard to say. The distribution is likely unimodal and bell-curved. You could probably model this using a normal distribution rounded off to the nearest integer.\n\n\n\n\nWe count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe observation window is the circular area, and the 10 minutes during observation. Assuming the rate of rainfall is constant, the number of raindrops in the circle can be modeled using a Poisson random variable.\n\n\n\n\nWe count the number of moth eggs on our window screen.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCounting indicates a discrete random variable. A binomial or a rounded normal distribution may be appropriate, but we lack enough details to be sure.\n\n\n\n\nWe count the number of blades of grass in a one square foot patch of land.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe location of the sprouting grass could be modeled well by a Poisson random variable - the \\(\\lambda\\) parameter would likely be very large, in the range of 1000 or 10000, and as such the distribution would look very much like a normal distribution.\n\n\n\n\nWe count the number of pats on a baby’s back until (s)he burps.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we define a geometric random variable, we let \\(X\\) be the number of failures before the first success. The last pat (that causes the burp) is the success in this context. So we could use a geometric random variable, but we would have to add 1 to it in order to count all burps (the failures + 1 success)."
  },
  {
    "objectID": "distr_practice.html#two-normal-rvs",
    "href": "distr_practice.html#two-normal-rvs",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.11 Two Normal RVs",
    "text": "9.11 Two Normal RVs\nLet X and Y be zero-mean, unit-variance independent Gaussian random variables. Find the value of r for which the probability that \\((X, Y )\\) falls inside a circle of radius r is 1/2.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000)\n\nr &lt;- seq(1.1, 1.2, .005)\np &lt;- 0\nfor (i in 1:length(r)){\n  p[i] &lt;- mean(sqrt(x^2+y^2) &lt;= r[i])\n}\ndata.frame(r,p)\n\n       r      p\n1  1.100 0.4525\n2  1.105 0.4542\n3  1.110 0.4568\n4  1.115 0.4600\n5  1.120 0.4622\n6  1.125 0.4647\n7  1.130 0.4690\n8  1.135 0.4721\n9  1.140 0.4754\n10 1.145 0.4787\n11 1.150 0.4818\n12 1.155 0.4843\n13 1.160 0.4869\n14 1.165 0.4894\n15 1.170 0.4920\n16 1.175 0.4946\n17 1.180 0.4975\n18 1.185 0.5007\n19 1.190 0.5032\n20 1.195 0.5065\n21 1.200 0.5089\n\n#X^2 + Y^2 ~ Chisq(2)\n#so the square root of the 50th percentile from that distribution should be the answer\nsqrt(qchisq(.5,2))\n\n[1] 1.17741"
  },
  {
    "objectID": "distr_practice.html#uniform-random-angle",
    "href": "distr_practice.html#uniform-random-angle",
    "title": "8  Random Variable Distribution Practice",
    "section": "9.12 Uniform Random Angle",
    "text": "9.12 Uniform Random Angle\nLet \\(\\Theta ∼ Uniform[0, 2\\pi]\\).\n\n\nIf \\(X = cos \\Theta\\), \\(Y = sin \\Theta\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, they are uncorrelated, because (x,y) can be any point on the circumference of a circle of radius 1 with uniform likelihood. However, they are not independent. If we know the value of \\(Y\\) for example, there are only 2 possible values of \\(X\\).\n\nthetas &lt;- runif(10000, 0, 2*pi)\ncor(cos(thetas), sin(thetas))\n\n[1] 0.0002646251\n\n\n\n\n\n\nIf \\(X = cos(\\Theta/4)\\), \\(Y = sin(\\Theta/4)\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case (x,y) can only be found in the first quadrant. In this case they are going to be negatively correlated, since that portion of the unit circle in the first quadrant slopes downwards.\n\ncor(cos(thetas/4), sin(thetas/4))\n\n[1] -0.9183002"
  },
  {
    "objectID": "distr_practice.html#variance-of-a-uniform-rv",
    "href": "distr_practice.html#variance-of-a-uniform-rv",
    "title": "8  Random Variable Distribution Practice",
    "section": "10.1 Variance of a Uniform RV",
    "text": "10.1 Variance of a Uniform RV\nCalculate the variance of \\(X \\sim \\text{Unif}(a,b)\\). (Hint: First calculate \\(\\mathbb{E}X^2\\))"
  },
  {
    "objectID": "distr_practice.html#expectation-and-binomial",
    "href": "distr_practice.html#expectation-and-binomial",
    "title": "8  Random Variable Distribution Practice",
    "section": "10.2 Expectation and Binomial",
    "text": "10.2 Expectation and Binomial\nIf \\(X \\sim \\text{Binom}(n,p)\\) show that \\(\\mathbb{E}X(X-1)=n(n-1)p^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can just expand the product \\(\\mathbb{E}(X^2-X)\\) and we can split this up into two expected values: \\(\\mathbb{E}X^2 - \\mathbb{E}X = \\mathbb{E}X^2-\\mu\\). Recall that \\(Var(X)=\\mathbb{E}X^2-\\mu^2\\) So \\(\\mathbb{E}X^2=Var(X)+\\mu^2\\). For a binomial, \\(Var(X)=np(1-p)\\) and \\(\\mu=np\\). Thus we have\n\\(\\mathbb{E}X^2 - \\mu=[np(1-p) + n^2p^2] - np = np\\left(1-p+np-1\\right)\\)\nTidying up a little bit we get \\(np(np-p)=np^2(n-1)\\), and we’re done."
  },
  {
    "objectID": "RV_summary.html#rv-summary",
    "href": "RV_summary.html#rv-summary",
    "title": "Appendix A — Random Variable Summary",
    "section": "A.1 RV summary",
    "text": "A.1 RV summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\nBinomial\nGeometric\nPoisson\nDiscrete Uniform\nNormal\n(Continuous) Uniform\nExponential\n\n\n\n\nType\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nContinuous\nContinuous\nContinuous\n\n\nParameters\n\\(n\\), \\(p\\)\n\\(p\\)\n\\(\\lambda\\)\n\\(a\\), \\(b\\)\n\\(\\mu\\), \\(\\sigma^2\\)\n\\(a\\), \\(b\\)\n\\(\\lambda\\)\n\n\nDescription\nNumber of successes in \\(n\\) independent trials with \\(p\\) probability of success for each trial  (Note: Bernoulli is just Binomial with \\(n\\!=\\!1\\))\nNumber of failures BEFORE the first success while independently repeating trial with \\(p\\) probability of success\nCount of number of occurrences of an event with constant mean rate \\(\\lambda\\) that’s independent of previous occurrences\n\\(n\\)-sided fair die\nNormal distributions usually arise from CLT (i.e. they’re processes that are the sum of many smaller independent processes)\nGeneralizing \\(n\\)-sided fair die to a continuous interval\nWaiting time between Poisson events\n\n\nOutcomes\n\\(0,1,\\ldots,n\\)\n\\(0,1,\\ldots\\)\n\\(0,1,\\ldots\\)\n\\(a,a\\!+\\!1,\\ldots,b\\)\n\\((-\\infty,\\infty)\\)\n\\([a,b]\\)\n\\([0,\\infty)\\)\n\n\nPDF/PMF at \\(k\\)\n\\({n\\choose k}p^k(n-p)^{n-k}\\)\n\\(p(1-p)^k\\)\n\\(\\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\n\\(\\frac1{b-(a-1)}\\)\n\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac12\\left(\\frac{x-\\mu}\\sigma\\right)^2}\\)\n\\(\\frac1{b-a}\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(P(X\\le k)\\)\n\n\\(1-(1-p)^{\\lfloor k\\rfloor+1}\\)\n\n\\(\\frac{\\lfloor k\\rfloor-(a-1)}{b-(a-1)}\\)\n\n\\(\\frac{x-a}{b-a}\\)\n\\(1-e^{-\\lambda x}\\)\n\n\nMean\n\\(np\\)\n\\(\\frac{1-p}p\\)\n\\(\\lambda\\)\n\\(\\frac{a+b}2\\)\n\\(\\mu\\)\n\\(\\frac{a+b}2\\)\n\\(\\frac1\\lambda\\)\n\n\nVariance\n\\(np(1-p)\\)\n\\(\\frac{1-p}{p^2}\\)\n\\(\\lambda\\)\n\\(\\frac{(b-(a-1))^2-1}{12}\\)\n\\(\\sigma^2\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\\(\\frac1{\\lambda^2}\\)\n\n\nR functions\ndbinom, pbinom, qbinom, rbinom\ndgeom, pgeom, qgeom, rgeom\ndpois, ppois, qpois, rpois\nsample\ndnorm, pnorm, qnorm, rnorm\ndunif, punif, qunif, runif\ndexp, pexp, qexp, rexp"
  },
  {
    "objectID": "R02_Distributions.html#bernoulli",
    "href": "R02_Distributions.html#bernoulli",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.1 Bernoulli",
    "text": "7.1 Bernoulli\nLike the flip of a fair or unfair coin. X=1 if the coin comes up heads, 0 on tails.\n\n#Parameters\np &lt;- .3 # the probability of a success\n\n#Generating 10 random values\nrbinom(10, size=1, prob=p)\n\n [1] 0 1 0 1 1 1 1 0 1 1\n\n\nExpected value is \\[\\sum_k k\\cdot Pr[X=k]\\]\n\n#All possible values\nk &lt;- 0:1\n#Associated probabilities\nPr &lt;- dbinom(k, size=1, prob=p)\n#expectation\nsum(k*Pr)\n\n[1] 0.3\n\n\nThe expected value is \\(p\\) and the variance is \\(p(1-p)\\)\n\nbernoulli.sim &lt;- rbinom(100, size=1, prob=p)\n\n# Expectation\np\n\n[1] 0.3\n\n# from sample\nmean(bernoulli.sim)\n\n[1] 0.25\n\n# Variance\np*(1-p)\n\n[1] 0.21\n\n#from sample\nvar(bernoulli.sim)\n\n[1] 0.1893939"
  },
  {
    "objectID": "R02_Distributions.html#binomial",
    "href": "R02_Distributions.html#binomial",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.2 Binomial",
    "text": "7.2 Binomial\nLike flipping a fair (or unfair) coin \\(n\\) times, counting the number of heads.\n\n#Parameters\nn &lt;- 8  #the number of flips / attempts\np &lt;- .4 #the probability of success\n\n#Generate 10 random values\nrbinom(10, size=n, prob=p)\n\n [1] 2 4 1 5 3 3 6 3 3 1\n\n#Calculate P(X=3)\ndbinom(x=3, size=n, prob=p)\n\n[1] 0.2786918\n\nchoose(n, 3)*p^3*(1-p)^(n-3)\n\n[1] 0.2786918\n\n#Calculate all probabilities\ndbinom(x=0:8, size=n, prob=p)\n\n[1] 0.01679616 0.08957952 0.20901888 0.27869184 0.23224320 0.12386304 0.04128768\n[8] 0.00786432 0.00065536\n\nsum(dbinom(x=0:8, size=n, prob=p))\n\n[1] 1\n\n#Calculate P(X &lt;= 3)\npbinom(q=3, size=n, prob=p)\n\n[1] 0.5940864\n\n\nVisualize the binomial probability mass function:\n\nk &lt;- 0:8\npk &lt;- dbinom(x=0:8, size=n, prob=p)\nbarplot(height=pk, names=k, main=paste0(\"pmf of Binom(\",n,\",\",p,\")\"))\n\n\n\n\nExpected value is \\[\\sum_k k\\cdot Pr[X=k]\\]\n\n#All possible values\nk &lt;- 0:n\n#Associated probabilities\nPr &lt;- dbinom(k, size=n, prob=p)\n#expectation\nsum(k*Pr)\n\n[1] 3.2\n\nn*p\n\n[1] 3.2\n\n\nThe probability mass function\n\nbarplot(Pr, names=k, main=\"Probability Mass Function of Binomial(8,.4)\")\n\n\n\n\nThe cumulative distribution function\n\nx &lt;- 0:8\ncdfx &lt;- pbinom(x, size=n,prob=.4)\nplot(x, cdfx, type=\"s\", main=\"Cumulative Distribution Function of Binomial(8,.4)\", ylim=c(0,1))\n\n\n\n\nThe expected value is \\(np\\) and the variance is \\(np(1-p)\\)\n\nbinomial.sim &lt;- rbinom(10000, size=n, prob=p)\n\n# Expectation\nn*p\n\n[1] 3.2\n\n# from sample\nmean(binomial.sim)\n\n[1] 3.2267\n\n# Variance\nn*p*(1-p)\n\n[1] 1.92\n\n#from sample\nvar(binomial.sim)\n\n[1] 1.9307"
  },
  {
    "objectID": "R02_Distributions.html#geometric",
    "href": "R02_Distributions.html#geometric",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.3 Geometric",
    "text": "7.3 Geometric\nCounting how many tails before the first head; the number of failures before the first success (independent trials, probability of success remains constant)\n\n#Parameters\np &lt;- .4 #The probability of success on each trial\n\n#Generate 10 random values\nrgeom(10, prob=p)\n\n [1] 1 3 0 1 2 0 0 0 2 0\n\n\nThe probability mass function…. only going out to 20, but the support is infinite!\n\nk &lt;- 0:20\nPr &lt;- dgeom(k, prob=p)\nbarplot(Pr, names=k, main=\"Probability Mass Function of Geom(.4)\")\n\n\n\n\nThe cumulative distribution function\n\ncdfx &lt;- cumsum(Pr)\n\n# if you want it to look really proper\nn &lt;- length(k)\nplot(x = NA, y = NA, pch = NA, \n     xlim = c(0, max(k)), \n     ylim = c(0, 1),\n     ylab = \"Cumulative Probability\",\n     main = \"Cumulative Distribution Function of Geom(.4)\")\npoints(x = k[-n], y = cdfx[-n], pch=19)\npoints(x = k[-1], y = cdfx[-n], pch=1)\nfor(i in 1:(n-1)) points(x=k[i+0:1], y=cdfx[c(i,i)], type=\"l\")\n\n\n\n\nTo calculate probabilities from a geometric rv, use the pgeom function\n\n#Pr(X &lt;= 3)\npgeom(3, prob=p)\n\n[1] 0.8704\n\n\nTo get individual probabilities at k\n\n#Pr(X=3)\ndgeom(3, prob=p)\n\n[1] 0.0864\n\n\nBut it’s probably faster to just use a step type plot. The vertical lines are not technically part of the plot though.\n\nplot(k, cdfx, type=\"s\", main=\"Cumulative Distribution Function of Geom(.4)\", ylim=c(0,1))\npoints(k, cdfx, pch = 16, col = \"blue\")\n\n\n\n\nYou can get the cumulative probabilities from the pgeom function\n\nplot(k, pgeom(k, prob=.4), type=\"s\", main=\"Cumulative Distribution Function of Geom(.4)\", ylim=c(0,1))\npoints(k, cdfx, pch = 16, col = \"blue\")\n\n\n\n\nThe expected value is \\(\\dfrac{1-p}{p}\\) and the variance is \\(\\dfrac{1-p}{p^2}\\)\n\ngeom.sim &lt;- rgeom(10000, prob=p)\n\n# Expectation\n(1-p)/p\n\n[1] 1.5\n\n# from sample\nmean(geom.sim)\n\n[1] 1.5067\n\n# Variance\n(1-p)/(p^2)\n\n[1] 3.75\n\n#from sample\nvar(geom.sim)\n\n[1] 3.776933"
  },
  {
    "objectID": "R02_Distributions.html#poisson",
    "href": "R02_Distributions.html#poisson",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.4 Poisson",
    "text": "7.4 Poisson\nLike the number of times something occurs during a fixed time window\n\n#Parameters\nl &lt;- 10.5 #The rate parameter, average occurrences per unit time\n     #It is lambda, but I'll call it \"l\"\n\n#Generate 10 random values\nrpois(10, lambda=l)\n\n [1]  8  7 13 15  5 12 13  8  8 14\n\n\nPMF and CDF\n\npar(mfrow=c(1,2))\nk &lt;- 0:20\nPr &lt;- dpois(k,l)\n\nbarplot(Pr, names=k, main=\"PMF of Pois(3)\")\nplot(k, ppois(k, l), type=\"s\", main=\"CDF of Pois(3)\", ylim=c(0,1))\npoints(k, ppois(k, l), pch = 16, col = \"blue\")\n\n\n\n\nThe expected value and variance are both is \\(\\lambda\\)\n\npois.sim &lt;- rpois(10000, lambda=l)\n\n# Expectation & Variance\nl\n\n[1] 10.5\n\n# mean from sample\nmean(pois.sim)\n\n[1] 10.4707\n\n#variance from sample\nvar(pois.sim)\n\n[1] 10.5538"
  },
  {
    "objectID": "R02_Distributions.html#uniform-discrete",
    "href": "R02_Distributions.html#uniform-discrete",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.5 Uniform (Discrete)",
    "text": "7.5 Uniform (Discrete)\nLike rolling a fair die\n\n#Parameters\na &lt;- 1 #lower bound, inclusive\nb &lt;- 6 #upper bound, inclusive\n\n#Generate 10 random values\nsample(a:b, 10, replace=TRUE) #replace=TRUE is important\n\n [1] 4 3 2 1 6 4 1 4 4 3\n\n\nThe PMF and CDF\n\npar(mfrow=c(1,2))\nk &lt;- a:b\nPr &lt;- rep(1/(b-a+1), length(k))\n\nbarplot(Pr, names=k, main=\"PMF of Unif(1,6)\")\nplot(k, cumsum(Pr), type=\"s\", main=\"CDF of Unif(1,6)\", ylim=c(0,1))\npoints(k, cumsum(Pr), pch = 16, col = \"blue\")"
  },
  {
    "objectID": "R02_Distributions.html#continuous-uniform",
    "href": "R02_Distributions.html#continuous-uniform",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.6 Continuous Uniform",
    "text": "7.6 Continuous Uniform\nThe backbone of random variable generation - for example a random decimal between 0 and 1\n\n# Parameters\na &lt;- 0 #lower bound\nb &lt;- 1 #upper bound\n\n#generate 10 random values\nrunif(10, min=a, max=b)\n\n [1] 0.4265918 0.7514635 0.4387945 0.6073087 0.5939819 0.6572275 0.7774568\n [8] 0.7139346 0.5636088 0.6903111\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(a,b, length.out=100)\n\nplot(x, dunif(x, a, b), type=\"l\", main=\"PDF of Unif(0,1)\", ylab=\"density\", ylim=c(0,1))\nplot(x, punif(x, a, b), type=\"l\", main=\"CDF of Unif(0,1)\", ylab=\"F(x)\")\n\n\n\n\nThe expected value is \\(\\frac{a+b}{2}\\) and the variance is \\(\\frac{(b-a)^2}{12}\\)\n\nunif.sim &lt;- runif(10000, min=a, max=b)\n\n# Expectation\n(a+b)/2\n\n[1] 0.5\n\n# mean from sample\nmean(unif.sim)\n\n[1] 0.4973751\n\n#variance\n(b-a)^2/12\n\n[1] 0.08333333\n\n#variance from sample\nvar(unif.sim)\n\n[1] 0.08360614"
  },
  {
    "objectID": "R02_Distributions.html#normal-gaussian",
    "href": "R02_Distributions.html#normal-gaussian",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.7 Normal / Gaussian",
    "text": "7.7 Normal / Gaussian\nMany things in the world are normally distributed - useful for modeling when the distribution is symmetric and probability is densest in the middle with decreasing tails.\n\n#Parameters\nmu &lt;- 5 #the mean/location of the distribution\nsigma2 &lt;- 4 #the variance is in squared units!!\nsigma &lt;- sqrt(sigma2) #sigma is the standard deviation, not the variance\n\n#generate 10 random values\nrnorm(10, mean=mu, sd=sigma)\n\n [1] 5.801261 7.958501 3.289157 4.331514 5.160004 4.634221 4.495469 9.406295\n [9] 6.611756 4.606657\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(mu-3*sigma, mu+3*sigma, length.out=100)\n\nplot(x, dnorm(x, mu, sigma), type=\"l\", main=\"PDF of Normal(5, 2^2)\", ylab=\"density\")\nplot(x, pnorm(x, mu, sigma), type=\"l\", main=\"CDF of Normal(5, 2^2)\", ylab=\"F(x)\")\n\n\n\n\nApproximate the expected value numerically\n\\[\\int_{-\\infty}^\\infty t f(t) dt\\]\n\nmu &lt;- 15\nsigma &lt;- 4\nt &lt;- seq(mu-4*sigma, mu+4*sigma, length.out=1000)\nd &lt;- dnorm(t, mean=mu, sd=sigma)\n\nw &lt;- t[2]-t[1]\n\nhead(t)\n\n[1] -1.0000000 -0.9679680 -0.9359359 -0.9039039 -0.8718719 -0.8398398\n\nhead(d)\n\n[1] 3.345756e-05 3.454551e-05 3.566656e-05 3.682162e-05 3.801165e-05\n[6] 3.923763e-05\n\n#Expected value\nsum( t * (d*w))\n\n[1] 14.99907\n\n\nThe expected value is \\(\\mu\\) and the variance is \\(\\sigma^2\\)\n\nnormal.sim &lt;- rnorm(10000, mean=mu, sd=sigma)\n\n# Expectation\nmu\n\n[1] 15\n\n# mean from sample\nmean(normal.sim)\n\n[1] 15.00065\n\n#variance\nsigma2\n\n[1] 4\n\n#variance from sample\nvar(normal.sim)\n\n[1] 15.73667\n\n\n\n7.7.1 Simulate a Normal probability\n\nnormal.sim &lt;- rnorm(1000000)\n\nx &lt;- .25 &lt;= normal.sim & normal.sim &lt;=.75\n\nsum(x)/1000000\n\n[1] 0.174158\n\npnorm(.75)-pnorm(.25)\n\n[1] 0.1746663"
  },
  {
    "objectID": "R02_Distributions.html#two-normally-distributed-random-variables",
    "href": "R02_Distributions.html#two-normally-distributed-random-variables",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.8 Two normally distributed random variables",
    "text": "7.8 Two normally distributed random variables\n\n# X1 ~ Normal(1,1^2)\nx1 &lt;- rnorm(10000, mean=1, sd=1)\n\n#X2 ~ Normal(2, 2^2)\nx2 &lt;- rnorm(10000, mean=2, sd=2)\n\npar(mfrow=c(1,2))\nhist(x1)\nhist(x2)\n\n\n\n\nLet’s check the variances individually\n\nvar(x1)\n\n[1] 1.003988\n\nvar(x2)\n\n[1] 3.991855\n\n\nIn theory, Var(X1+X2) = Var(X1) + Var(X2) We check with sample variance\n\nvar(x1+x2)\n\n[1] 5.022937\n\nvar(x1)+var(x2)\n\n[1] 4.995843\n\nvar(x1)+var(x2)+2*cov(x1,x2)\n\n[1] 5.022937\n\n\nLook at the plot of these two variables\n\nmean(x1+x2)\n\n[1] 3.032148\n\nmean(x1)+mean(x2)\n\n[1] 3.032148\n\nplot(x1,x2)\n\n\n\nhist(x1+x2, breaks=50)"
  },
  {
    "objectID": "R02_Distributions.html#uniform-uniform",
    "href": "R02_Distributions.html#uniform-uniform",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.9 Uniform + uniform",
    "text": "7.9 Uniform + uniform\n\nX &lt;- runif(100000, 0, 1)\nhist(X)\n\n\n\nY &lt;- runif(100000, 0, 1)\nhist(Y)\n\n\n\nhist(X+Y)\n\n\n\nZ &lt;- runif(100000, 0, 1)\nhist(X+Y+Z, breaks=100)\n\n\n\n\n\n7.9.1 Geometric Distribution Limit\nLet’s consider a geometric that counts the number of days before a electronic component malfunctions Suppose that every day there’s a 25% chance of malfunction.\n\np=.25\nrandom.geom &lt;- rgeom(10000, prob=p)\nhist(random.geom, breaks=25)\n\n\n\n\nWhat if more than one malfunction could happen per day? Like we replace the part immediately when it malfunctions. We could divide the day into \\(N\\) parts and use a geometric for each part of the day. If \\(X\\)=k, then the proportion of the day it took to break is \\(k/N\\) Let’s start with \\(N\\)=2 and go up from there\n\npar (mfrow=c(2,2))\nN &lt;- 2\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.5055\n\nN &lt;- 6\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.8146\n\nN &lt;- 24\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 4.016775\n\nN &lt;- 100\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\n\n\n\nmean(geom.sim)\n\n[1] 4.028912"
  },
  {
    "objectID": "R02_Distributions.html#exponential",
    "href": "R02_Distributions.html#exponential",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.10 Exponential",
    "text": "7.10 Exponential\nFor modeling waiting times - the length of time until an event occurs. It’s a continuous version of a geometric, if you start looking at smaller and smaller time units (e.g. days -&gt; hours -&gt; minutes -&gt; seconds)\n\n#Parameters\nl &lt;- 3 #The rate parameter, the average number of occurrences per unit time\n\n#Generate 10 random values\nrexp(10, rate=l)\n\n [1] 1.05014281 0.07847060 0.37205927 0.29786303 0.11721248 0.11242535\n [7] 0.31164338 0.67525800 0.37148634 0.05199775\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(0, 6/l, length.out=100)\n\nplot(x, dexp(x, l), type=\"l\", main=\"PDF of Exp(3)\", ylab=\"density\")\nplot(x, pexp(x, l), type=\"l\", main=\"CDF of Exp(3)\", ylab=\"F(x)\")\n\n\n\n\nThe expected value is \\(\\dfrac{1}{\\lambda}\\) and the variance is \\(\\frac{1}{\\lambda^2}\\)\n\nexp.sim &lt;- rexp(10000, l)\n\n# Expectation\n1/l\n\n[1] 0.3333333\n\n# mean from sample\nmean(exp.sim)\n\n[1] 0.337423\n\n#variance\n1/l^2\n\n[1] 0.1111111\n\n#variance from sample\nvar(exp.sim)\n\n[1] 0.115274\n\n\nIf rate = 1, what is \\(Pr(X &lt; 5)\\)\n\npexp(5, rate=1)\n\n[1] 0.9932621\n\n\n\n7.10.1 Relationship between an exponential and a Poisson\nlambda = 10\n\nsimulated.exp &lt;- rexp(10000, rate=10)\nhist(simulated.exp)\n\n\n\nhead(simulated.exp)\n\n[1] 0.007536642 0.061080740 0.008189010 0.001095974 0.079767192 0.547604518\n\nconverted.poisson &lt;- as.vector(table(floor(cumsum(simulated.exp))))\npar(mfrow=c(1,2))\n    hist(converted.poisson)\n    hist(rpois(500, lambda=10))"
  },
  {
    "objectID": "R02_Distributions.html#simulate-stock-prices",
    "href": "R02_Distributions.html#simulate-stock-prices",
    "title": "7  Random Variable Distributions R Examples",
    "section": "7.11 Simulate stock prices",
    "text": "7.11 Simulate stock prices\nHere’s an example of how you could combine normally distributed random variables for a model of daily stock prices. This is an example of a random walk.\n\ndeltas &lt;- rnorm(30)\nX &lt;- 50 + c(0,cumsum(deltas))\nplot(x=1:31, y=X, type=\"l\")"
  },
  {
    "objectID": "mc_practice.html",
    "href": "mc_practice.html",
    "title": "11  Monte Carlo Practice",
    "section": "",
    "text": "12 Practice Problems",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#integration-by-darts",
    "href": "mc_practice.html#integration-by-darts",
    "title": "11  Monte Carlo Practice",
    "section": "12.1 Integration by darts",
    "text": "12.1 Integration by darts\nEstimate \\(\\int_0^3 (x^3-3x^2+x+3)\\,dx\\). To do so, use the fact that \\[\\int_a^b g(x)dx = \\int_a^b \\frac{g(x)}{f(x)}f(x)dx\\]. Let \\(f(x)\\) be the density function for a uniform random variable over \\([0,3]\\).\n\nf&lt;-function(x){return(x^3-3*x^2+x+3)}\nplot(x=seq(0,3,.1), f(seq(0,3,.1)), type=\"l\", ylim=c(0,7), xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Solution\n\n#We can define a uniform density function between 0 and 3\n#call this g(x)=dunif(x,0,3)\n#The integral trick says that int_0^7 f(x)dx = E(f(x)/g(x)).\nNMC &lt;- 1000\nx &lt;- runif(NMC, 0,3)\nmean(f(x)/dunif(x,0,3))\n\n[1] 6.84278",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#eulers-constant",
    "href": "mc_practice.html#eulers-constant",
    "title": "11  Monte Carlo Practice",
    "section": "12.2 Euler’s Constant",
    "text": "12.2 Euler’s Constant\nSuppose \\(X_1, X_2,\\ldots, X_n \\sim Unif(0,1)\\). Let \\(N\\) be the lowest index such that \\(X_1+X_2+\\cdots+X_N &gt; 1\\). \\(\\mathbb{E}(N)=e\\). Use this fact to estimate Euler’s constant \\(e\\) using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 1000\nNs &lt;- 0\nfor(i in 1:NMC){\n  sum &lt;- 0;   N &lt;- 0\n  while(sum &lt; 1){\n    sum &lt;- sum + runif(1)\n    N &lt;- N+1\n  }\n  Ns[i] = N\n}\nmean(Ns)\n\n[1] 2.718\n\nexp(1)\n\n[1] 2.718282",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#dice-probability",
    "href": "mc_practice.html#dice-probability",
    "title": "11  Monte Carlo Practice",
    "section": "12.3 Dice Probability",
    "text": "12.3 Dice Probability\nI roll 3 six sided dice. What is the probability that the sum of the dice is at least 12? Estimate the answer with Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\nresults &lt;- 0\nfor(i in 1:NMC){\n  results[i] &lt;- sum(sample(6, 3, replace=TRUE))&gt;=12\n}\nmean(results)\n\n[1] 0.37",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#sales-and-price",
    "href": "mc_practice.html#sales-and-price",
    "title": "11  Monte Carlo Practice",
    "section": "12.4 Sales and Price",
    "text": "12.4 Sales and Price\nBased on your market research, you believe that there are equal chances that the market will be Slow, OK, or Hot.\n\nIn the “Slow market” scenario, you expect to sell 50,000 units at an average selling price of $11.00 per unit.\nIn the “OK market” scenario, you expect to sell 75,000 units, but you’ll likely realize a lower average selling price of $10.00 per unit.\nIn the “Hot market” scenario, you expect to sell 100,000 units, but this will bring in competitors who will drive down the average selling price to $8.00 per unit.\n\nAnother uncertain variable is Unit Cost.B Your firm’s production manager advises you that unit costs may be anywhere from $5.50 to $7.50. Use a Monte Carlo simulation to estimate the expected net profit (revenue - cost).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\nprofit &lt;- 0\nprices &lt;- c(11, 10, 8)\nquantity &lt;- c(50000, 75000, 100000)\nfor(i in 1:NMC){\n  market &lt;- sample(1:3, 1)\n  unitCost &lt;- runif(1, 5.5, 7.5)\n  profit[i] &lt;- quantity[market]*(prices[market]-unitCost)\n}\nmean(profit)\n\n[1] 220447.1",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#black-scholes-option-pricing",
    "href": "mc_practice.html#black-scholes-option-pricing",
    "title": "11  Monte Carlo Practice",
    "section": "12.5 Black-Scholes Option Pricing",
    "text": "12.5 Black-Scholes Option Pricing\nWe start with the Black-Scholes-Merton formula (\\(1973\\)) for the pricing of European call options on an underlying (e.g. stocks and indexes) without dividends:\n\\(\\begin{eqnarray*} C(S_t, K, t, T, r, \\sigma) &=& S_t\\cdot N(d_1) - e^{-r(T-t)}\\cdot K \\cdot N(d_2)\\newline\\newline N(d) &=& \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^d e^{-\\frac{1}{2}x^2}dx \\newline\\newline d_1 &=& \\frac{\\log\\frac{S_t}{K} + (T-t)\\left(r + \\frac{\\sigma^2}{2}\\right)}{\\sigma\\sqrt{T-t}}\\newline\\newline d_2 &=& \\frac{\\log\\frac{S_t}{K} + (T-t)\\left(r - \\frac{\\sigma^2}{2}\\right)}{\\sigma\\sqrt{T-t}}. \\end{eqnarray*}\\)\nIn the equations above \\(S_t\\) is the price of the underlying at time \\(t\\), \\(\\sigma\\) is the constant volatility (standard deviation of returns) of the underlying, \\(K\\) is the strike price of the option, \\(T\\) is the maturity date of the option, \\(r\\) is the risk-free short rate.\nThe Black-Scholes-Merton (\\(1973\\)) stochastic differential equation is given by \\(dS_t = rS_t dt + \\sigma S_t dZ_t,\\) where $Z(t)$ is the random component of the model (a Brownian motion). In this model, the risky underlying follows, under risk neutrality, a geometric Brownian motion with a stochastic differential equation (SDE).\nWe will look at the discretized version of the BSM model (Euler discretization), given by \\(S_t = S_{t-\\Delta t} \\exp\\left(\\left(r - \\frac{\\sigma^2}{2}\\right)\\Delta t + \\sigma\\sqrt{\\Delta t}z_t \\right).\\)\nThe variable \\(z\\) is a standard normally distributed random variable, \\(0 &lt; \\Delta t &lt; T\\), a (small enough) time interval. It also holds \\(0 &lt; t \\leq T\\) with \\(T\\) the final time horizon.\nIn this simulation we use the values \\(S_0 = 100\\), \\(K = 105\\), \\(T = 1.0\\), \\(r = 0.05\\), \\(\\sigma = 0.2\\). Let’s see what is the expected option price using these parameters and assuming \\(t=0\\), then we will run a Monte Carlo simulation to find the option price under the same conditions.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nS_0 &lt;- 100; K &lt;- 105; endT &lt;- 1.0; r &lt;- 0.05; sigma &lt;- 0.2;\ndt &lt;- 0.01\n\nST &lt;- 0 #vector to hold values\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  St &lt;- S_0\n  t &lt;- 0\n  while(t &lt; endT){\n    St &lt;- St*exp((r-sigma^2/2)*dt + sigma*sqrt(dt)*rnorm(1))\n    t &lt;- t+dt\n  }\n  ST[i] &lt;- St\n}\nhist(ST)\n\n\n\n\n\n\n\nmean(ST)\n\n[1] 104.3716",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-sphere",
    "href": "mc_practice.html#average-distance-in-a-sphere",
    "title": "11  Monte Carlo Practice",
    "section": "12.6 Average Distance in a sphere",
    "text": "12.6 Average Distance in a sphere\nEstimate the average distance between two points in a sphere of radius 1 using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\nfor(i in 1:NMC){\n  #get a random point\n  repeat{\ncoord1 &lt;- runif(2, -1,1)\nif(sqrt(sum(coord1^2))&lt;=1){\n  break\n}\n  }\n  #get another random point\n  repeat{\ncoord2 &lt;- runif(2, -1,1)\nif(sqrt(sum(coord2^2))&lt;=1){\n  break\n}\n  }\n  dist[i] = sqrt(sum((coord2-coord1)^2))\n}\nmean(dist)\n\n[1] 0.9080386",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-square",
    "href": "mc_practice.html#average-distance-in-a-square",
    "title": "11  Monte Carlo Practice",
    "section": "12.7 Average distance in a square",
    "text": "12.7 Average distance in a square\nEstimate the average distance between two points in a square with side lengths 1 using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\nfor(i in 1:NMC){\n  coord1 &lt;- runif(2, 0, 1)\n  coord2 &lt;- runif(2, 0, 1)\n  dist[i] = sqrt(sum((coord2-coord1)^2))\n}\nmean(dist)\n\n[1] 0.5248204",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-simplex",
    "href": "mc_practice.html#average-distance-in-a-simplex",
    "title": "11  Monte Carlo Practice",
    "section": "12.8 Average distance in a simplex",
    "text": "12.8 Average distance in a simplex\nEstimate the average distance between two points on the 3-simplex (points \\((x,y,z)\\) such that \\(0\\leq x,y,z \\leq 1\\) and \\(x+y+z=1\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\n\nfor(i in 1:NMC){\n  \n#A good way to think of the sampling is to pick two random dividing points between 0 and 1. Add 0 and 1 to the list and then sort them, then find the differences\n  pt1 &lt;- diff(sort(c(0,1,runif(2))))\n  pt2 &lt;- diff(sort(c(0,1,runif(2))))\n\n  dist[i] &lt;- sqrt(sum((pt1-pt2)^2))\n}\nmean(dist)\n\n[1] 0.5603156",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#completion-time",
    "href": "mc_practice.html#completion-time",
    "title": "11  Monte Carlo Practice",
    "section": "12.9 Completion Time",
    "text": "12.9 Completion Time\nLet’s assume we have a process constructed from 3 stages (X1, X2, X3). Each one has an average duration (5, 10 and 15 minutes) which vary following the normal distribution and we know their standard deviation (all 1 minute). We want to know what is the probability that the process will exceed 34 minutes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC&lt;-10000\nduration &lt;- rnorm(NMC, 5, 1)+rnorm(NMC, 10, 1) + rnorm(NMC, 15, 1)\nmean(duration &gt; 34)\n\n[1] 0.0104",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#fitting-blocks",
    "href": "mc_practice.html#fitting-blocks",
    "title": "11  Monte Carlo Practice",
    "section": "12.10 Fitting Blocks",
    "text": "12.10 Fitting Blocks\nIn this example let’s assume we want to assemble three blocks inside a container of a given width. The box has a nominal width of 16.5mm, the three blocks have nominal widths of 4, 6 and 6 mm. By design there is a nominal gap of 0.5mm. However there are variations in the production of the blocks.\n\nThe box has variation in width between -0.1mm to +0.1mm uniformly at random\nThe other boxes have margins of error of 0.2, 0.3 and 0.25 respectively (+ or - uniformly at random)\n\nEstimate the probability that the 3 blocks will fit in the box.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\n\nbox.w &lt;- runif(NMC, 16.5-.1, 16.5+.1)\nblock1.w &lt;- runif(NMC, 4-.2, 4+.2)\nblock2.w &lt;- runif(NMC, 6-.3, 6+.3)\nblock3.w &lt;- runif(NMC, 6-.25, 6+.25)\n\nfits &lt;- box.w &gt; block1.w+block2.w+block3.w\nmean(fits)\n\n[1] 0.97",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#roomba-zoomba",
    "href": "mc_practice.html#roomba-zoomba",
    "title": "11  Monte Carlo Practice",
    "section": "12.11 Roomba Zoomba",
    "text": "12.11 Roomba Zoomba\nA circular vacuum robot that has a radius of 20cm will roll around a room that is 10m x 10m square. It begins in the exact center of the room. The way it moves is:\n\nIt picks a direction uniformly at random from 0 to 360 degrees (\\(0\\) to \\(2\\pi\\))\nIt travels for anywhere between 0m and 1m, uniformly at random.\nIf it hits a wall it bounces off at the angle of incidence.\nIt repeats\n\n\nIf there is a speck of dirt located at coordinates (1,1), what is the average amount of time it will take for the robot to collect this dirt?\nIf the robot travels at 1m/sec, what is the expected length of time for the robot to clean 50% of the floor?\n\nSolution in-progress!\n\nx &lt;- 5; y&lt;- 5\n\nmaxN &lt;- 1000\n\nfor(step in 1:maxN){\n  theta &lt;- runif(1, 0, 2*pi)\n  dist &lt;- runif(1,0,1)\n  xy_new &lt;- c(x[step],y[step]) + dist*c(cos(theta), sin(theta))\n  if(xy_new[1]&lt;.2){xy_new[1]=2*.2-xy_new[1]}\n  if(xy_new[2]&lt;.2){xy_new[2]=2*.2-xy_new[2]}\n  if(xy_new[1]&gt;9.8){xy_new[1]=2*9.8-xy_new[1]}\n  if(xy_new[2]&gt;9.8){xy_new[2]=2*9.8-xy_new[2]}\n  x[step+1] &lt;- xy_new[1]\n  y[step+1] &lt;- xy_new[2]\n}\nplot(x,y, type=\"l\", xlim=c(0,10), ylim=c(0,10))\nabline(h=c(0,10), col=\"red\")\nabline(v=c(0,10), col=\"red\")",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#chutes-and-ladders",
    "href": "mc_practice.html#chutes-and-ladders",
    "title": "11  Monte Carlo Practice",
    "section": "12.12 Chutes and Ladders",
    "text": "12.12 Chutes and Ladders\nConsider the game Chutes and Ladders. On each turn you roll a 6 sided die and move that many spaces. If you land on a ladder you move up to a new spot, and if you land on a slide you move down. Perform a monte Carlo simulation to answer the following questions:\n\n\nWhat’s the average number of rolls to win?\nHow many chutes and ladders will a user typically hit?\nAre the chutes and ladders balanced?\nAre some squares hit more often than others?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#We need to create a vector of the board\nboard &lt;- 1:100\n#ladders\nboard[1]&lt;- 38\nboard[4] &lt;- 14\nboard[9] &lt;- 31\nboard[21] &lt;- 42\nboard[36] &lt;- 44\nboard[28] &lt;- 84\nboard[51] &lt;- 67\nboard[80] &lt;- 100\nboard[71] &lt;- 91\n#chutes\nboard[16] &lt;- 6\nboard[49] &lt;- 11\nboard[48] &lt;- 26\nboard[62] &lt;- 19\nboard[87] &lt;- 24\nboard[56] &lt;- 53\nboard[64] &lt;- 60\nboard[93] &lt;- 73\nboard[95] &lt;- 75\nboard[98] &lt;- 78\n\nNMC &lt;- 100\nset.seed(1)\n\nrolls &lt;- rep(0, NMC)\nnChutes &lt;- rep(0, NMC)\nnLadders &lt;- rep(0,NMC)\nlandOn &lt;- matrix(rep(0, NMC*100),nrow=NMC)\n\nfor(i in 1:NMC){\n  position &lt;- 1\n  repeat{\nlandOn[i,position] &lt;- landOn[i,position] + 1\nif(position==100){break;}\nroll &lt;- sample(6,1)\nrolls[i] &lt;- rolls[i] + 1\nposition.new &lt;- min(100,position+roll)\nif(board[position.new] &lt; position.new){\n  nChutes[i] &lt;- nChutes[i] + 1\n}\nif(board[position.new] &gt; position.new){\n  nLadders[i] &lt;- nLadders[i] +1\n}\nposition &lt;- board[position.new]\n  }\n}\n\nmean(rolls)\n\n[1] 34.66\n\nmean(nChutes)\n\n[1] 3.93\n\nmean(nLadders)\n\n[1] 3.08\n\nplot(colMeans(landOn))\n\n\n\n\n\n\n\nlandOnDF &lt;- data.frame(space=1:100, proportion &lt;- colMeans(landOn))\ntail(landOnDF[order(landOnDF[,2]),],10)\n\n    space proportion....colMeans.landOn.\n42     42                           0.68\n31     31                           0.83\n24     24                           0.84\n6       6                           0.86\n11     11                           0.86\n26     26                           0.86\n44     44                           0.93\n1       1                           1.00\n100   100                           1.00\n84     84                           1.02\n\n\nIt takes an average of 36.78 turns to finish the game The average number of chutes slid down is 4 per game. The average number of ladders climbed is 3.22 per game. Chutes are more likely than ladders apparently; even though there are 10 chutes compared to 9 ladders, the expected number of chutes is much higher than ladders.\nThe most common spots (besides 1 and 100) are 44 and 26.\n44 is the result of landing on one of the ladders. 26 is the result of landing on one of the chutes.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#battleship",
    "href": "mc_practice.html#battleship",
    "title": "11  Monte Carlo Practice",
    "section": "12.13 Battleship",
    "text": "12.13 Battleship\nBuild a Battleship AI (https://github.com/mitchelljy/battleships_ai)\n(Rules of the game: https://www.hasbro.com/common/instruct/battleship.pdf)\nBattleship is a game played on 10x10 grid. There are 5 boats of lengths 2, 3, 3, 4, and 5. The Monte Carlo AI is built like this:\n\nTake current board state\nSimulate \\(N\\) samples, each is a random placement of a remaining ships (knowing where hits and misses have been observed.) . Be sure to not put boats in a space that has been guessed, and if a hit has been found, be sure that boats cover those spots.\nStack all of the simulations and sum the total number of ships in each square (emphasise ships that overlap existing hits)\nTake the mean for each sqaure, giving us a frequency matrix or heatmap\nPick the largest value corresponding to a legal move in the matrix\n\nThis is how the AI decides which square to guess at every state of the game. As an added challenge, you can create additional logic to have the computer make the guess - taking the spot that has the highest likelihood - and determine if it is a hit, or if it sinks a boat. You will need to initialize the board with a random arrangement of the boats.\n\nYou will need a function generate_board &lt;- function(width=10, height=10, boats=c(TRUE,TRUE,TRUE,TRUE,TRUE)). This function should randomly place boats on the board, each either vertically or horizontally, and return The layout. Number the boats 1,2,3,4,5. Be sure that numbers do not overlap each other when placed randomly. This function should return a matrix with values 0 if there is nothing placed in a spot, or the boat number. For example, when you place boat 5, it is 5 units long, so there should be 5 spots with the number 5 in them (And they should be in a straight line).\nYou will also need a function validate_board &lt;- function(board, state) which will check to see if the simulated board is legal given the state. The state should have 0s in locations that have not been tested, -1 in locations where misses have been numbers 1 through 5 where hits have been observed. If the number of \\(i\\)’s on the board equals the number of pegs in boat \\(i\\), then the AI will now that boat \\(i\\) has been sunk - you can convert all of these \\(i\\)’s into -1 so they will be ignored, and in the future we can dispance with randomly placing boat \\(i\\). A board is valid if there are no boats in a -1 spot, and any spot in an \\(i\\) spot contains boat \\(i\\).\nTo determine the next move you will need to generate boards until you have had at least 1 valid board (10 is better) to make a choice. If many spots have the same frequency, then you can just pick one at random, or pick the one with the lowest row and column.\n\nSee an implementation here",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#infection-spread-simulation-source",
    "href": "mc_practice.html#infection-spread-simulation-source",
    "title": "11  Monte Carlo Practice",
    "section": "12.14 Infection spread simulation source",
    "text": "12.14 Infection spread simulation source\nModel a city as a 1x1 square; The population is 2.5k, randomly located throughout the city. Pick one single individual to be infected.\nOn each time step, each individual walks in a random direction and moves .005 units (if it hits a wall, just place it at the edge of the city). After each time step, if an unaffected person is near an infected person (within .002) then the infection spreads to them, and they become infected.\n\nEstimate how many time steps it takes for 99% of the population to become infected.\nWhat if each person will become resistant after \\(X_i\\) time steps, where \\(X_i \\sim Geom(p)\\). Suppose \\(p=.5\\). This gives us the standard SIR model - individuals are susceptible, infected or resistant (either cured or dead). resistant individuals cannot become infected, and they cannot infect others. How does the esimate change?\nWhat if we change the population to 1000 individuals?\nWhat if we change the infection distance to be 0.004?\n\nSolution in progress",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#stock-price",
    "href": "mc_practice.html#stock-price",
    "title": "11  Monte Carlo Practice",
    "section": "12.15 Stock Price",
    "text": "12.15 Stock Price\nSuppose a stock price varies from day to day through a scaled process. If the stock price is \\(X_i\\) on day \\(i\\), \\(X_{i+1} = (1.01+Z/25) X_i\\) where \\(Z\\) is a standard normal random variable. Today the stock price is $50 per share. Use a Monte Carlo simulation to estimate the distribution of stock prices 30 days from now.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 1000\n  plot(NA, xlim=c(1,31), ylim=c(0,100))\n  finalPrice &lt;- 0\nfor(i in 1:NMC){\n  price &lt;- 50\n  for(day in 1:30){\nprice[day+1] &lt;- price[day] * (1.01 + rnorm(1, 0, 1)/25)\n  }\n  finalPrice[i] &lt;- price[31]\n  lines(x=1:31, price, col=rgb(0,0,0,.15))\n}\n\n\n\n\n\n\n\nmean(finalPrice)\n\n[1] 67.90084\n\nhist(finalPrice, prob=TRUE)\nlines(density(finalPrice))\n\n\n\n\n\n\n\n\n\n\n\nYou can buy a 30 day put option for $10 with a strike price of $65 from Sleazy Jim, the stock broker. This terms of this contract are these:\n\nYou pay $10 to Sleazy Jim today.\nIf after 30 days the stock price is below $65, you will buy a share of stock at the market price and Sleazy Jim will buy it from you for $65. You pocket the difference.\nIf after 30 days the stock price is above $65, you will do nothing.\n\n(In essence, Sleazy Jim is betting that the stock price will be higher than $65 after 30 days.)\nEven though interest rates are high now, let’s ignore the affect of interest rate. Using Monte Carlo simulation, estimate the expected value of this put option (the net profit) to determine whether it is a good idea to buy the put option.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngain &lt;- 65 - pmin(finalPrice, 65)\ncost &lt;- 10\n\nmean(gain - 10)\n\n[1] -5.383399\n\n\nIf you enter into this trade with Sleazy Jim, you have an expected loss of $5.53. Don’t make the trade!",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#patent-pending",
    "href": "mc_practice.html#patent-pending",
    "title": "11  Monte Carlo Practice",
    "section": "12.16 Patent Pending",
    "text": "12.16 Patent Pending\nYou have decided to apply for a patent to protect your IP, but you also did that in order to increase sales as you are aware that businesses deem a patented product more worthy.\nLet’s make the following assumptions:\n\nThere is a 50% chance that your product gets patented\nIf it does get patented, your sales go up by 25% — 75%, with 50% being the most likely case. (uniform distribution)\nWithout a patent you expect to sell between $1 — $9 million next year, with $3 million being the most likely case. Let the probabilities be given in the following table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales ($M)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nprobability\n1/33\n4/33\n7/33\n6/33\n5/33\n4/33\n3/33\n2/33\n1/33\n\n\n\n\nWe do not have to consider any costs or expenses\n\nProblem: Suppose a wholesaler offers to buy your entire production and inventory for the year for $6 million (you won’t be able to sell anything else), would you accept the offer?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsales.p &lt;- c(1,4,7,6,5,4,3,2,1)/33\nsales.v &lt;- 1:9\n\nrevenue &lt;- 0\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  getPatent &lt;- sample(0:1, size=1, prob=c(.5,.5))\n  sales.scalar &lt;- 1\n  if(getPatent){\nsales.scalar &lt;- 1 + runif(1, .25, .75)\n  }\n  revenue[i] &lt;- sample(sales.v, size=1, prob=sales.p) * sales.scalar\n}\nhist(revenue)\n\n\n\n\n\n\n\nmean(revenue)\n\n[1] 5.401985\n\n\nThe expected revenue is 5.75 million. The offer of 6 million is an attractive offer. I would go for that.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#social-network",
    "href": "mc_practice.html#social-network",
    "title": "11  Monte Carlo Practice",
    "section": "12.17 Social Network",
    "text": "12.17 Social Network\nConsider a social network with people \\(1,2,\\ldots, n\\). Suppose we have a symmetric social structure, in that if person \\(i\\) is friends with person \\(j\\), then person \\(j\\) is friends with person \\(i\\). Thus we can define a symmetric friendship matrix with entries \\(p_{ij}=p_{ji}\\).\nSuppose that each pair of people can be friends independently with probability \\(\\theta\\). The question we want to answer here is this: How big must \\(\\theta\\) be in order to be more than 95\\% certain that the entire social network will be connected - this means that every two people will be connected either directly or through one or more intermediate friends.\nYour goal is to write a function estimateProportionConnected(n,theta, NMC), which takes three parameters:\n\nn, the number of people in the network,\ntheta, the probability of a connection,\nNMC, the number of Monte Carlo simulations to run.\n\nThe function should\n\nCreate an empty vector to store the results (whether or not each simulation resulted in a connected network)\nIn a loop,\n\n\ngenerate a random symmetric social network of size n (a function generateNetwork(n, theta) will be useful)\nDetermine whether the network is connected or not\nSave the result in your results vector\n\n\nReturn the proportion of simulations that resulted in a connected network.\n\nHint: You can use the function concomFromMatAdj in the concom package. It takes a symmetric adjacency matrix and returns the connected components. In particular, you want to look at the number of connected components, in the ncomponents variable.\nGiven a matrix M, you just call concomFromMatAdj(M)$ncomponents\nIf the number of components is more than 1 then the social network is not connected.\nBonus: For a social network of size \\(50\\), come up with an estimate of the lowest value of \\(\\theta\\) that produces connected social networks 95% of the time.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#install.packages(\"concom\")\nlibrary(concom)\n\nWarning: package 'concom' was built under R version 4.3.3\n\ngenerateMatrix &lt;- function(n, theta){\n  M &lt;- matrix(data=rep(0, n*n), nrow=n)\n  for(i in 1:(n-1)){\nfor(j in 2:n){\n  if(runif(1) &lt; theta){\nM[i,j]=1\nM[j,i]=1\n  }\n}\n  }\n  return(M)\n}\n\nestimateProportionConnected &lt;- function(n,theta, NMC){\n  results &lt;- 0\n  for(i in 1:NMC){\nM &lt;- generateMatrix(n, theta)\nresults[i] &lt;- concomFromMatAdj(M)$ncomponents==1\n  }\n  return(mean(results))\n}\n\nfor(theta in seq(.08,.07,by=-.001)){\n  p &lt;- estimateProportionConnected(50, theta, 500)\n  print(paste(\"When theta =\",theta,\"connected with prob\",p))\n}\n\n[1] \"When theta = 0.08 connected with prob 0.956\"\n[1] \"When theta = 0.079 connected with prob 0.966\"\n[1] \"When theta = 0.078 connected with prob 0.958\"\n[1] \"When theta = 0.077 connected with prob 0.968\"\n[1] \"When theta = 0.076 connected with prob 0.964\"\n[1] \"When theta = 0.075 connected with prob 0.956\"\n[1] \"When theta = 0.074 connected with prob 0.948\"\n[1] \"When theta = 0.073 connected with prob 0.934\"\n[1] \"When theta = 0.072 connected with prob 0.946\"\n[1] \"When theta = 0.071 connected with prob 0.924\"\n[1] \"When theta = 0.07 connected with prob 0.93\"",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#other-examples",
    "href": "mc_practice.html#other-examples",
    "title": "11  Monte Carlo Practice",
    "section": "12.18 Other examples",
    "text": "12.18 Other examples\nCheck out some examples here",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html",
    "href": "R03_MonteCarloExamples.html",
    "title": "10  Monte Carlo Examples",
    "section": "",
    "text": "10.1 A few structures for Monte Carlo",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#a-few-structures-for-monte-carlo",
    "href": "R03_MonteCarloExamples.html#a-few-structures-for-monte-carlo",
    "title": "10  Monte Carlo Examples",
    "section": "",
    "text": "10.1.1 For Loop\nConsider two random variables: \\(X\\sim N(5, 3^2)\\) is normally distributed with a mean of \\(\\mu=5\\) and a variance of \\(\\sigma^2=3^2\\). \\(Y\\sim exp(.2)\\), independent of X, is exponentially distributed with a rate parameter of \\(\\lambda=.2\\). The Questions is what is the \\(Pr[X &gt; Y]\\)?\nUse Monte Carlo to estimate this probability.\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nresults &lt;- rep(FALSE, NMC) # a vector for storing results\nfor(i in 1:NMC){\n  X &lt;- rnorm(1,5,3)\n  Y &lt;- rexp(1, .2)\n  results[i] &lt;- (X &gt; Y) #TRUE or FALSE\n}\nmean(results)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57455\n\n#equivalently\nsum(results) / length(results) # how many TRUEs / total replications\n\n[1] 0.57455\n\n\n\n\n10.1.2 Generate Many RVs at once\nYou can also avoid the loop entirely by just generating many random variables at once\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nX &lt;- rnorm(NMC,5,3)\nY &lt;- rexp(NMC, .2)\nmean(X &gt; Y)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57829\n\n\n\n\n10.1.3 The replicate() function\nYou can also use the replicate function. It can avoid the need for a loop\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nresults &lt;- replicate(NMC, rnorm(1,5,3)&gt;rexp(1,.2))\n#replicate( how many times, expression)\n#this creates a vector of replicates! So easy!\nmean(results)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57455",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#expected-value-estimation-using-monte-carlo",
    "href": "R03_MonteCarloExamples.html#expected-value-estimation-using-monte-carlo",
    "title": "10  Monte Carlo Examples",
    "section": "10.2 Expected Value Estimation using Monte Carlo",
    "text": "10.2 Expected Value Estimation using Monte Carlo\n\n10.2.1 Weak Law of Large Numbers Example\nDemonstration of how larger samples leads the sample mean to approach expected value\nLet’s consider the following random variable: (I’m generating the probabilities of this random variable randomly!!! So this is not any know discrete distribution)\n\nx&lt;- 1:10\npx &lt;- runif(10)\npx &lt;- px / sum(px)\nbarplot(height=px, names=x)\n\n\n\n\n\n\n\n#Let's peek behind the curtain\n(EX &lt;- sum( x * px))\n\n[1] 4.923099\n\n(VarX &lt;- sum(x^2*px) - EX^2)\n\n[1] 7.508671\n\nsum((x-EX)^2*px)\n\n[1] 7.508671\n\n\nLet’s demonstrate how our estimate of EX gets better with M growing\n\nmyMeans &lt;- vector(\"numeric\")\nMs &lt;- seq(100, 10000, 100)\nfor(M in Ms){\n  mySample &lt;- sample(x, prob=px, size=M, replace=TRUE)\n  myMeans[M/100] &lt;- mean(mySample)\n}\nplot(x= Ms, y=myMeans)\nabline(h=EX, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n10.2.2 Demonstration of the law of large numbers in Monte Carlo expected value estimation\nFor example, say you want to estimate the mean of an exponential distribution. In principal, we can simulate many values \\(X_1, X_2, \\ldots, X_M\\) from this distribution, average them and that’s going to be our estimate of the \\(EX\\)\n\nNMC &lt;- 10000\nrandomExp &lt;- rexp(NMC, .4)\nnumerator &lt;- cumsum(randomExp)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=1/.4, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n10.2.3 Estimate the expected value of some weird random variable\nGamma distribution takes 2 parameters - shape and scale\nSay X ~ Gamma(shape = 5, scale = 0.5)\nLet’s estimate it’s expected value using Monte carlo - with just 100 runs. We’ll do it 5 times to compare the estimates.\n\nset.seed(1)\n\nMCN &lt;- 100   #stands for Monte Carlo N\n\nfor(i in 1:5){\n  #generate a bunch of values from the random variable\n  myData &lt;- rgamma(MCN, shape=5, scale=.5)\n#  hist(myData)\n  \n  #Law of Large Numbers says that the sample average should be close to EX\n  print(mean(myData))\n  \n}\n\n[1] 2.46491\n[1] 2.381638\n[1] 2.44396\n[1] 2.536307\n[1] 2.514453\n\n\nWhat’s your guess as to the true expected value? Hint: the parameters are 5 and .5. Do you have a guess?\nLet’s generate 10 million random values and see what we get for the mean.\n\nMCN &lt;- 100000   #stands for Monte Carlo N\n\n#generate a bunch of values from the random variable\nmyData &lt;- rgamma(MCN, shape=5, scale=.5)\n\n#Law of Large Numbers says that the sample average should be close to EX\nmean(myData)\n\n[1] 2.500456\n\n\nLet’s consider a random variable Y which is the square root of a exponential random variable X with rate parameter 3\n\\(E(X) = 1/3 = .333333\\)\nGuess what might be \\(E(Y)=E(\\sqrt{X})\\)? maybe \\(\\sqrt{1/3} = 0.5773503\\)?\nLet’s check using MC method.\n\nX &lt;- rexp(1000000, rate=3)\nY &lt;- sqrt(X)\nmean(Y)\n\n[1] 0.5117741\n\n\nNo ! it turns out that EY = .511 or so.\nIn general \\(E(g(X))\\) is not \\(g(E(X))\\)\n\n\n10.2.4 Estimate the expected value of log(X) where X~Normal\n\nmu &lt;- 50\nsigma &lt;- 3\n\nNMC &lt;- 10000\n\nX &lt;- rnorm(NMC, mean=mu, sd=sigma)\n\n#Look a the histogram\nhist(X)\n\n\n\n\n\n\n\nY &lt;- log(abs(X))\n#histogram of log|X|\nhist(Y)\n\n\n\n\n\n\n\nmean(Y)\n\n[1] 3.910573\n\n\n\n\n10.2.5 Example A complicated random variable\nRoll 3 6-sided dice and multiply their values. Let’s find the expected product\n\nNMC &lt;- 100000\n\nresults &lt;- FALSE\nfor(i in 1:NMC){\n  results[i] &lt;- prod(sample(6, size=3, replace=TRUE)) #multiples the values in the vector\n  \n}\nmean(results)\n\n[1] 42.96147\n\nhist(results)\n\n\n\n\n\n\n\n\n\n\n10.2.6 Expected value of a composition of random variables\nSuppose \\(X\\sim Unif(0,1)\\) and \\(Y \\sim Exp(1)\\)., the two are independent random variables. Let \\(W\\) be defined as \\(Y^X\\). What is the expected value of \\(W\\)?\n\nNMC &lt;- 10000\n#generate many instances of $X$ and $Y$\nx &lt;- runif(NMC, 0, 1)\ny &lt;- rexp(NMC, rate=1)\n#produce the values of W\nw &lt;- y^x\n#let's look at the distribution of W, the approximation anyway\nhist(w)\n\n\n\n\n\n\n\n#expectation is approximated by taking a sample average\nmean(w)\n\n[1] 0.9160806\n\n\n\n\n10.2.7 A time when Monte Carlo fails - when the expected value does not exist.\nThe Cauchy Distribution is a weird one. It has no defined expected value. It is actually just a T distribution with 1 degree of freedom! Same thing.\nHere’s a picture of its density function from -10 to 10.\n\nplot(x=seq(-10,10,.01), y=dcauchy(seq(-10,10,.01)), type=\"l\", main=\"density of Cauchy Distribution\", xlab=\"x\", ylab=\"density\")\n\n\n\n\n\n\n\n\nHere’s an example of a sample from the Cauchy:\n\nhist(rcauchy(1000))\n\n\n\n\n\n\n\n\nChances are you get at least one extreme extreme value. That’s the effect of having FAT tails.\nLet’s just look at what the long term average would be\n\nNMC &lt;- 10000\nrandomCauchy &lt;- rcauchy(NMC)\nnumerator &lt;- cumsum(randomCauchy)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\n\n\n\n\nThe cumulative average approaches 0 until one extreme value is sampled and then it throws off the average. Then the average slowly approaches 0 again until another extreme value throws everything off. This is what happens when expected value is not defined - law of large numbers cannot take effect.\nBut when the t distribution has 2 degrees of freedome we see a very different pattern emerge:\n\nNMC &lt;- 10000\nrandomT2 &lt;- rt(NMC,2)\nnumerator &lt;- cumsum(randomT2)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\n\n\n\n\nProblem is the variance is still undefined. The variance is infinity for \\(df \\leq 2\\). When the mean is defined and the variance is finite we start to see the Law of Large Numbers get involved\n\nNMC &lt;- 10000\nrandomT3 &lt;- rt(NMC,3)\nnumerator &lt;- cumsum(randomT3)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n10.2.8 St Petersburg Game - Another weird case\nThe game goes like this:\nI (the Casino) put one dollar in the pot I flip a coin. If it’s a tails, I double the pot If it is heads, you win the pot.\nIT costs money to play the game!!! First question: what is a fair price to play the game?\nA random variable without a defined expected value; Monte Carlo will fail us!\n\n#we can simulate M plays of the game with simply M geometric random values from geom(.5)\nM &lt;- 1000000\n\nt &lt;- rgeom(M, .5)  #t is number tails per game\nwinnings &lt;- 2^t\nmean(winnings)\n\n[1] 10.69882",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#probability-calculation-by-monte-carlo",
    "href": "R03_MonteCarloExamples.html#probability-calculation-by-monte-carlo",
    "title": "10  Monte Carlo Examples",
    "section": "10.3 Probability Calculation by Monte Carlo",
    "text": "10.3 Probability Calculation by Monte Carlo\n\n10.3.1 A Normal Example\nSuppose we want to calculate a normal RV probability. Say X follows a normal distribution with mean 8 and standard deviation 2.\nWhat is the probability that X is between 8.4 and 9.9? And let’s also imagine we don’t know how to use pnorm.\n\nM &lt;- 1000000 #this is the number of Monte Carlo replicates to make\nX &lt;- rnorm(M, mean=8, sd=2)\nmean(X &gt; 8.4 & X &lt; 9.9)\n\n[1] 0.249053\n\n\n\npnorm(9.9, 8,2)- pnorm(8.4, 8,2)\n\n[1] 0.2496842\n\n\n\n\n10.3.2 The Birthday Problem\nIf you have n people together, what is the probability that at least 2 of them share a birthday?\n\nsimulateBirthdays &lt;- function(n){\n  #birthdays will be numbers from 1 to 365\n  #we'll assume that each day of the year is equally likely (this is probably not true in real life)\n  return (sample(x = 1:365, size=n, replace=TRUE))\n}\n\ncheckSharedBirthdays &lt;- function(birthdays){\n  #We'll use the unique function\n  #given a vector X of values, unique(X) gives the unique values\n  return (length(unique(birthdays)) &lt; length(birthdays))\n}\n\nWe’ll do a Monte Carlo simulation to estimate the probability of a shared birthday when we have 19 people\n\nNMC &lt;- 10000\nresults &lt;- vector(\"logical\")\nfor(i in 1:NMC){\n  simBirthdays &lt;- simulateBirthdays(19)\n  results[i] &lt;- checkSharedBirthdays(simBirthdays)\n}\n#estimate the probability\nmean(results)\n\n[1] 0.3859\n\n\nRepeat with other values of n\n\nNMC &lt;- 100000\nresults &lt;- vector(\"logical\")\nfor(i in 1:NMC){\n  simBirthdays &lt;- simulateBirthdays(23)\n  results[i] &lt;- checkSharedBirthdays(simBirthdays)\n}\n#estimate the probability\nmean(results)\n\n[1] 0.50754\n\n\n\n\n10.3.3 Buffon’s Needles\nWe want to modify the example from class to take 3 parameters: L: length of the needles W: the width of the floorbords M: the number of Monte Carlo replicates\nThis will simulate the Buffon Needle experiment with M needles of length L on a floor with line width W. The needle.crosses function returns a single needle’s result - did it cross a floor line (TRUE or FALSE).\n\n#I will dispense with the x coordinates entirely. \nneedle.crosses &lt;- function(L, W){\n  y &lt;- runif(1, -W, W)\n  angle &lt;- runif(1, 0, pi)\n  dy &lt;- L/2 * sin(angle)\n  y1 &lt;- y-dy\n  y2 &lt;- y+dy\n  return( floor(y1/W)!= floor(y2/W)) #divide by W tells us which board number the endpoint is on, \n}\n\nbuffon.prob &lt;- function(L, W, M){\n  results &lt;- rep(0, M)\n  for(i in 1:M){\n    results[i] &lt;- needle.crosses(L,W)\n  }\n  return(mean(results))\n}\n\n#test\nbuffon.prob(1, 1, 10000)\n\n[1] 0.6468\n\n#What if the floor boards are 2 units wide??\nbuffon.prob(1, 2, 10000)\n\n[1] 0.3141\n\n\n\n\n10.3.4 Example: the Monty Hall Problem\nOkay, it’s time for a probability and statistics rite of passage: the Monty Hall problem.\nThe problem is named after Monty Hall, the original host of the game show Let’s Make a Deal.\nThe setup is as follows: you, the contestant, are faced with three doors. Behind one of the doors is the grand prize (a million dollars, a new car, a MacGuffin; use your imagination). Behind the other two doors are goats (in this hypothetical universe, a goat is not a prize you want; I disagree with this sentiment, but that’s beside the point). You get to choose a door, and you win the prize behind that door.\nHere’s a helpful illustration from Wikipedia:\n\n\n\nImage credit Wikipedia; https://en.wikipedia.org/wiki/Monty_Hall_problem\n\n\nSuppose that you choose a door at random. Having chosen that door, the host Monty Hall opens one of the other doors to reveal a goat (i.e., not the Grand Prize), and Monty Hall offers you a choice: you can stick with the door you chose and win whatever prize is behind that door, or you can switch your choice to the other door, which Monty Hall has not yet opened.\nShould you switch your choice of door?\nOn first glance, most people agree that it should make no difference– your original choice of door was random, so whether you switch your guess or not, you’re still equally likely to have chosen the door with the Grand Prize.\nBut this intuition is incorrect! Let’s check with a simulation, first.\n\ngenerate_game_state &lt;- function() {\n  # Generate a random instance of the Monty Hall game.\n  # That is, a random assignment of prizes to doors\n  # and an initial guess for our contestant.\n  # Generate an assignment of prizes/goats to doors.\n  # We'll encode the Grand Prize as a 1 and the goats as 0s.\n  # Reminder: sample(v) just returns a random permutation\n  # of the entries of v.\n  prizes &lt;- sample( c(1,0,0) );\n  # Now, let's randomly pick a door to guess.\n  # We pick door number 1, 2 or 3.\n  # Use sample() to choose from {1,2,3} uniformly at random\n  doors &lt;- c(1,2,3)\n  guess &lt;- sample( doors, size=1);\n  # Record the other two doors.\n  # x[-c] returns the entries of x NOT in vector c.\n  otherdoors &lt;- doors[-guess];\n  \n  # Return a list object, which will be easier to work with\n  # when we want to access the different pieces of game\n  # information\n  game &lt;- list(prizes=prizes, guess=guess,\n               otherdoors=otherdoors );\n  return( game )\n}\n\nrun_game_noswitch &lt;- function() {\n  # Run one iteration of Let's Make a Deal,\n  # in which we do NOT switch our guess.\n  \n  # Generate a game.\n  game &lt;- generate_game_state()\n  # Now, Monty Hall has to reveal a door to us.\n  # If we were switching our guess, we would need to do some\n  # extra work to check some conditions (see below for that),\n  # but since we're not switching, let's just cut to the\n  # chase and see if we won the prize or not.\n  # Remember, game$prizes is a vector of 0s and 1s, encoding\n  #     the prizes behind the three doors.\n  # game$guess is 1, 2 or 3, encoding which door we guessed\n  return( game$prizes[game$guess] )\n}\n\nrun_game_yesswitch &lt;- function() {\n  # Run one iteration of Let's Make a Deal,\n  # in which we DO switch our guess.\n  \n  # Generate a game.\n  game &lt;- generate_game_state()\n  guess &lt;- game$guess; # We're going to switch this guess.\n  # Now, Monty Hall has to reveal a door to us.\n  # To do that, we need to look at the other doors,\n  # and reveal that one of them has a goat behind it.\n  # game$otherdoors is a vector of length 2, encoding the\n  # two doors that we didn't look at.\n  # So let's look at them one at a time.\n  # Note: there are other, more clever ways to write\n  # this code, but this is the simplest implementation\n  # to think about, in my opinion.\n  if( game$prizes[game$otherdoors[1]]==0 ){\n    # The first non-guessed door doesn't have a goat\n    # behind it, so Monty Hall shows us the goat behind\n    # that door, and we need to switch our guess to the\n    # *other* door that we didn't choose.\n    guess &lt;- game$otherdoors[2];\n  } else {\n    # If the Grand Prize is behind otherdoors[1],\n    # so that game$otherdoors[1]]==1,\n    # then Monty Hall is going to show us a goat behind\n    # otherdoors[2], and we have the option to switch our\n    # guess to otherdoors[1],\n    # and we will exercise that option\n    guess &lt;- game$otherdoors[1];\n  }\n  # Now check if we won the prize!\n  return( game$prizes[guess] )\n}\n\nOkay, we’ve got simulations implemented for both of our two different game strategies. Let’s simulate both of these a bunch of times and compare the long-run average success.\n\nM &lt;- 1e4;\nnoswitch_wins &lt;- 0;\nfor(i in 1:M) {\n  noswitch_wins &lt;- noswitch_wins + run_game_noswitch()\n}\n\nnoswitch_wins/M\n\n[1] 0.3406\n\n\nNow, let’s see how we do if we switch our guesses.\n\nM &lt;- 1e4;\nyesswitch_wins &lt;- 0;\nfor(i in 1:M) {\n  yesswitch_wins &lt;- yesswitch_wins + run_game_yesswitch()\n}\n\nyesswitch_wins/M\n\n[1] 0.6606\n\n\nWow! That’s a lot better than the strategy where we don’t switch our guess!\nThis discrepancy can be explained using Bayes’ rule, but it gets a bit involved (see the wikipedia page if you’re really curious).\nInstead, let’s just think about the following: suppose that the Grand Prize is behind door number 1. There are three possibilities, all equally likely (because we chose uniformly at random among the three doors):\n\nWe pick door 1. We have chosen the door with the Grand Prize behind it. In this situation, the other two doors both have goats behind them, and Monty Hall reveals one of those two goats to us. In this situation, we (mistakenly, so sad!) switch our Grand Prize door for a goat door and we lose.\nWe pick door 2. We have chosen a door with a goat behind it. Of the other two doors, only one has a goat, door 3. Monty Hall shows us the goat behind that door, and we switch our guess to the other door, door 1, which has the Grand Prize behind it. Hooray!\nWe pick door 3. We have chosen a door with a goat behind it. Of the other two doors, only one has a goat, door 2. Monty Hall shows us the goat behind that door, and we switch our guess to the other door, door 1, which has the Grand Prize behind it. Hooray!\n\nSo, of the three equally likely situations, we win the Grand Prize in two of them, and our probability of winning is thus \\(2/3\\). Compare that with our \\(1/3\\) probability of winning in the situation where we don’t switch doors. Not bad!\nThe important point here is that our decision to switch doors is made conditional upon the information from Monty Hall that eliminates one of the three doors for us.\n\n\n10.3.5 A Combination of Random Variables\nLet X be the product of 3 normal random variables Y1, Y2 and Y3, with means 3, 6, and -2 and standard deviations 5, 6 and 9\nWhat is Pr[X &lt; 13]?\n\nNMC&lt;- 100000\nresults &lt;- rep(0, NMC)\nfor(i in 1:NMC){\n  Y1 &lt;- rnorm(1, 3, 5)\n  Y2 &lt;- rnorm(1, 6, 6)\n  Y3 &lt;- rnorm(1, -2, 9)\n  X &lt;- Y1*Y2*Y3\n  results[i] &lt;- (X &lt; 13)\n}\nmean(results)\n\n[1] 0.61014\n\n\n\n\n10.3.6 Example: non-transitive dice\nsee here\n\ndieY &lt;- c(3,3,3,3,3,3)\ndieB &lt;- c(0,0,4,4,4,4)\ndieG &lt;- c(1,1,1,5,5,5)\ndieR &lt;- c(2,2,2,2,6,6)\n\nnotTdice &lt;- data.frame(dieY,dieB,dieG,dieR)\n\nrollDice &lt;- function(die1,die2,N){\n  #true if player 1 wins\n  return(sample(notTdice[,die1],N,replace=TRUE) &gt; sample(notTdice[,die2],N,replace=TRUE))\n}\n\nMCN &lt;- 100000\n\ndieColors &lt;- c(\"yellow\",\"blue\",\"green\",\"red\")\n\nfor(i in 1:3){\n  for(j in ((i+1):4)){\n    results =rollDice(i,j,MCN)\n    print(paste(dieColors[i],\n                \"vs\",\n                dieColors[j],\n                \":\",\n                dieColors[i],\n                \"win proportion is \",\n                mean(results)))\n  }\n}\n\n[1] \"yellow vs blue : yellow win proportion is  0.33286\"\n[1] \"yellow vs green : yellow win proportion is  0.50163\"\n[1] \"yellow vs red : yellow win proportion is  0.66667\"\n[1] \"blue vs green : blue win proportion is  0.33467\"\n[1] \"blue vs red : blue win proportion is  0.44264\"\n[1] \"green vs red : green win proportion is  0.33375\"\n\n\nA 4 color non-transitive cycle emerges:\nBlue tends to beat Yellow, yellow tends to beat red, red tends to beat green, and green tends to beat blue!\nYellow vs green is fair, and red has a slight advantage vs blue.\n\n\n10.3.7 Miwin’s Dice\nMiwin’s Dice were invented in 1975 by the physicist Michael Winkelmann.\nConsider a set of three dice such that\ndie A has sides 1, 2, 5, 6, 7, 9 die B has sides 1, 3, 4, 5, 8, 9 die C has sides 2, 3, 4, 6, 7, 8\nWhat are the winning probabilities for these dice in the game?\n\ndieA &lt;- c(1, 2, 5, 6, 7, 9)\ndieB &lt;- c(1, 3, 4, 5, 8, 9)\ndieC &lt;- c(2, 3, 4, 6, 7, 8)\n\nnotTdice &lt;- data.frame(dieA,dieB,dieC)\n\nrollDice &lt;- function(die1,die2,N){\n  #true if player 1 wins\n  results &lt;- rep(0,N)\n  P1Rolls &lt;- sample(notTdice[,die1],N,replace=TRUE)\n  P2Rolls &lt;- sample(notTdice[,die2],N,replace=TRUE)\n  results[ P1Rolls &gt; P2Rolls] &lt;- 1\n  results[P1Rolls &lt; P2Rolls] &lt;- -1\n  return(results)\n}\nMCN &lt;- 1000000\ndieNames &lt;- c(\"A\",\"B\",\"C\")\nresultsTable &lt;- data.frame(P1&lt;-vector(\"character\"),\n                           P2&lt;-vector(\"character\"),\n                           Win&lt;-vector(\"numeric\"),\n                           Tie&lt;-vector(\"numeric\"),\n                           Lose&lt;-vector(\"numeric\"))\nfor(i in 1:2){\n  for(j in ((i+1\n             ):3)){\n    results =rollDice(i,j,MCN)\n    resultsTable &lt;- rbind(resultsTable,\n                          c(dieNames[i],\n                            dieNames[j],\n                            mean(results == 1),\n                            mean(results == 0),\n                            mean(results == -1)))\n  }\n}\nresultsTable\n\n  X.A. X.B. X.0.470773. X.0.083681. X.0.445546.\n1    A    B    0.470773    0.083681    0.445546\n2    A    C     0.44495    0.083876    0.471174\n3    B    C    0.472675    0.083433    0.443892\n\nnames(resultsTable) &lt;- c(\"P1 Die\",\"P2 Die\",\"P1 win\",\"Tie\",\"P2 Win\")",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#integration-by-monte-carlo-simulation",
    "href": "R03_MonteCarloExamples.html#integration-by-monte-carlo-simulation",
    "title": "10  Monte Carlo Examples",
    "section": "10.4 Integration by monte carlo simulation",
    "text": "10.4 Integration by monte carlo simulation\n\n10.4.1 Example 1\n\\[\\int_{7}^{14} (3x-2\\ln x-x^2) dx\\]\n\ng &lt;- function(x){\n  return(3*x - 2*log(x)-x^2)\n}\n\nM &lt;- 100000\nresults &lt;- vector(\"numeric\") #empty numerical vector\n\n#I am going to sample from X~unif(7,14) and I want to evalute h(x) = g(x)/f(x), where f(x) is the density of X\n\nfor(i in 1:M){\n  x &lt;- runif(1, 7, 14)\n  results[i] &lt;- g(x)/dunif(x, 7, 14) #this is h(x)\n}\nmean(results)\n\n[1] -611.9162\n\n\n\n-2*log(13492928512)-3395/6\n\n[1] -612.4842\n\n\n\n\n10.4.2 Example 2\n\\[\\int_1^7\\ln(3x^2-2)\\sin x dx\\]\n\nM &lt;- 10000\n\n#step 1, sample from unif(1,7) \nx &lt;- runif(M, 1,7)\ng &lt;- function(x){\n  log(3*x^2-2)*sin(x)\n}\nf &lt;- function(x){\n  dunif(x, 1, 7)\n}\nh &lt;- g(x)/f(x)\n\nmean(h)\n\n[1] -4.144071",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#other-examples",
    "href": "R03_MonteCarloExamples.html#other-examples",
    "title": "10  Monte Carlo Examples",
    "section": "10.5 Other Examples",
    "text": "10.5 Other Examples\n\n10.5.1 Estimating Euler’s constant\nA fact - if you sample \\(X_1, X_2, \\ldots\\) from Unif(0,1), and let \\(Y\\) be the index for which the sum exceeds 1, the expected value of \\(Y\\) is \\(e\\).\n\nNMC &lt;- 100000\nNs &lt;- 0\nfor(i in 1:NMC){\n  Ns[i] = min(which(cumsum(runif(100))&gt;1))\n}\nmean(Ns)\n\n[1] 2.72073\n\n\n\n#Check the value\nexp(1)\n\n[1] 2.718282\n\n\n\n\n10.5.2 Estimating the value of pi\n\nNMC &lt;- 10000; # Number of Monte Carlo replicates\nplot(NA,NA, xlim=c(-1,1), ylim=c(-1,1))\n\nin_circ &lt;- 0; # Count how many points landed in the circle\n# For-loop over the MC replicates\nfor(i in 1:NMC){\n  \n  # for each point, generate (x,y) randomly between -1 and 1\n  point &lt;- runif(n=2, min=-1, max=1);\n  # to be inside circle, our point must satisfy xˆ2 + yˆ2 &lt;= 1\n  if(point[1]^2 + point[2]^2 &lt;= 1){\n  # if inside, add to count\n    in_circ &lt;- in_circ+1\n    points(point[1],point[2])\n  }\n}\n\n\n\n\n\n\n\n#To get proportion of square covered, take in_circ/N\nprop &lt;- in_circ/NMC\n# to get our estimate of pi, multiply by 4.\npi.mc &lt;- 4*prop\npi.mc\n\n[1] 3.1344",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#random-variable-generation",
    "href": "R03_MonteCarloExamples.html#random-variable-generation",
    "title": "10  Monte Carlo Examples",
    "section": "10.7 Random Variable Generation",
    "text": "10.7 Random Variable Generation\n\n10.7.1 Distribution of F(X) is uniform(0,1)\n\n#Sample from the normal distribution, let's get 1000 values from N(mean=5, var=8^2)\nX &lt;- rnorm(10000, 5, sd=8)\n\nhist(X)\n\n\n\n\n\n\n\n#What are the first 5 sampled values?\nX[1:5]\n\n[1]  4.090129  6.802556 11.395173  4.966809 -6.029658\n\n#SO far we haven't looked at F(X). Remember, the CDF, F(x) is defined as Pr(X &lt;= x)\n#To find these we can look at pnorm\npnorm(X[1:5], 5, sd=8)\n\n[1] 0.45472440 0.58913464 0.78796975 0.49834486 0.08399251\n\n#what if we looked at pnorm for ALL values\n#How would the pnorms be distributed?\n\nhist(pnorm(X, 5, sd=8))\n\n\n\n\n\n\n\n\n\n\n10.7.2 Simulating Random Values\nLet’s look at a random sample of values from a normal distribution - \\(N(6,2^2)\\)\n\nn.values &lt;- rnorm(10000, mean=6, sd=2)\n\nWe can look a histogram of these values\n\nhist(n.values)\n\n\n\n\n\n\n\n\nYup. It’s a normal distribution. For any value, let p=P(X&lt;x) What about the left-tailed probabilities associated? Use pnorm.\n\nn.values[1:5]\n\n[1] 5.894536 3.620964 5.133187 7.241246 4.677117\n\npnorm(n.values[1:5], mean=6, sd=2)\n\n[1] 0.4789728 0.1171179 0.3323598 0.7325762 0.2541646\n\n\nWhat does the distribution of left-tail probabilities look like?\n\nhist(pnorm(n.values, mean=6, sd=2), breaks=10)\n\n\n\n\n\n\n\n\nWhat about some other random variable distribution? Let’s look at an exponential random variable.\n\nexp.values &lt;- rexp(10000, rate=3)\nhist(exp.values)\n\n\n\n\n\n\n\n\nAgain- that’s the shape of the distribution. What about the distribution of left-tail probabilities?\n\nhist(pexp(exp.values, rate=3))\n\n\n\n\n\n\n\n\nWhy is this the case? Think about the theory. X ~ Normal Distribution with mean=6, sd=2 What is the probability that P(X&lt;x) &lt; .1?\nWell, .1 of course! There’s a 10% probability that .2 &lt; P(X&lt;x) &lt; .3, and for each interval in this histogram.\n\nprobs &lt;- seq(.01,.99,.005)\npercentiles &lt;- qnorm(probs)\n\nplot(x=seq(-2,2,.1), pnorm(seq(-2,2,.1)), type=\"l\")\nsegments(percentiles,0,percentiles, probs, lwd=3, col=rgb(0,0,0,.5))\nsegments(-2,probs,percentiles, probs)\n\n\n\n\n\n\n\n\nWe can use this idea to generate random values from any distribution we want as long as we can specify the inverse CDF.\nWe’re going to use the inverse CDF trick to simulate a bunch of values from a normal distribution with mean 6 and standard deviation 2.\n\nu &lt;- runif(10000)  #These are my values sampled uniformly\n                   # at random from 0 to 1. These represent\n                   # Left tail probabilities\nx &lt;- qnorm(u, mean=6, sd=2)\n\nmean(x)\n\n[1] 6.002453\n\nsd(x)\n\n[1] 1.978054\n\nhist(x, probability=TRUE)\nlines(seq(0,12,.1), dnorm(seq(0,12,.1),6,2), col=\"blue\")\n\n\n\n\n\n\n\n\n\n\n10.7.3 CDF plot for some other distribution\nThe PDF of an exponential\n\nx&lt;- seq(0, 6, .1)\nplot(x, dexp(x,.4), type=\"l\", main=\"Exp(.4) density curve\")\n\n\n\n\n\n\n\n\n\nprobs &lt;- seq(.01,.99,.01)\npercentiles &lt;- qexp(probs, .4)\n\nplot(x=percentiles, pexp(percentiles,.4), type=\"l\")\nsegments(percentiles,0,percentiles, probs, lwd=3, col=rgb(0,0,0,.5))\nsegments(-2,probs,percentiles, probs)\n\n\n\n\n\n\n\n\n\n\n10.7.4 Generating random values\nSay I want to get 1000 values from an exponential distribution\n\nx&lt;- seq(0, 20,.1)\ny&lt;- dexp(x, rate=1)\nplot(x,y, type=\"l\", main=\"the density function of X ~ exp(1)\")\n\n\n\n\n\n\n\n\n\n#Here is what R would do\nU &lt;- runif(1000)\nX &lt;- qexp(U, rate=1) #This is the inverse CDF of X\n#in theory, X data should be distributed by exp(1)\nhist(X, probability=TRUE)\nlines(x,y, col=\"red\")\n\n\n\n\n\n\n\n\n\\(f(X)=\\frac38x^2\\) over \\([0,2]\\)\n\nU &lt;- runif(10000)\nX &lt;- (8*U)^(1/3)\nhist(X, probability=TRUE)\nlines(x=seq(0, 2, .1), y=3/8*seq(0, 2, .1)^2, col=\"red\")\n\n\n\n\n\n\n\n\n\n\n10.7.5 Inverse CDF Trick\nHere is a density function that doesn’t have a name The support is from 0 to 2\n\\(f(x) = \\frac38 x^2\\)\n\nt &lt;- seq(0,2, by=.01)\nplot(t, 3/8*t^2, type=\"l\", main=\"pdf of x\")\n\n\n\n\n\n\n\n\n\\(F(x) = \\frac18 x^3\\)\nThe inverse CDF is fairly simple to get\n\\(F^{-1}(x) = \\sqrt[3]{8x}\\)\n\nU &lt;- runif(10000, min=0, max=1)\nX &lt;- (8*U)^(1/3)\n\nhist(X, probability=TRUE)\nlines(x =seq(0,2,.1), y=3/8*(seq(0,2,.1)^2), col=\"red\")",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 340: Data Science II",
    "section": "",
    "text": "0.1 Collection of Shiny Apps",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>STAT 340</span>"
    ]
  },
  {
    "objectID": "probability_rv.html",
    "href": "probability_rv.html",
    "title": "3  Probability and Random Variables",
    "section": "",
    "text": "3.1 Learning objectives\nAfter this lesson, you will be able to",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#learning-objectives",
    "href": "probability_rv.html#learning-objectives",
    "title": "3  Probability and Random Variables",
    "section": "",
    "text": "Compute the probabilities of simple events under different probability distributions using R.\nExplain what a random variable is\nExplain what it means for variables to be dependent or independent and assess how reasonable independence assumptions are in simple statistical models.\nExplain expectations and variances of sums of variables are influenced by the dependence or independence of those random variables.\nExplain correlation, compute the correlation of two random variables, and explain the difference between correlation and dependence.\nDefine the conditional probability of an event \\(A\\) given an event \\(B\\) and calculate this probability given the appropriate joint distribution.\nUse Bayes’ rule to compute \\(\\Pr[B \\mid A]\\) in terms of \\(\\Pr[A \\mid B]\\), \\(\\Pr[A]\\) and \\(\\Pr[B]\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#probability-refresher",
    "href": "probability_rv.html#probability-refresher",
    "title": "3  Probability and Random Variables",
    "section": "3.2 Probability refresher",
    "text": "3.2 Probability refresher\nA random experiment (or random process) is a procedure which produces an uncertain outcome. The set of possible outcomes is often denoted \\(\\Omega\\).\n\nWhen we flip a coin, it can land either heads (\\(H\\)) or tails (\\(T\\)), so the outcomes are \\(\\Omega = \\{H, T\\}\\).\nWhen we roll a six-sided die, there are six possible outcomes, \\(\\Omega = \\{1,2,3,4,5,6\\}\\).\nIn other settings, the outcomes might be an infinite set.\n\nEx: if we measure the depth of Lake Mendota, the outcome may be any positive real number (theoretically)\n\n\nUsually \\(\\Omega\\) will is discrete (e.g., \\(\\{1,2,\\dots\\}\\)) or continuous (e.g., \\([0,1]\\)). We call the associated random variable discrete or continuous, respectively.\nA subset \\(E \\subseteq \\Omega\\) of the outcome space is called an event.\nA probability is a function that maps events to numbers, with the properties that\n\n\\(\\Pr[ E ] \\in [0,1]\\) for all events \\(E\\)\n\\(\\Pr[ \\Omega ] = 1\\)\nFor \\(E_1,E_2 \\in \\Omega\\) with \\(E_1 \\cap E_2 = \\emptyset\\), \\(\\Pr[ E_1 \\cup E_2 ] = \\Pr[ E_1 ] + \\Pr[ E_2 ]\\)\n\nIf \\(\\Pr[E_1 \\cap E_2]=0\\), we say that \\(E_1\\) and \\(E_2\\) are mutually exclusive (or disjoint)\nTwo events \\(E_1\\) and \\(E_2\\) are independent if \\(\\Pr[ E_1 \\cap E_2 ] = \\Pr[ E_1 ] \\Pr[ E_2 ]\\).\n\n3.2.1 Example: Coin flipping\nConsider a coin toss, in which the possible outcomes are \\(\\Omega = \\{ H, T \\}\\).\nThis is a discrete random experiment. If we have a fair coin, then it is sensible that \\(\\Pr[ X=1 ] = \\Pr[ X=0 ] = 1/2\\).\nExercise (optional): verify that this probability satisfies the above properties!\nWe will see in a later lecture that this is a special case of a Bernoulli random variable, which you are probably already familiar with.\n\n\n3.2.2 Example: Six-sided die\nIf we roll a die, the outcome space is \\(\\Omega = \\{1,2,3,4,5,6\\}\\), and the events are all the subsets of this six-element set.\nSo, for example, we can talk about the event that we roll an odd number \\(E_{\\text{odd}} = \\{1,3,5\\}\\) or the event that we roll a number larger than \\(4\\), \\(E_{&gt;4} = \\{5,6\\}\\).\n\n\n3.2.3 Example: Human heights\nWe pick a random person and measure their height in, say, centimeters. What is the outcome space?\n\nOne option: the outcome space is the set of positive reals, in which case this is a continuous random variable.\nAlternatively, we could assume that the outcome space is the set of all real numbers.\n\nHighlight: the importance of specifying our assumptions and the outcome space we are working with in a particular problem.\n\n\n3.2.4 A note on models, assumptions and approximations\nNote that we are already making an approximation– our outcome sets aren’t really exhaustive, here.\n\nWhen you toss a coin, there are possible outcomes other than heads and tails: it is technically possible to land on the edge.\nSimilarly, perhaps the die lands on its edge.\n\nHuman heights:\n\nWe can only measure a height to some finite precision (say, two decimal places), so it is a bit silly to take the outcome space to be the real numbers.\nAfter all, if we can only measure a height to two decimal places, then there is no way to ever obtain the event, “height is 160.3333333… centimeters”.\n\nGood to be aware of these approximations - but they usually won’t bother us.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#events-and-independence.",
    "href": "probability_rv.html#events-and-independence.",
    "title": "3  Probability and Random Variables",
    "section": "3.3 Events and independence.",
    "text": "3.3 Events and independence.\nTwo events \\(E_1\\) and \\(E_2\\) are independent if \\(\\Pr[ E_1 \\cap E_2 ] = \\Pr[ E_1 ] \\Pr[ E_2 ]\\).\nWe may write \\(\\Pr[ E_1, E_2]\\) to mean \\(\\Pr[ E_1 \\cap E_2 ]\\), read as “the probability of events \\(E_1\\) and \\(E_2\\)” or “the probability that \\(E_1\\) and \\(E_2\\) occur”.\n\nFor example, if each of us flip a coin, it is reasonable to model them as being independent.\nLearning whether my coin landed heads or tails doesn’t tell us anything about your coin.\n\n\n3.3.1 Example: dice and coins\nSuppose that you roll a die and I flip a coin. Let \\(D\\) denote the (random) outcome of the die roll, and let \\(C\\) denote the (random) outcome of the coin flip. So \\(D \\in \\{1,2,3,4,5,6\\}\\) and \\(C \\in \\{1,0\\}\\). Suppose that for all \\(d \\in \\{1,2,3,4,5,6\\}\\) and all \\(c \\in \\{1,0\\}\\), \\(\\Pr[ D=d, C=c ] = 1/12\\).\nQuestion: Verify that the random variables \\(D\\) and \\(C\\) are independent, or at least check that it’s true for two particular events \\(E_1 \\subseteq \\{1,2,3,4,5,6\\}\\) and \\(E_2 \\subseteq \\{1,0\\}\\).\n\n\n3.3.2 Example: more dice\nSuppose that we roll a fair six-sided die. Consider the following two events: \\[\n\\begin{aligned}\nE_1 &= \\{ \\text{The die lands on an even number} \\} \\\\\nE_2 &= \\{ \\text{The die lands showing 3} \\}.\n\\end{aligned}\n\\]\nAre these two events independent? If I tell you that the die landed on an even number, then it’s certainly impossible that it landed showing a 3, since 3 isn’t even. These events should not be independent.\nLet’s verify that \\[\n\\Pr[ E_1 \\cap E_2 ] \\neq \\Pr[ E_1 ] \\Pr[ E_2 ].\n\\]\nThere are six sides on our die, numbered 1, 2, 3, 4, 5, 6, and three of those sides are even numbers, so \\(\\Pr[ E_1 ] = 1/2\\).\nThe probability that the die lands showing 3 is exactly \\(\\Pr[ E_2 ] = 1/6\\).\nPutting these together, \\(\\Pr[E_1] \\Pr[E_2] = 1/12\\).\nOn the other hand, let’s consider \\(E_1 \\cap E_2\\) (the die is even and it lands showing three). These two events cannot both happen!\nThat means that \\(E_1 \\cap E_2 = \\emptyset\\). Thus \\(\\Pr[ \\emptyset ] = 0.\\)\n(Aside: why? Hint: \\(\\Pr[ \\Omega ] = 1\\) and \\(\\emptyset \\cap \\Omega = \\emptyset\\); now use the fact that the probability of the union of disjoint events is the sum of their probabilities).\nSo we have \\[\n\\Pr[ E_1 \\cap E_2 ] = 0 \\neq \\frac{1}{12} =\\Pr[ E_1 ] \\Pr[ E_2 ].\n\\]\nOur two events are indeed not independent.\n\n\n3.3.3 Mutually Exclusive vs Independent\nTwo events \\(E_1\\) and \\(E_2\\) are mutually exclusive if the probability of both occurring simultaneously is zero \\[P(E_1 \\cap E_2) = 0\\]\nCould two events \\(E_1\\) and \\(E_2\\) with non-zero probabilities be mutually exclusive and independent? Think about it!",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#conditional-probability",
    "href": "probability_rv.html#conditional-probability",
    "title": "3  Probability and Random Variables",
    "section": "3.4 Conditional probability",
    "text": "3.4 Conditional probability\nWe can’t talk about events and independence without discussing conditional probability.\nTo motivate this, consider the following: suppose I roll a six-sided die. What is the probability that the die lands showing 2?\nNow, suppose that I don’t tell you the number on the die, but I do tell you that the die landed on an even number (i.e., one of 2, 4 or 6). Now what is the probability that the die is showing 2?\nWe can work out the probabilities by simply counting possible outcomes. Are the probabilities the same?\n\n3.4.1 Example: disease screening\nHere’s a more real-world (and more consequential example): suppose we are screening for a rare disease. A patient takes the screening test, and tests positive. What is the probability that the patient has the disease, given that they have tested positive for it?\nWe will need to establish the rules of conditional probability before we can tackle a problem such as this.\n\n\n3.4.2 Introducing conditional probability\nThese kinds of questions, in which we want to ask about the probability of an event given that something else has happened, require that we be able to define a “new kind” of probability, called conditional probability.\nLet \\(A\\) and \\(B\\) be two events.\n\nExample: \\(A\\) could be the event that a die lands showing 2 and \\(B\\) is the event that the die landed on an even number.\nExample: \\(A\\) could be the event that our patient has a disease and \\(B\\) is the event that the patient tests positive on a screening test.\n\nProvided that \\(\\Pr[ B ] &gt; 0\\), we define the conditional probability of \\(A\\) given \\(B\\), written \\(\\Pr[ A \\mid B]\\), according to \\[\n\\Pr[ A \\mid B ] = \\frac{ \\Pr[ A \\cap B ] }{ \\Pr[ B ] }.\n\\] Note that if \\(\\Pr[B] = 0\\), then the ratio on the right-hand side is not defined, hence why we demanded that \\(\\Pr[B] &gt; 0\\).\nLet’s try computing one of these conditional probabilities: what is the probability that the die is showing 2 conditional on the fact that it landed on an even number?\nWell,\n\n\\(\\Pr[ \\text{ even } ] = 1/2\\), because there are three even numbers on the die, and all six numbers are equally likely: \\(3/6 = 1/2\\).\n\\(\\Pr[ \\text{ die lands 2 } \\cap \\text{even} ] =  \\Pr[ \\text{ die lands 2 }]\\), since \\(2\\) is an even number.\n\nSo the conditional probability is \\[\n\\begin{aligned}\n\\Pr[ \\text{ die lands 2 } \\mid \\text{ even }]\n&= \\frac{ \\Pr[ \\text{ die lands 2 } \\cap \\text{even} ] }{ \\Pr[ \\text{ even } ] } \\\\\n&= \\frac{ \\Pr[ \\text{ die lands 2 }]}\n    { \\Pr[ \\text{ even } ] } \\\\\n&= \\frac{ 1/6 }{ 1/2 } = 1/3.\n\\end{aligned}\n\\] This makes sense– given that the die lands on an even number, we are choosing from among three outcomes: \\(\\{2,4,6\\}\\). The probability that we choose \\(2\\) from among these three possible equally-likely outcomes is \\(1/3\\).\n\n\n3.4.3 Example: disease screening (continued)\nWhat about our disease testing example? What is the probability that our patient has the disease given that they tested positive?\nWell, applying the definition of conditional probability, \\[\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n= \\frac{ \\Pr[ \\text{ disease} \\cap \\text{ positive test }] }{ \\Pr[ \\text{positive test} ] }\n\\]\nOkay, but what is \\(\\Pr[ \\text{ positive test} ]\\)? I guess it’s just the probability that a random person (with the disease or not) tests positive? For that matter, what is \\(\\Pr[ \\text{ disease} \\cap \\text{ positive test }]\\)? These can be hard events to assign probabilities to! Luckily, there is a famous equation that often gives us a way forward.\n\n\n3.4.4 Bayes’ rule\nThe Reverend Thomas Bayes was the first to suggest an answer to this issue. Bayes’ rule, as it is now called, tells us how to relate \\(\\Pr[ A \\mid B]\\) to \\(\\Pr[ B \\mid A]\\): \\[\n\\Pr[ A \\mid B ] = \\frac{ \\Pr[ B \\mid A ] \\Pr[ A ]}{ \\Pr[ B ]}.\n\\]\nThis is useful, because it is often easier to write one or the other of these two probabilities.\nApplying this to our disease screening example, \\[\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n=\n\\frac{ \\Pr[\\text{ positive test } \\mid \\text{ disease}]\n        \\Pr[ \\text{ disease}]}\n        { \\Pr[ \\text{ positive test } ]  }\n\\]\nThe advantage of using Bayes’ rule in this context is that the probabilities appearing on the right-hand side are all straight-forward to think about (and estimate!).\n\n\\(\\Pr[ \\text{ disease}]\\) is the just the probability that a randomly-selected person has the disease. This is known as the prevelance of the diseases in the population. We could estimate this probability by randomly selecting a random group of people and determining if they have the disease (hopefully not using the screening test we are already using…).\n\\(\\Pr[\\text{ positive test } \\mid \\text{ disease}]\\) is the probability that when we give our screening test to a patient who has the disease in question, the test returns positive. This is often called the sensitivity of a test, a term you may recall hearing frequently in the early days of the COVID-19 pandemic.\n\\(\\Pr[ \\text{ positive test } ]\\) is just the probability that a test given to a (presumably randomly selected) person returns a positive result. We just said about that this is the hard thing to estimate.\n\n\n\n3.4.5 Example: testing for a rare disease\nSuppose that we are testing for a rare disease, say, \\[\n\\Pr[ \\text{ disease}] = \\frac{1}{10^6},\n\\]\nand suppose that a positive test is also rare, in keeping with the fact that our disease is rare and our test presumably has a low false positive rate: \\[\n\\Pr[ \\text{ positive test} ] = 1.999*10^{-6}\n\\] Note that this probability actually depends on the sensitivity \\(\\Pr[\\text{ positive test } \\mid \\text{ disease}]\\) and the specificity \\(1-\\Pr[\\text{ positive test } \\mid \\text{ healthy}]\\) of our test. You’ll explore this part more on your homework, but we’re just going to take this number as given for now.\nFinally, let’s suppose that our test is 99.99% accurate: \\[\n\\Pr[\\text{ positive test } \\mid \\text{ disease}]\n= 0.9999 = 1-10^{-4}\n\\]\nTo recap, \\[\n\\begin{aligned}\n\\Pr[ \\text{ disease}] &= \\frac{1}{10^6} \\\\\n\\Pr[ \\text{ positive test} ] &= 1.999*10^{-6} \\\\\n\\Pr[\\text{ positive test } \\mid \\text{ disease}]\n&= 0.9999.\n\\end{aligned}\n\\]\nNow, suppose that a patient is given the screening test and receives a positive result. Bayes’ rule tells us \\[\n\\begin{aligned}\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n&=\n\\frac{ \\Pr[\\text{ positive test } \\mid \\text{ disease}]\n        \\Pr[ \\text{ disease}]}\n        { \\Pr[ \\text{ positive test } ]  }\n= \\frac{ 0.9999 * 10^{-6} }{ 1.999*10^{-6} } \\\\\n&= 0.5002001.\n\\end{aligned}\n\\]\nSo even in light of our positive screening test result, the probability that our patient has the disease in question is still only about 50%!\nThis is part of why, especially early on in the pandemic when COVID-19 was especially rare, testing for the disease in the absence of symptoms was not considered especially useful.\nMore generally, this is why most screenings for rare diseases are not done routinely– doctors typically screen for rare diseases only if they have a reason to think a patient is more likely to have that disease for other reasons (e.g., family history of a genetic condition or recent exposure to an infectious disease).\n\n\n3.4.6 Calculating the denominator in Bayes’ Rule\nThe denominator can be decomposed into two parts using a property known as the Law of Total Probability.\n\\[\n\\Pr[ \\text{ positive test} ] = \\Pr[ \\text{ positive test} \\cap \\text{disease}]+\\Pr[ \\text{ positive test} \\cap \\text{no disease}]\n\\]\nIn other words, all positive results are either true positives or false positives. Because these are mutually exclusive events, the total probability of a positive result is the probability of a true positive plus the probability of a false positive. We can expand each of these terms using the conditional probability rule.\n\\[\n\\Pr[ \\text{ positive test} \\cap \\text{disease}] = \\Pr[\\text{ positive test } \\mid \\text{ disease}]  \\Pr[ \\text{ disease}]\n\\] \\[\n\\Pr[ \\text{ positive test} \\cap \\text{no disease}] = \\Pr[\\text{ positive test } \\mid \\text{ no disease}]  \\Pr[ \\text{no disease}]\n\\]\nFor example, suppose that a genetic condition occurs in roughly 1 out of 800 individuals. A simple saliva test is available. If a person has the gene, the test is positive with 97% probability. If a person does not have the gene, a false positive occurs with 4% probability.\nTo simplify notation, let \\(G\\) represent “the individual has the gene” and \\(G'\\) be the complementary event that “the individual does not have the gene.” Furthermore, let \\(Pos\\) and \\(Neg\\) represent the test results.\nIf a random person from the population takes the test and gets a positive result, what is the probability they have the genetic condition?\nBayes’ Rule to the rescue:\n\\[\n\\begin{aligned}\nP[G | Pos] &= \\dfrac{P[Pos | G] P[G]}{P[Pos | G] P[G] + P[Pos | G'] P[G']}\\\\\n&= \\dfrac{(.97)(1/800)}{(.97)(1/800)+(.04)(799/800)}\\\\\n&\\approx 0.0295\n\\end{aligned}\n\\] In other words, a positive test result would raise the likelihood of the gene being present from \\(1/800=0.00125\\) up to \\(.0295\\).\n\n\n3.4.7 Dependent free throw shots\nSuppose a basketball player’s likelihood of making a basket when making a free throw depends on the previous attempt. On the first throw, they have a probability of \\(0.67\\) of making the basket. On the second throw, following a basket the probability goes up to \\(.75\\). If the first throw is a miss, the probability of a basket on the second throw goes down to \\(0.62\\).\nExercise: If the second throw is a basket, what is the likelihood the first throw is a basket?\nExercise: Given that the player scores at least 1 point, what is the probability that they score 2 points total?",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#random-variables",
    "href": "probability_rv.html#random-variables",
    "title": "3  Probability and Random Variables",
    "section": "3.5 Random Variables",
    "text": "3.5 Random Variables\nConsider the following quantities/events:\n\nWhether or not a coin flip comes up heads or tails.\nHow many people in the treatment group of a vaccine trial are hospitalized.\nThe water level measured in Lake Mendota on a given day.\nHow many customers arrive at a store between 2pm and 3pm today.\nHow many days between installing a lightbulb and when it burns out.\n\nAll of these are examples of events that we might reasonably model according to different random variables.\nInformal definition of a random variable: a (random) number \\(X\\) about which we can compute quantities of the form \\(\\Pr[ X \\in S ]\\), where \\(S\\) is a set.\n\n3.5.1 Random variables (formal definition)\nA random variable \\(X\\) is specified by an outcome set \\(\\Omega\\) and a function that specifies probabilities of the form \\(\\Pr[ X \\in E ]\\) where \\(E \\subseteq \\Omega\\) is an event. For our purposes though it’s helpful to think of a random variable as a variable that can take random values with specified probabilities.\nBroadly speaking random variables fall into two categories: discrete and continuous. A discrete random variable can take a value from a countable set (for example, the integers). The set of possible values could be finite or possibly infinite. A continuous random variable can take any real number value along an interval (or union of intervals).\n\n\n3.5.2 Discrete Random Variables\n\n3.5.2.1 Probability Mass Function\nTo define a discrete random variable it is enough to define a function mapping possible values to their probabilities. This is known as a probability mass function (pmf). It can be represented in a table or mathematically.\nFor example, suppose the random variable \\(X\\) can take values \\(0,1,2,3\\) or \\(4\\). We could define the probability mass function using a table:\n\n\n\n\\(k\\)\n0\n1\n2\n3\n4\n\n\n\\(Pr[X=k]\\)\n.10\n.15\n.20\n.30\n.25\n\n\n\nAnother random variable \\(Y\\) may be defined as taking any non-negative integer \\(k\\geq0\\), with \\[Pr[Y=k]=(.6)(.4^k)\\]\n\n\n3.5.2.2 The Cumulative Distribution Function\nWe are often interested not just in the probability that a random variable \\(X\\) takes one particular value \\(k\\), but a value within a range. From a probability mass function \\(f(k)=Pr[X=k]\\) we can define a cumulative distribution function (cdf) \\(F(k)\\). \\[F(k) = Pr[X \\leq k] = \\sum_{x \\leq k}Pr[X=x]\\] (we can just add up these probabilities since they are disjoint events).\n\n\n3.5.2.3 CDF example\nFor example, we can expand the pmf from the previous example to a cdf.\n\n\n\n\\(k\\)\n0\n1\n2\n3\n4\n\n\n\\(Pr[X=k]\\)\n.10\n.15\n.20\n.30\n.25\n\n\n\\(F(k)\\)\n.10\n.25\n.45\n.76\n1.0\n\n\n\nNotice that the cdf increases (technically it is “non-decreasing”) as \\(k\\) increases, going from 0 to 1 and no higher. While the table above only gives cdf values for integers, it is technically a step function defined for all real numbers. The cdf would be 0 for any value \\(k\\) lower than the lowest possible value of \\(X\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#expectation",
    "href": "probability_rv.html#expectation",
    "title": "3  Probability and Random Variables",
    "section": "3.6 Expectation",
    "text": "3.6 Expectation\nBefore we continue with more random variables, let’s take a pause to discuss one more important probability concept: expectation. You will hopefully recall from previous courses in probability and/or statistics the notion of expectation of a random variable.\nExpectation: long-run averages\nThe expectation of a random variable \\(X\\), which we write \\(\\mathbb{E} X\\), is the “long-run average” of the random variable.\nWhat we would see on average if we observed many realizations of \\(X\\).\nThat is, we observe \\(X_1,X_2,\\dots,X_n\\), and consider their average, \\(\\bar{X} = n^{-1} \\sum_{i=1}^n X_i\\).\n\n3.6.1 Expected value and the Law of Large Numbers (first peek)\nThe law of large numbers (LLN) states that in a certain sense, as \\(n\\) gets large, \\(\\bar{X}\\) gets very close to \\(\\mathbb{E} X\\).\nWe would like to say something like \\[\n\\lim_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i = \\mathbb{E} X.\n\\] But \\(n^{-1} \\sum_i X_i\\) is a random sum, so how can we take a limit?\nRoughly speaking, for \\(n\\) large, with high probability, \\(\\bar{X}\\) is close to \\(\\mathbb{E}\\).\n\n\n3.6.2 Expectation: formal definition\nMore formally, if \\(X\\) is a discrete random variable, we define its expectation to be \\[\n\\mathbb{E} X = \\sum_k k \\Pr[ X = k],\n\\] where the sum is over all \\(k\\) such that \\(\\Pr[ X=k ] &gt; 0\\).\n\nNote that this set could be finite or infinite.\nIf the set is infinite, the sum might not converge, in which case we say that the expectation is either infinite or doesn’t exist. But that won’t be an issue this semester.\n\nQuestion: can you see how this definition is indeed like the “average behavior” of \\(X\\)?\n\n\n3.6.3 Example: Calculating Expectation\nCalculate the expected value of the random variable \\(X\\) with the pmf\n\n\n\n\\(k\\)\n0\n1\n2\n3\n4\n\n\n\\(Pr[X=k]\\)\n.10\n.15\n.20\n.30\n.25\n\n\n\n\n\n3.6.4 LLN Important take-away\nThe law of large numbers says that if we take the average of a bunch of independent RVs, the average will be close to the expected value.\n\nSometimes it’s hard to compute the expected value exactly (e.g., because the math is hard– not all sums are nice!)\nThis is where Monte Carlo methods come in– instead of trying to compute the expectation exactly, we just generate lots of RVs and take their average!\nIf we generate enough RVs, the LLN says we can get as close as we want.\nWe’ll have lots to say about this in our lectures on Monte Carlo methods next week.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#continuous-random-variables",
    "href": "probability_rv.html#continuous-random-variables",
    "title": "3  Probability and Random Variables",
    "section": "3.7 Continuous random variables",
    "text": "3.7 Continuous random variables\nContinuous random variables, which take values in “continuous” sets like the interval \\([0,1]\\) or the real \\(\\mathbb{R}\\).\nContinuous random variables have probability density functions, which we will usually write as \\(f(x)\\) or \\(f(t)\\).\nThese random variables are a little trickier to think about at first, because it doesn’t make sense to ask about the probability that a continuous random variable takes a specific value. That is, \\(\\Pr[ X = k ]\\) doesn’t really make sense when \\(X\\) is continuous (actually– in a precise sense this does make sense, but the probability is always zero; you’ll see why below).\n\n3.7.1 The probability density function\nRather than defining the probability measure for specific values \\(k\\) that the random variable can take, it only makes sense to describe the relative likelihood of different values. We use a probability density function (pdf) to do this. The pdf, typically denoted \\(f(x)\\) defines the how much probability per unit is to be found at \\(X=x\\).\nLet’s look at an example.\n\n\n3.7.2 PDF Example\nDefine continuous rv \\(X\\) with density function \\(f(x)=.5x\\) for \\(x \\in [0,2]\\).\n\n\n\n\n\n\n\n\n\n\n\n3.7.3 Calculating probabilities for continuous RVs\nProbability can be calculated by integration: For some interval \\([a,b]\\) we calculate \\[Pr\\left[a\\leq X\\leq b\\right] = \\int_a^b f(x)dx\\]\nBecause the definite integral correspond to the area under the curve, we can sometimes find probabilities using geometry rather than calculus.\nExample: Calculate \\(Pr[.5 \\leq X \\leq 1.5]\\).\n\n\n3.7.4 Probability that X=k?\nBecause of how we define probability for continuous random variables, \\(Pr[X=k]=\\int_k^k f(x)dx=0\\), so there is no measurable probability for any one particular value for a continuous random variable. Therefore we can say \\(Pr[X &lt; k]\\) or \\(Pr[X \\leq k]\\). It doesn’t matter since \\(Pr[X=k]=0\\).\n\n\n3.7.5 CDF for a continuous random variable\nThe CDF is defined as \\(F(x) = Pr[X \\leq x]\\) (using \\(x\\) instead of \\(k\\) for now). This is the same definition for continuous random variables, but mathematically it arises from an integral rather than a summation: \\[F(x) = Pr[X \\leq x] = \\int_{-\\infty}^x f(t)dt\\] For this course we won’t be integrating density functions. On occasion you’ll be provided with a cdf but you won’t have to produce them yourself.\nBut it’s worth noting that \\(F(x) \\in [0,1]\\) for all random variables, and \\(F(x)\\) is a non-decreasing function, with \\(F(x)\\to0\\) as \\(x \\to -\\infty\\), and \\(F(x)\\to 1\\) as \\(x \\to +\\infty\\).\n\n\n3.7.6 Expectation for continuous random variables\nPreviously, we defined the expectation of a discrete random variable \\(X\\) to be \\[\n\\mathbb{E} X = \\sum_k k \\Pr[ X = k ],\n\\] with the summand \\(k\\) ranging over all allowable values of \\(X\\). When \\(X\\) is continuous how should we define the expectation? Change the sum to an integral!\n\\[\n\\mathbb{E} X = \\int_\\Omega t f(t) dt,\n\\]\nwhere \\(f(t)\\) is the density of \\(X\\) and \\(\\Omega\\) is the support.\nThis type of exercise is best left to another statistics course. For us it’s enough to think of the expectation as a balancing point (center of mass) for a continuous random variable’s distribution function.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#random-variables-and-independence",
    "href": "probability_rv.html#random-variables-and-independence",
    "title": "3  Probability and Random Variables",
    "section": "3.8 Random Variables and Independence",
    "text": "3.8 Random Variables and Independence\nTwo random variables \\(X\\) and \\(Y\\) are independent if for all sets \\(S_1,S_2\\), we have \\(\\Pr[ X \\in S_1 ~\\&~ Y \\in S_2 ] = \\Pr[ X \\in S_1 ] \\Pr[ Y \\in S_2 ]\\).\nRoughly speaking, two random variables are independent if learning information about one of them doesn’t tell you anything about the other.\n\nFor example, if each of us flips a coin, it is reasonable to model them as being independent.\nLearning whether my coin landed heads or tails doesn’t tell us anything about your coin.\n\n\n3.8.1 Independent Random Variables\nInformally, we’ll say that two random variables \\(X\\) and \\(Y\\) are independent if any two events concerning those random variables are independent.\nThat is, for any event \\(E_X\\) concerning \\(X\\) (i.e., \\(E_X = \\{ X \\in S \\}\\) for \\(S \\subseteq \\Omega)\\) and any event \\(E_Y\\) concerning \\(Y\\), the events \\(E_X\\) and \\(E_Y\\) are independent.\ni.e., if two random variables \\(X\\) and \\(Y\\) are independent, then for any two sets \\(S_1, S_2 \\subset \\Omega\\), \\[\n\\Pr[ X \\in S_1, Y \\in S_2 ]\n=\n\\Pr[ X \\in S_1] \\Pr[ Y \\in S_2 ].\n\\] \nIf \\(X\\) and \\(Y\\) are both discrete, then for any \\(k\\) and \\(\\ell\\), \\[\n\\Pr[ X=k, Y=\\ell ]\n=\n\\Pr[ X=k ] \\Pr[ Y=\\ell ].\n\\]\nSimilarly, if \\(X\\) and \\(Y\\) are continuous, then the joint density has the same property: \\[\nf_{X,Y}(s,t) = f_X(s) f_Y(t).\n\\]\n\n\n3.8.2 How reasonable is independence?\nIn most applications, it is pretty standard that we assume that our data are drawn independently and identically distributed according to some distribution. We say “i.i.d.”. For example, if \\(X_1, X_2, \\ldots, X_n\\) are all continuous uniform random variables between 0 and 1, we would say \\[\nX_i \\overset{\\text{iid}}{\\sim} \\text{Uniform}(0,1), \\text{ for } i=1,\\ldots, n\n\\]\nThis notation is common to denote iid random variables.\nAs another example, when we perform regression (as you did in STAT240, and which we’ll revisit in more detail later this semester), we imagine that the observations (i.e., predictor-response pairs) \\((X_1,Y_1),(X_2,Y_2),\\dots,(X_n,Y_n)\\) are independent.\nMost standard testing procedures (e.g., the t-test) assume that data are drawn i.i.d.\nHow reasonable are these assumptions?\nIt depends on where the data comes from! We have to draw on what we know about the data, either from our own knowledge or from that of our clients, to assess what assumptions are and aren’t reasonable.\nLike most modeling assumptions, we usually acknowledge that independence may not be exactly true, but it’s often a good approximation to the truth!\nExample: suppose we are modeling the value of a stock over time. We model the stock’s price on days 1, 2, 3, etc as \\(X_1, X_2, X_3, \\dots\\). What is wrong with modeling these prices as being independent of one another? Why might it still be a reasonable modeling assumption?\nWhat if instead we look at the change in stock price from day to day? For example, let \\(Y_i = X_{i+1}-X_{i}\\). In other words, \\(X_{i+1}=X_i+Y_i\\). Would it be more reasonable to assume that the \\(Y_i\\)’s are independent?\nWhat if instead of considering a stock’s returns on one day after another, we look at a change in stock price on one day, then at the change 10 days from that, and 10 days from that, and so on? Surely there is still dependence, but a longer time lag between observations might make us more willing to accept that our observations are close to independent (or at least have much smaller covariance!).\nNote: Tobler’s first law of geography states ‘Everything is related to everything else, but near things are more related than distant things.’ Does that ring true in this context?\nExample: suppose we randomly sample 1000 UW-Madison students to participate in a survey, and record their responses as \\(X_1,X_2,\\dots,X_{1000}\\). What might be the problem with modeling these responses as being independent? Why might be still be a reasonable modeling assumption?\n\n\n3.8.3 (in)dependence, expectation and variance\nRecall the definition of the expectation: If \\(X\\) is continuous with density \\(f_X\\), \\[\n\\mathbb{E} X = \\int_\\Omega t f_X(t) dt,\n\\]\nand if \\(X\\) is discrete with probability mass function \\(\\Pr[ X=k]\\), \\[\n\\mathbb{E} X = \\sum_{k \\in \\Omega} k \\Pr[X=k]\n\\]\nWith the expectation defined, we can also define the variance, \\[\n\\operatorname{Var} X = \\mathbb{E} (X - \\mathbb{E} X)^2\n= \\mathbb{E} X^2 - \\mathbb{E}^2 X.\n\\]\nThat second equality isn’t necessarily obvious– we’ll see why it’s true in a moment.\nNote: we often write \\(\\mathbb{E}^2 X\\) as short for \\((\\mathbb{E} X)^2\\).\nA basic property of expectation is that it is linear. For any constants (i.e., non-random) \\(a,b \\in \\mathbb{R}\\), \\[\n\\mathbb{E} (a X + b) = a \\mathbb{E} X + b.\n\\] If \\(X,Y\\) are random variables, then \\[\n\\mathbb{E}( X + Y) = \\mathbb{E} X + \\mathbb{E} Y.\n\\]\nNote that derivatives and integrals are linear, too. For example, \\[\n(a ~f(t) + b ~g(t))' = a ~ f'(t) + b ~g'(t)\n\\]\nand \\[\n\\int(a f(t) + b g(t)) dt = a \\int f(t) dt + b \\int g(t) dt\n\\]\nBecause expected value is simply an integral (or summation), the linearity of expectation follows directly from the definition.\nExercise: prove that \\(\\mathbb{E} (a X + b) = a \\mathbb{E} X + b\\) for discrete r.v. \\(X\\).\nExercise: prove that \\(\\mathbb{E}( X + Y) = \\mathbb{E} X + \\mathbb{E} Y\\) for discrete \\(X\\) and \\(Y\\).\nExercise: Use the linearity of expectation to prove that \\(\\mathbb{E} (X - \\mathbb{E} X)^2\n= \\mathbb{E} X^2 - \\mathbb{E}^2 X\\). Hint: \\(\\mathbb{E}( X \\mathbb{E} X) = \\mathbb{E}^2 X\\) because \\(\\mathbb{E} X\\) is NOT random– it pops right out of the expectation just like \\(a\\) does in the equation above.\nThe definition of variance and the linearity of expectation are enough to give us a property of variance:\nFor any constants (i.e., non-random) \\(a,b \\in \\mathbb{R}\\), \\[\n\\operatorname{Var} (a X + b) = a^2 \\operatorname{Var} (X).\n\\]\nExercise: Use the definition \\(\\operatorname{Var}(X)=\\mathbb{E} X^2 - \\mathbb{E}^2 X\\) to prove the above.\nThis linearity property implies that the expectation of a sum is the sum of the expectations: \\[\n\\mathbb{E}[ X_1 + X_2 + \\dots + X_n]\n= \\mathbb{E} X_1 + \\mathbb{E} X_2 + \\dots + \\mathbb{E} X_n.\n\\]\nHowever, the variance of the sum the is not always the sum of the variances.\nConsider RVs \\(X\\) and \\(Y\\). \\[\n\\begin{aligned}\n\\operatorname{Var}(X + Y)\n&= \\mathbb{E}[ X + Y - \\mathbb{E}(X + Y) ]^2 \\\\\n&= \\mathbb{E}[ (X - \\mathbb{E} X) + (Y - \\mathbb{E} Y) ]^2,\n\\end{aligned}\n\\]\nwhere the second equality follows from applying linear of expectation to write \\(\\mathbb{E}(X+Y) = \\mathbb{E}X + \\mathbb{E}Y\\).\nNow, let’s expand the square in the expectation. \\[\n\\begin{aligned}\n\\operatorname{Var}(X + Y)\n&=\n\\mathbb{E}[ (X - \\mathbb{E} X) + (Y - \\mathbb{E} Y) ]^2 \\\\\n&= \\mathbb{E}[(X - \\mathbb{E} X)^2 + 2(X - \\mathbb{E} X)(Y - \\mathbb{E} Y)\n              + (Y - \\mathbb{E} Y)^2 ] \\\\\n&= \\mathbb{E} (X - \\mathbb{E} X)^2 + 2 \\mathbb{E} (X - \\mathbb{E} X)(Y - \\mathbb{E} Y) + \\mathbb{E} (Y - \\mathbb{E} Y)^2,\n\\end{aligned}\n\\] where the last equality is just using the linearity of expectation.\nNow, the first and last terms there are the variances of \\(X\\) and \\(Y\\): \\[\n\\operatorname{Var} X = \\mathbb{E}(X - \\mathbb{E} X)^2,~~~\n\\operatorname{Var} Y = \\mathbb{E}(Y - \\mathbb{E} Y)^2.\n\\]\nSo \\[\n\\operatorname{Var}(X + Y)\n= \\operatorname{Var} X + 2 \\mathbb{E} (X - \\mathbb{E} X)(Y - \\mathbb{E} Y)\n  + \\operatorname{Var} Y.\n\\]\nThe middle term is (two times) the covariance of \\(X\\) and \\(Y\\), often written \\[\n\\operatorname{Cov}(X,Y)\n= \\mathbb{E}( X - \\mathbb{E}X)( Y - \\mathbb{E} Y).\n\\]\nIf \\(\\operatorname{Cov}(X,Y) = 0\\), then \\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var} X + \\operatorname{Var} Y.\n\\]\nBut when does \\(\\operatorname{Cov}(X,Y) = 0\\)?\nIf \\(X\\) and \\(Y\\) are independent random variables, then \\(\\operatorname{Cov}(X,Y) = 0\\) (but causality does not work the other way)\nNote: We will skip the proof that independence of \\(X\\) and \\(Y\\) implies \\(Cov(X,Y)=0\\), but you can find this proof in many places online.\n\n\n\n3.8.4 (Un)correlation and independence\nCovariance might look familiar to you from a quantity that you saw in STAT240 (and a quantity that is very important in statistics!). The (Pearson) correlation between random variables \\(X\\) and \\(Y\\) is defined to be \\[\n\\rho_{X,Y} = \\frac{ \\operatorname{Cov}(X,Y) }{ \\sqrt{ (\\operatorname{Var} X)(\\operatorname{Var} Y)} }.\n\\]\nNote that if \\(X\\) and \\(Y\\) are independent, then \\(\\rho_{X,Y}=0\\) and we say that they are uncorrelated.\nBut the converse isn’t true– it is possible to cook up examples of random variables that are uncorrelated (i.e., \\(\\rho_{X,Y} = 0\\)), but which are not independent.\n\n\n3.8.5 Example: Uncorrelated but not independent\nSuppose \\(X\\) can take three values: \\(-1, 0,\\) and \\(1\\) with probabilities \\(.25, .5\\) and \\(.25\\). We can define a second random variable \\(Y=X^2\\). You can see from the definition that \\(Y\\) is most definitely dependent on \\(X\\). If, for example, you know that \\(x=1\\), then you know that \\(y=1^2=1\\).\nWith a little bit of work, we can show that \\(EX=0, EY=.5\\), \\(VarX=.5\\) and \\(VarY=.25\\) (do this as an exercise!)\nWhat is the covariance?\n\\[\n\\begin{aligned}\nCov(X,Y)  &= E((X-EX)(Y-EY)) \\\\\n          &= E((X-0)(X^2-.5)) \\\\\n          &= E(X^3-.5X)\\\\\n          &= EX^3 - .5EX\\\\\n          &= EX - 0\\\\\n          &= 0\n\\end{aligned}\n\\] (Note that \\(Y=X^2\\) and \\(X^3=X\\))\n\\(Cov(X,Y)=0\\) but \\(X\\) and \\(Y\\) are not independent.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "probability_rv.html#review",
    "href": "probability_rv.html#review",
    "title": "3  Probability and Random Variables",
    "section": "3.9 Review:",
    "text": "3.9 Review:\nIn these notes we covered:\n\nThe basic rules of probability: outcome spaces, events\nThe concept of independent events\nWhen the independence assumption is reasonable\nConditional probability & the general multiplication rule\nBayes’ rule\nThe concept of a random variable\nPMF, PDF and CDF\nThe concept of expected value\nIndependent random variables\nDefinition of variance\nExpectation of a linear combination of r.v.s\nVariance of a linear combination of r.v.s\nCovariance and correlation\nRelationship between correlation and independence",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability and Random Variables</span>"
    ]
  },
  {
    "objectID": "R01_Prob_RVs.html#a-random-variable",
    "href": "R01_Prob_RVs.html#a-random-variable",
    "title": "4  Probability and Random Variable R Examples",
    "section": "4.1 A Random Variable",
    "text": "4.1 A Random Variable\n\\(X\\) is a random variable that can take values 0,1,2,3 and 4 with probabilities .10, .15, .20, .30, and .25.\nWe can define the values and probabilities in vectors\n\nx &lt;- 0:4 #Or c(0,1,2,3,4).\npx &lt;- c(.10, .15, .20, .30, .25)\n\nWe can visualize the probability mass function\n\nplot(x,px, ylim=c(0, max(px)))\nsegments(x,0,x,px)\n\n\n\n\nSimulating values from the distribution can be easily done using the sample() function. Every time this code chunk is run you should get 10 different values\n\nsample(x, #first we provide the values to sample from \n       size=10, #how many values do we want to sample\n       replace=TRUE, #so it is possible to get the same value more than once.\n       prob=px) #finish by providing the probability vector\n\n [1] 1 3 0 1 4 1 1 3 3 2\n\n\nIf we sample a bunch and plot a histogram it should resemble the pmf\n\nhist(\n  sample(x, size=1000, replace=TRUE, prob=px), \n  breaks=seq(-.5,4.5,1) )\n\n\n\n\nExpected value can be calculated using the formula. Remember that \\(E(X)=\\sum_k k \\cdot Pr(X=k)\\)\n\nsum(x * px) # we can harness the fact that R works easily on vectors\n\n[1] 2.45\n\n\nWe can check if this is the long-term average by sampling a lot and averaging the values\n\nmean(\n  sample(x, size=1000, replace=TRUE, prob=px)\n)\n\n[1] 2.44\n\n\nVariance can be calculated as well using the definition \\(Var(X)=E((X-\\mu)^2)\\)\n\nsum((x-2.45)^2*px)\n\n[1] 1.6475\n\n\nOr we can use the alternate formula \\(Var(X)=E(X^2)-\\mu^2\\)\n\nsum(x^2*px) - 2.45^2\n\n[1] 1.6475"
  },
  {
    "objectID": "R01_Prob_RVs.html#example",
    "href": "R01_Prob_RVs.html#example",
    "title": "4  Probability and Random Variable R Examples",
    "section": "4.2 Example",
    "text": "4.2 Example\nTwo random variables that are Not independent, but have a 0 covariance\n\nX = seq(-1,1,.001)\nY = X^2\nplot(X,Y)\nabline(h=mean(Y))\nabline(v=mean(X))\n\n\n\ncov(X,Y)\n\n[1] 1.39546e-17\n\ncor(X,Y)\n\n[1] 8.090705e-17\n\n\n\n#A estimate of the pdf of Y\nplot(density(Y), xlim=c(0,1))\n\n\n\nhist(Y)"
  },
  {
    "objectID": "R01_Prob_RVs.html#conditional-probability",
    "href": "R01_Prob_RVs.html#conditional-probability",
    "title": "4  Probability and Random Variable R Examples",
    "section": "4.3 Conditional Probability",
    "text": "4.3 Conditional Probability\nExample: X: roll a 6 sided die Y: flip a coin x times, count the number of heads\nWe could ask the question: What is the covariance ?!?!?\nSuppose we were to simulate doing this thing many many times. Each outcome from the simulation is representing one equally likely outcome, right?\nThis is the idea of Monte Carlo sampling - which we’ll look at soon\n\nX &lt;- sample(1:6, size=10000, replace=TRUE) #sample the values 1-6 with equal probability\nY &lt;- vector(\"numeric\", 10000)\nfor(i in 1:10000){\n  #represent heads as 1, tails as 0.\n  Y[i] &lt;- sum(sample(0:1, size=X[i], replace=TRUE)) #sample x 0 or 1s and add them up - this counts the 1s\n  #next week we'll learn a faster way to do this using the binomial distribution\n}\ncor(X,Y)\n\n[1] 0.6754856\n\nplot(jitter(Y)~jitter(X), col=rgb(0,0,0,.01), pch=16)"
  },
  {
    "objectID": "R01_Prob_RVs.html#disease-screening",
    "href": "R01_Prob_RVs.html#disease-screening",
    "title": "4  Probability and Random Variable R Examples",
    "section": "4.4 Disease Screening",
    "text": "4.4 Disease Screening\nSay that the flu is currently infecting 3% of the population (prevelance). A person can take a flu test that has a sensitivity of 99% (i.e. if they have the virus, there is a 99% chance the test will give a positive result) and a specificity of 95% (i.e. if they don’t have the virus, there’s a 95% chance the test gives them a negative result).\nThe sensitivity is also known as the True positive rate (TPR). The complement of specificity is the False Positive Rate (FPR), and in this case it is 1-.95=.05 or 5%.\nSo a person takes the test and gets a positive test result, what is the probability that they actually have the flu? (This is known as the positive predictive value or PPV)\n\np.flu &lt;- .03; p.noflu &lt;- 1-p.flu\np.pos.given.flu &lt;- .99; p.neg.given.flu &lt;- 1-p.pos.given.flu\np.neg.given.noflu &lt;- .95; p.pos.given.noflu &lt;- 1-p.neg.given.noflu\n\n# P(flu | pos) = P(flu)*P(pos|flu)/ [P(flu)*P(pos|flu) + P(noflu)*P(pos|noflu) ]\np.flu*p.pos.given.flu / (p.flu*p.pos.given.flu + p.noflu*p.pos.given.noflu)\n\n[1] 0.3797954\n\n\nSurprising? Well, if we didn’t do the test we’d guess a 5% chance of flu. Now that the test results are in that estimation increases by more than 7x to about 38%. Why isn’t it higher? There is a high chance of false positives muddying the waters.\nThe negative predictive value is the probability they truly do not have the flu if they get a negative test result.\n\n# P(no flu | neg) = P(no flu) * P(neg | no flu) / [ P(no flu) * P(neg|no flu) + P(flu) * P(neg|flu)]\n(1-p.flu)*p.neg.given.noflu / ((1-p.flu)*p.neg.given.noflu + (p.flu)*p.neg.given.flu)\n\n[1] 0.9996745\n\n\nVery high - but remember that 97% of the population does not have the flu. A negative test result has raised our subjective likelihood of no flu from 97% up to 99.97% - there’s still a chance this was a false negative but its not likely because the base rate of the flu is small to begin with."
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "2  Introduction",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn this introductory set of notes, we will:\n\nget a flavor for what data science is\ndiscuss why you might want to take a course like this one and\nhighlight the topics that we will cover this semester."
  },
  {
    "objectID": "intro.html#motivating-questions",
    "href": "intro.html#motivating-questions",
    "title": "2  Introduction",
    "section": "2.2 Motivating Questions",
    "text": "2.2 Motivating Questions\nConsider the following questions:\n\nHow effective is the Pfizer vaccine against the delta variant of COVID-19?\nDo early intervention programs (e.g., Head Start) improve educational outcomes for low-income children?\nAre record-high temperatures in recent years explainable by chance?\n\nThese questions are a little too complicated for an introductory course like this, but they are the kinds of questions that data science is equipped to answer.\nOur job as data scientists is to draw on tools from statistics, computer science and mathematics, in collaboration with domain experts, to answer questions like these.\nThat being said, collecting, cleaning and analyzing data is only part of the job.\nThe most important skill you will learn in this course is not a statistical or computational tool (though those are important!). It is the abilitity to clearly organize and explain your findings in a way that is appropriate for your intended audience."
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "2  Introduction",
    "section": "2.3 What is data science?",
    "text": "2.3 What is data science?\nIs “data science” just a rebranding of applied statistics?\nWell, perhaps, to an extent. But the emergence of “data science” has coincided with huge changes in how science and statistics gets done.\nComputers and the internet have made it easy to collect, share and analyze data in ways that were unimaginable only a decade or two ago.\nSeveral times this semester, we will talk about fundamental parts of the data science toolkit that are usable only because computers are fast and cheap.\nThis change in technology has changed how we do science. In this sense, “data science” is a culture or a way of thinking, more than it is a field."
  },
  {
    "objectID": "intro.html#wearing-different-hats",
    "href": "intro.html#wearing-different-hats",
    "title": "2  Introduction",
    "section": "2.4 Wearing different hats",
    "text": "2.4 Wearing different hats\nBeing a good data scientist requires that we be able to draw on many different fields and ways of thinking.\nA “well rounded” data scientist should move fluidly among multiple ways of thinking and approaching a problem.\nHere are a few of the different kinds of hats I (Keith) find myself wearing in my day-to-day work:\n\nscientist: understanding data domain, developing questions, “story telling”\nsoftware dev: data processing and wrangling / reproducibility\nmathematician: linear algebra, probability theory, optimization\nmethodologist: regression, unsupervised learning, visualizations\nscience communicator: summarizing results, explaining broader impacts\n\nThis hints at why it is so hard to become a truly good data scientist– you need to be pretty good at a lot of different things.\nIf nothing else, going into this line of work requires that you be ready and willing to be a life-long student There are always new techniques, methods, frameworks and application domains to be learned!"
  },
  {
    "objectID": "intro.html#topics-covered",
    "href": "intro.html#topics-covered",
    "title": "2  Introduction",
    "section": "2.5 Topics covered",
    "text": "2.5 Topics covered\nWe will cover five basic topics this semester:\n\nSampling\nEstimation\nTesting\nPrediction\nObservational/exploratory data analysis\n\nLet’s briefly discuss each of these.\n\n2.5.1 Sampling\nElection polls and other survey data often get reported with response rates and other details about how survey respondents were selected.\nExample: At the end of October 2020, pollsters were trying to predict the outcome of the 2020 presidential election. A common technique for this is “random digit dialing”, which is exactly what it sounds like.\nThis poll reached 806 registered voters in Wisconsin. After a significant amount of statistical work, the pollsters reported that\n\n48% of likely voters would choose Biden\n43% would vote for Trump\n2% for Jorgensen, and\n7% remained undecided.\n\nIn order to reach those 806 participants, many more numbers need to be dialed. In this poll, the response rate was 4.3%. If you read the methodology report, you’ll see that in fact over 100,000 numbers had to be dialed to get these 806 respondents. The vast majority of those 100,000 calls were never picked up. Among those who did pick up, 806 were registered voters and agreed to participate in the survey, but another were 1113 refused to participate (or hung up).\nIn the actual election,\n\nBiden received 49.45% of votes cast in Wisconsin and\nTrump received 48.82%.\n\nQuestions:\n\nHow does this compare with the predicted vote shares of 48% for Biden and 43% for Trump?\nHow might we explain the discrepancies?”\n\n\n\n2.5.2 Testing\nYou have no doubt heard that “correlation does not imply causation”. Well, this is true.\n\nIce cream consumption is correlated with drowning, but we don’t think that ice cream causes drowning.\nHospitals are full of sick people, but we don’t think that hospitals cause sickness.\n\nNonetheless, to paraphrase a relevant comic, causality often does give awfully strong hints.\n\n\n\nCredit: Randall Munroe, XKCD (https://xkcd.com/552)\n\n\nExample: On November 16, 2020 Moderna released results from their phase 3 clinical trial for their COVID-19 vaccine.\nThere were approximately 30,000 people in the trial, split (approximately) evenly between treatment (got the vaccine) and control (got a placebo).\n\nIn total, there were 95 cases of COVID-19 among the participants; 90 among the placebo group and 5 among the treated group.\nOf the 95 cases, 11 were severe cases, all in the placebo group.\n\nIn this study, vaccination is correlated with reduced risk of infection.\n\nDoes this mean that the vaccination causally reduced COVID-19 infection? Why or why not?\nHow do we know that we aren’t fooling ourselves when we say that the Moderna vaccine is effective?\n\n\n\n2.5.3 Estimation\nThe news is perpetually full of stories about different economic indicators and how they are changing over time.\n\nThe Consumer Price Index (CPI) is meant to measure the change over time in prices of consumer goods and services.\nMost surveys (e.g., public opinion or election surveys) are reported with a +/- 3% “confidence interval” or “sampling error”.\n\nWhat is that all about?\nExample: In the Wisconsin poll discussed above, the margin of error was reported to be +/- 4.3%.\nRecall that the pollsters predicted 48% of likely voters would choose Biden and 43% Trump, while the actual outcome was 49.45% for Biden and 48.82% for Trump.\n\nIs this outcome within the stated margin of error?\nMore generally, what does it mean to give a confidence interval for a quantity of interest?”\n\n\n\n2.5.4 Prediction\nInvesting successfully (in real estate, stocks, etc) requires that we be able to predict the future behavior of an asset based on what we know about it currently.\n\nBased on the size of a house, its proximity to schools or infrastructure, walkability of its neighborhood, etc., we might hope to predict its “true” price.\nMany psychology and sociology studies aim to predict future student outcomes based on performance on a standardized test\n\nIn these kinds of problems, our goal is to predict an outcome or response (e.g., house price) based on one or more predictors (e.g., square footage of the house).\nMost of machine learning is concerned with prediction problems. For example, labeling an image according to whether or not it contains a cat can be stated as a prediction problem.\nExample: this plot shows different people’s incomes (in tens of thousands of dollars per year) as a function of their years of education.\n\n\n\nEducation by income\n\n\nIt certainly looks like more years of education correlate with higher income.\nQuestion: Suppose I tell you that someone has 18 years of education. What would you predict their income to be?\n\n\n2.5.5 Observational/exploratory data analysis\nSuppose that a colleague or client gives you a data set that looks like this:\n\nWhat would you do? There is clearly some kind of a cluster structure present here.\nThe goal of exploratory data analysis is to identify interesting structures in our data that might warrant further study.\nExample: In my own research, I (Keith) collaborate a lot with neuroscientists, who are interested in identifying functional subnetworks of the brain. These are groups of neurons that work together, typically because they are associated with the same activity (e.g., attention, motion, speech).\nThis is an example of clustering, in which our goal is to group data points in a sensible way, without necessarily saying ahead of time what those groups mean\nOftentimes, we obtain observational data. That is, data that does not come from a carefully-designed experiment.\nExample: Much data in the modern age of “big data” is collected by scraping the web or collected in other “messy” ways.\n\nScraping data from a social media site such as Twitter.\nMeasuring the socioeconomic status of people in different zip codes\n\nThere are lots of interesting scientific questions we might like to ask about such a data set, but because this data isn’t the result of a carefully-controlled experiment, we are often much more limited in what we can say."
  },
  {
    "objectID": "intro.html#world-data-model-hat-tip-to-karl-rohe",
    "href": "intro.html#world-data-model-hat-tip-to-karl-rohe",
    "title": "2  Introduction",
    "section": "2.6 World, Data, Model (hat tip to Karl Rohe)",
    "text": "2.6 World, Data, Model (hat tip to Karl Rohe)\nUnless you subscribe to some rather outlandish philosophical beliefs (see, e.g., here), there is a world out there, which we would like to learn about.\nTo figure things out about the world, we take measurements. That is to say, we collect data. These data describe the world, but it remains to build a model that explains how these data came to be.\nThe process of inference is how we use a specific set of data to guide our beliefs about the world.\nAs a simple example, consider human height. What is the average adult human height?\nThis is a question about the world. In fact, we could compute the average adult human height exactly if we could go out and measure the height of every adult human.\n\nOf course this would be a complicated and expensive process.\nInstead, we could just measure the heights of a few adult humans (say, a few thousand).\n\nOf course, this small collection of humans would need to be chosen randomly and in such a way that they would be representative of the population as a whole, but let’s ignore that concern until later in the course.\nThe few thousand heights that we measure would constitute our data– measurements that were taken out there in the world.\nHere is a simulation of what that data might look like with a sample of size 2000.\n\n# heights.R contains code for generating our synthetic data set.\nsource('r_scripts/heights.R');\n\nLoading required package: ggplot2\n\npp &lt;- ggplot( df_heights, aes(x=heights ) );\npp &lt;- pp + geom_histogram(aes(y=..density..), fill=\"grey\", color=\"black\", binwidth=2, );\npp &lt;- pp + geom_vline(xintercept=mean(heights), color=\"black\", linetype='f8', linewidth=1);\npp\n\n\n\n\nThe vertical dashed line indicates the mean of these 2000 sampled heights.\nBecause this is a random sample (and because heights vary due to factors like nutrition and genetics), this sample mean need not be equal to the population mean (i.e., the true average adult human height).\nInstead, the heights in our sample (and their mean) will vary randomly about the population average height. We use a statistical model (i.e., probability theory) to describe this variation.\nA common choice for modeling random variation like this is the normal distribution (in the coming lectures we will see plenty of other distributions).\nWe assume that our data are generated by a normal distribution with some mean \\(\\mu\\) (i.e., the average height) and some standard deviation \\(\\sigma\\). We call these parameters of the model.\nEstimating the population average height then reduces to estimating the “true” value of the parameter \\(\\mu\\) based on the data.\nThis step of using our data to determine something about our model is called inference. In this case, our goal is to estimate the value of the model parameter \\(\\mu\\), which will in turn be our estimate of the average adult human height."
  },
  {
    "objectID": "intro.html#all-models-are-wrong-but-some-are-useful",
    "href": "intro.html#all-models-are-wrong-but-some-are-useful",
    "title": "2  Introduction",
    "section": "2.7 All models are wrong, but some are useful",
    "text": "2.7 All models are wrong, but some are useful\nAn important point in our human height example above was our assumption that heights are normally distributed. In practice, modeling assumptions like these are never strictly true.\nOur model is just that– a model of the world; a set of simplifying assumptions that we hope are at least a good approximation to the truth.\nThink of your physics courses, where we assume that things happen in a frictionless void and use Newtonian mechanics instead of quantum mechanics.\nWe make assumptions like these because they often make the math easier while still being a good approximation to reality.\nIn our example above, the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) says that with some (perhaps very small) probability, we might observe a negative number.\nIf we model human heights as being normally distributed, our model predicts that we should, on occasion, meet people who have negative height.\n\nWhy might that be a problem?\nWhy might it be okay to still use this model anyway?\n\nLet’s push ahead and “fit” a normal to our data. We’ll have lots to say about this later. For now, think of this as choosing, out of all the possible normal distributions, the one that “best agrees”” with our data.\n\n# The dataframe df_normal contains a column of x-values in the same range as our synthetic data.\n# df_normal$y contains the normal pdf with mean and standard deviation fit to our synthetic data,\n# evaluated at these x-values.\npp &lt;- pp + geom_line( aes(x=df_normal$x, y=df_normal$y), size=1, color='red' );\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\nHuh. Our data seems to be a bit “flatter” than the normal distribution would predict.\nPerhaps this is just due to random fluctuation, but in fact there is a very simple reason for this: We didn’t tell you about it, but this sample includes both males and females.\nLet’s plot the same histogram, but this time, let’s break out the heights according to sex.\n\npp &lt;- ggplot( df_heights, aes(x=heights, color=sex, fill=sex ) );\npp &lt;- pp + geom_histogram( aes(y=..density..), position=\"identity\", alpha=0.5, binwidth=2);\npp\n\n\n\n\nHuman heights are bimodal– female heights are approximately normal about some mean, and male heights are approximately normal about another.\nWe can fit a normal to each of these separately and we see that our model agrees with the data much better.\n\n# df_bimodal$x contains x values that agree with the range of our height data.\n# df_bimodal$density is the density of a normal distribution, fit to the male or female heights,\n#        evaluated at these x-values.\n# df_bimodal$sex encodes male/female\\n\",\npp &lt;- pp + geom_line(data=df_bimodal, aes(x=x, y=density, color=sex), size=2, alpha=0.5);\npp\n\n\n\n\nThis is a good (albeit simple) illustration of the kind of iterative workflow that we typically use in data science.\n\nWe obtain our data, fit a model to it, and then we examine the shortcomings of that model.\nAfter some thought, it becomes clear how to improve our model.\nWe implement those changes (in this case, we incorporated the variable sex into our model), and examine our findings again.\n\nTypically, we repeat this cycle several times before reaching a conclusion that we are confident in."
  },
  {
    "objectID": "R04_Monte_Carlo_Testing.html#example-groundhogs-day",
    "href": "R04_Monte_Carlo_Testing.html#example-groundhogs-day",
    "title": "14  Monte Carlo Testing",
    "section": "14.1 Example: Groundhog’s Day",
    "text": "14.1 Example: Groundhog’s Day\nHow well has Punxsutawney Phil done in predicting the weather?\nThousands gather at Gobbler’s Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.\nSource: https://www.kaggle.com/datasets/groundhogclub/groundhog-day?resource=download\n\ngroundhog&lt;-read.csv(\"data/archive.csv\")\n\n#Get rid of rows with no record or partial shadow. Let's be serious!\n\ngroundhog &lt;- subset(groundhog, groundhog$Punxsutawney.Phil %in% c(\"Full Shadow\",\"No Shadow\") & !is.na(groundhog$February.Average.Temperature))\n\nLet’s do a permutation test\n\\(H_0:\\) Phil’s not a true forecasting groundhog \\(H_1:\\) Phil has some forecasting power.\n\n14.1.1 Early Spring = February warmer than average\nWe will compare mean temperature in February as a measure of early spring. We will take those years of “early spring” prediction and those of “regular spring” and compare the average average Feb temperature in those two groups.\n\n#Isolate feb avg temperature for full shadow years\n#isolate feb avg temperature for no shadow years\nfeb.avg.shadow &lt;- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\nfeb.avg.noshadow &lt;- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n#check\nfeb.avg.shadow\n\n  [1] 35.37 30.76 29.86 28.42 31.59 26.94 33.69 35.46 33.01 35.02 29.30 33.66\n [13] 31.08 29.50 29.52 35.94 33.03 30.09 33.82 32.25 33.69 36.90 31.95 29.57\n [25] 35.19 38.30 37.67 37.90 34.34 26.92 40.10 37.94 36.70 29.59 36.01 25.23\n [37] 31.73 36.00 29.98 33.85 33.53 34.50 35.28 34.88 31.41 31.62 31.91 34.54\n [49] 35.46 35.87 41.41 31.05 32.27 38.07 31.98 32.49 30.52 37.42 34.92 34.12\n [61] 31.77 32.52 31.24 32.88 32.92 32.81 33.48 33.55 33.58 34.74 39.56 36.03\n [73] 27.99 28.13 32.85 36.59 32.59 37.38 30.87 37.38 29.17 39.81 39.70 31.14\n [85] 31.87 35.69 38.71 39.78 33.98 36.39 32.79 33.57 37.94 34.83 34.70 36.77\n [97] 31.80 37.51 32.13 32.99\n\nfeb.avg.noshadow\n\n [1] 34.32 35.55 35.35 32.76 36.86 35.85 33.69 36.70 37.36 36.05 39.49 32.41\n[13] 33.04 34.77 39.47\n\n\n\n#We will use mean feb no shadow - mean feb shadow; if this &gt;0 that is evidence that the predictions work.\n\nmean(feb.avg.noshadow)-mean(feb.avg.shadow)\n\n[1] 1.8655\n\npermute_and_compute &lt;- function(sampleA, sampleB){\n  #remember - sampleA is no shadow\n  #sample B is shadow\n  pooledData &lt;- c(sampleA, sampleB)\n  shuffledData &lt;- sample(pooledData)\n  sim.sampleA &lt;- shuffledData[1:length(sampleA)]\n  sim.sampleB &lt;- shuffledData[(length(sampleA)+1):length(shuffledData)]\n  #we may modify this if we want to use a different test statistic\n  return(mean(sim.sampleA)-mean(sim.sampleB)) \n}\nt_obs &lt;- mean(feb.avg.noshadow)-mean(feb.avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(feb.avg.noshadow, feb.avg.shadow)\n}\nhist(test.stats)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs) #p-value\n\n[1] 0.0171\n\n\n\n\n14.1.2 Early Spring = warmer than average March temperatures\nMaybe he’s better at predicting March temperatures?\n\nmar.avg.shadow &lt;- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\nmar.avg.noshadow &lt;- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(mar.avg.noshadow)-mean(mar.avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(mar.avg.noshadow, mar.avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0578\n\n\n\n\n14.1.3 Early Spring = Mean of Feb & March temps above average\nWhat if we average Feb and March temperatures together.\n\ngroundhog$FebMarAvg &lt;- (groundhog$February.Average.Temperature+groundhog$March.Average.Temperature)/2\navg.shadow &lt;- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\navg.noshadow &lt;- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(avg.noshadow)-mean(avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(avg.noshadow, avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0076\n\n\nI don’t know about you, but this is staring to seem a little bit weird.\n\n\n14.1.4 Measuring Accuracy\n(New idea of what “early spring” means)\nIf both February temperatures and march temperature are greater than average, then we say we have an early spring.\n\nfeb.avg &lt;- mean(groundhog$February.Average.Temperature)\nmar.avg &lt;- mean(groundhog$March.Average.Temperature)\n\ngroundhog$EarlySpring &lt;- groundhog$February.Average.Temperature&gt;feb.avg & groundhog$March.Average.Temperature&gt;mar.avg\n\naddmargins(table(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))\n\n             \n              FALSE TRUE Sum\n  Full Shadow    75   25 100\n  No Shadow       7    8  15\n  Sum            82   33 115\n\n\nwhen earlySpring = TRUE, and prediction was “no shadow” that would be correct when earlySpring = FALSE and prediction is “full shadow” that would be correct\n\naccuracy &lt;- function(guesses, weather){\n  predictEarlySpring &lt;- guesses==\"No Shadow\"\n  nCorrect &lt;- sum(weather==predictEarlySpring)\n  return(nCorrect/length(weather))\n}\n\naccuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring)\n\n[1] 0.7217391\n\n\nLet’s call our simulated Groundhog “Bernoulli Phil”\n\nprop.table(table(groundhog$Punxsutawney.Phil))\n\n\nFull Shadow   No Shadow \n  0.8695652   0.1304348 \n\n#A randomly guessing groundhog would see his shadow this proportion of the time\n\np.shadow &lt;- prop.table(table(groundhog$Punxsutawney.Phil))[1]\n\n\n(accuracy.obs &lt;- accuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))\n\n[1] 0.7217391\n\n#simulate guesses by Bernoulli Phil\nrandomGuesses &lt;- function(n=115, p=0.8695652){\n  return(sample(c(\"Full Shadow\",\"No Shadow\"), size=n, replace=TRUE, prob=c(p,1-p)))\n}\n\nNMC &lt;- 10000\nresults &lt;- rep(0,NMC)\nn &lt;- nrow(groundhog)\n\nfor(i in 1:NMC){\n  results[i] &lt;- accuracy(randomGuesses(n, p.shadow), groundhog$EarlySpring)\n}\n\nhist(results, breaks=40)\nabline(v=accuracy.obs)\n\n\n\nmean(results &gt;= accuracy.obs)\n\n[1] 0.0242\n\n\nThe actual groundhog’s accuracy is statistically much better than the simulated “random guessing” groundhog - at least better at predicting if both Feb & March will be above average temperature.\nThe scatterplot colored by shadow status makes this pattern suspiciously clear\n\nplot(groundhog$February.Average.Temperature, groundhog$March.Average.Temperature, col=(as.numeric(groundhog$Punxsutawney.Phil==\"Full Shadow\") +1), xlab=\"Avg. Feb Temp\", ylab=\"Avg. Mar Temp\", main=\"National Feb/Mar Temp vs Shadow\", pch=16)\nabline(v=mean(groundhog$February.Average.Temperature))\nabline(h=mean(groundhog$March.Average.Temperature))\nlegend(x=25, y=50, legend=c(\"early spring\",\"6 wks winter\"), pch=16, col=1:2)\n\n\n\n\nO ye unbelievers! Witness the prognosticating powers of Phil the groundhog!\n\n\n14.1.5 Looking at Deviation from a moving average\nHowever we have to be cautious - think about climate change and Phil’s predictions.\n\nlibrary(smooth)\n\nLoading required package: greybox\n\n\nPackage \"greybox\", v2.0.6 loaded.\n\n\n\nAttaching package: 'greybox'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    accuracy\n\n\nThis is package \"smooth\", v4.3.0\n\n\n\nAttaching package: 'smooth'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    accuracy\n\nMA &lt;-sma(groundhog$FebMarAvg, order=20)\n#predict(MA)\nplot(y=groundhog$FebMarAvg, x=groundhog$Year, col=as.numeric((as.factor(groundhog$Punxsutawney.Phil)))+1, pch=16)\nlines(y=predict(MA)$mean, x=groundhog$Year)\n\n\n\n\nNotice that he has been predicting early springs more often since the 60s, and also we’ve been having warmer and warmer temperatures. Perhaps he’s predicting early spring more often because of climate change, and thus he is correct more often.\nThis whole analysis really hinges on what we mean by an “early spring”. One could argue that with rising global temperatures, spring is actually coming earlier more often than it used to, and THAT’s why he’s seeing no shadow more often.\nIf we repeat this analysis and look not at whether the temperature is above average, but rather whether the temperature is above the moving average then it changes the conclusion a bit\n\ngroundhog$tempDev &lt;- groundhog$FebMarAvg - as.numeric(predict(MA)$mean)\nplot(y=groundhog$tempDev,x=groundhog$Year, col=as.numeric((as.factor(groundhog$Punxsutawney.Phil)))+1, pch=16, ylab=\"dev from moving avg\")\n\n\n\n\n\navg.shadow &lt;- groundhog$tempDev[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\navg.noshadow &lt;- groundhog$tempDev[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(avg.noshadow)-mean(avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(avg.noshadow, avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0589\n\n\nLast let’s consider an “early spring” when both Feb and March average temps are above the moving average.\n\nMAfeb &lt;-sma(groundhog$February.Average.Temperature, order=20)\nMAmar &lt;-sma(groundhog$March.Average.Temperature, order=20)\n\nearlySpring &lt;- groundhog$February.Average.Temperature &gt; as.numeric(predict(MAfeb)$mean)  &\n  groundhog$March.Average.Temperature &gt; as.numeric(predict(MAmar)$mean)\n\naccuracy(groundhog$Punxsutawney.Phil, earlySpring)\n\n[1] 0.7130435\n\n\nThe accuracy is now not much better than a flip of a coin. But that’s not statistical thinking - we need to see if he is better than a randomly guessing groundhog.\n\n(accuracy.obs &lt;- accuracy(groundhog$Punxsutawney.Phil, earlySpring))\n\n[1] 0.7130435\n\n#simulate guesses by Bernoulli Phil\nrandomGuesses &lt;- function(n=115, p=0.8695652){\n  return(sample(c(\"Full Shadow\",\"No Shadow\"), size=n, replace=TRUE, prob=c(p,1-p)))\n}\n\nNMC &lt;- 10000\nn &lt;- nrow(groundhog)\nresults &lt;- replicate(NMC, accuracy(randomGuesses(n, p.shadow), earlySpring))\n\nhist(results, breaks=40)\nabline(v=accuracy.obs)\n\n\n\nmean(results &gt;= accuracy.obs)\n\n[1] 0.0294\n\n\nI’m still convinced - the groundhog does significantly better than a random guesser."
  },
  {
    "objectID": "R04_Monte_Carlo_Testing.html#example-autocorrelation",
    "href": "R04_Monte_Carlo_Testing.html#example-autocorrelation",
    "title": "14  Monte Carlo Testing",
    "section": "14.2 Example: Autocorrelation",
    "text": "14.2 Example: Autocorrelation\nA question of particular importance in time series analysis is whether there is any autocorrelation. If your data is recorded regularly at timesteps \\(1,2,\\ldots\\) we could look at the relationship between \\(X_i\\) and \\(X_{i+1}\\) to see if they exhibit correlation.\nconsider the following time series data:\n\ntime_series &lt;- read.csv(\"data/time_series_data.csv\")\nplot(time_series$x, type=\"l\")\n\n\n\n\nOne possible model for how this data came about is that it is independent draws from a random variable. A quick check of the histogram can help us determine a good random variable to model the data:\n\nhist(time_series$x, breaks=20)\n\n\n\n\nThe distribution is somewhat symmetric, bell-shape. It could be a normal distribution. A better check for the distribution fit is a qqnorm plot.\n\nlibrary(car)\nqqPlot(time_series$x)\n\n\n\n\n[1] 98 83\n\n\nLet’s just pull the point estimates for the \\(\\mu\\) and \\(\\sigma\\) parameters for a normal distribution.\n\n(mu &lt;- mean(time_series$x))\n\n[1] 100.1767\n\n(sigma &lt;- sd(time_series$x))\n\n[1] 4.166514\n\n\nIf we are reasonably comfortable with the assumption that the data came from a normal distribution (seems supported by the QQ plot) we could model \\(X\\) as independent draws from \\(N(100.17, 4.17^2)\\).\nA natural test statistic to look at which measures autocorrelation is the correlation of \\(X_i\\) with \\(X_{i+1}\\).\n\ntest.stat &lt;- function(X){\n  return(cor(X[-1], X[-length(X)] ) )\n}\n(obs.t &lt;- test.stat(time_series$x))\n\n[1] 0.2991791\n\n\n\n14.2.1 A Parametric Test of Autocorrelation\nTo perform a Monte Carlo test we can simulate datasets from our null hypothesis model and record the simulated test statistic.\n\nt.sim &lt;- replicate(10000, test.stat(rnorm(100, mu, sigma)))\nhist(t.sim)\n\n\n\n\nWe will consider what proportion of the simulated test statistics are more extreme than we observed. Because this is a two-tailed distribution - and evidence against the null would be found in either the upper or the lower tail, we should make our p-value calculation by doubling the smaller of the two tails\n\n2 * min(mean(t.sim &gt;= obs.t),\n        mean(t.sim &lt;= obs.t))\n\n[1] 0.0022\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nWe have a very low p-value, indicating that if the data were in fact independent draws, we’d have a very very small chance of seeing data like this.\n\n\n14.2.2 A Permutation Test of Autocorrelation\nWe could also take a different approach for our null model. We could use a permutation test. Under the null hypothesis, every permutation of the data is equally likely. So we could produce a simulated test statistic distribution by shuffling the data around each time. This is consistent with our null hypothesis because there’s no trend in this model.\n\nt.sim &lt;- replicate(10000,\n                   test.stat(sample(time_series$x)))\n2 * min(mean(t.sim &gt;= obs.t),\n        mean(t.sim &lt;= obs.t))\n\n[1] 0.0018\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nThe p-value is pretty much the same, and so is our conclusion.\n\n\n14.2.3 A Permutation test of constant variance\nAnother violation of the null model would be if the variance of the simulated values is not consistent. We could simply split the data in half and calculate the variance of the first half of the data, comparing it with the variance of the second half of the data. We can look at the absolute value of the difference.\n\ntest.stat &lt;- function(x){\n  n1 &lt;- round(length(x)/2)\n  abs(var(x[1:n1]) - var(x[-(1:n1)]))\n}\n(obs.t &lt;- test.stat(time_series$x))\n\n[1] 13.25346\n\n\nThis test statistic is designed to be a one-sided test statistic, only large values of \\(T\\) represent evidence against the null hypothesis that \\(\\sigma^2\\) is constant.\n\nt.sim &lt;- replicate(10000, test.stat(sample(time_series$x)))\nmean(t.sim &gt;= obs.t) #right-tailed p-value\n\n[1] 0.0039\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nThe \\(p\\)-value is small once again. We have strong evidence that the the variance of the model generating these values is not constant."
  },
  {
    "objectID": "testing2_practice.html",
    "href": "testing2_practice.html",
    "title": "18  Testing and Power Practice",
    "section": "",
    "text": "19 Practice Problems",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "testing2_practice.html#precise-tea-testing",
    "href": "testing2_practice.html#precise-tea-testing",
    "title": "18  Testing and Power Practice",
    "section": "19.1 Precise Tea Testing",
    "text": "19.1 Precise Tea Testing\nReturning to the example of Muriel Bristol, suppose that Fisher wanted to establish a decision rule that would have achieve an exact \\(\\alpha\\) level of 0.05. How would that be done? (Don’t simply say reject \\(H_0\\) with probability .05, you can make a more powerful test than that)\nRemember that the test goes like this: 4 cups of “milk-first” tea and 4 cups of “milk-second” tea are shuffled. Muriel tastes each one and attempts to identify the 4 “milk-first” cups. Under the null hypothesis she is a random guesser.\nEstablish your rule and demonstrate that its type 1 error rate is 0.05 using Monte Carlo.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nRecall the functions used for this Monte Carlo test\n\ngenerate_cups &lt;- function(k, n){\n  return(sample(c(rep(0,k), rep(1, n-k))))\n}\ngenerate_guesses &lt;- function(k,n){\n  return(generate_cups(k,n))\n}\n\n#Establish test statistic distribution\nNMC &lt;- 10000\nt.sim &lt;- replicate(NMC, sum(generate_cups(4,8) ==\n                            generate_guesses(4,8)))\n\nthreshold &lt;- quantile(t.sim, .95)\np.above &lt;- mean(t.sim &gt; threshold)\np.at &lt;- mean(t.sim == threshold)\nprob.reject.at &lt;-  (.05-p.above)/p.at\n\nThe rule is to reject \\(H_0\\) if there are more than 6 correct guesses. If there are exactly 6 correct guesses, reject \\(H_0\\) with probability 0.1628521.\n\n#verify\nNMC.2 &lt;- 10000\nresult &lt;- logical(NMC.2)\nfor(i in 1:NMC.2){\n  n.correct &lt;- sum(generate_cups(4,8) ==\n                            generate_guesses(4,8))\n  if(n.correct&gt;threshold | \n     (n.correct==threshold & runif(1)&lt;prob.reject.at))\n    result[i] = TRUE\n}\nmean(result)\n\n[1] 0.0519\n\n\nThe code above checks the rejection rate of \\(H_0\\) when \\(H_0\\) is true, this is the type 1 error rate. It is very close to 0.05.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#type-1-errors-and-the-p-value-distribution",
    "href": "R05_testing_Additional_Examples.html#type-1-errors-and-the-p-value-distribution",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.1 Type 1 Errors and the P-Value Distribution",
    "text": "17.1 Type 1 Errors and the P-Value Distribution\nI want to sample from \\(N(\\mu, 5^2)\\)\nSay I want to test \\(H_0: \\mu = 10\\) vs \\(H_A: \\mu &gt; 10\\)\nI can use the test statistic simply being \\(\\bar{X}\\). Naturally if \\(\\bar{X}\\) is high then I would consider that evidence against \\(H_0\\).\nHere’s a function to generate sample data given \\(\\mu\\) and a test statistic function.\n\ngenerate_data &lt;- function(n, mu){\n  return(rnorm(n, mu, 5))\n}\n\ntest_stat &lt;- function(someData){\n  return(mean(someData))\n}\n\nSuppose we want to use a significance level of 7%. Why 7%? Why not!?! Imagine here is some data - note that this data is being generated from an alternative model (\\(\\mu=11\\)).\n\nset.seed(1)\nmyData &lt;- rnorm(25, 11, 5)\nmyData\n\n [1]  7.86773095 11.91821662  6.82185694 18.97640401 12.64753886  6.89765808\n [7] 13.43714526 14.69162353 13.87890676  9.47305806 18.55890584 12.94921618\n[13]  7.89379710 -0.07349944 16.62465459 10.77533195 10.91904868 15.71918105\n[19] 15.10610598 13.96950661 15.59488686 14.91068150 11.37282492  1.05324152\n[25] 14.09912874\n\n\nLet’s generate a test statistic distribution under the null hypothesis. What are typical values of \\(\\bar{X}\\) when the null hypothesis is true? If you remember the central limit theorem it will come as no surprise that this distribution is \\(N(10, 5^2/25)\\)\n\nNMC &lt;- 10000\nt.sim &lt;- replicate(NMC, test_stat(generate_data(25, 10)))\nhist(t.sim)\n\n\n\nmean(t.sim);var(t.sim)\n\n[1] 9.994043\n\n\n[1] 0.9879012\n\n\nWe want to pick a rejection threshold (a critical value) \\(t_\\alpha\\) designed to achieve a significance level (type 1 error rate) of .07. If \\(\\bar{X} \\geq t_\\alpha\\) we reject \\(H_0\\), and this should occur 7% of the time when \\(H_0\\) is true. Rather than using the theoretical normal distribution for \\(\\bar{X}\\), we’ll use the simulated test statistics to approximate the threshold.\n\nt.crit &lt;- quantile(t.sim, .93)\n#verify\nmean(t.sim &gt;= t.crit)\n\n[1] 0.07\n\n\nLet’s just make sure that if the null hypothesis is true that we end up rejecting 7%\n\nresult &lt;- FALSE\nfor(i in 1:NMC){\n  nullData &lt;- generate_data(25, 10)\n  result[i] &lt;- test_stat(nullData) &gt;= t.crit\n  # result will be TRUE if the test stat is beyond the critical\n  # value, ie. we reject the null\n  #False otherwise\n}\nmean(result)\n\n[1] 0.0732\n\n\nI’m curious about the p-values you get from this test. I mean - if the null hypothesis is true, what p-values would you expect to get? Let’s repeat with the null hypothesis, and just record p-values.\nTo put it another way - Because \\(T(D_0)\\) is a random variable, \\(Pr[T(D) &gt; T(d_0)]\\) is a random variable as well; the \\(p\\)-value under the null hypothesis can be thought of as a random variable and thus it has a distribution. What is the shape of this distribution? Let’s use Monte Carlo to estimate it.\n\np.val &lt;- 0\nfor(i in 1:NMC){\n  nullData &lt;- generate_data(25, 10)\n  p.val[i] &lt;- mean(t.sim &gt;= test_stat(nullData))\n}\nhist(p.val)\n\n\n\n\nIf the null hypothesis is true, and we get data and calculate a p-value, the p-value will be uniformly distributed according to a uniform(0,1). This turns out to be the case for ANY hypothesis test. It’s a little less precise when your test statistic is a discrete r.v., but it is more or less going to be the case.\nWhat about the distribution of p-values if the mean = 11 (i.e. the null is false) - this is the model from which we got our initial data.\nReminder: \\(H_0: \\mu=10\\) and \\(H_A: \\mu &gt; 10\\)\n\np.val &lt;- 0\nfor(i in 1:NMC){\n  altData &lt;- generate_data(25, 11)\n  p.val[i] &lt;- mean(t.sim &gt;= test_stat(altData))\n}\nhist(p.val)\n\n\n\nmean(p.val &lt;= 0.07)\n\n[1] 0.3277\n\nmean(p.val &gt;0.07)\n\n[1] 0.6723\n\n\nSo it seems if the data was drawn from a normal distribution with a mean of 11, we have a 32.77% probability of getting “strong evidence” to reject H_0. That’s the power of this test."
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#achieving-exact-alpha-with-a-discrete-test-statistic-distribution",
    "href": "R05_testing_Additional_Examples.html#achieving-exact-alpha-with-a-discrete-test-statistic-distribution",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.2 Achieving exact alpha with a discrete test statistic distribution",
    "text": "17.2 Achieving exact alpha with a discrete test statistic distribution\nGoing back to the example from lecture, suppose we want to achieve an exact type 1 error rate of 2.5%. We’re flipping a coin and we want to know what rejection rule we should use for the lower-bound on the number of heads - to conclude that the coin is not fair.\nTo remind you - we are flipping a coin 200 times, we have to come up with a rule such that if the flips are producing too low of heads, we reject the null hypothesis (and we would do something similar for the upper tail).\nThe goal is to find some rejection rule for low X that will reject 2.5% of the time for a fair coin.\n\n#P (X &lt;= 85) for X~Binom(200, .5)\npbinom(85, 200, .5)\n\n[1] 0.0200186\n\n#P (X &lt;= 86) for X~Binom(200, .5)\npbinom(86, 200, .5)\n\n[1] 0.02798287\n\n#P(X = 86)\ndbinom(86, 200, .5)\n\n[1] 0.007964274\n\n\n\\[P(X \\leq 85) = 0.0200186\\]\n\\[P(X \\leq 86) = 0.02798287\\]\n\\[P(X=86) = .00796\\]\nThe idea is we will reject always when X &lt;= 85, sometimes when X = 86, and never when X &gt; 86. What do we want for the sometimes? We want \\[P(X \\leq 85) + P(reject \\cap X=86) = 0.025\\] In other words:\n\\[P(reject \\cap X=86) = 0.025-P(X \\leq 85)\\] Expanding the left hand side\n\\[P(reject | X=86)\\cdot P(X=86) = 0.025-P(X \\leq 85)\\] Divide both sides by \\(P(X=86)\\) \\[P(reject | X=86) = \\dfrac{0.025-P(X \\leq 85)}{P(X=86)}\\]\n\n#We look at the probability under H0 that we get 86 heads\nP.reject.86 &lt;- (.025 - pbinom(85, 200, .5) )/dbinom(86, 200, .5)\nP.reject.86\n\n[1] 0.6254687\n\n\nSo if we get 86 heads, we will reject 62.5% of the time. That’s the idea.\n\nhack_decision_rule &lt;- function(T){\n  #T is the number of heads\n  if(T &lt;= 85){ return (TRUE)} #always reject if T &lt;= 85\n  else if(T==86) {return(rbinom(1, 1, P.reject.86))}\n  else {return (FALSE)}\n}\n\n#verify that we actually are able to achieve a precise 2.5% type 1 error rate using this \"hack\" rule.\ntrials &lt;- rbinom(100000, 200, .5)\nreject &lt;- FALSE\nfor(i in 1:100000){\n  reject[i] &lt;- hack_decision_rule(trials[i])\n}\nmean(reject)\n\n[1] 0.02499\n\n\nThere are two other ways that we could attempt (that come to my mind) to achieve an exact \\(\\alpha\\) with a discrete test statistic distribution.\n\nIf \\(X \\leq 86\\) reject \\(p\\) of the time\nAlways reject 2.5% of the time.\n\nBoth of these will attain an \\(\\alpha\\) of 0.025, but they are not as powerful (probably). Let’s consider different probabilities from 0 to .5 and compare the powers We’re only refining the rejection rule for the lower tail, so we don’t need to look at the upper tail right now.\n\nset.seed(1)\n\nrule1.p &lt;- 0.025/pbinom(86,200, .5)\n\nhack_decision_rule1 &lt;- function(T){\n  if(T &lt;= 86){\n    return(runif(1) &lt; rule1.p)\n  } else {\n    return(FALSE)\n  }\n}\nhack_decision_rule2 &lt;- function(T){\n  return(runif(1) &lt; 0.025)\n}\n\n\nif(file.exists(\"discrete_exact_alpha.rds\")){\n  #So this knits faster- load the results if they exist.\n  discreteRR &lt;- readRDS(\"discrete_exact_alpha.rds\")\n} else {\n  pseq &lt;- seq(0,.5,.01)\n  rr_0 &lt;- rep(0,length(pseq))\n  rr_1 &lt;- rr_0\n  rr_2 &lt;- rr_0\n  NMC &lt;- 5000\n  for(j in 1:length(pseq)){\n    p &lt;- pseq[j]\n    for(i in 1:NMC){\n      #generate data\n      nheads &lt;- rbinom(1, 200, p)\n      rr_0[j] &lt;- rr_0[j] + hack_decision_rule(nheads)\n      rr_1[j] &lt;- rr_1[j] + hack_decision_rule1(nheads)\n      rr_2[j] &lt;- rr_2[j] + hack_decision_rule2(nheads)\n    }\n  }\n  rr_0 &lt;- rr_0 / NMC\n  rr_1 &lt;- rr_1 / NMC\n  rr_2 &lt;- rr_2 / NMC\n  discreteRR &lt;- data.frame(pseq, rr_0, rr_1, rr_2)\n  saveRDS(discreteRR, \"discrete_exact_alpha.rds\")\n}\n\nplot(discreteRR$rr_0~discreteRR$pseq, type=\"l\", ylab=\"rejection rate\", xlab=\"Prob of H\", main=\"Power Comparison of Hack Rejection Rules\", data=discreteRR)\nabline(h=0.025, col=\"red\")\nlines(x=discreteRR$pseq, y=discreteRR$rr_1, col=\"blue\")\nlines(x=discreteRR$pseq, y=discreteRR$rr_2, col=\"orange\")\nlegend(x=0, y=.6, legend=c(\"Hack rule\", \"reject sometimes if T&lt;=86\", \"reject sometimes\"), col=c(\"black\",\"blue\",\"orange\"), lwd=1)\n\n\n\n\nCompare their levels:\n\ndiscreteRR$rr_0[51]\n\n[1] 0.0256\n\ndiscreteRR$rr_1[51]\n\n[1] 0.0252\n\ndiscreteRR$rr_2[51]\n\n[1] 0.0262"
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#example-comparing-consistency",
    "href": "R05_testing_Additional_Examples.html#example-comparing-consistency",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.3 Example: Comparing Consistency",
    "text": "17.3 Example: Comparing Consistency\nStudent A and Student B are in the same class. We are curious to know about their academic performance, in particular their consistency in grade performance.\n\nA.grades &lt;- c(83.4,91.4,83.0,77.2,81.9,76.8,82.4,89.0,82.0,74.9,77.6,78.7,79.1,77.8,80.3)\nB.grades &lt;- c(75.9,66.4,98.1,83.0,72.4,70.6,67.1,73.0,89.1,87.0,83.9,63.7,88.6,90.0,75.5,86.5,77.5)\nA.grades &lt;- A.grades - mean(A.grades)\nB.grades &lt;- B.grades - mean(B.grades)\nboxplot(A.grades, B.grades, horizontal=TRUE)\n\n\n\n\nThe question is this: are the two students equally consistent in their grades? Let’s think about consistency in terms of variance\nLet’s fill in the following code:\n\n#Takes in two data vectors, compares them in a meaningful way and returns a statistic that is useful to compare consistency.\ntestStat &lt;- function(dataA, dataB){\n  return(var(dataA)-var(dataB))\n}\n\n#This function takes in two vector, combines them, shuffles,\n#And splits them\n#Then calculates a test statistic\npermuteAndCompute &lt;- function(dataA, dataB){\n  combinedData &lt;- c(dataA, dataB)\n  shuffledData&lt;- sample(combinedData)\n  simA &lt;- shuffledData[1:length(dataA)]\n  simB &lt;- shuffledData[-(1:length(dataA))] #I use negative index\n  return(testStat(simA, simB))\n}\n\n#Takes in the two data sets and a number of monte carlo runs. It will perform the MC simulation NMC times and it will return a distribution of test statistics.\nmcDist &lt;- function(dataA, dataB, MNC){\n  results &lt;- 0\n  for(i in 1:NMC){\n    results[i] &lt;- permuteAndCompute(dataA, dataB)\n  }\n  return(results)\n}\n\n\nobsTestStat &lt;- testStat(A.grades, B.grades)\nmcSamplingDistribution &lt;- mcDist(A.grades, B.grades, 10000)\n\nhist(mcSamplingDistribution)\nabline(v=obsTestStat)\n\n\n\nobsTestStat\n\n[1] -76.26533\n\nmean(mcSamplingDistribution &gt;= obsTestStat)\n\n[1] 0.9993\n\nmean(mcSamplingDistribution &lt;= obsTestStat)\n\n[1] 7e-04\n\n\nWe have about .1% in the left-tail, more extreme than our observed test statitic. However the question we were investigating is “Are they equally consistent or not”\nSo - if the test statisitc was very positive (+75 or so) we would also consider that evidence for the alternative.\nThis is a two-sided test; we need to calculate probability from both sides of the distribution. What we want to do then is to follow this model:\n\ncalculate the probability &lt;= obs. test stat\ncalculate the probability &gt;= obs. test stat\ntake the smaller of the two\nDouble it\n\n\n#This is my formula for a 2-tailed p-value\n2*min(mean(mcSamplingDistribution&gt;=obsTestStat),\n      mean(mcSamplingDistribution&lt;=obsTestStat))\n\n[1] 0.0014\n\n\nIf this was a symmetric distribution, this would be the same as:\n\nmean(mcSamplingDistribution&gt;= abs(obsTestStat)) + mean(mcSamplingDistribution&lt;= -abs(obsTestStat))\n\n[1] 0.0021\n\n\nLet’s suppose we used some other measure of “difference of consistency” For example, we could take the ratio of the variances.\n\n#redefine the test statistic\ntestStat &lt;- function(dataA, dataB){\n  return(var(dataA)/var(dataB))\n}\n\nobsTestStat &lt;- testStat(A.grades, B.grades)\nmcSamplingDistribution &lt;- mcDist(A.grades, B.grades, 10000)\n\nhist(mcSamplingDistribution, breaks=50)\nabline(v=obsTestStat)\n\n\n\n2*min(mean(mcSamplingDistribution &gt;= obsTestStat),\n      mean(mcSamplingDistribution &lt;= obsTestStat))\n\n[1] 0.002"
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#coin-flips",
    "href": "R05_testing_Additional_Examples.html#coin-flips",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.4 Coin Flips",
    "text": "17.4 Coin Flips\nBelow are two sequences of 300 “coin flips” (H for heads, T for tails). One of these is a true sequence of 300 independent flips of a fair coin. The other was generated by a person typing out H’s and T’s and trying to seem random. Which sequence is truly composed of coin flips?\nSequence 1:\nTTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHH TTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHH TTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHT HTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTT HHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\nSequence 2:\nHTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTH THTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHH TTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTT THTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTH HHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT\nUseful functions: flips &lt;- strsplit(\"HHTHTHHTHTHHHT\",\"\") with(rle(flips[[1]]), lengths[values==\"H\"])\n\nseq1 =\"TTHHTHTTHTTTHTTTHTTTHTTHTHHTHHTHTHHTTTHHTHTHTTHTHHTTHTHHTHTTTHHTTHHTTHHHTHHTHTTHTHTTHHTHHHTTHTHTTTHHTTHTHTHTHTHTTHTHTHHHTTHTHTHHTHHHTHTHTTHTTHHTHTHTHT THHTTHTHTTHHHTHTHTHTTHTTHHTTHTHHTHHHTTHHTHTTHTHTHTHTHTHTHHHTHTHTHTHHTHHTHTHTTHTTTHHTHTTTHTHHTHHHHTTTHHTHTHTHTHHHTTHHTHTTTHTHHTHTHTHHTHTTHTTHTHHTHTHTTT\"\n\nseq2 = \"HTHHHTHTTHHTTTTTTTTHHHTTTHHTTTTHHTTHHHTTHTHTTTTTTHTHTTTTHHHHTHTHTTHTTTHTTHTTTTHTHHTHHHHTTTTTHHHHTHHHTTTTHTHTTHHHHTHHHHHHHHTTHHTHHTHHHHHHHTTHTHTTTHHTTTTHTHHTTHTTHTHTHTTHHHHHTTHTTTHTHTHHTTTTHTTTTTHHTHTHHHHTTTTHTHHHTHHTHTHTHTHHHTHTTHHHTHHHHHHTHHHTHTTTHH HTTTHHTHTTHHTHHHTHTTHTTHTTTHHTHTHTTTTHTHTHTTHTHTHT\"\n\nflips1 &lt;- strsplit(seq1,\"\")\nH.run.lengths1 &lt;- with(rle(flips1[[1]]), lengths[values==\"H\"])\ntable(H.run.lengths1)\n\nH.run.lengths1\n 1  2  3  4 \n66 27  8  1 \n\nflips2 &lt;- strsplit(seq2,\"\")\nH.run.lengths2 &lt;- with(rle(flips2[[1]]), lengths[values==\"H\"])\ntable(H.run.lengths2)\n\nH.run.lengths2\n 1  2  3  4  5  6  7  8 \n45 15  9  5  1  1  1  1 \n\n\nWhat if we use maximum run length as a test statistic?\n\nt.dist &lt;- 0 #empty vector for our simulated distribution of test statistics\nMCN &lt;- 10000\nn &lt;- 300\nfor(i in 1:MCN){\n  flips &lt;- sample(c(\"H\",\"T\"), 300, replace=TRUE)\n  H.run.lengths &lt;- with(rle(flips), lengths[values==\"H\"])\n  t.dist[i] &lt;- max(H.run.lengths)\n}\n\nhist(t.dist)\n\n\n\n\nWhat are the two test statistics? We would use a 2-tailed p-value. We would consider a sequence of Hs and Ts to be weird if the max run length was too long or too short.\n\n(t1 &lt;- max(H.run.lengths1))\n\n[1] 4\n\n2*(min(mean(t.dist &lt;= t1),mean(t.dist &gt;= t1)))\n\n[1] 0.013\n\n(t2 &lt;- max(H.run.lengths2))\n\n[1] 8\n\n2*(min(mean(t.dist &lt;= t2),mean(t.dist &gt;= t2)))\n\n[1] 0.8762\n\n\nIt probably comes as no surprise that the first sequence of Hs and Ts is suspicious. It does not contain enough long runs of Hs- when flips are done randomly you tend to get at least some much longer runs.\nWe could repeat this with the “average” run length as well.\n\nt.dist &lt;- 0 #empty vector for our simulated distribution of test statistics\nMCN &lt;- 10000\nn &lt;- 300\nfor(i in 1:MCN){\n  flips &lt;- sample(c(\"H\",\"T\"), 300, replace=TRUE)\n  H.run.lengths &lt;- with(rle(flips), lengths[values==\"H\"])\n  t.dist[i] &lt;- mean(H.run.lengths)\n}\n\nhist(t.dist)\n\n\n\n(t1 &lt;- mean(H.run.lengths1))\n\n[1] 1.45098\n\n2*(min(mean(t.dist &lt;= t1),mean(t.dist &gt;= t1)))\n\n[1] 0\n\n(t2 &lt;- mean(H.run.lengths2))\n\n[1] 1.897436\n\n2*(min(mean(t.dist &lt;= t2),mean(t.dist &gt;= t2)))\n\n[1] 0.5612\n\n\nThe average run length of 1.45 would virtually never happen if a coin was actually flipped 300 times. That’s conclusive evidence that the first sequence was made up by a human.\nWhat are some other test statistics that could have worked?"
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#example-hot-hands",
    "href": "R05_testing_Additional_Examples.html#example-hot-hands",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.5 Example: hot hands",
    "text": "17.5 Example: hot hands\nFor the next example, let’s do something slightly more complicated.\nA certain professional basketball player believes he has “hot hands” when shooting 3-point shots (i.e. if he makes a shot, he’s more likely to also make the next shot). His friend doesn’t believe him, so they make a wager and hire you, a statistician, to settle the bet.\nAs a sample, you observe the next morning as the player takes the same 3-point shot 200 times in a row (assume he is well rested, in good physical shape, and doesn’t feel significantly more tired after the experiment), so his level of mental focus doesn’t change during the experiment). You obtain the following results, where Y denotes a success and N denotes a miss:\nYNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNNNYNNYYYYNNYY\nNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNNYNNNNNNYNNNYNNYNNNNNYNYY\nYNNYYYNYNNNNYNNNNNNNYYNNYYNNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNN\nNYYYYYYNYYNNYN\nThe hypotheses being tested are: \\(H_0:\\) there is no hot hand effect - shots are independent \\(H_A:\\) there is some dependency between shots\nNote that the existence of a “hot hands” effect means the shots are not independent. Also note that there’s a third possibility: that the player is more likely to “choke” and miss the next shot if he scored the previous one (e.g. maybe scoring a shot makes him feel more nervous because he feels like he’s under pressure).\n\n17.5.1 Attempt 1: run length\nSince the existence of a hot hands effect tends to increase the run lengths of Ys compared to if the shots were independent, we can use the longest run length as a way of comparing independence vs hot hands (note if the player is a choker, they will tend to have shorter runs of Ys than if they were independent, so you can simply ignore this case for now and compare hot hands v. independence for simplicity).\nNow, how exactly do you compare these two situations and determine which is a better fit for the data?\nOne thing that’s worth noting is that if a sequence of repeated experiments is independent, then it shouldn’t matter what order the results are in. This should be fairly easy to understand and agree with.\nLet’s assume that the throws are totally independent. Recall we also assume he doesn’t get tired so his baseline shot-making ability doesn’t change over the course of the experiment. Therefore, we should be able to (under these assumptions) arbitrarily reorder his shots without affecting any statistical properties of his shot sequence. So let’s do that!\nWe begin by parsing the throws into a vector of Y and N.\n\n# the sequence of throws is broken up into 4 chunks for readability, then\n# paste0 is used to merge them into a single sequence, then\n# strplit(\"YN...N\",split=\"\") is used to split the string at every \"\", so\n# we get a vector of each character, and finally\n# [[1]] is used to get the vector itself (strsplit actually outputs a list\n# with the vector as the first element; [[1]] removes the list wrapper)\n# \n# for more info about the strsplit function, see\n# https://www.journaldev.com/43001/strsplit-function-in-r\n\nthrows = strsplit(\n   paste0(\"YNNNNYYNNNYNNNYYYNNNNNYNNNNNNNNNYNNNNNYYNYYNNNYNNN\",\n          \"NYNNYYYYNNYYNNNNNNNNNNNNNNNYYYNNNYYYYNNNNNYNYYNNNN\",\n          \"YNNNNNNYNNNYNNYNNNNNYNYYYNNYYYNYNNNNYNNNNNNNYYNNYY\",\n          \"NNNNNNYNNNYNNNNNNNNYNNNYNNNNNYYNNNNNNYYYYYYNYYNNYN\"), split=\"\")[[1]]\n\nthrows\n\n  [1] \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"N\"\n [19] \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\"\n [37] \"N\" \"N\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\"\n [55] \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\"\n [73] \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"N\" \"N\"\n [91] \"N\" \"N\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\"\n[109] \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"Y\" \"Y\" \"Y\" \"N\"\n[127] \"N\" \"Y\" \"Y\" \"Y\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\"\n[145] \"Y\" \"Y\" \"N\" \"N\" \"Y\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\"\n[163] \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\"\n[181] \"Y\" \"N\" \"N\" \"N\" \"N\" \"N\" \"N\" \"Y\" \"Y\" \"Y\" \"Y\" \"Y\" \"Y\" \"N\" \"Y\" \"Y\" \"N\" \"N\"\n[199] \"Y\" \"N\"\n\n\nNext, we write a function to get the longest run of Ys in the throw sequence. Here we use a convenient function called rle( ) which is short for run length encoding, which turns our sequence of throws into sequences of runs (e.g. YNNNNYYNNNY becomes something like “1 Y, 4 Ns, 2 Ys, 3 Ns, and 1 Y”). We can then simply take the longest of the Y runs.\n\nlongestRun = function(x, target = 'Y'){\n    max(0, with(rle(x), lengths[values==target]))\n}\n\nlongestRun(throws)\n\n[1] 6\n\n\nNow, we randomly shuffle the sequence of throws many times and see what the longest Y runs look like for these shuffled sequences.\n\n# set number of reps to use\nMCN = 10000\n\n# create vector to save results in\nmc.runs = rep(0,MCN)\n\n# for each rep, randomize sequence and find longest run of Y\nfor(i in 1:MCN){\n    mc.runs[i] = longestRun(sample(throws))\n}\n\n\noptions(max.print=50)\nmc.runs\n\n [1] 3 3 4 4 3 4 4 4 5 4 4 5 4 4 4 4 4 4 6 7 5 6 3 6 4 5 7 4 4 4 4 5 3 5 5 5 5 4\n[39] 3 5 4 5 3 3 4 7 4 4 4 5\n [ reached getOption(\"max.print\") -- omitted 9950 entries ]\n\n\n\nbarplot(table(mc.runs))\n\n\n\n\nFor a two tailed test - if you would reject the null hypothesis if the test statistic lived in either the upper or the lower tail of the distribution, a general rule to use is to take the proportion &gt;= observed and &lt;= observed, whichever one is smaller, double that.\n\nmean(mc.runs &gt;= 6) * 2 #because it's a 2 tailed test\n\n[1] 0.2164\n\n\nCompared to other shuffled sequences, our run length doesn’t seem that unlikely. Therefore, this method seems inconclusive.\nCan we find an even better “statistic” to use?\n\n\n17.5.2 Attempt 2: running odds ratio\nConsider every pair of consecutive throws and make a table of the outcomes. For example, the first 8 throws in the sequence are YNNNNYYN. Breaking this into consecutive pairs, we have YN, NN, NN, NN, NY, YY, YN. This gives the table:\n\n\n\n\n\nNN\nNY\nYN\nYY\n\n\n\n\n3\n1\n2\n1\n\n\n\n\n\nSuppose we do this for the entire sequence of 200 throws (note this gives you 199 pairs). If we divide the number of NY by the number of NN, we get an estimate for how much more likely he is to make the next shot assuming he missed his last shot.\nSimilarly, we can divide the number of YY by the number of YN to get an estimate for how much more likely he is to make the next shot assuming he scored his last shot.\nNow, note that if the “hot hands” effect really exists in the data, then YY/YN should be larger than NY/NN in a large enough sample. We use this fact to define the following quantity:\n\\[\nR=\\frac{(\\text{# of YY})/(\\text{# of YN})}{(\\text{# of NY})/(\\text{# of NN})}\n\\]\nThe ratio \\(R\\) represents, in some sense, how much more likely the player is to make the next shot if he made the previous shot vs if he didn’t make the previous shot (note the vs). This is exactly what we’re trying to investigate!\nIf there is a “hot hands” effect, the numerator should be greater than the denominator and we should have \\(R&gt;1\\). If the throws are independent and do not affect each other then in theory we should have \\(R=1\\). If the player is actually a choker (i.e. he is more likely to miss after a successful shot), then we should have \\(R&lt;1\\). (Side note: this is basically an odds ratio).\nNow, we can use the same general method as the first attempt. If we assume his throws are independent and his shot probability doesn’t change significantly during the experiment, then we can randomly shuffle his throws and no properties should change. So let’s do that!\nFirst, I wrote a function to split the sequence of throws into consecutive pairs and then tabulates them.\n\n# define function for tabulating consecutive pairs\ntableOfPairs = function(x) {n=length(x);Rfast::Table(paste(x[1:(n-1)],x[2:n],sep=\"\"))}\n\n# test function for correct output\ntableOfPairs(strsplit(\"YNNNNYYN\",split=\"\")[[1]])\n\nNN NY YN YY \n 3  1  2  1 \n\n\n\n# run function on original sequence of throws\ntableOfPairs(throws)\n\n NN  NY  YN  YY \n102  34  35  28 \n\n\nNext, I wrote a function that takes the above table as an input and returns the ratio R as defined above.\n\nratioFromTable = function(tb) setNames((tb[\"YY\"]/tb[\"YN\"])/(tb[\"NY\"]/tb[\"NN\"]),\"R\")\n\n# run on our data\nratioFromTable(tableOfPairs(throws))\n\n  R \n2.4 \n\n\n\n# we can check this is correct by manually computing it\n(28/35)/(34/102)\n\n[1] 2.4\n\n\nNow we just need to shuffle the sequence and see what this ratio looks like for other sequences.\n\n# set number of reps to use\nN = 10000\n\n# create another vector to save results in\nmc.ratios = rep(NA,N)\n\n# for each rep, randomize sequence and find ratio R\n#for(i in 1:N){\n#    mc.ratios[i] = ratioFromTable(tableOfPairs(sample(throws)))\n#}\n\n# alternatively, use replicate\nmc.ratios = replicate(N,ratioFromTable(tableOfPairs(sample(throws))))\n\n\noptions(max.print=50)\nround(mc.ratios,2)\n\n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n1.04 1.53 0.97 0.51 0.90 1.01 0.39 0.86 0.90 0.97 1.04 1.24 0.84 0.84 0.64 0.54 \n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n1.59 0.51 1.12 1.95 0.57 1.38 0.67 0.57 1.59 0.93 1.43 1.08 0.72 0.72 0.77 1.01 \n   R    R    R    R    R    R    R    R    R    R    R    R    R    R    R    R \n0.90 0.86 0.40 0.81 0.64 0.90 0.67 1.16 1.38 1.24 0.25 0.67 0.81 0.84 1.65 0.81 \n   R    R \n0.48 0.90 \n [ reached getOption(\"max.print\") -- omitted 9950 entries ]\n\n\n\nhist(mc.ratios, breaks=20)\n\n\n\n\n\nmean(mc.ratios&gt;=ratioFromTable(tableOfPairs(throws)))\n\n[1] 0.002\n\n\nNow we can see our original ratio of \\(R=2.4\\) seems extremely unlikely! In particular, most of the shuffled statistics are centered around 1 (which is what we expect, since we established \\(R=1\\) for independent sequences).\nThis method (which is a little more refined than the simpler run length method) appears to show that our original sequence isn’t well explained by the throws being independent. Since \\(R=2.4\\gg1\\) and this result appears unlikely to happen under independence, we may conclude the player does actually have hot hands."
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#permutation-test-drug-trials-extended",
    "href": "R05_testing_Additional_Examples.html#permutation-test-drug-trials-extended",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.6 Permutation test: Drug Trials (Extended)",
    "text": "17.6 Permutation test: Drug Trials (Extended)\nApologies- this part of the document is a little messy, I have to remove duplicate work!\nThe results from a medical trial; larger numbers are “better” We want to know if there is evidence that the drug (treatment group) performs on average better than control.\n\ncontrol &lt;- c(-0.10, -0.55, 1.24, -0.97, -0.76,  0.21,-0.27, -1.02, 0.58, 1.67, -1.07, 0.17, 1.45, 0.34, 1.15, 0.18, -0.97, 0.43, -1.39, -2.76 );\n\ntreatment &lt;- c( 0.54, 0.36, 0.59, -0.57, 0.53, -0.78, -0.44, -0.98, 1.31, 0.50, 0.57, 0.49, -0.96, 2.41, 0.85, 1.93, 0.95, 1.45, 1.61, -1.16 );\n\nboxplot(treatment, control)\n\n\n\n\nThink about the logic of how a monte carlo test would work. We need to come up with a null model corresponding with the null hypothesis - but we really should be clear about the hypothesis that we’re testing.\nClaim (alt hypothesis) - the drug works. Null hypothesis - the drug does not work.\nThe null model implies - these 40 values just happened to be partitioned randomly into control group and treatment group.\nSo in a Monte Carlo simulation, we can take these 40 values, shuffle them around, and re-partition them into “control” and “Treatment”\nThe second important decision we have to make: on each simulation of the data we need to calculate some statistic or number - some function of the data - that captures the… unusualness of it somehow. Something that would lend evidence towards the alt. hypothesis.\nThe alt hypothesis (“the drug works”) - evidence in support of this would be: the mean for the treatment group &gt; the mean for the control group.\nOne idea: Our test statistic could be “TRUE/FALSE is the mean_Trt &gt; mean_Cntrl”\n\n#Create a test_statistic function\ntestStat &lt;- function(dataA, dataB){\n\n  #true if mean dataA &gt; mean dataB\n  #return(mean(dataA) &gt; mean(dataB)) \n  #Oops that doesn't work well, because it's only TRUE or FALSE\n  \n  return(mean(dataA)-mean(dataB))\n}\n\nNow let’s get the test statistic for the observed data:\n\ntestStat(treatment, control)\n\n[1] 0.582\n\n#MC simulation\nNMC &lt;- 10000\nresults &lt;- FALSE\nfor(i in 1:NMC){\n  #We need to re-partition the data into two groups.\n  combinedData &lt;- c(treatment, control)\n  #we next get 'simulated' treatment and control data\n  shuffledData&lt;- sample(combinedData)\n  treatmentSim &lt;- shuffledData[1:length(treatment)]\n  controlSim &lt;- shuffledData[-(1:length(treatment))]\n  results[i] &lt;- testStat(treatmentSim, controlSim)\n}\nhist(results)\n\n\n\n#Recall the alternative hypothesis:\n# The drug works\n# We only would consider evidence of this if the meanT-meanC &gt;0\n# So we only are going to calculate a p-value from the upper-tail\n# of the distribution.\n\nmean(results &gt;= testStat(treatment, control))\n\n[1] 0.0442\n\n\nIt might be interesting to check what a parametric T test would give us.\n\nt.test(treatment, control, var.equal=TRUE, alternative=\"greater\")\n\n\n    Two Sample t-test\n\ndata:  treatment and control\nt = 1.7437, df = 38, p-value = 0.04465\nalternative hypothesis: true difference in means is greater than 0\n95 percent confidence interval:\n 0.01928295        Inf\nsample estimates:\nmean of x mean of y \n    0.460    -0.122 \n\n\nSome other test statistics we could possibly use:\n\ndifference of the medians\ndifference of the max values\n\n\n#Create a test_statistic function\ntestStat &lt;- function(dataA, dataB){\n  return(max(dataA)-max(dataB))\n}\n\npermute_and_compute &lt;- function( treatment, control ){\n    combinedData &lt;- c(treatment, control)\n    shuffledData&lt;- sample(combinedData)\n    treatmentSim &lt;- shuffledData[1:length(treatment)]\n    controlSim &lt;- shuffledData[-(1:length(treatment))]\n    return(testStat(treatmentSim, controlSim))\n}\n\nNMC &lt;- 10000\nresults &lt;- FALSE\nfor(i in 1:NMC){\n  results[i] &lt;- permute_and_compute(treatment, control)\n}\n\nhist(results, breaks=50)\n\n\n\nmean(results &gt;= testStat(treatment, control))\n\n[1] 0.249\n\n\nWhat if we use a different test statistic? Let’s compare the medians.\n\ntestStat &lt;- function(A,B){\n  return(median(A)-median(B))\n}\n\n#rerun comparing the sample medians\ntest_statistics &lt;- 0\n\n(Tobsd &lt;- testStat(treatment, control))\n\n[1] 0.5\n\nfor(i in 1:NMC ) {\n  test_statistics[i] &lt;- permute_and_compute( treatment, control );\n}\nhist( test_statistics, breaks=60 )\nabline( v=Tobsd, lw=3, col='red' )\n\n\n\nsum(test_statistics &gt;= Tobsd)/NMC\n\n[1] 0.0949\n\n\nLet’s use KS test statistics - the max deviance between their empirical distributions!=\nwhat is that?\n\ncompare &lt;- function(x, y) {\n  n &lt;- length(x); m &lt;- length(y)\n  w &lt;- c(x, y)\n  o &lt;- order(w)\n  z &lt;- cumsum(ifelse(o &lt;= n, m, -n))\n  i &lt;- which.max(abs(z))\n  w[o[i]]\n}\nu&lt;-compare(treatment, control) #this is where the max difference occurs\nabs(mean(treatment &lt; u) - mean(control &lt; u))\n\n[1] 0.35\n\n\n\necdf1 &lt;- ecdf(treatment)\necdf2 &lt;- ecdf(control)\nplot(ecdf1, verticals=TRUE, do.points=FALSE, col=\"blue\", main=\"Compare ecdfs\")\nplot(ecdf2, verticals=TRUE, do.points=FALSE, add=TRUE, col='red')\nlines(c(u,u), c(ecdf1(u), ecdf2(u)), lwd=2)\nlegend(x=min(control,treatment), y=1, legend=c(\"treatment\",\"control\"), col=c(\"blue\",\"red\"), lty=1, xjust=-1)\n\n\n\n\nWe can pull this out from the built-in ks.test function. The test statistic is the max deviance between the two empirical CDFs\n\ntestStat &lt;- function(A,B){\n  return(as.numeric(ks.test(A,B)[1]))\n}\ntestStat(treatment, control)\n\n[1] 0.4\n\n\n\n#rerun\n\n(Tobsd &lt;- testStat(treatment, control))\n\n[1] 0.4\n\ntest_statistics &lt;- rep( 0, NMC ); # Vector to store our \"fake\" test statistics\nfor(i in 1:NMC ) {\n  test_statistics[i] &lt;- permute_and_compute( treatment, control );\n}\nhist( test_statistics )\nabline( v=Tobsd, lw=3, col='red' )\n\n\n\nsum(test_statistics &gt;= Tobsd)/NMC\n\n[1] 0.0821\n\n\nWe can spend all our time picking different test statistics, and maybe we find one that gives us a low p-value for this data set. But we have to be careful that it would generalize well to other datasets. Furthermore, we want to make sure that it would not be giving us false positives if we were comparing two populations that really did have the same distribution\nLet’s demonstrate this comparing our 3 test statistics:\n\ndifference of means\ndifference of maximums\ndifference of medians\nmax ecdf deviation\n\nAnd we can compare them over datasets that are generated from increasingly different populations.\n\ngenerate_experiment &lt;- function(nA, nB, effectSize){\n  #we will assume that the values in our data are drawn from a distribution - \n  #here we'll use a normal distribution. We could use permutation methods but \n  #that doesn't really change the point we're trying to make.\n  return(list(rnorm(nA, mean=effectSize, sd=1),\n              rnorm(nB, mean=0, sd=1)))\n}\n#for example\ngenerate_experiment(20,20,.5)\n\n[[1]]\n [1]  1.14210423 -0.03205957  0.82557421 -0.22970126  0.69222803 -0.04971073\n [7]  0.12339363 -0.33214334  1.61048497  0.94078033 -0.22787996 -1.04021845\n[13] -0.32305908  1.38665797 -0.44129337  2.51240507  1.27427885  2.38525221\n[19]  0.22467590  0.21917225\n\n[[2]]\n [1] -0.94739967 -2.49238258  1.76135946 -0.88882364  0.59388380  0.07096781\n [7]  1.40419070  0.57009621  0.19407728 -0.54706896  0.75325827 -0.04731243\n[13] -0.71945015 -1.56398442 -2.68816161  1.73276146  0.72895625  0.66229371\n[19]  0.05474118 -1.58360254\n\n\n\nnDatasets &lt;- 200\nNMC &lt;- 200\nset.seed(1)\neffects &lt;- seq(0,1,.1)\nrr.mean &lt;- 0; rr.max &lt;- 0; rr.median &lt;-0; rr.ks &lt;- 0\nfor(k in 1:length(effects)){\n  effect &lt;- effects[k]\n  rej.mean &lt;- rep(FALSE,nDatasets)\n  rej.max &lt;- rep(FALSE,nDatasets)\n  rej.median &lt;- rep(FALSE,nDatasets)\n  rej.ks &lt;- rep(FALSE,nDatasets)\n  #generate 100 different datasets\n  for(j in 1:nDatasets){\n    exp.data &lt;- generate_experiment(20,20,effect)\n    \n    obs.mean &lt;- mean(exp.data[[1]])-mean(exp.data[[2]])\n    obs.median &lt;- median(exp.data[[1]])-median(exp.data[[2]])\n    obs.max &lt;- max(exp.data[[1]])-max(exp.data[[2]])\n    obs.ks &lt;- as.numeric(ks.test(exp.data[[1]],exp.data[[2]])[1])\n    \n    #permute and compute 1000 times\n    t.means &lt;- 0\n    t.medians &lt;- 0\n    t.maxes &lt;- 0\n    t.kss &lt;- 0\n    for(i in 1:NMC){\n      shuffledData &lt;- sample(c(exp.data[[1]],exp.data[[2]]))\n      simA &lt;- shuffledData[1:length(exp.data[[1]])]\n      simB &lt;- shuffledData[-(1:length(exp.data[[1]]))]\n      \n      t.means[i] &lt;- mean(simA)-mean(simB)\n      t.medians[i] &lt;- median(simA)-median(simB)\n      t.maxes[i] &lt;- max(simA)-max(simB)\n      t.kss[i] &lt;- as.numeric(ks.test(simA,simB)[1])\n    }    \n    rej.mean[j] &lt;- mean(t.means &gt;= obs.mean) &lt;=0.05\n    rej.median[j] &lt;- mean(t.medians &gt;= obs.median) &lt;=0.05\n    rej.max[j] &lt;- mean(t.maxes &gt;= obs.max) &lt;=0.05\n    rej.ks[j] &lt;- mean(t.kss &gt;= obs.ks) &lt;=0.05\n  }\n  rr.mean[k] &lt;- mean(rej.mean)\n  rr.max[k] &lt;- mean(rej.max)\n  rr.median[k] &lt;- mean(rej.median)\n  rr.ks[k] &lt;- mean(rej.ks)\n}\n\nplot(x=effects, y=rr.mean, type=\"l\", ylim=c(0,1), main=\"Rejection rate of different test statistics\", xlab=\"effect size\", ylab=\"rejection rate\")\nlines(x=effects, y=rr.max, col=\"red\")\nlines(x=effects, y=rr.median, col=\"blue\")\nlines(x=effects, y=rr.ks, col=\"darkgreen\")\nabline(h=.05)\nlegend(x=0, y=1, legend=c(\"means\",\"medians\",\"deviance\",\"max\"), col=c(\"black\",\"blue\",\"darkgreen\",\"red\"), lty=1)"
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#random-number-string",
    "href": "R05_testing_Additional_Examples.html#random-number-string",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.7 Random number string",
    "text": "17.7 Random number string\nI want is the numbers 1 through 20 shuffled up. I ask both a computer and a person to provide me with this number string. Suppose I get the string, but I don’t know whether it came from the computer or the person.\nfrom 2/20/2025 afternoon lecture 4 7 5 18 11 12 14 2 3 17 10 1 9 16 20 8 13 19 6 15\n\nobs.Seq &lt;- c(4,7,5,18,11,12,14,2,3,17,10,1,9,16,20,8,13,19,6,15)\n\n\\(H_0:\\) data generated truly randomly \\(H_A:\\) data generated by humans, not randomly, but attempted\nLet’s use The number of values that are not in their original place\n\ncount_moved &lt;- function(x) {\n  n &lt;- length(x)\n  sum((1:n) == x)\n}\n\nt.sim &lt;- replicate(10000, count_moved(sample(20)))\nhist(t.sim)\n\n\n\nmean(t.sim &gt;= count_moved(obs.Seq))\n\n[1] 1\n\n\n19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8\n(generated 9/26: 1 6 18 12 15 19 20 5 2 17 11 3 9 16 4 7 8 10 13 14)\nHow can I use a permutation test to determine the likely source of this string of numbers???? Said more precisely, is there evidence that sequence came from a human (not a true randomizer). The idea here is that humans are actually not great at mentally randomizing. We put patterns into things subconsciously.\n\nnumbers.obs &lt;- c(19, 7, 5, 4, 13, 11, 2, 12, 3, 1, 15, 10, 18, 16, 9, 17, 20, 14, 6, 8)\n#We need a way to generate a random shuffling of the numbers\n\ngenerate_numbers &lt;- function(n.numbers){\n  return(sample(n.numbers))\n}\n\nWe also need some statistic or measurement of our data which indicates the weirdness of it. Let’s think about what wouldn’t be random.\n1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20\nAn increasing sequence of integers is very much not random. We can quantify this by counting how many differences are positive. In a random shuffling we would have some decreases and some increases. So let’s start with that statistic: The number of positive differences.\n\n17.7.1 Mean Absolute deviation\nWe can look at the distance of each digit from where it “started” from. Rather than just averaging all of their deviations (which would have positive deviations cancelling out with negative deviations) we can look at absolute deviations (absolute values).\n\nmean_abs_dev &lt;- function(x){\n  n &lt;- length(x)\n  return (mean(abs(x - 1:n)))\n}\n\nt.obs &lt;- mean_abs_dev(obs.Seq)\n\nset.seed(1)\nmean.abs.dev.sim &lt;- replicate(10000, mean_abs_dev(generate_numbers(20)))\nhist(mean.abs.dev.sim)\nabline(v=t.obs)\n\n\n\n#2 tail p-value\n2 * min(mean(mean.abs.dev.sim &lt;= t.obs), mean(mean.abs.dev.sim &gt;= t.obs))\n\n[1] 0.4122\n\n\nA two-tailed \\(p\\)-value of .354 is unconvincing; we have no evidence that this sequence of numbers wasn’t truly “randomly generated.”\n\n\n17.7.2 Number of positive differences\nIn an ordered sequence the difference between every pair of consecitive digits is +1 or -1. We could count how many (out of n-1) of the differences are positive. If the number of positive differences is too high OR too low we would suspect non-randomness; this is a two-tailed test.\n\ncount_pos_diff &lt;- function(numbers){\n return(sum(diff(numbers)&gt;0 ))\n}\n\ncount_pos_diff(numbers.obs)\n\n[1] 7\n\nNMC &lt;- 10000\nset.seed(1)\nt.posdiff &lt;- 0\nfor(i in 1:NMC){\n  t.posdiff[i] &lt;- count_pos_diff(generate_numbers(20))\n}\nhist(t.posdiff)\nabline(v=count_pos_diff(numbers.obs))\n\n\n\n(p.val &lt;- 2*min(\n  mean(t.posdiff &lt;= count_pos_diff(numbers.obs)),\n  mean(t.posdiff &gt;= count_pos_diff(numbers.obs))\n))\n\n[1] 0.1278\n\n\nDrawback of this distribution we see is that it is discrete. I’m not a big fan of discrete distributions because they can be sensitive to just a single data value, and the p-value you get increases in big jumps as you move through the distribution.\nIf looking only at number of positive differences we don’t see an unusually large (or unusually small) number of positive difference. No evidence of randomness. However look at this dataset:\n1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20\n\ncount_pos_diff(c(1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20))\n\n[1] 10\n\nplot(c(1, 3, 2, 5, 4, 7, 6, 9, 8, 11, 10, 13, 12, 15, 14, 17, 16, 19, 18, 20))\n\n\n\n\nIt’s very close to the middle of the distribution, but it’s a clear pattern. So the SIZE of the differences is clearly meaningful. We would want the differences to have the right amount of variation. If we take the differences of each consecutive pair of numbers, we can look at the standard deviation of those differences. That might be a good statistic to use.\n\n\n17.7.3 Standard Deviation of Differences\nAgain note that this is a two-tailed test, because if the variation is too high or too low that would be evidence of non-randomness.\n\ncalc_sd_diff &lt;- function(numbers){\n return(sd(diff(numbers) ))\n}\n\nNMC &lt;- 10000\nset.seed(1)\nt.sddiff &lt;- 0\nfor(i in 1:NMC){\n  t.sddiff[i] &lt;- calc_sd_diff(generate_numbers(20))\n}\nhist(t.sddiff)\nabline(v=calc_sd_diff(numbers.obs))\n\n\n\n2*min(mean(t.sddiff &lt;= calc_sd_diff(numbers.obs)),\n      mean(t.sddiff &gt;= calc_sd_diff(numbers.obs)))\n\n[1] 0.2702\n\n\n\n\n17.7.4 Mean absolute difference\nAnother statistic we could look at is related to the absolute values of the differences - the average absolute difference. Again- this is a two-tailed test. Too high or too low would be evidence of nonrandomness.\n\ncalc_mean_abs_diff &lt;- function(numbers){\n return(mean(abs(diff(numbers))))\n}\n\nNMC &lt;- 10000\nt.meanabsdiff &lt;- 0\nfor(i in 1:NMC){\n  t.meanabsdiff[i] &lt;- calc_mean_abs_diff(generate_numbers(20))\n}\nhist(t.meanabsdiff, breaks = 50)\nabline(v=calc_mean_abs_diff(numbers.obs))\n\n\n\n2*min(mean(t.meanabsdiff &lt;= calc_mean_abs_diff(obs.Seq)),\n      mean(t.meanabsdiff &gt;= calc_mean_abs_diff(obs.Seq)))\n\n[1] 0.9274\n\n\nNo evidence of nonrandomness! We could be pretty satisfied that the string of integers was properly shuffled by a computer.\nWhat about this one?\n2 9 3 13 18 4 17 10 1 15 7 5 12 20 6 19 8 16 14 11\nDo you think this was generated by computer or by human?\n\nnew.obs.data&lt;- c(2,9,3,13,18,4,17,10,1,15,7,5,12,20,6,19,8,16,14,11)\n\n#Mean absolute deviation\n2*min(mean(mean.abs.dev.sim &gt;= mean_abs_dev(new.obs.data)),\n    mean(mean.abs.dev.sim &lt;= mean_abs_dev(new.obs.data))) \n\n[1] 0.3034\n\n#Positive Differences\n2*min(mean(t.posdiff &gt;= count_pos_diff(new.obs.data)),\n    mean(t.posdiff &lt;= count_pos_diff(new.obs.data))) \n\n[1] 0.9904\n\n#SD Differences\n2*min(mean(t.sddiff &gt;= calc_sd_diff(new.obs.data)),\n    mean(t.sddiff &lt;= calc_sd_diff(new.obs.data))) \n\n[1] 0.2834\n\n#Mean Abs Differences\n2*min(mean(t.meanabsdiff &gt;= calc_mean_abs_diff(new.obs.data)),\n    mean(t.meanabsdiff &lt;= calc_mean_abs_diff(new.obs.data))) \n\n[1] 0.104\n\n\n\n\n17.7.5 Comparing The Test Statistics\nI want to compare the power of the test statistics as the randomness of an array increases. In order to do this we would have to have a sense of an “effect size” and look at rejection rates as effect size increases - as the vector gets more and more shuffled.\nA function that randomly chooses one value in an array and moves it to the first index can be used to incrementaly shuffle an array, so we can have some that are very unrandom, and very random. There are other ways to create gradually more “randomized” lists, but this is the one I’ll use.\n\npopOnce &lt;- function(x){\n  #pops a random element to the top of the list\n  ridx &lt;- sample(length(x),1)\n  return(c(x[-ridx],x[ridx]))\n}\n\nThen the shuffleN function takes the numbers in a vector x and calls the popOnce function n times.\n\nshuffleN &lt;- function(x,n=1){\n  for(i in 1:n){\n    x &lt;- popOnce(x)\n  }\n  return(x)\n}\n\nmaxPops &lt;- 40\npops &lt;- 1:maxPops\nrr.madev &lt;- rep(0, maxPops)\nrr.pd &lt;- rep(0,maxPops)\nrr.sdd &lt;- rep(0,maxPops)\nrr.mad &lt;- rep(0,maxPops)\nNMC &lt;- 5000\n\nfor(k in pops){\n  for(i in 1:NMC){\n     shuffledData &lt;- shuffleN(1:20,k)\n        #Absolute Deviation\n        rr.madev[k] &lt;- rr.madev[k] + (2*min(mean(mean.abs.dev.sim &gt;= mean_abs_dev(shuffledData)),\n            mean(mean.abs.dev.sim &lt;= mean_abs_dev(shuffledData))) &lt;=0.05 )\n        #Positive Differences\n        rr.pd[k] &lt;- rr.pd[k] + (2*min(mean(t.posdiff &gt;= count_pos_diff(shuffledData)),\n            mean(t.posdiff &lt;= count_pos_diff(shuffledData))) &lt;=0.05 )\n        #SD Differences\n        rr.sdd[k] &lt;- rr.sdd[k] + (2*min(mean(t.sddiff &gt;= calc_sd_diff(shuffledData)),\n            mean(t.sddiff &lt;= calc_sd_diff(shuffledData))) &lt;= 0.05)\n        #Mean Abs Differences\n        rr.mad[k] &lt;- rr.mad[k] + (2*min(mean(t.meanabsdiff &gt;= calc_mean_abs_diff(shuffledData)),\n            mean(t.meanabsdiff &lt;= calc_mean_abs_diff(shuffledData))) &lt;=0.05) \n  }\n}\nplot(pops, rr.pd/NMC, type=\"l\", col=\"darkgreen\")\nlines(pops, rr.sdd/NMC, col=\"blue\")\nlines(pops, rr.mad/NMC, col=\"red\")\nlines(pops, rr.madev/NMC, col=\"orange\")\nlegend(col=c(\"darkgreen\",\"blue\",\"red\", \"orange\"), x=maxPops*.65, y=1, legend=c(\"#posDiff\",\"sdDiff\",\"meanAbsDiff\", \"meanAbsDev\"), lty=1)\nabline(h=0.05, lty=2)\n\n\n\n\nFrom this plot it seems that the number of positive differences in the sequence is the most powerful test statistic to detect a non-randomized list. BUt of course, this is only if “non-randomized” is measured in the way we’ve defined in terms of the “effect size”."
  },
  {
    "objectID": "R05_testing_Additional_Examples.html#random-visual-noise-detection",
    "href": "R05_testing_Additional_Examples.html#random-visual-noise-detection",
    "title": "17  Monte Carlo Testing: Additional Examples",
    "section": "17.8 Random Visual Noise detection",
    "text": "17.8 Random Visual Noise detection\nSuppose we see an image on a monitor and want to determine if it is just random noise or if there is a pattern. Maybe this is data from a transmission from deep space. Is it just random noise, or is it possibly a message from an intelligent alien species?\nFor simplicity let’s say it is black/white pixels in a 16x16 grid:\n\nobsData &lt;- matrix(\n           c(0,0,0,1,1,1,0,1,1,1,0,1,1,0,0,1,\n             1,1,0,1,0,1,0,0,0,0,1,1,0,1,0,1,\n             0,0,1,1,0,1,0,1,1,0,1,1,0,1,0,1,\n             0,0,1,0,1,1,0,1,1,1,0,0,0,1,0,1,\n             1,1,1,1,0,1,1,1,0,1,1,1,1,0,0,1,\n             1,0,0,0,0,1,1,1,1,1,0,0,1,1,1,0,\n             1,0,1,0,1,0,0,0,1,1,0,0,0,1,0,1,\n             0,1,1,0,1,0,0,0,1,1,1,1,0,1,1,1,\n             0,0,1,1,1,1,1,0,0,0,1,0,1,1,1,0,\n             0,0,1,1,0,0,0,1,0,1,1,0,1,1,1,1,\n             0,1,1,1,1,1,1,0,1,1,1,1,1,0,1,1,\n             1,0,0,1,1,0,0,1,0,0,1,0,0,1,0,0,\n             0,1,0,1,1,1,1,1,0,1,1,0,1,1,0,0,\n             1,0,1,0,1,0,0,0,0,1,0,1,1,1,1,1,\n             1,0,0,0,0,1,0,0,1,0,0,0,0,1,1,0,\n             0,1,1,1,0,1,1,0,1,1,0,1,0,1,0,1),\n           nrow=16,byrow=TRUE)\npar(pty=\"s\")\nrequire(raster)\n\nLoading required package: raster\n\n\nLoading required package: sp\n\nimage(obsData, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n\n\n\n\nWe want to devise a hypothesis test that will test for randomness. What can we use? Let’s look at some summary stats.\n\n17.8.1 Test Stat 1: variability in number of 1s per region\nIdea: Slice it into 4x4 chunks, and count the number of 1s, then calculate the standard deviation. If we have random noise data then we should have some variability in each of these 16 sub-squares.\n\nsd_chunk_1s &lt;- function(matrix){\n  #I will assume that the data is 16x16. This function is not going to\n  #generalize well to other sized inputs.\n  results &lt;- numeric(0)\n  for(i in seq(1,15, 4)){\n    for(j in seq(1,15, 4)){\n      chunk &lt;- matrix[i:(i+3), j:(j+3)]\n      results &lt;- rbind(results, sum(chunk))\n    }\n  }\n  return(sd(results))\n}\nsd_chunk_1s(obsData)\n\n[1] 1.181454\n\n\nWe can perform our permutation test, randomizing the grid and calculating the test statistic on each run.\n\n#Permutation test\ntest.stat.sim &lt;- 0\nfor(i in 1:1000){\n  test.stat.sim[i] &lt;- sd_chunk_1s(matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE))\n}\nhist(test.stat.sim, breaks=25)\nabline(v=sd_chunk_1s(obsData))\n\n\n\n#two tailed test, ie if sd too low or too high that is unusual\n2*min(mean(test.stat.sim &lt;= sd_chunk_1s(obsData)), \n      mean(test.stat.sim &gt;= sd_chunk_1s(obsData)))\n\n[1] 0.02\n\n\n\n\n17.8.2 Test Stat 2: Horizontal Symmetry\nWe can look at what proportion of pixels are the same as their mirror pixel. A fast way to calculate that is to compare the image with it’s flip, do a conditional comparison of values, and divide by 2.\n\ncalculate_horizontal_symmetry &lt;- function(matrix){\n  nCols &lt;- dim(matrix)[2]\n  matrix.flip &lt;- matrix[,nCols:1]\n  return(sum(matrix==matrix.flip)/2)\n}\n\nNMC &lt;- 10000\nresults &lt;- 0\nfor(i in 1:NMC){\n  shuffledMatrix &lt;- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n  results[i] &lt;- calculate_horizontal_symmetry(shuffledMatrix)\n}\nhist(results)\nabline(v=calculate_horizontal_symmetry(obsData))\n\n\n\ncalculate_horizontal_symmetry(obsData)\n\n[1] 63\n\n\n\n\n17.8.3 Test stat 3: Neighbor edges\nLook at vertical neighbors (and horizontal neighbors)\nA 00 edge will sum to 0 A 01 or 10 edge will sum to 1 A 11 edge will sum to 2\n\nvEdges &lt;- obsData[-1,]+obsData[-16,]\nhEdges &lt;- obsData[,-1]+obsData[,-16]\ntable(vEdges)\n\nvEdges\n  0   1   2 \n 46 121  73 \n\ntable(hEdges)\n\nhEdges\n  0   1   2 \n 43 126  71 \n\n\n\n#Add two tables together!\ntable(vEdges)+table(hEdges)\n\nvEdges\n  0   1   2 \n 89 247 144 \n\n\n\n#Let's let the BW edges be the test statistic to see how well it works:\ncount_bw_edges &lt;- function(matrix){\n  dims &lt;- dim(matrix)\n  vEdges &lt;- matrix[-1,]+matrix[-dim(matrix)[1],]\n  hEdges &lt;- matrix[,-1]+matrix[,-dim(matrix)[2]]\n  bwEdges &lt;- as.numeric((table(factor(vEdges, levels=0:2))+\n                           table(factor(hEdges, levels=0:2)))[2])\n  return(bwEdges)\n}\n\ncount_bw_edges(obsData)\n\n[1] 247\n\n\n\nNMC &lt;- 5000\nresults &lt;- 0\nfor(i in 1:NMC){\n  shuffledMatrix &lt;- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n  results[i] &lt;- count_bw_edges(shuffledMatrix)\n}\nhist(results)\nabline(v=count_bw_edges(obsData))\n\n\n\n\nThis test statistic doesn’t detect all kinds of non-randomness. Let’s see how well this test statistic does against some image that is definitely not random:\n\nobsData2 &lt;- matrix(rep(c(0,1),8*16),nrow=16, byrow=TRUE)\nimage(obsData2, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n\n\n\npar(pty=\"s\")\ncount_bw_edges(obsData2)\n\n[1] 240\n\n\nOr this one\n\nobsData3 &lt;- matrix(rep(c(rep(c(0,0,1,1),8),rep(c(1,1,0,0),8)),4), ncol=16, byrow=TRUE)\npar(pty=\"s\")\nimage(obsData3, useRaster=TRUE, axes=FALSE, col=c(\"black\",\"white\"))\n\n\n\ncount_bw_edges(obsData3)\n\n[1] 224\n\n\nLet’s just create a function to automate the MC simulation\n\ngenerateMCsim &lt;- function(data, NMC){\n  results &lt;- 0\n  for(i in 1:NMC){\n    shuffledMatrix &lt;- matrix(data=sample(data), nrow=dim(data)[1], byrow=TRUE)\n    results[i] &lt;- count_bw_edges(shuffledMatrix)\n  }\n  return(results)  \n}\n\nAnd now the Monte Carlo part.\n\nmcDist &lt;- generateMCsim(obsData2,5000)\nhist(mcDist)\n\n\n\ncount_bw_edges(obsData2)\n\n[1] 240\n\n\n\n\n17.8.4 Test Stat 4: Moran’s I\nWe define neighbors to be cardinally adjacent. Moran’s index is a statistic that measures how correlated neighboring cells are\n\\[I = \\frac{N}{W} \\frac{\\sum_{i=1}^N \\sum_{j=1}^N w_{ij}(x_i-\\bar{x})(x_j-\\bar{x})}{\\sum_{i=1}^N(x_i-\\bar{x}^2)}\\]\n\nmoranI &lt;- function(matrix){\n  N &lt;- prod(dim(matrix))\n  #The number of adjacencies is the product of the dimensions -1\n  W &lt;- prod(dim(matrix)-1)\n  xbar &lt;- mean(matrix)\n  nRow &lt;- dim(matrix)[1]\n  nCol &lt;- dim(matrix)[2]\n  I &lt;- 2*(sum((matrix-xbar)[-1,]*(matrix-xbar)[-nRow,]) +\n    sum((matrix-xbar)[,-1]*(matrix-xbar)[,-nCol]) )/\n    sum((matrix-xbar)^2) * N/W\n  return(I)\n}\n\nt.sim &lt;- replicate(NMC, moranI(matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)))\nt.obs &lt;- moranI(obsData2)\nhist(t.sim); abline(v=t.obs, col=\"red\");\n\n\n\n2*min( mean(t.sim&gt;=t.obs), mean(t.sim&lt;=t.obs))\n\n[1] 0.9392\n\n\n\n\n17.8.5 Test Stat 5: Number of clumps\nLet’s count the number of contiguous clumps\n\nlibrary(igraph)\nlibrary(raster)\n\nnClumps &lt;- function(matrix){\n   return(max(as.matrix(clump(raster(matrix), directions=4)), na.rm=TRUE))\n}\n\nnClumps(obsData) + nClumps(1-obsData)\n\n[1] 49\n\n\nDistribution of clump counts on data example 2\n\nNMC &lt;- 1000\nresults &lt;- 0\n  for(i in 1:NMC){\n    shuffledMatrix &lt;- matrix(data=sample(obsData2), nrow=dim(obsData2)[1], byrow=TRUE)\n    results[i] &lt;- nClumps(shuffledMatrix) + nClumps(1-shuffledMatrix)\n  }\nhist(results)\nabline(v=nClumps(obsData2)+nClumps(1-obsData2))\n\n\n\nnClumps(obsData2)+nClumps(1-obsData2)\n\n[1] 16\n\n\nHow does counting clumps work on the first dataset?\n\nNMC &lt;- 1000\nresults &lt;- 0\n  for(i in 1:NMC){\n    shuffledMatrix &lt;- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n    results[i] &lt;- nClumps(shuffledMatrix)+nClumps(1-shuffledMatrix)\n  }\nhist(results)\nabline(v=nClumps(obsData)+nClumps(1-obsData))\n\n\n\nnClumps(obsData)+nClumps(1-obsData)\n\n[1] 49\n\n\n\n\n17.8.6 Test Stat 6: Variation in clump sizes\nPerhaps the standard deviation of the clump sizes might be useful.\n\nsdClumpSize &lt;- function(matrix){\n  clumps1 &lt;- as.vector(table(as.matrix(clump(raster(matrix), directions=4))))\n  clumps0 &lt;- as.vector(table(as.matrix(clump(raster(1-matrix), directions=4))))\n  return(sd(c(clumps0,clumps1)))\n}\n\n\nNMC &lt;- 1000\nresults &lt;- 0\n  for(i in 1:NMC){\n    shuffledMatrix &lt;- matrix(data=sample(obsData), nrow=dim(obsData)[1], byrow=TRUE)\n    results[i] &lt;- sdClumpSize(shuffledMatrix)\n  }\nhist(results)\n\n\n\nsdClumpSize(obsData)\n\n[1] 10.27186"
  },
  {
    "objectID": "testing2_practice.html#an-exponential-test",
    "href": "testing2_practice.html#an-exponential-test",
    "title": "18  Testing and Power Practice",
    "section": "19.2 An Exponential Test",
    "text": "19.2 An Exponential Test\nConsider a population \\(X\\sim exponential(\\lambda)\\). The test \\(H_0: \\lambda = 5\\) vs \\(H_A: \\lambda &gt; 5\\)\nConsider a sample size of 15 from this population. Because \\(E(X)=\\frac{1}{\\lambda}\\), we can use \\(\\frac{1}{\\bar{X}}\\) as a test statistic.\n\nWould you consider large or small values of \\(\\bar{X}\\) to be evidence supporting the alternative?\nSuppose we want to test this claim with \\(\\alpha=0.05\\). Find a \\(\\bar{X}^*\\) for which we will reject \\(H_0\\) if \\(\\bar{X}\\) is more extreme (which tail are we testing???)\nTake \\(\\lambda_A\\) values from 5 to 25, by 1s. Using the rejection rule you chose in part b, calculate the power of the test if the true lambda is \\(\\lambda_A\\) and the sample size remains at 15.\nSuppose that \\(\\lambda = 6\\) not \\(5\\) as the null hypothesis asserts. What sample size \\(n\\) would you need to achieve a power of 80% to reject \\(H_0\\) while maintaining \\(\\alpha=0.05\\)? Note - you will need a different threshold for each sample size!\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nSince \\(\\bar{X}\\approx E(X)=\\frac{1}{\\lambda}\\), a small \\(\\bar{X}\\) would indicate a large \\(\\lambda\\). So using \\(\\bar{X}\\) as our test statistic, we’d consider this a left-tailed test.\nUnder the null hypothesis, we’d like to simulate a distribution of test statistics.\n\n\nset.seed(1)\nt.sim &lt;- replicate(100000, mean(rexp(15, rate=5)))\nquantile(t.sim, prob=.05)\n\n       5% \n0.1230636 \n\n\nSo if \\(\\bar{X}&lt;.12306\\) we could consider this strong evidence that \\(\\lambda&gt;5\\).\n\n\n\n\nlambdas &lt;- seq(5, 25, 1)\nrr &lt;- numeric(length(lambdas))\nfor(i in 1:length(lambdas)){\n  t.alt &lt;- replicate(10000, mean(rexp(15, rate=lambdas[i])))\n  rr[i] &lt;- mean(t.alt &lt; .1230636)  \n}\nplot(x=lambdas, y=rr, type=\"l\",ylab=\"null rejection rate\", xlab=\"lambda\", main=\"power curve varying lambda\", ylim=c(0,1))\nabline(h=.05, col=\"red\")\n\n\n\n\n\n\n\n\n\nWe can repeat the above, but let N vary instead. In order to not take too long, I’ll just check 10 sample sizes and estimate.\n\n\nns &lt;- seq(15, 300, length.out=20)\nrr &lt;- numeric(length(ns))\nfor(i in 1:length(ns)){\n  t.sim &lt;- replicate(5000, mean(rexp(ns[i], rate=5)))\n  t.alt &lt;- replicate(5000, mean(rexp(ns[i], rate=6)))\n  #continue to use the 5th percentile from the t.sim as our threshold\n  #this will vary with n.\n  rr[i] &lt;- mean(t.alt &lt; quantile(t.sim, 0.05))  \n}\nplot(x=ns, y=rr, type=\"l\",ylab=\"null rejection rate\", xlab=\"n\", main=\"power curve varying n\", ylim=c(0,1))\nabline(h=.05, col=\"red\")\nabline(h=.80, col=\"red\", lty=2)\n\n\n\n\n\n\n\n\nSeems to be a sample size around 190. We can repeat with a zoom in to get a more precise estimate:\n\nns &lt;- seq(180, 200, by=5)\nrr &lt;- numeric(length(ns))\nfor(i in 1:length(ns)){\n  t.sim &lt;- replicate(40000, mean(rexp(ns[i], rate=5)))\n  t.alt &lt;- replicate(40000, mean(rexp(ns[i], rate=6)))\n  #continue to use the 5th percentile from the t.sim as our threshold\n  #this will vary with n.\n  rr[i] &lt;- mean(t.alt &lt; quantile(t.sim, 0.05))  \n}\nplot(x=ns, y=rr, type=\"l\",ylab=\"null rejection rate\", xlab=\"n\", main=\"power curve varying n\", ylim=c(.75,.85))\nabline(h=.05, col=\"red\")\nabline(h=.80, col=\"red\", lty=2)\n\n\n\n\n\n\n\n\nIncreasing the number of replicates, and zooming in to sample sizes between 180 to 200, it looks like 190 is a pretty good estimate.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "testing2_practice.html#type-1-and-type-2-errors",
    "href": "testing2_practice.html#type-1-and-type-2-errors",
    "title": "18  Testing and Power Practice",
    "section": "19.3 Type 1 and Type 2 errors",
    "text": "19.3 Type 1 and Type 2 errors\nIn the following situations, describe the null and alternative hypothesis, and what would be a type 1 error or a type 2 error for each situation.\n\nWe are testing to see if a new medical procedure is better than the standard procedure.\nWe are testing to see if people can tell the difference between Pepsi and Coke.\nWe are trying to figure out if a painting in a gallery is a forgery.\nWe are testing a patient to see if they have a certain disease.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe null hypothesis would be that the new procedure is no better than the standard, the alternative would be that the new procedure is better. A type 1 error would be concluding that the new procedure is better when in fact it is not. A type 2 error would be failing to realize that the new procedure is an improvement when in fact it is.\nThe null hypothesis is that people cannot, on average, tell the difference. The alternative hypothesis is that people can tend to tell the difference. A type 1 error would be to conclude that people can tend to taste the difference even though they cannot. A type 2 error is failing to recognize people taste the difference when they do.\nThe null hypothesis is the painting is legit. The alternative hypothesis is that it is a forgery. A type 1 error is concluding it is a forgery when it is actually legit. A type 2 error is being fooled by the forgery!\nThe null hypothesis is that the patient does not have the disease, the alternative hypothesis is that they do have the disease. A type 1 error is a false positive (conclude they have the disease when they don’t) and a type 2 error is a false negative (not detecting the presence of the disease even though it is there).",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "testing2_practice.html#a-poisson-test",
    "href": "testing2_practice.html#a-poisson-test",
    "title": "18  Testing and Power Practice",
    "section": "19.4 A Poisson Test",
    "text": "19.4 A Poisson Test\nSuppose we have some data and we wish to determine if it comes from a Poisson distribution.\n\nmysteryData &lt;- c(7,8,6,7,9,8,8,6,9,8,7,7,9,8,10,\n                 5,8,7,7,7,8,9,6,8,9,9,9,7,9,7)\n\n\nTest \\(H_0: \\lambda=7\\) vs \\(H_A: \\lambda \\not=7\\), assuming the distribution is a Poisson in either case. What test statistic would be good? Perform a Monte Carlo test.\nNow test \\(X\\sim Poisson(7)\\) vs \\(X \\not\\sim Poisson(7)\\). Consider two test statistics: \\(\\bar{X}/S^2\\) and \\(\\bar{X}-S^2\\). Calculate a p-value for either.\nSuppose the alternative distribution is a binomial. In particular, recall that if \\(p\\) is small and \\(n\\) is large, a binomial looks awful similar to a Poisson distribution. If \\(\\lambda=7\\) that’s similar to \\(p=7/n\\) for some large \\(n\\). As \\(n\\) gets smaller the distribution looks less and less like a Poisson. Consider \\(n\\) from 100 to 10. Use Monte Carlo to plot a power curve of the two possible test statistics from part b, maintaining \\(\\alpha=0.05\\) for all \\(n\\)s. Is either more powerful?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe sample mean \\(\\bar{X}\\) is a reasonable test statistic to use since \\(E(X)=\\lambda\\) for a Poisson distribution.\n\n\nset.seed(1)\nt.sim &lt;- replicate(10000, \n                   mean(rpois(length(mysteryData), 7)))\n#2 sided test\n(xbar &lt;- mean(mysteryData))\n\n[1] 7.733333\n\n2* min(mean(t.sim &lt;= xbar), \n       mean(t.sim &gt;= xbar))\n\n[1] 0.1416\n\n\nWe estimate a \\(p\\)-value of .1416. This indicates our data would not be unusual if \\(\\lambda=7\\).\n\n\n\n\n#First we use Xbar/S^2\nset.seed(1)\ntestStat &lt;- function(poisData){\n  return (mean(poisData)/var(poisData))\n}\n\nt.sim &lt;- replicate(10000, \n                   testStat(rpois(length(mysteryData), 7)))\n#2 sided test\n(t.obs &lt;- testStat(mysteryData))\n\n[1] 5.625418\n\nhist(t.sim, breaks=40)\n\n\n\n\n\n\n\n2* min(mean(t.sim &lt;= t.obs), \n       mean(t.sim &gt;= t.obs))\n\n[1] 0\n\n\n\n#First we use Xbar/S^2\nset.seed(1)\ntestStat &lt;- function(poisData){\n  return (mean(poisData)-var(poisData))\n}\n\nt.sim &lt;- replicate(10000, \n                   testStat(rpois(length(mysteryData), 7)))\n#2 sided test\n(t.obs &lt;- testStat(mysteryData))\n\n[1] 6.358621\n\nhist(t.sim, breaks=40)\n\n\n\n\n\n\n\n2* min(mean(t.sim &lt;= t.obs), \n       mean(t.sim &gt;= t.obs))\n\n[1] 0\n\n\nThe conclusion from either of these tests indicate that we have overwhelming evidence that the data could not have come from \\(Poisson(7)\\). Coupled with the conclusion from part a, it’s reasonable to conclude the data unlikely came from a Poisson distribution at all.\n\n\n\n\nNs &lt;- seq(100, 10, by=-10)\nps &lt;- 7/Ns\nrr1 &lt;- numeric(length(ps))\nrr2 &lt;- numeric(length(ps))\n\ntestStat1 &lt;- function(poisData){return (mean(poisData)/var(poisData))}\ntestStat2 &lt;- function(poisData){return (mean(poisData)-var(poisData))}\n\nfor(i in 1:length(rr1)){\n  crits1 &lt;- quantile(replicate(10000, \n                      testStat1(rpois(length(mysteryData), 7))),\n                     c(.025, .975))\n  t.alt1 &lt;- replicate(10000,\n                      testStat1(rbinom(length(mysteryData),\n                                       Ns[i],\n                                       ps[i])))\n  rr1[i] &lt;- mean(t.alt1 &lt;= crits1[1] | t.alt1 &gt;= crits1[2])\n\n  crits2 &lt;- quantile(replicate(10000, \n                      testStat2(rpois(length(mysteryData), 7))),\n                     c(.025, .975))\n  t.alt2 &lt;- replicate(10000,\n                      testStat2(rbinom(length(mysteryData),\n                                       Ns[i],\n                                       ps[i])))\n  rr2[i] &lt;- mean(t.alt2 &lt;= crits2[1] | t.alt2 &gt;= crits2[2])\n  \n}\nplot(x=Ns, y=rr1, type=\"l\", col=\"red\", ylab=\"rejection rate\")\nlines(x=Ns, y=rr2, col=\"blue\")\n\n\n\n\n\n\n\n\nIt is very hard to see much difference. Neither test statistic seems to be more sensitive to deviations from “Poissonity” under this setting.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "R06_More_Estimation_Examples.html#trying-to-achieve-a-2-margin-of-error",
    "href": "R06_More_Estimation_Examples.html#trying-to-achieve-a-2-margin-of-error",
    "title": "21  Estimation Examples",
    "section": "21.1 Trying to achieve a 2% margin of error",
    "text": "21.1 Trying to achieve a 2% margin of error\nPlay around with \\(n\\) in the above code to find how large our sample size has to be so that \\(\\Pr[ |S-p| &lt; 0.02 ] \\approx 0.95\\).\n\n# This is the exact same setup except we're changing n from 200 to 400.\nN_MC &lt;- 2000; n &lt;- 400; p &lt;- 0.8; \n# Still using p=0.8 and 2000 Monte Carlo trials.\n\n# Note that we don't really have to create the data frame again. We\n# could, if we wanted, just overwrite it, but this is a good habit to\n# be in to make sure we don't accidentally \"reuse\" old data.\nmonte_carlo &lt;- data.frame( replicate = 1:N_MC,\n                           S = rep(NA, N_MC),\n                           S_good = rep(NA, N_MC));\nm &lt;- 0.02\nfor(i in 1:N_MC){\n  monte_carlo$S[i] &lt;- simulate_S( n, p );\n  monte_carlo$S_good[i] &lt;- check_if_S_is_good(monte_carlo$S[i], p, m)\n}\nsum( monte_carlo$S_good )/N_MC\n\n[1] 0.691\n\n\nBy changing the margin from 2% to 1% or up to 4% we are changing the precision of our estimate.\nWhether or not our estimate is within the margin (yes/no) can be seen as the accuracy of our estimate."
  },
  {
    "objectID": "R06_More_Estimation_Examples.html#estimate-of-lambda-for-an-exponential-distribution",
    "href": "R06_More_Estimation_Examples.html#estimate-of-lambda-for-an-exponential-distribution",
    "title": "20  Estimation Examples",
    "section": "20.2 Estimate of \\(\\lambda\\) for an exponential distribution",
    "text": "20.2 Estimate of \\(\\lambda\\) for an exponential distribution\nAs per the slides, we are using \\(1/\\bar{X}\\) as our estimate of \\(\\lambda\\) since this makes sense from a “method of moments” perspective. Is the estimate biased?\nWhen using estimator \\(\\hat{\\theta}\\) to estimate parameter \\(\\theta\\), bias is defined as \\(E(\\hat{\\theta})-\\theta\\)\n\nlambda = 5\nn &lt;- 200\nNMC &lt;- 10000 # how many samples I want to estimate lambda on\n\nestimates &lt;- replicate(NMC,\n                       1/mean(rexp(n=n, rate=lambda)))\n\n#I want to look at the BIAS of this estimator\n#Bias = E(estimator) - target\n\nmean(estimates)-lambda #this should be 0 if the estimator is unbiased\n\n[1] 0.02608835\n\n\nIn this case the bias is positive, and it decreases (gets closer to zero) as sample size increases (not as NMC increases. NMC just gives us a better estimate of the bias, n gives us a better estimate of lambda)\nThis property - bias goes to zero as sample size increases - is called being “asymptotically unbiased”. It is closely related to a concept called “consistency.”",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Estimation Examples</span>"
    ]
  },
  {
    "objectID": "R06_More_Estimation_Examples.html#experiments-with-estimator-bias",
    "href": "R06_More_Estimation_Examples.html#experiments-with-estimator-bias",
    "title": "20  Estimation Examples",
    "section": "20.3 Experiments with Estimator Bias",
    "text": "20.3 Experiments with Estimator Bias\nLet’s consider \\(S^2\\), the sample variance.\nLet’s consider a population that is normally distributed, say \\(N(100, 5^2)\\), so \\(\\sigma=5\\). Let’s look at samples of size 20.\n\nsample_vars &lt;- 0\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  mysample &lt;- rnorm(20, 100, 5)\n  sample_vars[i] &lt;- var(mysample)\n}\nhist(sample_vars)\nabline(v=mean(sample_vars), lwd=3)\nabline(v=5^2, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\nJust for kicks, let’s repeat with a Poisson population with parameter \\(\\lambda=5\\), still a sample size of \\(n=20\\). Refresher: if X follows a Poison(\\(\\lambda\\)), both EX and VarX = \\(\\lambda\\).\n\nsample_vars &lt;- 0\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  mysample &lt;- rpois(20, lambda=5)\n  sample_vars[i] &lt;- var(mysample)\n}\nhist(sample_vars)\nabline(v=mean(sample_vars), lwd=3)\nabline(v=5, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\nIt is a fact that \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\) (the population variance). This is true as long as the data is an independent sample, but the population distribution makes no difference.\nSince \\(E(S^2)=\\sigma^2\\) one might naturally think that the sample standard deviation is an unbiased estimator of the population standard deviation. Is it true that \\(E(S)=\\sigma\\)?\nWe can use the same simulation to look at the potential bias of the sample standard deviation (back to the normal distribution).\n\nNMC &lt;- 10000\nsample_sds &lt;- 0 #empty vector\nfor(i in 1:NMC){\n  mysample &lt;- rnorm(20, 100, 5)\n  sample_sds[i] &lt;- sd(mysample)\n}\nhist(sample_sds, main=\"Sampling distribution of Sample SD\")\nabline(v=mean(sample_sds), lwd=3)\nabline(v=5, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\nThis is no fluke. The sample standard deviation is not an unbiased estimator for the population standard deviation. The reason is because the expectation function does not handle square roots: \\(E(\\sqrt{X}) \\not= \\sqrt{E(X)}\\). For that reason: \\[E(S)=E(\\sqrt{S^2}) \\not= \\sqrt{E(S^2)} = \\sqrt{\\sigma^2}=\\sigma\\]\nThe bias may not be as bad as sample size increases. Let’s look at samples of size 5, 10, 15, …, 100\n\nns &lt;- seq(5, 100, 5)\nbiases &lt;- rep(0, length(ns))\nfor(i in 1:length(ns)){\nNMC &lt;- 10000\n  sample_sds &lt;- 0\n  for(j in 1:NMC){\n    mysample &lt;- rnorm(ns[i], 100, 5)\n    sample_sds[j] &lt;- sd(mysample)\n  }\n  biases[i] &lt;- mean(sample_sds) - 5\n}\nplot(x=ns, y=biases, type=\"l\", main=\"bias of s as n increases\", ylim=c(min(biases),0))\nabline(h=0, lty=2)",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Estimation Examples</span>"
    ]
  },
  {
    "objectID": "R06_More_Estimation_Examples.html#bias-of-sample-maximum",
    "href": "R06_More_Estimation_Examples.html#bias-of-sample-maximum",
    "title": "20  Estimation Examples",
    "section": "20.4 Bias of Sample Maximum",
    "text": "20.4 Bias of Sample Maximum\nLet’s look at the sample maximum as an estimate of the population maximum. Let’s sample data from a uniform(0, 1), and we’ll look at a sample of size 20.\n\nNMC &lt;- 10000\nsample_maxes &lt;- 0\nfor(i in 1:NMC){\n  mysample &lt;- runif(20, 0, 1)\n  sample_maxes[i] &lt;- max(mysample)\n}\nhist(sample_maxes, main=\"Sampling distribution of Sample Max\", breaks=50)\nabline(v=mean(sample_maxes), lwd=3)\nabline(v=1, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\nPerhaps unsurprisingly the sample maximum tends to under-estimates the population maximum.\nIt turns out that we can modify the estimator to correct for the bias. If we use \\(\\dfrac{n+1}{n}X_{(n)}\\) this gives us an unbiased estimator for population maximum (you can look this up if you don’t believe me). Let’s demonstrate:\n\nNMC &lt;- 10000\nn &lt;- 20\nsample_maxes_adj &lt;- 0\nfor(i in 1:NMC){\n  mysample &lt;- runif(n, 0, 1)\n  sample_maxes_adj[i] &lt;- (n+1)/(n)*max(mysample)\n}\nhist(sample_maxes_adj, main=\"Sampling distribution of adjusted Sample Max\", breaks=40)\nabline(v=mean(sample_maxes_adj), lwd=3)\nabline(v=1, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\n\n20.4.1 German Tank Example\nSuppose we have tanks produced each with a serial number from 1 up to N. We are able to capture and observe the serial numbers on a sample of say 5 tanks. Can we estimate the population size of tanks (get an estimate for N)???\nBy the same logic from above, one might guess that \\(\\frac{n+1}{n} \\max(X)\\) would be a good estimate of N. Let’s imagine that there are 100 tanks in the population.\n\nNMC &lt;- 10000\nN &lt;- 100\nn &lt;- 5\nsample_maxes &lt;- 0\nfor(i in 1:NMC){\n  mysample &lt;- sample(1:N, size=n, replace=FALSE)\n  sample_maxes[i] &lt;- (n+1)/(n)*max(mysample)\n}\nhist(sample_maxes, main=\"Sampling distribution of adjusted Sample Max\")\nabline(v=mean(sample_maxes), lwd=3)\nabline(v=N, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\nFor the discrete case we actually need to use \\(\\frac{n+1}{n}\\max(X) - 1\\) as our unbiased estimator. The reason is that the discrete uniform behaves slightly differently than the continuous - because the probability of observing the population max is &gt; 0.\n\nNMC &lt;- 10000\nN &lt;- 100\nn &lt;- 5\nsample_maxes &lt;- 0\nfor(i in 1:NMC){\n  mysample &lt;- sample(1:N, size=n, replace=FALSE)\n  sample_maxes[i] &lt;- (n+1)/(n)*max(mysample)-1\n}\nhist(sample_maxes, main=\"Sampling distribution of adjusted Sample Max\")\nabline(v=mean(sample_maxes), lwd=3)\nabline(v=N, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\n\n\n20.4.2 Capture - Recapture population estimation\nSuppose that a population is of size N. One day you capture and tag a random sample of \\(n_1\\) individuals and release them back into the wild. Several weeks later you capture a new batch of size \\(n_2\\) and \\(k\\) of the new sample are tagged. How can we estimate the population size from this?\nSuppose the population is of size 400 and we tag and release 25. Suppose our second sample is of size 20. How many would we expect to see in the second sample?\n\nN &lt;- 30000\nn1 &lt;- 450\nn2 &lt;- 500\npopulation &lt;- c(rep(1, n1), rep(0, N-n1))\n\nNMC &lt;- 10000\nrecaptured &lt;- replicate(NMC, \n                        sum(sample(population, n2)))\nbarplot(table(recaptured))\n\n\n\n\n\n\n\n\nWhat would be a good estimate of the population size?\nAfter the first tagging, we know that the population has exactly \\(n_1\\) tagged individuals. So assuming (1) that the population has been randomized and (2) that deaths are not consequential (either not many, or that both tagged/untagged were equally likely to have died) then the probability of a tagged fish being recaptured is \\(n_1/N\\). So the proportion of tagged fish in the second sample, which is \\(k/n_2\\) should be the same as \\(n_1/N\\).\n\\[\\frac{k}{n_2} = {n_1}{N}\\]\nUnder this logic, we could solve for \\(N\\) to find a point estimate\n\\[\\hat{N}=\\frac{n_1 n_2}{k}\\]\nFor example, if we found 7 in the second sample we’d calculate\n\n450*500/7\n\n[1] 32142.86\n\n\nIt seems to be an over-estimate. We can examine the bias by repeating the simulation Note - we have to throw out any recaptures where 0 are tagged - obviously infinity is not a useful estimate.\n\nestimates &lt;- n1*n2/recaptured\nhist(estimates, breaks=40)\nabline(v=mean(estimates[estimates&lt;Inf]), lwd=3)\nabline(v=N, col=\"red\", lty=2, lwd=3)\n\n\n\n\n\n\n\n\n\n\n20.4.3 Example: Estimating the rate parameter in the exponential distribution.\n\nrun_exprate_expt &lt;- function( n, rate ) {\n  data &lt;- rexp(n=n, rate=rate);\n  Xbar &lt;- mean( data )\n  return( 1/Xbar )\n}\n\n\nM &lt;- 1e4;\nreplicates &lt;- rep(NA, M);\nfor (i in 1:M ) {\n  replicates[i] &lt;- run_exprate_expt(2000, rate=5);\n}\nhist(replicates)\nabline( v=5, col='red', lwd=4 )\nabline(v=mean(replicates), col=\"black\", lwd=4, lty=2)\n\n\n\n\n\n\n\n\n\nmean(replicates)-5\n\n[1] 0.002511578\n\n\nHow does the bias change over increasing sample sizes? Does the bias go to zero? How does the standard deviation change?\n\nns &lt;- seq(25,1000, by=50)\nsd &lt;- vector(mode=\"numeric\")\nbias &lt;- vector(mode=\"numeric\")\n\nfor(k in 1:length(ns)){\n  M &lt;- 1e4;\n  replicates &lt;- rep(NA, M);\n  for (i in 1:M ) {\n    replicates[i] &lt;- run_exprate_expt(ns[k], rate=5);\n  }\n  sd[k] &lt;- sd(replicates)\n  bias[k] &lt;- mean(replicates)-5\n}\nplot(x=ns,y=bias, type=\"l\", ylim=c(0,bias[1]), main=\"Bias of 1/xbar as n increases\")\nabline(h=0)\n\n\n\n\n\n\n\nplot(x=ns,y=sd, type=\"l\", main=\"Standard error of 1/xbar as n increases\")\nlines(x=ns, y=sqrt(ns[1])*sd[1]/sqrt(ns), lty=2)",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Estimation Examples</span>"
    ]
  },
  {
    "objectID": "testing2_practice.html#elevated-homocysteine",
    "href": "testing2_practice.html#elevated-homocysteine",
    "title": "18  Testing and Power Practice",
    "section": "19.5 Elevated homocysteine",
    "text": "19.5 Elevated homocysteine\nNormal levels of homocysteine are low (e.g., 10-12 \\(\\mu\\)mol/L), while early kidney disease is associated with elevated, or hyperhomocysteinemia, levels because the kidneys are responsible for clearing homocysteine from the blood.\nSuppose that typical levels for a healthy individual are normally distributed with a mean of 11 \\(\\mu\\)mol/L and a standard deviation of 2. If early kidney disease is associated with levels that are normally distributed with a mean of \\(14\\) and a standard deviation of \\(3\\), let’s devise a hypothesis test based purely on a measure of homocysteine.\n\nSuppose that a single sample is drawn from a patient and the decision of the test is based on that. If this is the case, what is the threshold we’d use to determine there’s evidence of possible early kidney disease, if we aim to keep an alpha level of \\(0.05\\).\nWhat is the power of the test using the threshold above?\nIf we wanted the power to be 80%, what threshold would we have to use? What would the \\(\\alpha\\) be in this case?\nSuppose in a population (excluding individuals known to have kidney disease) that 3% of the population are at the early stage of kidney disease. What is the Positive Predictive Value and Negative Predictive Value of this test, using the thresholds you chose from part a.\nIf a person’s homocysteine level exceeds 15 then what is the likelihood they have Kidney disease?\nCreate a plot of positive predictive power for thresholds of 15 to 20. ::: {.callout-note collapse=“true” title=“Solution”}\nTo maintain a type 1 error rate of 5%, we’d need to find the upper 5th percentile from \\(N(11, 2^2)\\)\n\n\nqnorm(.05, 11, 2, lower.tail=F)\n\n[1] 14.28971\n\n\nSo if a patient’s homoscysteine level exceeds 14.28971 this would indicate a high level of homocysteine.\n\nThe power of the test is \\(Pr[X &gt; 14.28971 | H_A]\\)\n\n\npnorm(14.28971, 14, 3, lower.tail=F)\n\n[1] 0.4615339\n\n\nThe power of the test is 46.15339%\n\n\n\n\n(thresh.c &lt;- qnorm(.80, 14, 3, lower.tail=F))\n\n[1] 11.47514\n\npnorm(thresh.c, 11, 2, lower.tail=F)\n\n[1] 0.406108\n\n\nIf we wished to have a power of 80%, then we’d diagnose early kidney disease if homocysteine levels were above 11.47514, but the type 1 error rate would rise to 40.61%\n\n\n\n\nprob.pos.given.d &lt;- 0.4615339\nprob.pos.given.notd &lt;- 0.05\nprob.d &lt;- .03\n\n#PPV\nprob.d * prob.pos.given.d / \n  (prob.d*prob.pos.given.d + (1-prob.d)*prob.pos.given.notd)\n\n[1] 0.2220834\n\n#NPV\n(1-prob.d) * (1-prob.pos.given.notd) / \n  ((1-prob.d)*(1-prob.pos.given.notd) + (prob.d)*(1-prob.pos.given.d))\n\n[1] 0.9827719\n\n\n\nFor this I’ll simulate a population\n\n\nNMC &lt;- 100000\nstatus &lt;- sample(c(1,0),NMC, prob=c(.03,.97), replace = TRUE)\nhLevel &lt;- (1-status)*rnorm(NMC, 11, 2) + (status)*rnorm(NMC, 14,3)\n\nmean(status ==1 & hLevel &gt;15)/mean(hLevel &gt; 15)\n\n[1] 0.3444816\n\n\n\n\n\n\nthreshs &lt;- seq(15,20,by=.05)\nppv &lt;- numeric(length(threshs))\nfor(i in 1:length(threshs)){\n  ppv[i] &lt;- mean(status ==1 & hLevel &gt;threshs[i])/mean(hLevel &gt; threshs[i])\n}\nplot(x=threshs, y=ppv, type=\"l\", ylim=c(0,1))\n\n\n\n\n\n\n\n\n:::",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "estimation1_practice.html",
    "href": "estimation1_practice.html",
    "title": "21  Point Estimation Practice",
    "section": "",
    "text": "22 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "estimation1_practice.html#method-of-moments-i",
    "href": "estimation1_practice.html#method-of-moments-i",
    "title": "21  Point Estimation Practice",
    "section": "22.1 Method of Moments I",
    "text": "22.1 Method of Moments I\nUse the method of moments to construct an estimator for the parameter \\(N\\) when your data \\(X_1, X_2, \\ldots, X_n\\) each are drawn independently from \\(Binom(N, p)\\). In this case assume that \\(p\\) is known to you. Note the sample size you’re working with is not the same as the \\(N\\) parameter from the binomial distribution."
  },
  {
    "objectID": "estimation1_practice.html#method-of-moments-ii",
    "href": "estimation1_practice.html#method-of-moments-ii",
    "title": "21  Point Estimation Practice",
    "section": "22.2 Method of Moments II",
    "text": "22.2 Method of Moments II\nContinuing from the previous problem, suppose that your data \\(X_1, X_2, \\ldots, X_n\\) each are drawn independently from \\(Binom(N, p)\\) but both \\(N\\) and \\(p\\) are unknown. Use the method of moments to construct two simultaneous estimators \\(\\hat{N}\\) and \\(\\hat{p}\\). You can only use sample statistics in your estimator functions, you cannot use \\(N\\) or \\(p\\) because both of these parameters are unknown."
  },
  {
    "objectID": "estimation1_practice.html#method-of-moments-iii",
    "href": "estimation1_practice.html#method-of-moments-iii",
    "title": "21  Point Estimation Practice",
    "section": "22.3 Method of Moments III",
    "text": "22.3 Method of Moments III\nSuppose \\(X_1, \\ldots, X_n \\sim Unif(0, M)\\). Use the method of moments to construct an estimator \\(\\hat{M}\\)."
  },
  {
    "objectID": "estimation1_practice.html#method-of-moments-iv",
    "href": "estimation1_practice.html#method-of-moments-iv",
    "title": "21  Point Estimation Practice",
    "section": "22.4 Method of Moments IV",
    "text": "22.4 Method of Moments IV\nThe gamma distribution takes two parameters, the shape parameter \\(\\alpha&gt;0\\) and the scale parameter \\(\\theta&gt;0\\). For \\(X\\sim Gamma(\\alpha, \\theta)\\), we have two properties of the distribution: \\(EX=\\alpha\\theta\\) and \\(VarX=\\alpha\\theta^2\\). Suppose we have a random sample \\(X_1, \\ldots, X_n\\) from such a gamma distribution.\n\nIf you know \\(\\theta\\), use the method of moments to find an estimator \\(\\hat\\alpha\\).\nIf you do not know \\(\\theta\\), use the method of moments to find an estimator \\(\\hat\\alpha\\). Note this estimator will need to use both sample mean and sample variance in some way."
  },
  {
    "objectID": "estimation1_practice.html#method-of-moments-v",
    "href": "estimation1_practice.html#method-of-moments-v",
    "title": "21  Point Estimation Practice",
    "section": "22.5 Method of Moments V",
    "text": "22.5 Method of Moments V\nSuppose we observe a sequence of independent trials where each trial is a success or a failure. Let \\(X\\) denote the number of failures before the first success. This experiment is repeated \\(n\\) times. Use the method of moments to construct an estimator for \\(p\\), the probability of success."
  },
  {
    "objectID": "estimation1_practice.html#bias-sample-sd",
    "href": "estimation1_practice.html#bias-sample-sd",
    "title": "21  Point Estimation Practice",
    "section": "22.6 Bias Sample SD",
    "text": "22.6 Bias Sample SD\nSuppose we have a normally distributed population, and an independent sample of size \\(15\\) from this population. So our data \\(X_1,\\ldots,X_{15}\\sim N(10, \\sigma^2)\\). For parts (a) and (b) you can set \\(\\sigma\\) to be whatever value you want.\n\nUse Monte Carlo to demonstrate that \\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\).\nUse Monte Carlo to demonstrate that \\(S\\), the sample standard deviation, is a biased estimator of \\(\\sigma\\), the population variance."
  },
  {
    "objectID": "estimation1_practice.html#a-poisson-estimator",
    "href": "estimation1_practice.html#a-poisson-estimator",
    "title": "21  Point Estimation Practice",
    "section": "22.7 A Poisson Estimator",
    "text": "22.7 A Poisson Estimator\nYour random sample \\(X_1, \\ldots, X_20 \\sim Poisson(\\lambda)\\). For parts (a) and (b) let’s assume the true value of \\(\\lambda\\) is 5.\n\nUse Monte Carlo to demonstrate that \\(\\bar{X}\\) and \\(S^2\\) are both unbiased estimators of \\(\\lambda\\).\nWhich estimator \\(\\bar{X}\\) or \\(S^2\\) has a lower variance (i.e. is a more precise) estimator for \\(\\lambda\\)? Use Monte Carlo support your answer."
  },
  {
    "objectID": "estimation1_practice.html#the-tricky-sample-maximum",
    "href": "estimation1_practice.html#the-tricky-sample-maximum",
    "title": "21  Point Estimation Practice",
    "section": "22.8 The Tricky Sample Maximum",
    "text": "22.8 The Tricky Sample Maximum\nSuppose \\(X_1, \\ldots, X_n \\sim Unif(0, M)\\) (continuous). Consider two estimators for \\(M\\):\n\\[\\hat{M}_1 = \\max(X_1, \\ldots, X_n)\\] \\[\\hat{M}_2 = \\frac{n+1}{n}\\max(X_1, \\ldots, X_n)\\] Suppose that You have a sample size \\(n=25\\) and a true \\(M=100\\).\n\nUse Monte Carlo to estimate the bias of each of these estimators.\nUse Monte Carlo to estimate the variance of both of these estimators.\nThe Mean Squared Error is \\(E\\left[(\\hat\\theta-\\theta)^2\\right]\\). This can be approximated by Monte Carlo by estimating the parameter repeatedly, squaring the error and averaging. Use Monte Carlo to estimate the MSE of both \\(\\hat{M}_1\\) and \\(\\hat{M}_2\\)."
  },
  {
    "objectID": "estimation1_practice.html#tricky-sample-maximum-ii",
    "href": "estimation1_practice.html#tricky-sample-maximum-ii",
    "title": "21  Point Estimation Practice",
    "section": "22.9 Tricky Sample Maximum II",
    "text": "22.9 Tricky Sample Maximum II\nAnother couple estimators for \\(M\\) from the previous problem is \\(\\hat{M}_3=2\\bar{X}\\), twice the sample mean and \\(\\hat{M}_4=2\\tilde{X}\\), twice the sample median.\nSuppose that You have a sample size \\(n=25\\) and a true \\(M=100\\).\n\nUse Monte Carlo to estimate the bias and variance of each estimator.\nUse Monte Carlo to approximate the MSE of each estimator.\nBetween \\(\\hat{M}_1\\) through \\(\\hat{M}_4\\), which estimator would you prefer? What are some pros and cons of each of them?"
  },
  {
    "objectID": "estimation1_practice.html#exponential-data",
    "href": "estimation1_practice.html#exponential-data",
    "title": "21  Point Estimation Practice",
    "section": "22.10 Exponential Data",
    "text": "22.10 Exponential Data\nConsider the data below given by Wang (2000) on failure times of an electrical component. Assuming an exponential distribution \\(Exp(\\lambda)\\), find the method of moments estimate of \\(\\lambda\\)\n\nt &lt;- c(5, 11, 21, 31, 46, 75, 98, 122, 145, 165, 196, 224, 245, 293, 321, 330, 350, 420)"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#transcendant-constant-approximation",
    "href": "R03_MonteCarloExamples.html#transcendant-constant-approximation",
    "title": "10  Monte Carlo Examples",
    "section": "10.5 Transcendant Constant Approximation",
    "text": "10.5 Transcendant Constant Approximation\n\n10.5.1 Estimating Euler’s constant\nA fact - if you sample \\(X_1, X_2, \\ldots\\) from Unif(0,1), and let \\(Y\\) be the index for which the sum exceeds 1, the expected value of \\(Y\\) is \\(e\\).\n\nNMC &lt;- 100000\nNs &lt;- 0\nfor(i in 1:NMC){\n  Ns[i] = min(which(cumsum(runif(100))&gt;1))\n}\nmean(Ns)\n\n[1] 2.71519\n\n\n\n#Check the value\nexp(1)\n\n[1] 2.718282\n\n\n\n\n10.5.2 Estimating the value of pi\n\nNMC &lt;- 10000; # Number of Monte Carlo replicates\npar(pty = \"s\")\nplot(NA,NA, xlim=c(-1,1), ylim=c(-1,1))\n\nin_circ &lt;- 0; # Count how many points landed in the circle\n# For-loop over the MC replicates\nfor(i in 1:NMC){\n  \n  # for each point, generate (x,y) randomly between -1 and 1\n  point &lt;- runif(n=2, min=-1, max=1);\n  # to be inside circle, our point must satisfy xˆ2 + yˆ2 &lt;= 1\n  if(point[1]^2 + point[2]^2 &lt;= 1){\n  # if inside, add to count\n    in_circ &lt;- in_circ+1\n    points(point[1],point[2])\n  }\n}\n\n\n\n\n\n\n\n#To get proportion of square covered, take in_circ/N\nprop &lt;- in_circ/NMC\n# to get our estimate of pi, multiply by 4.\npi.mc &lt;- 4*prop\npi.mc\n\n[1] 3.1656",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R03_MonteCarloExamples.html#geometric-property-estimation",
    "href": "R03_MonteCarloExamples.html#geometric-property-estimation",
    "title": "10  Monte Carlo Examples",
    "section": "10.6 Geometric Property Estimation",
    "text": "10.6 Geometric Property Estimation\nIn a square with side lengths 1, if you pick 2 points uniformly at random, what is the average distance between them?\n\nNMC &lt;- 100000\ndistances &lt;- numeric(NMC)\n\nfor(i in 1:NMC){\n  x1 &lt;- runif(2, min=0, max=1)\n  x2 &lt;- runif(2, min=0, max=1)\n  distances[i] &lt;- sqrt((x1[1] - x2[1])^2 + \n                         (x1[2] - x2[2])^2)\n}\nmean(distances)\n\n[1] 0.5219016",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Monte Carlo Examples</span>"
    ]
  },
  {
    "objectID": "R06_More_Estimation_Examples.html",
    "href": "R06_More_Estimation_Examples.html",
    "title": "20  Estimation Examples",
    "section": "",
    "text": "20.1 Trying to achieve a 2% margin of error\nPlay around with \\(n\\) in the above code to find how large our sample size has to be so that \\(\\Pr[ |S-p| &lt; 0.02 ] \\approx 0.95\\).\n# This is the exact same setup except we're changing n from 200 to 400.\nN_MC &lt;- 2000; n &lt;- 400; p &lt;- 0.8; \n# Still using p=0.8 and 2000 Monte Carlo trials.\n\n# Note that we don't really have to create the data frame again. We\n# could, if we wanted, just overwrite it, but this is a good habit to\n# be in to make sure we don't accidentally \"reuse\" old data.\nmonte_carlo &lt;- data.frame( replicate = 1:N_MC,\n                           S = rep(NA, N_MC),\n                           S_good = rep(NA, N_MC));\nm &lt;- 0.02\nfor(i in 1:N_MC){\n  monte_carlo$S[i] &lt;- simulate_S( n, p );\n  monte_carlo$S_good[i] &lt;- check_if_S_is_good(monte_carlo$S[i], p, m)\n}\nsum( monte_carlo$S_good )/N_MC\n\n[1] 0.6855\nBy changing the margin from 2% to 1% or up to 4% we are changing the precision of our estimate.\nWhether or not our estimate is within the margin (yes/no) can be seen as the accuracy of our estimate.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Estimation Examples</span>"
    ]
  },
  {
    "objectID": "estimation1_practice.html#a-uniform-mystery",
    "href": "estimation1_practice.html#a-uniform-mystery",
    "title": "21  Point Estimation Practice",
    "section": "22.11 A uniform mystery",
    "text": "22.11 A uniform mystery\nSuppose \\(X_1, \\ldots, X_n \\sim Unif(a, b)\\), but both \\(a\\) and \\(b\\) are unknown.\n\nConstruct simultaneous method of moment estimators \\(\\hat{a}\\) and \\(\\hat{b}\\) from the facts that \\(E(X)=\\frac{b-a}{2}\\) and \\(Var(X)=\\frac{(b-a)^2}{12}\\).\nOne could also use the estimator \\(\\hat{b}=\\frac{n+1}{n}\\max(X_1, \\ldots, X_n)\\), which is unbiased. Show that \\(\\frac{n+1}{n}(\\max(X)-\\min(X))\\) is an unbiased estimator for \\(b-a\\) using Monte Carlo.\nFrom Part b, use algebra to figure out an estimator \\(\\hat{a}\\) for the lower bound of the support.\nIs the estimator from part (c) unbiased? Is it possible that \\(\\hat{a} &gt; \\min(X)\\)?"
  },
  {
    "objectID": "estimation1_practice.html#another-uniform-estimation",
    "href": "estimation1_practice.html#another-uniform-estimation",
    "title": "21  Point Estimation Practice",
    "section": "22.12 Another Uniform Estimation",
    "text": "22.12 Another Uniform Estimation\nSuppose \\(X_1, \\ldots X_n \\sim Unif(-\\theta, theta)\\) are an independent random sample.\n\nUse the method of moments to construct an estimator for \\(\\theta\\) (hint: the first moment will be useless!)\nConstruct another estimator based on the sample max and sample min. Is it biased?"
  },
  {
    "objectID": "estimation2_practice.html",
    "href": "estimation2_practice.html",
    "title": "24  Interval Estimation Practice",
    "section": "",
    "text": "25 Practice Problems\nFor the next few exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.\nlibrary(dslabs)\ndata(\"polls_us_election_2016\")\nSpecifically, we will use all the national polls that ended within one week before the election.\nlibrary(tidyverse)\n\npolls &lt;- polls_us_election_2016 %&gt;%\n\nfilter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "estimation2_practice.html#polling-part-1",
    "href": "estimation2_practice.html#polling-part-1",
    "title": "24  Interval Estimation Practice",
    "section": "25.1 Polling Part 1",
    "text": "25.1 Polling Part 1\nFor the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\n(N&lt;- polls$samplesize[1])\n\n[1] 2220\n\n(x_hat &lt;- polls$rawpoll_clinton[1]/100)\n\n[1] 0.47\n\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\)."
  },
  {
    "objectID": "estimation2_practice.html#polling-part-2",
    "href": "estimation2_practice.html#polling-part-2",
    "title": "24  Interval Estimation Practice",
    "section": "25.2 Polling Part 2",
    "text": "25.2 Polling Part 2\nNow use dplyr to add a confidence interval as two columns, call them lower and upper, to the object polls. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx_hat &lt;- polls$rawpoll_clinton/100\nse_hat &lt;- sqrt(x_hat*(1-x_hat)/polls$samplesize)\npolls$lower &lt;- x_hat-1.96*se_hat\npolls$upper &lt;- x_hat+1.96*se_hat"
  },
  {
    "objectID": "estimation2_practice.html#polling-part-3",
    "href": "estimation2_practice.html#polling-part-3",
    "title": "24  Interval Estimation Practice",
    "section": "25.3 Polling Part 3",
    "text": "25.3 Polling Part 3\nThe final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p = 0.482\\) or not.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npolls$hit &lt;- 0.482 &gt;=polls$lower & 0.482 &lt;= polls$upper"
  },
  {
    "objectID": "estimation2_practice.html#polling-part-4",
    "href": "estimation2_practice.html#polling-part-4",
    "title": "24  Interval Estimation Practice",
    "section": "25.4 Polling Part 4",
    "text": "25.4 Polling Part 4\nFor the table you just created, what proportion of confidence intervals included \\(p\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(polls$hit)\n\n[1] 0.3142857"
  },
  {
    "objectID": "estimation2_practice.html#polling-part-5",
    "href": "estimation2_practice.html#polling-part-5",
    "title": "24  Interval Estimation Practice",
    "section": "25.5 Polling Part 5",
    "text": "25.5 Polling Part 5\nIf these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n95% of the confidnece intervals should include p."
  },
  {
    "objectID": "estimation2_practice.html#polling-part-6",
    "href": "estimation2_practice.html#polling-part-6",
    "title": "24  Interval Estimation Practice",
    "section": "25.6 Polling Part 6",
    "text": "25.6 Polling Part 6\nA much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates d, which in this election was \\(0.482 − 0.461 = 0.021\\). Assume that there are only two parties and that \\(d = 2p − 1\\), redefine polls as below and re-do exercise 1, but for the difference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npolls &lt;- polls_us_election_2016 %&gt;%\nfilter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") %&gt;%\n\nmutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)\n\n(N&lt;- polls$samplesize[1])\n\n[1] 2220\n\n(x_hat &lt;- polls$d_hat[1])\n\n[1] 0.04"
  },
  {
    "objectID": "estimation2_practice.html#polling-part-7",
    "href": "estimation2_practice.html#polling-part-7",
    "title": "24  Interval Estimation Practice",
    "section": "25.7 Polling Part 7",
    "text": "25.7 Polling Part 7\nNow repeat Polling Part 3, but for the difference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx_hat &lt;- polls$rawpoll_clinton/100\nse_hat &lt;- sqrt(abs(x_hat*(1-x_hat))/polls$samplesize)\npolls$lower &lt;- polls$d_hat-1.96*se_hat\npolls$upper &lt;- polls$d_hat+1.96*se_hat\n\npolls$hit &lt;- .021 &gt;=polls$lower & .021 &lt;= polls$upper"
  },
  {
    "objectID": "estimation2_practice.html#polling-part-8",
    "href": "estimation2_practice.html#polling-part-8",
    "title": "24  Interval Estimation Practice",
    "section": "25.8 Polling Part 8",
    "text": "25.8 Polling Part 8\nNow repeat part 4, but for the difference.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(polls$hit)\n\n[1] 0.6\n\n\n\n\n\n#Polling Part 9\nAlthough the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual d = 0.021. Stratify by pollster.\n#Polling Part 10 Redo the plot that you made for exercise 9, but only for pollsters that took five or more polls."
  },
  {
    "objectID": "slr_practice.html",
    "href": "slr_practice.html",
    "title": "27  Simple Linear Regression Practice",
    "section": "",
    "text": "28 Practice Problems\n\nResearchers studying anthropometry collected body and skeletal diameter measurements, as well as age, weight, height and sex for 507 physically active individuals. The scatterplot below shows the relationship between height and shoulder girth (circumference of shoulders measured over deltoid muscles), both measured in centimeters.\n\n\nif(!require(openintro)){\ninstall.packages(\"openintro\")\nlibrary(openintro)\n}\n\nLoading required package: openintro\n\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\nplot(x=bdims$sho_gi, y=bdims$hgt, xlab=\"Shoulder girth (cm)\", ylab = \"Height(cm)\")\n\n\n\n\n\n\n\n\n\nDescribe the relationship between shoulder girth and height.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a positive, seemingly linear relationship between shoulder girth and height. It seems to be a moderately strong association.\n\n\n\n\nHow would the relationship change if shoulder girth was measured in inches while the units of height remained in centimeters?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe relationship would be the same if the units of shoulder girth were changed; the only change to the plot would be the labeling of the X axis. The scatterplot itself would appear unchanged.\n\n\n\n\nThe Coast Starlight Amtrak train runs from Seattle to Los Angeles. The scatterplot below displays the distance between each stop (in miles) and the amount of time it takes to travel from one stop to another (in minutes).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(x=coast_starlight$dist, y=coast_starlight$travel_time, xlab=\"Distance(miles)\", ylab=\"Travel Time (minutes)\")\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the relationship between distance and travel time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs distance tends to increase the travel time tends to increase as well, but there is a great deal of variability.\n\n\n\n\nHow would the relationship change if travel time was instead measured in hours, and distance was instead measured in kilometers?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the units were changed linearly (miles to km, and minutes to hours) the association would not be affected.\n\n\n\n\nCorrelation between travel time (in miles) and distance (in minutes) is r = 0.636. What is the correlation between travel time (in kilometers) and distance (in hours)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation between travel time in km and distance in hours would be the exact same as the correlation with the original units. Correlation is a standardized statistic and is unaffected by affine (linear) transformations in the variables.\n\n\n\n\nWrite the equation of the regression line for predicting travel time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nstarlight.lm &lt;- lm(travel_time ~ 1 + dist, data=coast_starlight)\nstarlight.lm\n\n\nCall:\nlm(formula = travel_time ~ 1 + dist, data = coast_starlight)\n\nCoefficients:\n(Intercept)         dist  \n    50.8842       0.7259  \n\n\nThe predicted model is \\(\\hat{time} = 50.8842 + 0.7259\\cdot dist\\)\n\n\n\n\nInterpret the slope and the intercept in this context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe slope can be interpreted as the average increase in travel time (.7259 minutes) for a 1 mile increase in travel distance. The slope does not have a meaningful interpretation as a zero distance trip would not take 50.88 minutes.\n\n\n\n\nCalculate \\(R^2\\) of the regression line for predicting travel time from distance traveled for the Coast Starlight, and interpret \\(R^2\\) in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(starlight.lm)$r.sq\n\n[1] 0.4044083\n\n\nThe \\(R^2\\) statistic equals 0.4044, which can be interpreted as variation in travel distance accounts for 40.44% of the variation in travel time along the Amtrak Starlight train line.\n\n\n\n\nThe distance between Santa Barbara and Los Angeles is 103 miles. Use the model to estimate the time it takes for the Starlight to travel between these two cities.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(coef(starlight.lm)*c(1, 103))\n\n[1] 125.6537\n\n#or\npredict(starlight.lm, newdata=data.frame(dist=103))\n\n       1 \n125.6537 \n\n\nUsing our model we would predict the travel time between Santa Barbara and Los Angeles to be 125.65 minutes.\n\n\n\n\nIt actually takes the Coast Starlight about 168 mins to travel from Santa Barbara to Los Angeles. Calculate the residual and explain the meaning of this residual value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n168 - 125.65\n\n[1] 42.35\n\n\nThe residual is 42.35 minutes; That means that our model under-estimates the travel time between Santa Barbara and Los Angeles by 42.35 minutes.\n\n\n\n\nSuppose Amtrak is considering adding a stop to the Coast Starlight 500 miles away from Los Angeles. Would it be appropriate to use this linear model to predict the travel time from Los Angeles to this point?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrange(coast_starlight$dist)\n\n[1]  10 352\n\n\nBecause the dataset that generated this model included distances from 10 to 352 miles, it would only be appropriate to use this model to predict travel time for distances in this range. We lack any evidence from the dataset that the modeled relationship exists for distances beyond 352 miles.\n\n\n\n\nWhat would be the correlation between the ages of partners if people always dated others who are\n\n\n\n3 years younger than themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, it would be close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a -3\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\n2 years older than themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a +2\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\nhalf as old as themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, and it would again be close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a / 2\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\nSuppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apple’s residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe residual is \\(y-\\hat{y}\\) or \\(observed-predicted\\). A negative residual indicates that \\(\\hat{y} &gt; y\\), so our model would have over-estimated shelf life for this apple.\n\n\n\n\nExplore the starbucks dataset in the openintro package, specifically the relationship between the number of calories and the amount of protein (in grams) in Starbucks food. Since Starbucks only lists the number of calories on the display items, we might be interested in predicting the amount of protein a menu item has based on its calorie content.\n\n\n\nCreate a scatter plot with protein as the Y variable and calories as the X variable.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(y=starbucks$protein, x=starbucks$calories, xlab=\"Calories\", ylab=\"Protein (g)\", main=\"Starbucks menu items\")\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the relationship between number of calories and amount of protein (in grams) that Starbucks food menu items contain. b.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a general trend that foods with higher calorie levels tend to have higher amounts of protein, but there is a great deal of variability. The positive trend is apparent though.\n\n\n\n\nIn this scenario, what are the predictor and outcome variables\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictor variable is calories, since that is the one piece of information labeled on the menu items. The outcome variable is protien (g), since we would try to predict that based on calories.\n\n\n\n\nWhy might we want to fit a regression line to these data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe relationship between calories and protein seems like it might be linear, and a straight relationshup lends itself to OLS regression.\n\n\n\n\nThe following linear model is fit to the data:\n\n\nstarbucks.lm &lt;- lm(protein ~ 1 + calories, data=starbucks)\nplot(y=starbucks$protein, x=starbucks$calories, xlab=\"Calories\", ylab=\"Protein (g)\", main=\"Starbucks menu items\")\nabline(starbucks.lm, col=\"red\")\n\n\n\n\n\n\n\nplot(starbucks.lm, which=1)\n\n\n\n\n\n\n\n\n\\(\\hat{protein} = -1.18211 + 0.03147 \\cdot calories\\)\nWhat does the residuals vs. predicted plot tell us about the variability in our prediction errors based on this model for items with lower vs. higher predicted protein?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis plot demonstrates very clearly that the variability is not constant; we have much higher variation in protein for foods with higher predicted protein than in foods with lower predicted protein. This indicates a violation in the assumption of constant variance. In order to fit a linear model perhaps the data should be transformed first, for example a log-transformation to the variables.\n\n\n\n\nThe scatterplot shows the relationship between socioeconomic status measured as the percentage of children in a neighborhood receiving reduced-fee lunches at school (lunch) and the percentage of bike riders in the neighborhood wearing helmets (helmet). The average percentage of children receiving reduced-fee lunches is 30.833% with a standard deviation of 26.724% and the average percentage of bike riders wearing helmets is 30.883% with a standard deviation of 16.948%.\n\n\nlibrary(openintro)\nplot(helmet$helmet ~ helmet$lunch, xlab=\"Rate of Receiving a Reduced-Fee Lunch (%)\", ylab=\"Rate of Wearing a Helmet (%)\")\n\n\n\n\n\n\n\n\n\nIf the \\(R^2\\) for the least-squares regression line for these data is 72%, what is the correlation between lunch and helmet?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCorrelation is precisely the square root of \\(R^2\\), the coefficient of determination. Therefore, the correlation would be\n\nsqrt(.72)\n\n[1] 0.8485281\n\n\n\n\n\n\nCalculate the slope and intercept for the least-squares regression line for these data.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhelmet.lm &lt;- lm(helmet ~ lunch, data=helmet)\ncoef(helmet.lm)\n\n(Intercept)       lunch \n 47.4904464  -0.5386091 \n\n\nThe slope is -.5386 and the intercept is 47.49.\n\n\n\n\nInterpret the intercept of the least-squares regression line in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe intercept would be interpreted that in a neighborhood where 0% of the children receive reduced fare lunches, we predict that 47.49% of bike riders wear helmets.\n\n\n\n\nInterpret the slope of the least-squares regression line in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor every percentage point increase in rate of children receiving reduced fare lunches, we predict the rate of bike-riders who wear helmets to decrease on average by 0.5386 percentage points.\n\n\n\n\nWhat would the value of the residual be for a neighborhood where 40% of the children receive reduced-fee lunches and 40% of the bike riders wear helmets? Interpret the meaning of this residual in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40-predict(helmet.lm, newdata=data.frame(lunch=40))\n\n       1 \n14.05392 \n\n\nThe residual comes to 14.05392, which means our model under-estimates the helmet rate in this neighborhood by 14.04392 percentage points.\n\n\n\n\n\n29 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>27</span>  <span class='chapter-title'>Simple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "prediction1.html#learning-objectives",
    "href": "prediction1.html#learning-objectives",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.1 Learning objectives",
    "text": "25.1 Learning objectives\nAfter this lesson, you will be able to\n\nExplain both simple and multiple linear regression\nUse R to run linear regression on a given data set and interpret the resulting coefficient estimates\nExplain what it means to associate a p-value to an estimated coefficient"
  },
  {
    "objectID": "prediction1.html#prediction-an-overview",
    "href": "prediction1.html#prediction-an-overview",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.2 Prediction: an overview",
    "text": "25.2 Prediction: an overview\nIn a prediction problem, we are given data pairs \\((X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)\\) and we want to use \\(X_i\\) to predict \\(Y_i\\).\nWe call the \\(X_i\\) values the predictors (also called the independent variables), and we call our \\(Y_i\\) values the responses (or the dependent variables or the outcomes).\nLet’s look at an example that we discussed in our very first lecture.\n\nHere, our \\((X_i,Y_i)\\) pairs correspond to years of education (\\(X_i\\)) and income (\\(Y_i\\)). That is, our predictors are years of education, and our responses are income.\nOur goal is to use this data to learn a function that maps years of education to income. That is, we want a function that takes years of education as input and outputs a prediction as to how much income we predict for a person with that income.\nNow, the very first problem we run into is what we mean by learning a function– there are lots of functions out there!\nLinear regression makes a particular choice: we will learn a linear function that maps our predictors to (predicted) responses.\nYou discussed this problem at some length in STAT240, in the setting where our \\(X_i\\) and \\(Y_i\\) variables were all real-valued. That is, you learned about simple linear regression, in which we model our data as \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\n\\]\nwhere \\(\\beta_0\\) is the intercept term, \\(\\beta_1\\) is the coefficient associated with our predictors, and \\(\\epsilon_i\\) is an error term.\nOne way to think of this is that we imagine that given a value for the predictor \\(X_i\\), say \\(X_i = x_i\\), the “true” value of \\(Y_i\\) would be \\(\\beta_0 + \\beta_1 x_i\\). However, we don’t see this quantity exactly. Instead, due either to uncertainty in our data collection or to random factors (or both), we see a noisy version of this quantity, namely \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\).\nExample: income and education\nWe might predict that a person with \\(X_i\\) years of education makes \\[\nY_i = 20 + 4 X_i\n\\] thousands of dollars in salary. But there are lots of other factors in addition to years of education that might influence how much money someone makes. Including the error \\(\\epsilon_i\\) is a way to account for these unmeasured variables.\nExample: astronomy\nIn the early days of astronomy, scientists like Johannes Kepler and Tycho Brahe were trying to fit the trajectories of planets in the sky to curves. When an astronomer takes a measurement of where a planet is, there are many sources of noise that cause them to make the measurement imprecisely. These sources of noise include both human error (e.g., imprecision in positioning the telescope, misreading a sight) and external sources (e.g., variations in humidity in the atmosphere change the way light bends as it reaches the observer’s eye). If we model the position of a planet at time \\(X_i\\) as \\[\nY_i = \\beta_0 + \\beta_1 X_i\n\\] for some choice of coefficients \\(\\beta_0\\) and \\(\\beta_1\\), we might account for these sources of measurement error via our error term \\(\\epsilon_i\\). Of course, this linear model is incorrect– the planets follow quite complicated trajectories in the night sky, not simple straight lines. We’ll come back to the matter of when linear models are or are not appropriate."
  },
  {
    "objectID": "prediction1.html#writing-down-the-model",
    "href": "prediction1.html#writing-down-the-model",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.3 Writing down the model",
    "text": "25.3 Writing down the model\nTo recap, a simple linear model has an outcome \\(y\\) and single predictor \\(x\\). It is defined by the following equation:\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n    &= ~~~~~~~\\hat{y}_i ~~~~~~ + \\epsilon_i\n\\end{aligned}\n\\]\nwhere \\(i = 1,2, \\dots, n\\), and the error terms \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) are independent over \\(i=1,2,\\dots,n\\). The \\(\\hat{y}_i\\) notation is to stress that once we have chosen values for the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), our prediction of the response of the \\(i\\)-th data point is \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\).\nThis equation represents our model, not the truth! We want to choose \\(\\beta_0\\) and \\(\\beta_1\\) so that this model describes our observed data as well as possible, but we have to bear in mind that this linearity assumption, that \\(y_i\\) is (exactly or approximately) expressible as \\(\\beta_0 + \\beta_1 x_i\\), is an assumption. Our model will be good at predicting outcomes only in so far as this model agrees with reality.\nThe subscript \\(i\\) in our regression equation indexes the \\(n\\) observations in the dataset. Think of \\(i\\) as a row number. So another way to think about our model \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\n\\]\nis as a system of \\(n\\) equations, \\[\n\\begin{aligned}\ny_1 &= \\beta_0 + \\beta_1 x_1 + \\epsilon_1 \\\\\ny_2 &= \\beta_0 + \\beta_1 x_2 + \\epsilon_2 \\\\\n    &\\vdots \\\\\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n&\\vdots \\\\\ny_{n-1} &= \\beta_0 + \\beta_1 x_{n-1} + \\epsilon_{n-1} \\\\\ny_{n} &= \\beta_0 + \\beta_1 x_{n} + \\epsilon_{n}.\n\\end{aligned}\n\\]\nThe error terms \\(\\epsilon_i\\) in a linear model correspond, essentially, to the part of the variation in the data that remains unexplained by the deterministic portion of the model (encoded in the linear function \\(\\beta_0 + \\beta_1 x\\)).\nOne of the key assumptions of a linear model is that the residuals are independent and have mean zero, \\(\\mathbb{E} \\epsilon_i = 0\\). Most typically, we further assume that they are normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\). We’ll do that here in these notes, but this choice can sometimes be “relaxed”, in the sense that we may not need to assume that the errors are normal for linear regression to work, depending on what we want to do downstream. Later in your studies, when you take your mathematical statistics course, you’ll put that statement on firmer ground; for now, we’ll have to leave it vague.)"
  },
  {
    "objectID": "prediction1.html#interpreting-simple-linear-regression",
    "href": "prediction1.html#interpreting-simple-linear-regression",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.4 Interpreting simple linear regression",
    "text": "25.4 Interpreting simple linear regression\nSo let’s suppose that we’re using the linear model \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i.\n\\]\nTo simplify things, let’s just ignore the error term for a moment. After all, \\(\\epsilon_i\\) just captures uncertainty in our measurements. In the ideal world of no measurement error, our model predicts that for a particular choice of predictor \\(x\\), we will measure the response \\[\ny = \\beta_0 + \\beta_1 x.\n\\]\nNow, let’s first consider what happens when \\(x=0\\). Then \\(y=\\beta_0\\). Said another way, if we plotted the line \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) would be the intercept of our model.\nFor example, here’s the function with \\(\\beta_0 = -2\\) and \\(\\beta_1 = 1.\\)\n\nx &lt;- seq(-2,8,0.1)\nbeta0 &lt;- -2;\nbeta1 &lt;- 1;\ny &lt;- beta0 + beta1 * x;\nplot(x,y)\nabline(h=0);\nabline(v=0);\n\n\n\n\nEquivalently, since we know that this function forms a line:\n\n# Pass NULL to plot to create an empty plot with axes.\nplot(NULL, xlab=\"\", ylab=\"\", xlim=c(-2, 8), ylim=c(-4, 6))\nabline(a=-2, b=1, col='red', lw=3);\nabline(h=0);\nabline(v=0);\n\n\n\n\nLooking at those two plots, it’s clear that \\(\\beta_0 = -2\\) is indeed the intercept of our function. But to reiterate, the “typical” interpretation of the parameter \\(\\beta_0\\) is as describing what would happen if we observed a data point for which our predictor \\(x\\) were equal to zero.\nNow, it’s pretty obvious that \\(\\beta_1\\) is the slope of our function. But how do we interpret it? Well, let’s suppose that we take one measurement with predictor \\(x\\). Our model says that (again, ignoring the error term for now) we will see a response \\[\ny = \\beta_0 + \\beta_1 x.\n\\]\nNow, let’s suppose we take another measurement, this time at predictor value \\(x+1\\). Our model predicts that we will measure the response \\[\ny' = \\beta_0 + \\beta_1 (x+1).\n\\]\nIf we subtract one from the other, we have \\[\ny' - y = \\beta_0 + \\beta_1 (x+1) - (\\beta_0 + \\beta_1 x)\n= \\beta_1.\n\\]\nIn other words, \\(\\beta_1\\) is the change in response that our model predicts if we increase the value of our predictor by one unit.\nExample: income and education\nLet’s come back to our model predicting income (in tens of thousands of dollars) from education, and suppose that we have fit a model of the form \\[\ny = 20 + 4 x.\n\\]\nSo our coefficients are \\(\\beta_0 = 20\\) and \\(\\beta_1 = 4\\). Thus, our model predicts that an increase in education by \\(1\\) year is associated with in an increase of $40K in salary (4 times our unit of measurement, $10K/year).\nSimilarly, since \\(\\beta_0=20\\), our model “predicts” that a person with zero years of education will receive a salary of $20K per year.\nExample: a cautionary tale\nInterpreting the intercept as describing the response at \\(x=0\\) can get a little bit weird if we push the idea too far. Let’s consider a similar problem, this time of predicting income from height. Suppose that we fit a model that predicts income (in thousands of dollars) from height (in centimeters), \\[\ny = 10 + 0.4 x,\n\\]\nwhere \\(x\\) is height in centimeters (note that the units on this example don’t really make sense– don’t let that bother you; it’s not the point).\nThe intercept of this model is \\(\\beta_0 = 10\\). So our model “predicts” that a person with height \\(x=0\\) would make a salary of $10,000 per year. Now, that’s all fine and good, except that I, for one, have never encountered a person with height 0 cm.\nSo our model makes a prediction, but it is making a prediction on an input that we don’t really every expect to encounter in the real word.\nThe high-level point is that often our linear regression model really only makes sense over a certain range of values, and we should be careful when using our model to extrapolate to “strange” values of \\(x\\).\nEven though we might be able to associate a response with any particular input \\(x\\), that doesn’t mean that every such input is realistic. These matters will mostly have to wait for later courses on modeling, but it’s a point we’ll come back to a couple of times over the next few weeks, and it’s a common pitfall in interpreting linear regression models, so it’s worth bringing it to your attention now.\n\n25.4.1 Caution: causality\nIt is always tempting in talking about models like this to say, having fit a model, that “increasing the value of the predictor by one unit causes an increase in the response by one unit” or that “increasing the predicor by one unit results in an increase of such and such amount”. Indeed, many statisticians will say something like this when speaking informally. Still, we should be careful to avoid giving a causal interpretation to our findings.\nFor example, suppose that we fit a linear regression model to predict the number of cancer cases per year in Wisconsin based on pollution levels (measured in, say, PM2.5), and we estimate \\(\\beta_1 = 10.2\\). We might be tempted to say that a unit increase of PM2.5 causes, on average*, an additional 10.2 cancer cases.\nFor better or worse, this is a stronger statement than what we can conclude from a linear model fitted in this way. We can only say that a unit increase in PM2.5 is associated with an increase of 10.2 cancer cases. This is the old “correlation is not causation” saying, wearing a slightly different hat.\nThere is a whole area in statistics called causal inference that attempts to use statistics to make causal statements, but it is, unfortunately, outside the scope of the course."
  },
  {
    "objectID": "prediction1.html#fitting-the-model",
    "href": "prediction1.html#fitting-the-model",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.5 Fitting the model",
    "text": "25.5 Fitting the model\nSuppose that we have chosen values of our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in our regression model. How do we decide how “good” or “bad” this choice of coefficients is? We need a function that takes a particular choice of \\(\\beta_0\\) and \\(\\beta_1\\) and outputs a number that measures how well or poorly the resulting model describes our data.\nIn the setting where larger values of this function correspond to worse model fit, we call this kind of a function a loss function: it takes a choice of model parameters (i.e., coefficients \\(\\beta_0\\) and \\(\\beta_1\\), in our case), and outputs a number that measures how poorly our model fits the data.\n\n25.5.1 Choosing a loss: sum of squares\nThere are lots of functions we could choose to use as our loss function, but by far the most common choice is the residual sum of squares (RSS), sometimes called the sum of squared errors (SSE): \\[\n\\ell( \\beta_0, \\beta_1 )\n= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n= \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2.\n\\]\nThe terms \\(y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1x_i)\\) are called the residuals. The word residual comes from the word residue (cue flashback to chem lab?), which refers to something that is left over. The residuals are what is left over after we try to predict the responses from the predictors \\(x_1,x_2,\\dots,x_n\\).\nOur goal is then to choose our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the sum of squared residuals loss in the equation above. We call this ordinary least squares (OLS) regression. “Least squares” because, well, we’re minimizing the sum of squares. “Ordinary” because there are other sums of squares we could look at that would be a little less ordinary (see here for details, if you’re curious).\nLet’s note that the sum of squared errors is not the only possible loss we could choose. For example, we might try to minimize the sum of absolute deviations, \\[\n\\sum_{i=1}^n |y_i - \\hat{y}_i|\n= \\sum_{i=1}^n \\left|y_i - (\\beta_0 + \\beta_1x_i) \\right|.\n\\]\nAs we’ve seen in recent lectures, though, trying to minimize this loss with respect to our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) can be challenging.\n\n\n25.5.2 Minimizing the loss\nSo we have a loss function \\[\n\\ell(\\beta_0,\\beta_1) = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2,\n\\] and we want to choose \\(\\beta_0\\) and \\(\\beta_1\\) to minimize this quantity.\nTo do that, we are going to dust off our calculus textbooks, take derivatives, set those derivatives equal to zero, and solve for \\(\\beta_0\\) and \\(\\beta_1\\). That is, we want to solve \\[\n\\frac{ \\partial \\ell( \\beta_0, \\beta_1 )}{ \\partial \\beta_0 } = 0\n~~~\\text{ and } ~~~\n\\frac{ \\partial \\ell( \\beta_0, \\beta_1 ) }{ \\partial \\beta_1 }\n= 0.\n\\]\nI’ll spare you the mathematical details; if you’re curious, you can find a derivation of the solution in any introductory regression book, or here\nThe important point is that we find that our estimates should be \\[\n\\begin{aligned}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\n\\hat{\\beta}_1 &= \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\sum_{i=1}^n (x_i - \\bar{x})^2 },\n\\end{aligned}\n\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of the predictors and responses, respectively: \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n~~~\\text{ and }~~~\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]"
  },
  {
    "objectID": "prediction1.html#interpreting-the-estimates",
    "href": "prediction1.html#interpreting-the-estimates",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.6 Interpreting the estimates",
    "text": "25.6 Interpreting the estimates\nLet’s pause and try to interpret what these two estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) actually mean. Let’s start with \\(\\hat{\\beta}_1\\).\n\n25.6.1 Interpreting the estimated slope\nBy our definition of \\(\\hat{\\beta}_1\\), we have\n\\[\n\\hat{\\beta}_1\n=\n\\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\sum_{i=1}^n (x_i - \\bar{x})^2 }\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 },\n\\] where we multiplied the numerator and denominator by \\(1/n\\).\nNow, let’s notice that the denominator is just the (uncorrected) sample variance of the predictors: \\[\ns_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\] If we define the analogous quantity for the predictors, \\[\ns_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2,\n\\] we have \\[\n\\hat{\\beta}_1\n= \\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{s_x^2}\n=\n\\frac{ s_y }{ s_x }\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ s_x s_x }.\n\\]\nNow, let’s look at the other sum, \\[\n\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}),\n\\] and notice that it is the sample covariance of the predictors and responses. Recalling our definition of the correlation as \\[\n\\rho_{x,y}\n= \\frac{ \\mathbb{E} (X - \\mathbb{E} X)( Y - \\mathbb{E}Y)}\n{\\sqrt{ (\\operatorname{Var} X)( \\operatorname{Var} Y) }},\n\\]\nwe notice that \\[\n\\hat{\\rho}_{x,y}\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ s_x s_x }\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ \\sqrt{ s_x^2 s_y^2 }}\n\\]\nis the sample correlation between our predictors and responses– we plugged in the sample versions of the covariance and the variances.\nSo, our estimated coefficient \\(\\hat{\\beta}_1\\) can be expressed as \\[\n\\hat{\\beta}_1\n=\n\\frac{ s_y }{ s_x } \\hat{\\rho}_{x,y}.\n\\]\nIn other words, the slope of our model is the ratio of the standard deviations, scaled by the correlation between our predictors and responses.\nAn interesting case to think about is when the predictors and responses are perfectly correlated (i.e., the predictors and responses form a perfect line, with no “jitter”). Then our estimated slope is \\(\\hat{\\beta}_1 = \\sqrt{ s_y^2/s_x^2 } = s_y/s_x\\). In other words, the slope of our model is just the ratio of the standard deviation of the responses to that of the predictors. Think of this as like a “change of units” from predictors to responses. If our predictors are measured in, say, years of education, and our responses are measured in dollars per year, then the ratio of the standard deviations has units \\[\n\\frac{ \\text{dollars per year} }{ \\text{years of education}},\n\\]\nand multiplying this by our predictor, which is measured in “years of education”, we get \\[\n\\text{ response }\n=\n\\frac{ \\text{dollars per year} }{ \\text{years of education}}\n\\cdot\n\\text{ years of education }\n=\n\\text{ dollars per year },\n\\] which is what we expect.\nThis kind of “dimension analysis”, which you may have seen before in physics, can be a useful way to make sure that you’re performing calculations correctly!\n\n\n25.6.2 Interpreting the intercept term \\(\\hat{\\beta}_0\\)\nTurning our attention to \\(\\hat{\\beta}_0\\), we have\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\] Why does this choice make sense?\nWell, let’s suppose that we decided to make our lives even harder by restricting our choice of prediction function to be a constant. That is, suppose we wanted to choose a prediction function that returns the same output, say, \\(\\hat{y}\\), no matter the input \\(x\\).\nIf we wanted to choose this output using the same least-squares approach that we used above, we would want to choose \\(\\hat{y}\\) so that it minimizes \\[\n\\sum_{i=1}^n (y_i - y)^2.\n\\] A little bit of calculus (seriously, this one’s easy– try it!) shows that the way to minimize this is to choose the output \\[\n\\hat{y}= \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}.\n\\] Now, thankfully, we are not actually trying to predict our data with a constant function. We are allowed to choose a slope!\nHaving chosen our slope \\(\\hat{\\beta}_1\\), our model without an intercept term predicts that the \\(i\\)-th observation should have response \\(\\hat{\\beta}_1 x_i\\). If we add an intercept term to the model, sticking with our least squares loss, we would like to choose \\(\\beta_0\\) so as to minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_1 x_i - \\beta_0 \\right)^2.\n\\]\nThe exact same kind of calculus argument (taking a derivative with respect to \\(\\beta_0\\), this time– again, give it a try!), gets us \\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\]"
  },
  {
    "objectID": "prediction1.html#variance-of-estimates",
    "href": "prediction1.html#variance-of-estimates",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.7 Variance of estimates",
    "text": "25.7 Variance of estimates\nAfter fitting, we can find our predicted \\(\\hat{y_i}\\), i.e. the \\(y\\) values on the line.\n\\[\n\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i\n\\]\nas well as our model residuals \\(\\hat{\\epsilon}_i\\)\n\\[\n\\hat{\\epsilon_i}=y_i-\\hat{y_i}\n\\]\nFrom this, we also get for free an estimate of the variance of the residuals \\(\\sigma^2\\), which happens to be very useful in computing other statistics. The reason is that the larger the residuals’ variance, the less precisely we can estimate our regression coefficients, which should make a lot of sense.\n\\[\n\\hat{\\sigma}^2=\\text{mean squared error}=\\frac{SSE}{n-2}=\\frac1{n-2}\\sum_i(y_i-\\hat{y_i})^2\n\\]\nWe can also easily derive the variance of the slope. First, observe that\n\\[\\begin{align}\n\\sum_i (x_i - \\bar{x})\\bar{y}\n&= \\bar{y}\\sum_i (x_i - \\bar{x})\\\\\n&= \\bar{y}\\left(\\left(\\sum_i x_i\\right) - n\\bar{x}\\right)\\\\\n&= \\bar{y}\\left(n\\bar{x} - n\\bar{x}\\right)\\\\\n&= 0\n\\end{align}\\]\nThis means that\n\\[\\begin{align}\n\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n&= \\sum_i (x_i - \\bar{x})y_i - \\sum_i (x_i - \\bar{x})\\bar{y}\\\\\n&= \\sum_i (x_i - \\bar{x})y_i\\\\\n&= \\sum_i (x_i - \\bar{x})(\\beta_0 + \\beta_1x_i + \\epsilon_i )\\\\\n\\end{align}\\]\nUsing this, we can easily derive \\(\\text{Var}(\\hat{\\beta_1})\\) as follows:\n\\[\\begin{align}\n\\text{Var}(\\hat{\\beta_1})\n& = \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2} \\right) \\\\\n&= \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})(\\beta_0 + \\beta_1x_i + \\epsilon_i )}{\\sum_i (x_i - \\bar{x})^2} \\right), \\;\\;\\;\\text{substituting in the above} \\\\\n&= \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})\\epsilon_i}{\\sum_i (x_i - \\bar{x})^2} \\right), \\;\\;\\;\\text{noting only $\\epsilon_i$ is a random variable} \\\\\n&=  \\frac{\\sum_i (x_i - \\bar{x})^2\\text{Var}(\\epsilon_i)}{\\left(\\sum_i (x_i - \\bar{x})^2\\right)^2} , \\;\\;\\;\\text{independence of } \\epsilon_i \\text{ and, Var}(cX)=c^2\\text{Var}(X) \\\\\n&= \\frac{\\sigma^2}{\\sum_i (x_i - \\bar{x})^2} \\\\\n\\end{align}\\]"
  },
  {
    "objectID": "prediction1.html#running-simple-linear-regression",
    "href": "prediction1.html#running-simple-linear-regression",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.8 Running simple linear regression",
    "text": "25.8 Running simple linear regression\nOkay, that’s enough abstraction. Let’s apply this to some real data and see how things go.\nIn the 1920s, Thomas Midgley Jr. discovered that adding tetraethyllead to gasoline decreased engine knocking (i.e., when fuel doesn’t fully ignite in an engine cylinder, which may damage the engine). He won the 1923 Nichols medal, a prestigious prize in chemistry, for his discovery.\nThe result of burning tetraethyllead in gasoline resulted in high levels of lead in the atmostphere. By the 1950s to 70s, researchers started to suspect that increased lead levels in the atmosphere was causing widespread lead poisoning, with symptoms ranging from depression, loss of appetite, and amnesia to anemia, insomnia, slurred speech, and cognitive impairment. Starting in the 1980s, the use of tetraethyllead in gasoline started to be phased out. At most gas stations in the United States, you’ll notice that gasoline is still marked as being “unleaded”, just in case you were worried!\nIn more recent years, a more controversial theory has emerged, suggesting that exposure to lead (be it in the atmosphere or in paint in older buildings) correlates with incidents of violent crime later in life 1 2. This study was first conducted in the US, but it was soon replicated in other countries and the similar results have been found elsewhere in the world.\n\n\n\nLet’s look at a dataset that contains atmospheric lead content levels and aggravated assault rates for several cities in the US and see if we can build a simple linear regression model to explain the trend and make predictions.\n\nlead &lt;- read.csv('data/lead.csv')\n# First things first: let's look at the data.\nhead(lead)\n\n     city air.pb.metric.tons aggr.assault.per.million\n1 Atlanta                421                     1029\n2 Atlanta                429                      937\n3 Atlanta                444                      887\n4 Atlanta                457                      533\n5 Atlanta                461                     1012\n6 Atlanta                454                      848\n\n\nThe variables we are interested in are lead levels in the atmosphere (measured in metric tons of lead emitted) and the aggravated assault rate per million 22 years later. We want to predict the assault rate from lead levels, so our predictor (or explanatory variable or independent variable, if you prefer) is lead levels, and our response (or dependent variable) is the assault rate. This data is available for a number of cities:\n\nlevels( as.factor(lead$city) )\n\n[1] \"Atlanta\"      \"Chicago\"      \"Indianapolis\" \"Minneapolis\"  \"New Orleans\" \n[6] \"San Diego\"   \n\n\nFor simplicity, let’s focus on the city of Atlanta.\n\natlanta_lead &lt;- lead[ lead$city=='Atlanta', ];\n## Alternative approach, using filter in dplyr:\n# library(dplyr)\n# atlanta_lead  lead %&gt;% filter(city == \"Atlanta\")\n\n# Plot the data to get a look at what's going on.\nlibrary(ggplot2)\npp &lt;- ggplot( atlanta_lead,\n          aes(x=air.pb.metric.tons, y=aggr.assault.per.million));\npp &lt;- pp + geom_point();\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\nVisually, it’s quite clear that assaults are well-predicted as a linear function of lead levels. One thing that is already perhaps of concern is that the data appears to be a bit more “spread out” in the vertical direction for lower lead levels. This is a bit concerning in light of our assumption that the error terms were all distributed according to a normal with mean \\(0\\) and variance \\(\\sigma^2\\). We’ll come back to this point below. For now, let’s press on.\nTo fit a linear model in R, we use the lm() function (lm for “linear model”). The syntax is as simple as lm(y ~ 1 + x, data=dframe), where dframe is the data frame containing our data, and y ~ 1 + x means to regress the variable y (i.e., the column y in the dataframe dframe) against the variable x and an intercept term (that’s the 1 in the model formula): \\[\ny = \\beta_1 x + \\beta_0\n\\]\nNote that the 1 in the model formula y ~ 1 + x is completely optional– R will include an intercept term automatically. Still, I recommend including it for the sake of clarity.\nThe function lm returns an object of the class lm. This is an object that contains a bunch of information about our fitted model. We’ll see some of that information below.\nSo in our case, we want to regress aggr.assault.per.million against air.pb.metric.tons. That is, we want a model like \\[\n\\text{agg.assault} = \\beta_0 + \\beta_1 \\cdot \\text{air.pb}\n\\]\nSo we’ll write that as aggr.assault.per.million ~ 1+ air.pb.metric.tons. Let’s try fitting the model and then we’ll ask R to summarize our model. Running summary() on the model object gives us a variety of useful summary statistics and other information about the fitted model.\n\natlanta_lead_lm &lt;- lm(aggr.assault.per.million ~ 1 + air.pb.metric.tons,\n                      data=atlanta_lead)\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nWe’ll come back a little later to talk about some of these numbers in more detail. For now, let’s notice in particular that we have estimated coefficients, accessible in the coefficients attribute of our model object:\n\natlanta_lead_lm$coefficients\n\n       (Intercept) air.pb.metric.tons \n        107.942757           1.403746 \n\n\nSo our model predicts that in Atlanta, an increase of one metric ton of lead in the atmosphere is associated with an increase of about 1.4 aggravated assaults per million people. Similarly, the intercept indicates that our model predicts that in the absence of any lead in the atmosphere, there would be about 108 aggravated assaults per million people."
  },
  {
    "objectID": "prediction1.html#working-with-lm-output-diagnostics",
    "href": "prediction1.html#working-with-lm-output-diagnostics",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.9 Working with lm() output: diagnostics",
    "text": "25.9 Working with lm() output: diagnostics\nLet’s look at how our fitted model tracks the data by overlaying the fitted line on our scatterplot above. One way to do this would be to use abline with the entries of atlanta_lead_lm$coefficients to specify the slope and intercept, but lm is so common that ggplot2 has this same basic functionality built-in, in the form of geom_smooth().\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\nLooks like a pretty good fit! Let’s look at some of the other information included in the output of lm().\n\n# gets fitted y-values (i.e., points on line of best fit)\nfitted(atlanta_lead_lm)\n\n        1         2         3         4         5         6         7         8 \n 698.9200  710.1499  731.2061  749.4548  755.0698  745.2436  787.3560  836.4871 \n        9        10        11        12        13        14        15        16 \n 849.1208  990.8992 1038.6266 1096.1802 1118.6401 1169.1750 1183.2125 1187.4237 \n       17        18        19        20        21        22        23        24 \n1194.4424 1197.2499 1302.5309 1351.6620 1414.8306 1529.9378 1597.3176 1689.9649 \n       25        26        27        28        29        30        31        32 \n1757.3447 1782.6122 1852.7995 1879.4707 1991.7704 2000.1929 2101.2626 2185.4874 \n       33        34        35        36 \n2209.3511 2231.8110 2217.7735 2236.0222 \n\n\n\n# residuals( model ) gets residuals (the difference between the\n# observed response y and the response predicted by our model)\nresiduals(atlanta_lead_lm)  # We can also use resid()\n\n          1           2           3           4           5           6 \n 330.080025  226.850054  155.793859 -216.454844  256.930171  102.756395 \n          7           8           9          10          11          12 \n-356.355996 -149.487118  382.879164 -273.899218 -268.626594 -280.180195 \n         13          14          15          16          17          18 \n 175.359863 -294.175006   23.787530  109.576291  -97.442440  -80.249933 \n         19          20          21          22          23          24 \n  -8.530910   45.337967  -43.830619  116.062179  -55.317646  -70.964906 \n         25          26          27          28          29          30 \n-129.344731   14.387834  -58.799484  143.529335  148.229626    9.807148 \n         31          32          33          34          35          36 \n 199.737410  -64.487372   16.648940   14.188998  -27.773538    3.977759 \n\n\nThese residuals reflect the error between our model’s prediction and the true observations, and they are often quite informative.\nRecall that our model assumes that the observation errors \\(\\epsilon_i\\) are normally distributed about zero, with a shared variance \\(\\sigma^2\\). To check that this assumption is (approximately) true, we can plot the residuals:\n\nresids &lt;- residuals(atlanta_lead_lm)\nhist(resids)\n\n\n\n\nThat looks… okay, at any rate. The residuals are (approximately) symmetric about zero, and the histogram looks normal-ish to me. We’ll come back to this point, and later in your studies (e.g., if you take our department’s regression course) you’ll learn lots of ways for assessing model fit (e.g., checking if the normal errors assumption is correct), but for the time being, we’ll be satisfied with the “ocular inspection” method.\n\n25.9.1 Homoscedasticity\nAnother important point, far more important that the normality assumption, is that the variance of the errors \\(\\epsilon_i\\) does not depend on the predictor \\(X_i\\). This is referred to as “homogeneity of variance”, more commonly called homoscedasticity. Its absence, heteroscedasticity, wherein the variance of the error terms varies with \\(X_i\\), can be a big problem for linear regression.\nSo let’s check for it, just visually for now. We want to plot the residuals as a function of their predictor values. If our errors are homoscedastic, we should observe the variance of the residuals about the horizontal line \\(y=0\\) to be more or less constant along the x-axis. R will do this for us automatically if we call plot on our model object. In fact, R will make several plots for us automatically, and return those plots in a list-like object. The residuals as a function of the x values is the first of these, and we can access it with the which keyword to plot().\n\nplot(atlanta_lead_lm, which=1)\n\n\n\n\nThe red line is fitted by R; if our residuals are reasonably well-behaved, this line should be horizontal. Inspecting this plot, it looks as we suspected– the residuals for smaller lead atmospheric levels have slightly higher variance, and tend to be biased toward positive values. Still (and this is an intuition that you’ll develop as you perform more analyses), this doesn’t look especially extreme.\n\n\n25.9.2 Assessing normality of the residuals\nThe stronger assumption, not required by linear regression per se, but a good assumption to check for use in downstream testing procedures (we’ll talk about that soon!), is that the errors are normal with mean zero. We saw that their histogram above looked pretty reasonable. A better check for fit is to construct a Q-Q plot (still no relation to the Chinese restaurant on University Ave, sadly).\n\nplot(atlanta_lead_lm, which=2)\n\n\n\n\nLet’s recall that a Q-Q-plot displays the quantiles (i.e., the percentiles) of our observed data against the quantiles of the normal distribution. If our data were perfectly normally distributed, then the Q-Q plot would look like a straight line with slope 1 (up to randomness in the data, of course). If our data is not normal, the Q-Q plot will look far different.\nJust to see an example of this, let’s generate some data from a t-distribution (which looks normal, but has “heavier tails”), and look at the Q-Q plot.\n\ndata &lt;- rt(n=36, df=3, ncp=0);\nhist(data)\n\n\n\n\n\nqqnorm(data);\n# Add a line to the plot to indicate the behavior we would\n# expect to see if the data were normal.\nqqline(data, col='red');\n\n\n\n\nSo this is an example of the kind of behavior we would expect to see if our data were not well-described by a normal. Here’s our lead level data again.\n\n# We could also call qqnorm(atlanta_lead_lm$residuals)\nplot(atlanta_lead_lm, which=2)\n\n\n\n\nThere are some slightly concerning spots there, especially in the bottom-left, but it’s not too extreme (in my opinion, anyway). Once again, later on you’ll learn more rigorous ways of checking model assumptions like this. We’re just trying to get an intuition for now."
  },
  {
    "objectID": "prediction1.html#testing-and-confidence-intervals-for-coefficients",
    "href": "prediction1.html#testing-and-confidence-intervals-for-coefficients",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.10 Testing and confidence intervals for coefficients",
    "text": "25.10 Testing and confidence intervals for coefficients\nSo we’ve established that our model is a reasonably good fit for the lead data, at least in the sense that the trend in the data follows our plotted line and such.\nCan we conclude from this that the association between lead and aggravated assault rate is “real”? It’s possible, after all, that the observed association is merely due to chance.\nWell, in our discussions of hypothesis testing we saw a number of tools for checking if observations were merely due to chance or not. Linear regression has its own set of tools for checking whether observed coefficient estimates are “merely due to chance”.\nLet’s look back at our model summary and let’s pay particular attention to the “coefficients” part of the output.\n\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nThe Coefficients table includes our estimates for the coefficients, standard errors for those estimates, t-values (i.e., a test statistic) for those statistics and, at the far right of the table, a column headed Pr[&gt;|t|]. Hey… that’s a p-value!\nNotice that in this case, the p-value associated to our estimate of the coefficient of lead levels is quite small. That indicates that an estimate this extreme (or more extreme) is highly unlikely to have arisen entirely by chance.\nNow, it is in dealing with these p-values that we need to be a bit careful about our model assumptions. If the assumption of normal, homoscedastic errors is violated, things can go wrong with these p-values. But since our Q-Q plot indicated that our residuals were reasonably normal-looking, we can be somewhat confident that this is reflecting a real effect (well, and it’s pretty clear just from the plot that there’s a linear relationship…).\nThis p-value arises, in essence, from a \\(t\\)-test. This t-test is designed to test the null hypothesis \\[\nH_0 : \\beta_1 = 0.\n\\]\nIn this case, our p-value associated with \\(\\beta_1\\) is quite small, indicating a correlation between lead levels and aggravated assault levels. It does not imply causation, though, as you well know by now. Nonetheless, this seems to be fairly convincing evidence that there is an association.\nThinking back to our brief discussion of the connection between confidence intervals and testing, you won’t be surprised to learn that we can also compute confidence intervals for the true value of the coefficients.\n\nconfint(atlanta_lead_lm, level=0.95)\n\n                        2.5 %     97.5 %\n(Intercept)        -55.579958 271.465472\nair.pb.metric.tons   1.238891   1.568602\n\n\nOf course, we also tested the hypothesis \\(H_0 : \\beta_0 = 0\\), and the p-value is not especially small (also reflected in the fact that the confidence interval includes \\(0\\)). This indicates that our intercept term was not statistically significantly different from zero.\nNote, however, that just because our p-value associated to the intercept term isn’t especially small, that doesn’t mean that the intercept isn’t useful for prediction. Let’s turn our attention to that matter."
  },
  {
    "objectID": "prediction1.html#making-predictions",
    "href": "prediction1.html#making-predictions",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.11 Making predictions",
    "text": "25.11 Making predictions\nSuppose that tomorrow a chemical company near Atlanta has an incident, and lead is released into the atmosphere. Suppose that the new atmospheric levels of lead are found to be 1300 metric tons. What would you predict the approximate aggravated assault rate to be 22 years later?\nWell, let’s start by just looking at a plot of our model again.\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\nJust looking at the plot, we see that at x-value 1300, our fitted line is just about exactly at 2000 aggravated assaults per million people.\nHopefully you can imagine how annoying it would be to perform this exercise by hand every time we need a new prediction. Luckily, R model objects (including linear regression) support a function called predict, which does exactly what it sounds like. We pass our model, and some data (i.e., x-values), and predict() outputs our model’s predicted responses at those values.\n\npredict(atlanta_lead_lm, newdata=data.frame(air.pb.metric.tons=1300))\n\n       1 \n1932.813 \n\n\nSuppose the company continues to release more lead into the atmosphere, and next year, the levels are measured to be 2000 metric tons. Can we use our model to predict what aggravated assault rates might look like 22 years later?\nWell, looking at the plot again, 2000 metric tons is rather far outside the range of our observed predictor values.\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\nAs we’ve alluded to earlier in these lecture notes, predictions made far outside the range of observed predictors have to be treated carefully They may be reliable, but they also may not. The reliability of a prediction usually decreases the further away it is from your data.\nFor example, perhaps once lead levels reach a certain point, there just isn’t much more damage they can do to human development. Then we would see the linear trend flatten out at higher lead levels. Our function would cease to be linear, and naively applying our linear models to those values would result in poor prediction performance.\nJust to drive the point home, here’s a cautionary tale:\n\n\n\n|t|)\n(Intercept) 17.965050 0.849663 21.144 &lt; 2e-16  disp -0.006622 0.004166 -1.590 0.12317\nhp -0.022953 0.004603 -4.986 2.88e-05  wt 1.485283 0.429172 3.461 0.00175 ** — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 1.062 on 28 degrees of freedom Multiple R-squared: 0.6808, Adjusted R-squared: 0.6466 F-statistic: 19.91 on 3 and 28 DF, p-value: 4.134e-07\n:::\n:::\n\n\nOnce again, before we get too eager about interpreting the model, we should check that our residuals are reasonable.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(mtc_model$residuals)\n\n\n\n:::\nHmm… that isn’t amazing, but on the other hand, there aren’t very many observations to begin with, so we shouldn’t expect a particularly normal-looking histogram.\nChecking heteroscedasticity isn’t so easy now, but we can still do things like compare the residuals with a normal via a Q-Q plot:\n\nplot(mtc_model, which=2)\n\n\n\n\nIn my opinion, this Q-Q plot would likely lead me to question the assumption of normal errors. That doesn’t mean that we can’t proceed with using our linear model, but it will mean that we should be a bit careful with how much credence we give to any quantities that depend on our normality assumption (e.g., our p-values).\nLet’s press on regardless, for now, mostly for the sake of demonstration of what we would do, if we were reasonably happy with our model assumptions. Still, we should bear in the back of our minds that perhaps our normality assumptions perhaps aren’t exactly true.\nLet’s return to our model output.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nLet’s start at the bottom. The residual standard error (RSE) is listed as being 1.062. This is related to the sum of squared errors we discussed earlier, and it’s exactly what it sounds like– the standard error of the model’s residuals. Ideally, we want this number to be small– after all, it measures the error in our model. We’ll come back to this matter later in the semester.\nThe output lists this as being the residual standard error on 28 degrees of freedom. Remember that as a rule of thumb, the degrees of freedom will be the number of data points less the number of parameters we estimate. In this case, there are 32 data points\n\nnrow(mtcars)\n\n[1] 32\n\n\nand our model has four parameters: the intercept and our three predictors’ coefficients, so 28 degrees of freedom checks out!\nMuch more on that whole matter soon, but let’s keep looking at that model summary.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nThe F-statistic associated with our residuals has a very small p-value associated to it, indicating, in essence, that our model fit is much better than would be expected by chance. You’ll learn a lot more about the F-distribution when you learn about analysis of variance (ANOVA) in a later class.\nScanning our way up the model summary, let’s look at the table of coefficient estimates. We see that our intercept term and the coefficients for horsepower (hp) and weight (wt) are flagged as being significant. Thus, briefly putting on our testing hats, we would reject the null hypotheses \\(H_0 : \\beta_0=0\\), \\(H_0: \\beta_{\\text{hp}} = 0\\) and \\(H_0: \\beta_{\\text{wt}} = 0\\). On the other hand, there is insufficient evidence to reject the null \\(H_0 : \\beta_{\\text{wt}} = 0\\).\nIn other words, it appears that horsepower and weight are associated with changes in quarter-mile time, but displacement is not."
  },
  {
    "objectID": "prediction1.html#assessing-model-fit",
    "href": "prediction1.html#assessing-model-fit",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.12 Assessing model fit",
    "text": "25.12 Assessing model fit\nOnce we’ve fit a model to the data, how do we tell if our model is good or not? We started talking about this above, and it is a trickier question that it might seem at first. We’ll have lots more to say about the problem in coming weeks. For now, though, let’s consider the most obvious answer to this question.\nWe fit our model to data by minimizing the sum of squares (we’re sticking with simple linear regression here for simplicity– this idea extends to multiple linear regression in the obvious way), \\[\n\\ell( \\beta_0, \\beta_1 )\n= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n= \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2.\n\\]\nSo what better way to measure how good our model is than using precisely this quantity? We define the residual sum of squares (RSS; also called the sum of squared errors, SSE) to be the sum of squared errors of our model. That is, letting \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) be our estimates of the coefficients, \\[\n\\operatorname{RSS}\n=\n\\operatorname{SSE}\n= \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - (\\hat{beta}_0 + \\hat{\\beta}_1 x_i) \\right)^2\n\\]\nThe number of degrees of freedom will have bearing on the distribution of this error term- Uunder the model where the errors are indeed normally distributed, the residual sum of squares (RSS) will have an F-distribution with degrees of freedom given by the number of observations minus the number of parameters (like the t-distribution, the F-distribution has the degrees of freedom as one of its parameters; the other, the 3 in the summary above, comes from the number of coefficients less one). Knowing this fact lets us build a rejection region for using the RSS as a test statistic, and that’s exactly where the overall p-value at the bottom of the summary comes from.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nAnother useful quantity in describing how well our model describes the data is the Coefficient of Determination, or \\(R\\)-squared, which can be interpreted as measuring the proportion (between 0 and 1) of the variation in \\(Y\\) that is explained by the variation in \\(X\\).\n\\[\nR^{2}=\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}}=1-\\frac{\\text{RSS}}{\\text{TSS}},\n\\]\nwhere \\[\n\\operatorname{TSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n\\]\nis the total sum of squares.\nIn the case of simple linear regression, things simplify so that \\(R^{2}=r^{2}\\), where \\(r\\) is the correlation coefficient between \\(X\\) and \\(Y\\),\n\\[\nr\n=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}\n{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n\\]\nWhen this quantity is close to 1, we can be confident that our linear model is accurately capturing a trend in the data."
  },
  {
    "objectID": "prediction1.html#looking-ahead-model-selection",
    "href": "prediction1.html#looking-ahead-model-selection",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.13 Looking ahead: model selection",
    "text": "25.13 Looking ahead: model selection\nOne important point that we’ve ignored in our discussion above is how we go about choosing what predictors to include in our model. For example, the mtcars data set has columns\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nIn our example above, we just chose a few of these to use as predictors. But suppose that we didn’t know ahead of time which predictors to use. How do we choose which ones to include in our model? Are there downsides to just including all of them?"
  },
  {
    "objectID": "prediction1.html#review",
    "href": "prediction1.html#review",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.14 Review",
    "text": "25.14 Review\n\nThe simple linear regression model\npredictor / response variables\nassumptions of SLR model\ninterpretation of SLR components\ncorrelation is not causation\nresiduals\nloss function of OLS: Sum of squared residuals\nslope estimate formula and interpretation\nintercept estimate formula and interpretation\nunits of \\(\\beta\\) estimates\nmean squared error, estimate of \\(\\sigma^2_\\epsilon\\)\nvariance of \\(\\beta\\) estimates\nOLS in R, and summary output\nOLS diagnostics (residual plot, residual QQ plot)\ninference and confidence intervals for coefficients\npredictions & prediction intervals\n\\(R^2\\) and \\(r\\)"
  },
  {
    "objectID": "prediction1.html#footnotes",
    "href": "prediction1.html#footnotes",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "",
    "text": "https://ir.lawnet.fordham.edu/ulj/vol20/iss3/1↩︎\nhttps://doi.org/10.1016/j.envres.2007.02.008↩︎"
  },
  {
    "objectID": "estimation1_practice.html#goldfish",
    "href": "estimation1_practice.html#goldfish",
    "title": "21  Point Estimation Practice",
    "section": "22.13 Goldfish",
    "text": "22.13 Goldfish\nIn 1997 Pepperidge Farm started putting faces on some of the goldfish crackers they produce. Roughly 40% of goldfish crackers have faces.\nSuppose you want to estimate this proportion, and you buy a 20 pack box as a representative sample of Goldfish. The box contains 20 1-oz packs of goldfish, each containing roughly 50 goldfish. Suppose we can model the number of goldfish in a pack using a normal distribution (rounded to the nearest integer), \\(Y_i \\sim N(50, 2^2)\\).\nTo estimate the proportion of smiley faces, you figure you could do three things:\n\nOpen all 20 bags in one pile and calculate the sample proportion\nCalculate the proportion in each of the 20 bags and average those proportions\nCalculate the proportions in each bag and calculate the weighted average (weighting by the number of goldfish in each bag)\n\nUsing a normal model for the number of fish in a bag, and a binomial for the number of smiley fish, determine the bias and variance of these three estimators.\n\nNMC &lt;- 10000\nestimates &lt;- data.frame(method1=numeric(NMC),\n                        method2=numeric(NMC),\n                        method3=numeric(NMC))\nfor(i in 1:NMC){\n  Y &lt;- round(rnorm(20, 50, 2))\n  X &lt;- rbinom(20, Y, .40)\n  estimates[i,] &lt;- c(sum(X)/sum(Y),\n                     mean(X/Y),\n                     sum(X/Y * Y/sum(Y))) \n}\n\ncolMeans(estimates)\n\n  method1   method2   method3 \n0.3999926 0.3999805 0.3999926 \n\napply(estimates, 2, FUN=var)\n\n     method1      method2      method3 \n0.0002396600 0.0002399836 0.0002396600 \n\n\nWhen working out the math, it becomes clear that method 1 and 3 are actually identical to each other. Running the MC a few times it seems that all 3 methods give an unbiased estimate. Method 2 however has a slightly larger variance. This seems consistent after re-running the MC multiple times. Thus method 1 is probably the best method to estimate \\(p\\)."
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html",
    "href": "R07_More_Estimation_and_CI_Examples.html",
    "title": "23  Interval Estimation Examples",
    "section": "",
    "text": "24 More Estimation Examples"
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#exercise-from-lecture",
    "href": "R07_More_Estimation_and_CI_Examples.html#exercise-from-lecture",
    "title": "23  Interval Estimation Examples",
    "section": "24.1 Exercise From Lecture",
    "text": "24.1 Exercise From Lecture\nI’ve modified the code to take in the number of MC replicates for the calculation of the LB and UB, as well as the number of MC replicates to estimate coverage rate.\n\nrun_exprate_expt &lt;- function( nsamp, true_rate, NMC=2000 ) {\n  obsd_data &lt;- rexp(n=nsamp, rate=true_rate);\n  estd_rate &lt;- 1/mean(obsd_data);\n  # Now generate \"fake\" data sets and estimate lambda on each.\n  replicates &lt;- rep(NA, NMC);\n  for ( i in 1:NMC) {\n    fake_data &lt;- rexp(nsamp, rate=estd_rate);\n    replicates[i] &lt;- 1/mean(fake_data);\n  }\n  CI &lt;- quantile( replicates, probs=c(0.025, 0.975), names=FALSE );\n  return( (CI[1] &lt; true_rate) & (true_rate &lt; CI[2]) );\n}\n\nestimate_exp_coverage &lt;- function(lambda_true=2,\n                                  nsamp = 100,\n                                  NMC1 = 2000,\n                                  NMC2 = 2000){\n  reps &lt;- replicate(NMC1, run_exprate_expt( nsamp, lambda_true,NMC2 ))\n  return(mean(reps))\n}\n\nestimate_exp_coverage(lambda_true=10,\n                      nsamp=25, 2000, 500)\n\n[1] 0.9355\n\n\nTry playing around with nsamp and lambda_true in the code above.\nConsider what happens if nsamp is small (e.g., 20 or 25): \\(\\bar{X}\\) will not necessarily be close to \\(\\mathbb{E} \\bar{X} = 1/\\lambda\\), and thus our estimate of \\(\\lambda\\), \\(1/\\bar{X}\\) will likely be far from \\(\\lambda\\), and thus our “fake” data will not actually look very much like it should.\n\nns &lt;- seq(10,100, 10) # we'll look at sample sizes 10 to 100\ncoverage &lt;- numeric(0)\nfor(n in ns){\n  coverage &lt;- c(coverage, estimate_exp_coverage(2,n,1000,1000))\n}\nplot(ns, coverage)\n\n\n\n\nThe coverage rate is a little on the low side, about 92.5% when n is low (10) but when N gets to about 40+ the coverage rate gets to 94 to 94.5%.\nIn another direction, try playing around with the parameter \\(\\lambda &gt; 0\\). What happens if you make \\(\\lambda\\) really big or really close to zero?\n\nlambdas &lt;- c(0.1, 0.5,1,2,5,10,50,100,200,500,1000)\ncoverage &lt;- numeric(0)\nfor(lambda in lambdas){\n  coverage &lt;- c(coverage, estimate_exp_coverage(lambda,100,1000,1000))\n}\nplot(log(lambdas), coverage)\n\n\n\n\nThe coverage rate seems to be between 93.5% and 95%. The trend is not obvious, it doesn’t seem like the lambda matters terribly much.\nThe reason for this? Changing lambda only changes the scale of the distribution, not its shape. It shouldn’t have an effect on how close \\(1/\\bar{X}\\) is to \\(\\lambda\\). What matters much more is the sample size \\(n\\) that we have to work with."
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#example-estimating-the-maximum-of-a-sample",
    "href": "R07_More_Estimation_and_CI_Examples.html#example-estimating-the-maximum-of-a-sample",
    "title": "23  Interval Estimation Examples",
    "section": "24.2 Example: Estimating the maximum of a sample",
    "text": "24.2 Example: Estimating the maximum of a sample\nSay we sample from a random variable \\(X \\sim \\text{Uniform}(0, M)\\) and get the following data\n\nX &lt;- read.csv(\"data/uniform_sample.csv\")$x\n\nOne point estimate we may naturally use is the sample maximum\n\nmax(X)\n\n[1] 0.9443383\n\n\nSuppose we were to use this as our point estimate of \\(M\\) (seems natural). Can we quantify our confidence in the estimate?\n\nsim_S &lt;- 0\nn &lt;- length(X) \nfor(i in 1:1000){\n  simData &lt;- runif(n, 0, max(X))\n  sim_S[i] &lt;- max(simData)\n}\nhist(sim_S)\n\n\n\n\nSay we want to give a 95% confidence interval for the population maximum \\(M\\). Monte Carlo would have us take the 2.5th and 97.5th percentiles.\n\nquantile(sim_S, c(.025, .975))\n\n     2.5%     97.5% \n0.8774311 0.9437517 \n\nmax(X)\n\n[1] 0.9443383\n\n\nIs this reasonable? Are we 95% sure that the population max is within that interval? Of course not! The sample max isn’t even within the interval. Clearly the sample maximum is not a good point estimate to use for the population maximum.\nIt turns out that \\(\\frac{n+1}{n}\\max(X_1,\\ldots, X_n)\\) is an unbiased estimator for the population maximum in this case.\n\n(M.hat &lt;- (n+1)*max(X)/n)\n\n[1] 0.963225\n\n\nLet’s see what the resulting interval looks like:\n\nsim_S &lt;- 0\nn &lt;- length(X) \nfor(i in 1:1000){\n  simData &lt;- runif(n, 0, M.hat)\n  sim_S[i] &lt;- (n+1)*max(simData)/n  #use the unbiased estimator each time\n}\nhist(sim_S, breaks=25)\nabline(v=max(X))\n\n\n\nquantile(sim_S, c(.025, .975))\n\n     2.5%     97.5% \n0.9119317 0.9819865 \n\n\nThis interval contains the sample maximum. And in fact the data was drawn from \\(\\text{Uniform}(0,.95)\\).\nWould it contain .95 (the true max) 95% of the time?\n\nNMC1 &lt;- 1000\nn &lt;- length(X) \ncontainsM &lt;- rep(FALSE, NMC1)\n\nfor(j in 1:NMC1){\n  X.sim &lt;- runif(n, 0, .95) #Generate a new sample\n  M.sim &lt;- (n+1)*max(X.sim)/n\n  sim_S &lt;- 0\n  for(i in 1:1000){\n    simData &lt;- runif(n, 0, M.sim)\n    sim_S[i] &lt;- (n+1)*max(simData)/n\n  }\n  containsM[j] &lt;- quantile(sim_S,.025) &lt;= .95 & quantile(sim_S, .975) &gt;= .95\n}\nmean(containsM)\n\n[1] 0.863\n\n\nOur coverage rate is not as good as we would hope. How does this improve with a larger sample size?\n\nns &lt;- seq(50,500,50)\nNMC1 &lt;- 200 #decreasing for speed\ncoverage &lt;- rep(0, length(ns))\nfor(k in 1:length(ns)){\n  n &lt;- ns[k]\n  containsM &lt;- rep(FALSE, NMC1)\n  \n  for(j in 1:NMC1){\n    X.sim &lt;- runif(n, 0, .95) #Generate a new sample\n    M.sim &lt;- (n+1)*max(X.sim)/n\n    sim_S &lt;- 0\n    for(i in 1:1000){\n      simData &lt;- runif(n, 0, M.sim)\n      sim_S[i] &lt;- (n+1)*max(simData)/n\n    }\n    containsM[j] &lt;- quantile(sim_S,.025) &lt;= .95 & quantile(sim_S, .975) &gt;= .95\n  }\n  coverage[k] &lt;- mean(containsM)\n}\n\nplot(x=ns, y=coverage, type=\"l\", main=\"coverage rate with increasing n\")\n\n\n\n\nOh that’s not pretty at all. Looks like our supposed 95% interval is more like an 86% interval. Why is this the case? Short answer - the population maximum / sample maximum relationship is not as friendly as the population mean/sample mean relationship. How could we modify our confidence interval estimate?\nTurns out (from a little research https://en.wikipedia.org/wiki/German_tank_problem) that the lower bound for a 95% confidence interval would be the sample max \\(\\max(X)\\) and the upper bound would be \\(\\max(X)/\\alpha^{1/n}\\)\nWe’ll produce the same plot, looking at sample sizes of 50, 100, …, 500 to see if there’s a trend\n\nns &lt;- seq(50,500,50)\nNMC1 &lt;- 1000 #decreasing for speed\ncoverage &lt;- rep(0, length(ns))\nfor(k in 1:length(ns)){\n  n &lt;- ns[k]\n  containsM &lt;- rep(FALSE, NMC1)\n  \n  for(j in 1:NMC1){\n    X.sim &lt;- runif(n, 0, .95) #Generate a new sample\n    containsM[j] &lt;- max(X.sim) &lt;= .95 & max(X.sim)/.05^(1/n) &gt;= .95\n  }\n  coverage[k] &lt;- mean(containsM)\n}\n\nplot(x=ns, y=coverage, type=\"l\", main=\"coverage rate with increasing n\")\n\n\n\n\nNo particular trend, a lot of fluctuation. It seems to be between .94 and .96 regardless of sample size. This is good justification for this approach."
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#central-limit-theorem-for-a-uniform-population",
    "href": "R07_More_Estimation_and_CI_Examples.html#central-limit-theorem-for-a-uniform-population",
    "title": "23  Interval Estimation Examples",
    "section": "24.3 Central Limit Theorem for a uniform population",
    "text": "24.3 Central Limit Theorem for a uniform population\n\ngenerate_data &lt;- function(n){\n  mydata &lt;- runif(n) #Letting the range be 0 to 1, because it\n                    # doesn't matter to me.\n  return(mean(mydata))\n}\n\nmy_means &lt;- replicate(10000, generate_data(5))\nhist(my_means, breaks=50)"
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#central-limit-theorem-of-a-very-very-not-normal-population",
    "href": "R07_More_Estimation_and_CI_Examples.html#central-limit-theorem-of-a-very-very-not-normal-population",
    "title": "23  Interval Estimation Examples",
    "section": "24.4 Central Limit Theorem of a very very not normal population",
    "text": "24.4 Central Limit Theorem of a very very not normal population\nSay the population looks like this: X takes two values, 0 and 100, with probabilities .10 and .90\n\nx &lt;- c(0, 100)\npx &lt;- c(.10, .90)\n\ngenerate_data &lt;- function(n,x,px){\n  mydata &lt;- sample(x, size=n, replace=TRUE, prob=px)\n  return(mean(mydata))\n}\n\nmy_means &lt;- replicate(10000, generate_data(300, x, px))\nhist(my_means, breaks=50)"
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#estimating-the-inter-quartile-range",
    "href": "R07_More_Estimation_and_CI_Examples.html#estimating-the-inter-quartile-range",
    "title": "23  Interval Estimation Examples",
    "section": "24.5 Estimating the inter-quartile range",
    "text": "24.5 Estimating the inter-quartile range\nThe population is assumed to be normally distributed with a mean of 100 but an unknown standard deviation. We wish to estimate the inter-quartile range - the difference between the 75th and 25th percentiles, and we want a 95% confidence interval for this range.\nHere is the data:\n\nmyNormalData &lt;- c(76.77379, 98.66781, 88.34336, 88.70260, 114.71026, 123.18747, 92.86254, 84.11664, 108.69649, 76.96178, 95.14971, 68.40800, 53.71018, 100.20321, 87.08193, 100.31221, 98.60364, 108.67020, 68.81813, 109.28832)\n\nIf we are to proceed with a Monte Carlo confidence interval for the IQR we need to do the following:\n\nEstimate any missing parameter in our model\nUse the estimated model to generate new data\nFrom each simulated dataset calculate a point estimate for the IQR\nRepeat until we have lots\nFind the 2.5th and 97.5th percentiles of those point estimates.\n\nNote: this is a little bit different than before because the missing parameter in the model is not the same thing as the parameter we’re trying to estimate.\n\nsigma.hat &lt;- sd(myNormalData) #I know this is slightly biased, but I \n                              #hope it's not going to matter much.\n                              #there are unbiased estimators of sigma \n                              #out there but I'm not going to use them.\nNMC &lt;- 10000\nsim.IQRs &lt;- 0 #lazy vector init\nfor(i in 1:NMC){\n  sim.data &lt;- rnorm(length(myNormalData),\n                    100, sigma.hat)\n  #I need a point estimate of the IQR.\n  #Let's use sample IQR\n  sim.IQRs[i] &lt;- IQR(sim.data)\n}\nhist(sim.IQRs)\n\n\n\nquantile(sim.IQRs, c(.025, .975)) #this is my approximate 95% conf interval.\n\n    2.5%    97.5% \n11.91755 33.83861 \n\n\nOr we could argue that the IQR from a normal population can be calculated from qnorm(.75,100,sigma) and qnorm(.25,100,sigma). If we use sigma.hat here we’d get the exact same value every time we calculate it. Maybe on each MC generated sample we re-calculate sigma.hat and use that. This seems like a valid method:\n\nNMC &lt;- 10000\nsim.IQRs2 &lt;- 0 #lazy vector init\nfor(i in 1:NMC){\n  sim.data &lt;- rnorm(length(myNormalData),\n                    100, sigma.hat)\n  #I need a point estimate of the IQR.\n  \n  sim.sigma.hat &lt;- sd(sim.data)\n  sim.IQRs2[i] &lt;- qnorm(.75,100,sim.sigma.hat)-qnorm(.25,100,sim.sigma.hat)\n}\nhist(sim.IQRs2)\n\n\n\nquantile(sim.IQRs2, c(.025, .975)) #this is my approximate 95% conf interval.\n\n    2.5%    97.5% \n16.23961 30.86386 \n\n\nIt’s a tighter interval - so the question is is it too narrow (is the coverage rate &lt; .95) or is the first interval using sample IQR too wide (coverage rate &gt; .95)\nWell, to answer that we’d have to run a simulation and estimate coverage rates of the two methods. Sounds like fun!\n\ncoverage.IQR &lt;- 0\ncoverage.qnorm &lt;- 0\n\nNMC1 &lt;- 1000\nNMC2 &lt;- 1000\ntrueIQR &lt;- qnorm(.75)-qnorm(.25)\nn &lt;- 20\nfor(i in 1:NMC1){\n  mc.data &lt;- rnorm(n) #what I use for mu and sigma doesn't matter, so we'll use a standard normal.\n  #I'll assume I know the mean is 0, but the sd is unknown.\n  sig.hat &lt;- sd(mc.data)\n  IQRs1 &lt;- 0; IQRs2 &lt;- 0;\n  for(j in 1:NMC2){\n    mc2.data &lt;- rnorm(n, 0, sig.hat)\n    IQRs1[j] &lt;- IQR(mc2.data)\n    IQRs2[j] &lt;- qnorm(.75,0,sd(mc2.data))-qnorm(.25,0,sd(mc2.data))\n  }\n  coverage.IQR[i] &lt;- quantile(IQRs1, .025) &lt;= trueIQR & quantile(IQRs1, .975) &gt;= trueIQR\n  coverage.qnorm[i] &lt;- quantile(IQRs2, .025) &lt;= trueIQR & quantile(IQRs2, .975) &gt;= trueIQR\n}\nmean(coverage.IQR)\n\n[1] 0.959\n\nmean(coverage.qnorm)\n\n[1] 0.892\n\n\nMy answer: the first interval was too wide and the second interval was too narrow. This procedure could probably be improved by using an unbiased estimate of \\(\\sigma\\). There is an unbiased \\(\\hat{\\sigma}\\) which you could read about (here)[https://en.wikipedia.org/wiki/Unbiased_estimation_of_standard_deviation]. A “rule of thumb” estimator we could use is \\(\\hat{\\sigma}=\\sqrt{\\frac{n-1}{n-1.5}}s\\)."
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#estimating-the-mean-of-a-normal-population",
    "href": "R07_More_Estimation_and_CI_Examples.html#estimating-the-mean-of-a-normal-population",
    "title": "23  Interval Estimation Examples",
    "section": "24.6 Estimating the mean of a normal population",
    "text": "24.6 Estimating the mean of a normal population\nSay that our population is a normal distribution With a mean of 50 and standard deviation of 13\n\nmu &lt;- 50\nsigma &lt;- 13\n\nLook at the variability of the sample mean with varying sample sizes\n\nsample.sizes &lt;- seq(1,200)\n\nx.bars &lt;- vector(\"numeric\")\nsds &lt;- vector(\"numeric\")\nfor(i in 1:200){\n  my.sample &lt;- rnorm(n=sample.sizes[i], mean=mu, sd=sigma)\n  x.bars[i]=mean(my.sample)  \n  sds[i]=sd(my.sample)\n}\nplot(x.bars)\nabline(h=mu)\n\n\n\nplot(sds)\nabline(h=sigma)\n\n\n\n\nEstimate the standard error of the sample mean based on NMC &lt;- 100\n\nNMC &lt;- 100\n\nstd.errors &lt;- vector(\"numeric\") \ntheoretical.standard.error &lt;- vector(\"numeric\")\nfor(i in 1:200){\n  \n  x.bars &lt;- vector(\"numeric\")\n  for(j in 1:NMC){\n    my.sample &lt;- rnorm(n=sample.sizes[i], mean=mu, sd=sigma)\n    x.bars[j]=mean(my.sample)  \n  }\n  std.errors[i] &lt;- sd(x.bars)\n  theoretical.standard.error[i] &lt;- sigma/sqrt(sample.sizes[i])\n}\n\nplot(std.errors, type=\"l\")\nlines(theoretical.standard.error, lty=2)\n\n\n\n\nSuppose we sample from this population with a sample size of 20\n\nNMC &lt;- 10000\nx.bars &lt;- vector(\"numeric\")\nfor(i in 1:NMC){\n  my.sample &lt;- rnorm(n=20, mean=mu, sd=sigma)\n  x.bars[i]&lt;- mean(my.sample)\n}\nhist(x.bars)\n\n\n\n\nFind an interval which contains 95% of the sample means\n\nquantile(x.bars, probs=c(0.025, 0.975))\n\n    2.5%    97.5% \n44.36661 55.84355 \n\n\nLet’s stop playing God. Let’s randomly pick the sample mean - some value between 0 and 100.\n\nmu &lt;- runif(1,0, 100)\n\n#The 'provided data', sigma = 12 I know, but let's pretend I don't\nmy.sample &lt;- rnorm(n=20, mean=mu, sd=sigma)\n\nmy.xbar &lt;- mean(my.sample)\nmy.sd &lt;- sd(my.sample) #I could possibly improve by using an unbiased estimate of population sd. \nNMC &lt;- 10000\nx.bars &lt;- vector(\"numeric\")\nfor(i in 1:NMC){\n  my.simulated.sample &lt;- rnorm(n=20, mean=my.xbar, sd=my.sd)\n  x.bars[i]&lt;- mean(my.simulated.sample)\n}\n\nEstimate my 95% confidence interval based on the quantiles\n\nhist(x.bars)\n\n\n\nbounds &lt;- quantile(x.bars, probs=c(0.025, 0.975), names=FALSE)\n\nDoes the confidence interval correctly capture the population mean?\n\nbounds[1] &lt;= mu & bounds[2] &gt;=mu\n\n[1] TRUE\n\n\nWhether it happened to accurately capture the population mean this time or not is not particularly interesting, what is more interesting is the overall coverage rate of this method.\nLet’s check the accuracy rate of this method\n\nmy.sample.size &lt;- 20\nNMC2 &lt;- 1000\ncorrect.count &lt;- 0\nmargin &lt;- vector(\"numeric\")\n\nfor(j in 1:NMC2){\n  my.sample &lt;- rnorm(n=my.sample.size, mean=mu, sd=sigma)\n  my.xbar &lt;- mean(my.sample)\n  my.sd &lt;- sd(my.sample)\n  NMC &lt;- 250\n  x.bars &lt;- vector(\"numeric\")\n  for(i in 1:NMC){\n    my.simulated.sample &lt;- rnorm(n=my.sample.size, mean=my.xbar, sd=my.sd)\n    x.bars[i]&lt;- mean(my.simulated.sample)\n  }\n  #Estimate my 95% confidence interval based on \n  bounds &lt;- quantile(x.bars, probs=c(.025, .975))\n  margin[j] &lt;- as.numeric(diff(bounds))\n  #does the confidence interval correctly capture the\n  #population mean?\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= mu & bounds[2] &gt;=mu)\n}\ncorrect.count/NMC2\n\n[1] 0.937\n\nmean(margin)\n\n[1] 10.93692\n\n\n95% middle probability, n=20 accuracy: 93.2% of the time mean margin: 10.96 is the average margin of error\nLet’s repeat this with a sample size of 40\n95% middle probability, n=40 accuracy:\nmean margin:\nThis increases the precision (i.e. decreases the margin) while keeping accuracy the same\nCan we get 100% accuracy rate? (nope!)\nwhat if the population standard deviation decreases\n\nsigma &lt;- 6\n\nLet’s check the accuracy rate of this method\n\nmy.sample.size &lt;- 20\nNMC2 &lt;- 1000\ncorrect.count &lt;- 0\nmargin &lt;- vector(\"numeric\")\n\nfor(j in 1:NMC2){\n  my.sample &lt;- rnorm(n=my.sample.size, mean=mu, sd=sigma)\n  my.xbar &lt;- mean(my.sample)\n  my.sd &lt;- sd(my.sample)\n  NMC &lt;- 250\n  x.bars &lt;- vector(\"numeric\")\n  for(i in 1:NMC){\n    my.simulated.sample &lt;- rnorm(n=my.sample.size, mean=my.xbar, sd=my.sd)\n    x.bars[i]&lt;- mean(my.simulated.sample)\n  }\n  #Estimate my 95% confidence interval based on \n  bounds &lt;- quantile(x.bars, probs=c(.025, .975))\n  margin[j] &lt;- as.numeric(diff(bounds))\n  #does the confidence interval correctly capture the\n  #population mean?\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= mu & bounds[2] &gt;=mu)\n}\ncorrect.count/NMC2\n\n[1] 0.924\n\nmean(margin)\n\n[1] 5.068632"
  },
  {
    "objectID": "R07_More_Estimation_and_CI_Examples.html#some-examples-of-confidence-interval-estimation",
    "href": "R07_More_Estimation_and_CI_Examples.html#some-examples-of-confidence-interval-estimation",
    "title": "23  Interval Estimation Examples",
    "section": "24.7 Some examples of confidence interval estimation",
    "text": "24.7 Some examples of confidence interval estimation\n\n24.7.1 Geometric distribution with unknown p\nUse MC to estimate the coverage rate of the CLT method for estimating the mean of a geometric with p=.33\n\np &lt;- .33 ; n &lt;- 30\ntrue_mean &lt;- (1-p)/p\nNMC2 &lt;- 1000 #this is for estimating the true coverage rate\n            # i.e. is it 95% or is it not?\ncorrect.count &lt;- 0\nfor(j in 1:NMC2){\n  my.sample &lt;- rgeom(n=n, prob=p)\n  xbar &lt;- mean(my.sample)\n  varx &lt;- var(my.sample)\n  #Let's do a 95% confidence interval for the population parameter p\n  bounds &lt;- c(xbar - 1.96 * sqrt(varx/n), \n              xbar + 1.96 *sqrt(varx/n))\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= true_mean & bounds[2] &gt;=true_mean)\n}\ncorrect.count / NMC2\n\n[1] 0.919\n\n\nThe central limit theorem, while this is nominally a “95%” confidence interval, in practice, for a Geometric population and sample size 30, it only gives about 92% coverage rate. Not amazing.\nLet’s compare to a MC method - this does not assume normality at all. But it does require that we estimate p in order to parameterize our model. Use MC to simulate many samples from a Geometric with p.hat as the parameter\n\nNMC2 &lt;- 1000   #this MC estimate is still for estimating coverage rate\ncorrect.count &lt;- 0\nfor(j in 1:NMC2){\n  my.sample &lt;- rgeom(n=n, prob=p)\n  p.hat &lt;- 1 / (mean(my.sample) + 1) #This here is a point estimate for p\n  \n  NMC &lt;- 1000 # this MC replication is for estimating the sampling\n              # distribution of XBAR\n  xbars &lt;- vector(\"numeric\")\n  for(i in 1:NMC){\n    simulated.sample &lt;- rgeom(n=n, prob=p.hat)\n    xbars[i] &lt;- mean(simulated.sample)\n  }\n  #Let's do a 95% confidence interval for the population parameter p\n  bounds &lt;- quantile(xbars, probs = c(.025, .975))\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= true_mean & bounds[2] &gt;=true_mean)\n}\ncorrect.count / NMC2\n\n[1] 0.926\n\n\nLooks like we do a better job by not assuming the sample means are normally distributed (because they are not!!!!)\n\n# how are the sample means distributed?\nhist(xbars, breaks=20)\n\n\n\n\nThe little bit of skew actually causes the CLT confidence interval to be off the mark and the coverage rate suffer because of that.\n\n\n24.7.2 Another Geometric Estimation example\nSuppose we have a geometric random variable (i.e population) with some unknown p\n\np &lt;- runif(1)\n\nfact: mean of a geometric distribution is \\((1-p)/p\\), variance of a geometric is \\((1-p)/p^2\\).\nUse MC to simulate many samples from a Geometric with p.hat as the parameter\n\nNMC2 &lt;- 1000\ncorrect.count &lt;- 0\nfor(j in 1:NMC2){\n  my.sample &lt;- rgeom(n=25, prob=p)\n  p.hat &lt;- 1 / (mean(my.sample) + 1)\n  NMC &lt;- 1000\n  p.hats &lt;- vector(\"numeric\")\n  for(i in 1:NMC){\n    simulated.sample &lt;- rgeom(n=25, prob=p.hat)\n    p.hats[i] &lt;- 1 / (mean(simulated.sample) + 1)\n  }\n  #Let's do a 90% confidence interval for the population parameter p\n  bounds &lt;- quantile(p.hats, probs = c(.05, .95))\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= p & bounds[2] &gt;=p)\n}\ncorrect.count / NMC2\n\n[1] 0.884\n\n\n\n\n24.7.3 Estimate the rate from a Poisson population\nLet’s use the central limit theorem for a Poisson population\nLet’s get a 95% confidence interval\n\nrate &lt;- runif(1,5, 100)\nNMC2 &lt;- 100000\ncorrect.count &lt;- 0\nfor(j in 1:NMC2){\n  my.sample &lt;- rpois(n=100, lambda=rate)\n\n  x.bar &lt;- mean(my.sample)\n  s &lt;- sd(my.sample)\n  \n  bounds &lt;- x.bar + qnorm(c(.025, .975)) * s/sqrt(100)\n  correct.count &lt;- correct.count + as.numeric(bounds[1] &lt;= rate & bounds[2] &gt;=rate)\n}\ncorrect.count / NMC2\n\n[1] 0.94797"
  }
]