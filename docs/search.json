[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 340: Data Science II",
    "section": "",
    "text": "0.1 Collection of Shiny Apps\nHere are the course notes, practice problems and other course materials for STAT 340: Data Science II at UW Madison.\nAuthor credits to Karl Rohe, Keith Levin, Bi Cheng Wu, and Brian Powers",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>STAT 340</span>"
    ]
  },
  {
    "objectID": "index.html#collection-of-shiny-apps",
    "href": "index.html#collection-of-shiny-apps",
    "title": "STAT 340: Data Science II",
    "section": "",
    "text": "Monte Carlo: Normal PDF CDF Inverse CDF \nTesting Part 2: Binomial One/Two Tail Test Power\nLogistic Regression: ROC Plot\nModel Selection: Visualization",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>STAT 340</span>"
    ]
  },
  {
    "objectID": "rv_practice.html",
    "href": "rv_practice.html",
    "title": "5  Probability and Random Variables Practice",
    "section": "",
    "text": "6 Practice Problems",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#phone-repair",
    "href": "rv_practice.html#phone-repair",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.1 Phone Repair",
    "text": "6.1 Phone Repair\n(From Introduction to Probability and Statistics for Data Science) The repair of a broken cell phone is either completed on time or it is late and the repair is either done satisfactorily or unsatisfactorily. What is the sample space for a cell phone repair?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sample space consists of two outcomes: {satisfactory, unsatisfactory}",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-i",
    "href": "rv_practice.html#dice-probabilities-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.2 Dice Probabilities I",
    "text": "6.2 Dice Probabilities I\nWhat is the probability that at least one of the following events occurs on a single throw of two fair six-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe probability that at least one of the following occurs requires us to look at the union of the events. “The dice total is 5, 6 or 7”.\nOn a single throw of two fair dice, the dice total can be a single number, so events A, B and C are mutually exclusive. We can calculate the probabilities individually and add them up.\nA 5 can occur in 4 ways: \\((1,4), (2,3), (3,2), (4,1)\\) are the four ways (we are distinguishinng the two dice).\nA 6 can occur in 5 ways: \\((1,5), (2,4), (3,3), (4,2), (5,1)\\).\nA 7 can occur in 6 ways: \\((1,6), (2,5), (3,4), (4,3), (5,2), (6,1)\\)\nIn the end, there are \\(4+5+6 = 15\\) ways in which one of these events can occur. Since there are \\(6\\times 6=36\\) two-dice outcomes, the probability is \\[\\frac{15}{36}\\]",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-ii",
    "href": "rv_practice.html#dice-probabilities-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.3 Dice Probabilities II",
    "text": "6.3 Dice Probabilities II\nWhat is the probability that at least one of the following events occurs on a single throw of two fair four-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA 5 can occur in 4 ways: \\((1,4), (2,3), (3,2), (4,1)\\) are the four ways (we are distinguishing the two dice).\nA 6 can occur in 3 ways: \\((2,4), (3,3), (4,2)\\).\nA 7 can occur in 2 ways: \\((3,4), (4,3)\\)\nIn the end, there are \\(4+3+2 = 9\\) ways in which one of these events can occur. Since there are \\(4\\times 4=16\\) two-dice outcomes, the probability is \\[\\frac{9}{16}\\]",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-fair-coins",
    "href": "rv_practice.html#two-fair-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 Two Fair Coins",
    "text": "6.4 Two Fair Coins\nAlex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets.\n\n\nFind and sketch the joint PMF \\(p_{X,Y} (x, y)\\).\nFind the marginal PMF \\(p_X(x)\\) and \\(p_Y (y)\\).\nFind the conditional PMF \\(P_{X|Y} (x | y)\\). Does \\(P_{X|Y} (x | y) = P_X(x)\\)? Why or why not?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- 0:1\npx &lt;- c(.25, .75) \n\ny&lt;- 0:1\npy &lt;- c(.75, .25)\n\npmf &lt;- px %*% t(py)\nrownames(pmf) &lt;- x\ncolnames(pmf) &lt;- y \naddmargins(pmf)\n\n         0      1  Sum\n0   0.1875 0.0625 0.25\n1   0.5625 0.1875 0.75\nSum 0.7500 0.2500 1.00\n\n\n\nprop.table(pmf, 2)\n\n     0    1\n0 0.25 0.25\n1 0.75 0.75\n\n\nEach column gives P(X=x|Y=y) for the two values of y; you can see that they are the same; the reason is because the value of X and Y are independent.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-fair-dice",
    "href": "rv_practice.html#two-fair-dice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 Two Fair Dice",
    "text": "6.5 Two Fair Dice\nTwo fair dice are rolled. Find the joint PMF of \\(X\\) and \\(Y\\) when\n\n\n\\(X\\) is the larger value rolled, and \\(Y\\) is the sum of the two values.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmax(die1, die2)\noutcomes$y &lt;- die1+die2\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             2          3          4          5          6          7\n  1 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.05555556 0.02777778 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.05555556 0.05555556 0.02777778 0.00000000\n  4 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556\n   \n             8          9         10         11         12\n  1 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  4 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000\n  5 0.05555556 0.05555556 0.02777778 0.00000000 0.00000000\n  6 0.05555556 0.05555556 0.05555556 0.05555556 0.02777778\n\n\n\n\n\n\n\\(X\\) is the smaller, and \\(Y\\) is the larger value rolled.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmin(die1, die2)\noutcomes$y &lt;- pmax(die1,die2)\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             1          2          3          4          5          6\n  1 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n  2 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556\n  3 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556\n  4 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#signal-and-noise",
    "href": "rv_practice.html#signal-and-noise",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.6 Signal and Noise",
    "text": "6.6 Signal and Noise\nLet \\(Y = X+N\\), where \\(X\\) is the input, \\(N\\) is the noise, and \\(Y\\) is the output of a system. Assume that \\(X\\) and \\(N\\) are independent random variables. It is given that \\(E[X] = 0\\), \\(Var[X] = \\sigma^2_X\\), \\(E[N] = 0\\), and \\(Var[N] = \\sigma^2_N\\).\n\n\nFind the correlation coefficient \\(\\rho\\) between the input \\(X\\) and the output \\(Y\\) .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\rho(X,Y)= \\dfrac{Cov(X,Y)}{SD(X)SD(Y)}=\\dfrac{E(XY)-E(X)E(Y)}{\\sigma_X \\cdot \\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(E(XY)=E(X^2+XN)=E(X^2)+E(X)E(N)\\). Because \\(E(X)=E(N)=0\\) this simplifies to\n\\(\\rho(X,y)=\\dfrac{E(X^2)}{\\sigma_x\\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(Var(X)=E(X^2)-E(X)^2 = E(X^2)\\) so we can replace the numerator with \\(\\sigma^2_X\\). So\n\\(\\rho(X,Y) = \\sqrt{\\dfrac{\\sigma^2_X}{\\sigma^2_X+\\sigma^2_N}}\\)\n\n#example: sigma_X = 5, sigma_N=2\nX &lt;- rnorm(10000, 0, 5)\nN &lt;- rnorm(10000, 0, 2)\ncor(X, X+N)\n\n[1] 0.9296617\n\nsqrt(5^2/(5^2+2^2))\n\n[1] 0.9284767\n\n\n\n\n\n\nSuppose we estimate the input \\(X\\) by a linear function \\(g(Y ) = aY\\) . Find the value of a that minimizes the mean squared error \\(E[(X − aY )^2]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E[(X-aY)^2]=E[X^2-2aXY+a^2Y^2]=E[X^2]-2aE[XY]+a^2E[Y^2]\\)\nBecause \\(E(Y)=0\\), \\(E[Y^2]=Var(Y)=\\sigma_X^2+\\sigma_N^2\\). We already have that \\(E(X^2)=\\sigma_X^2\\) and \\(E(XY)=\\sigma_X^2\\). Thus\n$E[(X-aY)^2]=(_X^2+_N2)a2- 2_X^2a+_X^2 $\nThis is a quadratic function in \\(a\\), and the vertex can be found at \\(-\\frac{B}{2A}\\) or in this case \\(a^*=\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\)\n\n\n\n\nExpress the resulting mean squared error in terms of \\(\\eta = \\sigma^2_X/\\sigma^2_N\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPlugging this in for \\(a\\) we get\n\\(E[(X-aY)^2]=(\\sigma_X^2+\\sigma_N^2)\\dfrac{\\sigma_X^4}{(\\sigma_X^2+\\sigma_N^2)^2}-2\\dfrac{\\sigma_X^4}{\\sigma_X^2+\\sigma_N^2}+\\sigma_X^2=\\sigma_X^2\\left(1-\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\right)\\)\n\\(=\\sigma_X^2\\left(\\dfrac{\\sigma_N^2}{\\sigma_X^2+\\sigma_N^2}\\right)=\\dfrac{\\sigma_X^2}{\\eta+1}\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#cat-genetics",
    "href": "rv_practice.html#cat-genetics",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.7 Cat Genetics",
    "text": "6.7 Cat Genetics\nThe gene that controls white coat color in cats, KIT , is known to be responsible for multiple phenotypes such as deafness and blue eye color. A dominant allele W at one location in the gene has complete penetrance for white coat color; all cats with the W allele have white coats. There is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. However, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors.\n\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes.\nAbout 73% of white cats with two blue eyes are deaf\n40% of white cats with one blue eye are deaf.\nOnly 19% of white cats with other eye colors are deaf.\n\n\nCalculate the prevalence of deafness among white cats.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn.blue &lt;- c(0,1,2)\np.n.blue &lt;- c(.60, .30, .10)\n\np.deaf.given.b &lt;- c(.19, .40, .73) #for 0, 1 ,2 blue eyes\n\n#P(Deafness) = P(0b)*P(deaf|0b) + P(1b)*P(deaf|1b) + P(2b)*P(deaf|2b)\n(p.deaf &lt;- sum(p.n.blue * p.deaf.given.b))\n\n[1] 0.307\n\n#Check\nnCats &lt;- 10000\nnblue &lt;- sample(n.blue, size=nCats, replace=TRUE, prob=p.n.blue)\nisdeaf &lt;- FALSE\nfor(i in 1:nCats){\n  isdeaf[i] &lt;- runif(1) &lt; p.deaf.given.b[nblue[i]+1]\n}\nmean(isdeaf)\n\n[1] 0.3036\n\n\n\n\n\n\nGiven that a white cat is deaf, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | deaf) = P(2b)*P(deaf|2b) / p(deaf)\np.n.blue[3] * p.deaf.given.b[3] / p.deaf\n\n[1] 0.237785\n\n#check\nmean(nblue[isdeaf]==2)\n\n[1] 0.2341897\n\n\n\n\n\n\nSuppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.\nWhat is the prevalence of blindness among deaf, white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\np.blind.given.nodeaf &lt;- 0.10\np.blind.given.deaf.and.nblue &lt;- c(0.20, 0.4, 0.2) #for 0, 1, 2 blue eyes\n\n#P(blind & deaf) = P(0b)*P(deaf|0b)*P(blind|deaf&0b)+\n#  P(1b)*P(deaf|1b)*P(blind|deaf&1b)+\n#  P(2b)*P(deaf|2b)*P(blind|deaf&2b)+\np.blind.and.deaf &lt;- sum(p.n.blue * p.deaf.given.b * p.blind.given.deaf.and.nblue)\n\n#P(blind | deaf ) = P(blind & deaf) / P(deaf)\n(p.blind.given.deaf = p.blind.and.deaf / p.deaf)\n\n[1] 0.2781759\n\n#check\nisBlind &lt;- FALSE\nfor(i in 1:nCats){\n  if(!isdeaf[i]){\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.nodeaf\n  } else {\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.deaf.and.nblue[nblue[i]+1]\n  }\n}\n#check\nmean(isBlind[isdeaf])\n\n[1] 0.2684453\n\n\n\n\n\n\nWhat is the prevalence of blindness among white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(blind) = P(deaf & blind) + P(nondeaf & blind)\n\n#P(nondeaf & blind ) = P(nondeaf) * P(blind | nondeaf)\np.blind.and.nondeaf &lt;- (1-p.deaf)*p.blind.given.nodeaf\n\n(p.blind &lt;- p.blind.and.deaf + p.blind.and.nondeaf)\n\n[1] 0.1547\n\n#check\nmean(isBlind)\n\n[1] 0.1539\n\n\n\n\n\n\nGiven that a cat is white and blind, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | blind) = P(2b & blind) / p(blind)\n#numerator: P(2b & blind) = P(2b) * [P(deaf|2b) * p(blind|deaf & 2b) + P(nondeaf|2b) * p(blind|nondeaf & 2b)]\np.blind.and.2b &lt;- p.n.blue[3] * (p.deaf.given.b[3]*p.blind.given.deaf.and.nblue[3] + \n (1-p.deaf.given.b[3]) * p.blind.given.nodeaf)\n\n(p.2b.given.blind = p.blind.and.2b / p.blind)\n\n[1] 0.1118293\n\n#check\nmean(nblue[isBlind]==2)\n\n[1] 0.1052632",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-i",
    "href": "rv_practice.html#gss-survey-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 GSS Survey I",
    "text": "6.8 GSS Survey I\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable?\n\n\nLinda is a banker.\nLinda is a banker and considers herself a liberal Democrat.\n\nTo answer this question we will use data from the GSS survey found at https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv.\nThe code for “Banking and related activities” in the indus10 variable is 6870. The values of the column sex are encoded like this:\n1: Male, 2: Female\nThe values of polviews are on a seven-point scale:\n1 Extremely liberal\n2 Liberal\n3 Slightly liberal\n4 Moderate\n5 Slightly conservative\n6 Conservative\n7 Extremely conservative\nDefine “liberal” as anyone whose political views are 3 or below. The values of partyid are encoded:\n0 Strong democrat\n1 Not strong democrat\n2 Independent, near democrat\n3 Independent\n4 Independent, near republican\n5 Not strong republican\n6 Strong republican\n7 Other party\nYou need to compute:\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngss &lt;- read.csv(\"data/gss_bayes.csv\")\n#banker : indus10==6870\n#female : sex == 2\n#liberal: polviews &lt;= 3\n#democrat: partyid &lt;= 2\n\n#In my reading, I am interpreteing that 'Linda is female' is given from the context;\n\n#P(banker | female) \nmean(gss[gss$sex==2,]$indus10==6870)\n\n[1] 0.02116103\n\n#P(liberal banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3,]$indus10==6870)\n\n[1] 0.01723195\n\n#P(liberal Dem banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3 & gss$partyid &lt;= 1,]$indus10==6870)\n\n[1] 0.01507289",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-ii",
    "href": "rv_practice.html#gss-survey-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.9 GSS Survey II",
    "text": "6.9 GSS Survey II\nCompute the following probabilities:\n\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$partyid&lt;=1,]$polviews&lt;=3)\n\n[1] 0.389132\n\n\n\n\n\n\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews&lt;=3,]$partyid&lt;=1)\n\n[1] 0.5206403",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#gss-survey-iii",
    "href": "rv_practice.html#gss-survey-iii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.10 GSS Survey III",
    "text": "6.10 GSS Survey III\nThere’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65. For these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\nCompute the following probabilities. For each statement, think about whether it is expressing a conjunction, a conditional probability, or both. For the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &lt;30 & gss$polviews &lt;=3)\n\n[1] 0.06579428\n\n\n\n\n\n\nWhat is the probability that a young person is liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$age &lt;30,]$polviews &lt;=3)\n\n[1] 0.3385177\n\n\n\n\n\n\nWhat fraction of respondents are old conservatives?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &gt;65 & gss$polviews &gt;= 5)\n\n[1] 0.06226415\n\n\n\n\n\n\nWhat fraction of conservatives are old?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews &gt;=5,]$age &gt;65)\n\n[1] 0.1820933",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-child-paradox",
    "href": "rv_practice.html#two-child-paradox",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.11 Two Child Paradox",
    "text": "6.11 Two Child Paradox\nSuppose you meet someone and learn that they have two children. You ask if either child is a girl and they say yes. What is the probability that both children are girls? (Hint: Start with four equally likely hypotheses.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBefore we know anything about their two kids, the number of girls \\(X\\) they have could be 0, 1 or 2. Simplifying the scenario with the ‘equally likely hypothesis’ means that we assume each kid has a 50% chance of being a girl, independently. Thus the probabilities are 0.25, 0.5 and 0.25 respectively.\nIf we learn that at least one of the kids is a girl, that tells us that the first possibility, that \\(x=0\\) is not possible. Thus \\(P(X=2 | X&gt;0) = .\\dfrac{25}{.5+.25}= \\frac{1}{3}\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#monty-hall",
    "href": "rv_practice.html#monty-hall",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.12 Monty Hall",
    "text": "6.12 Monty Hall\nThere are many variations of the Monty Hall problem. For example, suppose Monty always chooses Door 2 if he can, and only chooses Door 3 if he has to (because the car is behind Door 2).\n\n\nIf you choose Door 1 and Monty opens Door 2, what is the probability the car is behind Door 3?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#3 equally likely possibilities:\n#C1  -&gt; Monty chooses Door 2\n#C2  -&gt; Monty cannot choose Door 2, chooses Door 3\n#C3  -&gt; Monty chooses Door 2\n\n#So if Monty chooses Door 2, either C1 or C3 must be the case, each equally likely. \n#Thus P(C3 | Monty chooses 2) = .50\n\n\n\n\n\nIf you choose Door 1 and Monty opens Door 3, what is the probability the car is behind Door 2?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#If he chose Door 3, that means that it must be the case that the car is behind \n#door 2; he would *ONLY* choose door 3 in that case.\n\n#Thus P(C2 | Monty chooses 3) = 1.0",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#mms",
    "href": "rv_practice.html#mms",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.13 M&Ms",
    "text": "6.13 M&Ms\nM&M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M’s, changes the mixture of colors from time to time. In 1995, they introduced blue M&M’s.\n\nIn 1994, the color mix in a bag of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan.\nIn 1996, it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n\nSuppose a friend of mine has two bags of M&M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n(Hint: The trick to this question is to define the hypotheses and the data carefully.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(G|94) = .10, P(G|96) = .20\\) \\(P(Y|94) = .20, P(Y|96) = .14\\) \\(P(94)=P(96)=.50\\) assuming equally likely. It’s important to realize only one of two situations could have occurred:\n\nSituation 1: A G was chosen from the 94 bag and a Y was chosen from the 96 bag\nSituation 2: A Y was chosen from the 94 bag and a G was chosen from the 96 bag.\n\nThe corresponding probabilities are \\((.10)(.14)=.014\\) and \\((.20)(.20)=.04)\\). The question could be stated: What is the probability that Sit.1 occurred given that either Sit1 or Sit2 occured.\nThe likelihood is \\(.014 / (.014+.04) = .2592593\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-coins",
    "href": "rv_practice.html#two-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 Two Coins",
    "text": "6.14 Two Coins\nSuppose you have two coins in a box. One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides. You choose a coin at random and see that one of the sides is heads. What is the probability that you chose the trick coin?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is actually similar to the “family with two girls” problem. The equally likely coins are “HT” and “HH”.\n\\(P(Heads | HH) = 1\\) \\(P(Heads | TH) = .5\\)\nBayes Theorem tells us that \\(P(HH | Heads) = \\dfrac{P(HH)*P(Heads|HH)}{P(HH)P(Heads|HH)+P(TH)P(Heads|TH)}=\\dfrac{.5}{.5+.25}=\\frac23\\)\nSeems counter intuitive! If I pick a random coin from the two, you already know that one of the sides is a head without looking at it. Somehow then, when you look at just one side of the coin, seeing a H makes you 67% sure that it is the trick coin. The reason is that seeing the head side is like flipping it once and getting a H. That is less likely to occur with the fair coin, hence this outcome lends evidence to the “trick coin” hypothesis.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#online-sales-i",
    "href": "rv_practice.html#online-sales-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 Online Sales I",
    "text": "6.15 Online Sales I\n(From Introduction to Probability and Statistics for Data Science) Customers can buy one of four produts, each having its own web page with a ``buy’’ link. When they click on the link, they are redirected to a common page containing a registration and payment orm. Once there the customer eithe rbuys the desired product (labeled 1,2,3 or 4) or they fail to complete and a sale is lost. Let event \\(A_i\\) be that a customer is on produt \\(i\\)’s web page and let event \\(B\\) be the event that the customer buys the product. For the purposes of this problem, assume that each potential customer visits at most one product page and so he or she buys at most one produt. For the probabilities shown in the table below, find the probability that a customer buys a product.\n\n\n\nProduct \\((i)\\)\n\\(Pr[B|A_i]\\)\n$Pr[A_i]\n\n\n\n\n1\n\\(0.72\\)\n\\(0.14\\)\n\n\n2\n\\(0.90\\)\n\\(0.04\\)\n\n\n3\n\\(0.51\\)\n\\(0.11\\)\n\n\n4\n\\(0.97\\)\n\\(0.02\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#online-sales-ii",
    "href": "rv_practice.html#online-sales-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 Online Sales II",
    "text": "6.16 Online Sales II\nContinuing from the previous problem, if a random purchase is selected, find the probability that it was item 1 that was purchased. Do the same for items 2, 3 and 4.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#uav",
    "href": "rv_practice.html#uav",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 UAV",
    "text": "6.17 UAV\nConsider a new type of commercial unmanned aerial vehicle (UAV) that has been outfitted with a transponder so that if it crashes it can easily be found and reused. Other older UAVs do not have transponders. Eighty percent of al lUAVs are recovered and, of those recovered, 75% have a transponder. Further, of those not recovered, 90% do not have a transponder. Denote recovery as \\(R+\\) and failure to recover as \\(R-\\). Denote having a transponder as \\(T+\\) and not having a transponder as \\(T-\\). Find\n\n\\(Pr[T+]\\)\nThe probability of not recovering a UAV given that it has a transponder.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#disease-screening",
    "href": "rv_practice.html#disease-screening",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.18 Disease Screening",
    "text": "6.18 Disease Screening\nSuppose that the test for a disease has sensitivity 0.90 and specificity 0.999. The base rate for the disease is 0.002. Find\n\nThe probability that someone selected at random from the population tests positive.\nThe probability that the person has the diseases given a positive test result (the positive predictive value).\nThe probability that the person does not have the disease given a negative test result (the negative predictive value).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#valid-pmf",
    "href": "rv_practice.html#valid-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.19 Valid PMF",
    "text": "6.19 Valid PMF\n(From Introduction to Probability and Statistics for Data Science) Let \\(X\\) have pmf \\[p(x)=\\begin{cases}0.2,&x=1\\\\0.3,&x=2\\\\0.5,&x=4\\\\0,&\\text{otherwise}\\end{cases}\\] Show that this is a valid pmf.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-dice-pmf",
    "href": "rv_practice.html#two-dice-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.20 Two Dice PMF",
    "text": "6.20 Two Dice PMF\nLet \\(X\\) denote the sum of two fair, six-sided dice. Specify the domain that \\(X\\) can take on and then write out the equation for the PMF and show that it is valid.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-practice",
    "href": "rv_practice.html#pmf-practice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.21 PMF practice",
    "text": "6.21 PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#dice-difference",
    "href": "rv_practice.html#dice-difference",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.22 Dice Difference",
    "text": "6.22 Dice Difference\nTwo dice are tossed. Let \\(X\\) be the absolute difference in the number of dots facing up.\n\n\nFind and plot the pmf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#It may be simplest to calculate all possible values of X. \nx &lt;- vector(\"numeric\")\nfor(i in 1:6){\n  for(j in 1:6){\nx[length(x)+1] = abs(i-j)\n  }\n}\n#now that we have all equally likely values, we can just calculate the pmf in a prop.table\npX &lt;- prop.table(table(x))\n#And create a barplot.\nbarplot(pX)\n\n\n\n\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\leq 2\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The probability that X &lt;= 2 is easy to find using the pmf\n#The columns are named with strings, so we can convert 0 1 and 2 to strings to pull out the proper probabilities.\nsum(pX[as.character(0:2)])\n\n[1] 0.6666667\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\) and \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The expected value and variance can be calculated from the pmf.\n\n(EX &lt;- sum(0:5 * pX))\n\n[1] 1.944444\n\n(VarX &lt;- sum((0:5 - EX)^2 * pX))\n\n[1] 2.052469\n\n#or by taking the mean and population variance of the x values themselves\nmean(x)\n\n[1] 1.944444\n\nmean((x-mean(x))^2)\n\n[1] 2.052469",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#voltage-random-variable",
    "href": "rv_practice.html#voltage-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.23 Voltage random variable",
    "text": "6.23 Voltage random variable\nA modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term that is drawn from the set \\(\\{0,-1,-2,-3\\}\\) with respective probabilities \\(\\{4/10, 3/10, 2/10, 1/10\\}\\).\n\n\nFind the pmf of the output \\(Y\\) of the channel.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Let X be the noise\nk &lt;- seq(0,-3)\npk &lt;- c(4/10, 3/10, 2/10, 1/10)\n\ny &lt;- sort(2+k)\npy &lt;- pk[order(2+k)]\n\nbarplot(height=py, names=y, main=\"pmf of Y\")\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the probability that the channel’s output is equal to the input of the channel?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis happens when there’s no noise, with probability 4/10.\n\n\n\n\nWhat is the probability that the channel’s output is positive?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Interpreting 'positive' to be strictly positive, not zero:\nsum(py[y&gt;0])\n\n[1] 0.7\n\n\n\n\n\n\nFind the expected value and variance of \\(Y\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n(EY &lt;- sum(y*py))\n\n[1] 1\n\n(VarY &lt;- sum((y-EY)^2*py))\n\n[1] 1",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#golf-score",
    "href": "rv_practice.html#golf-score",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.24 Golf Score",
    "text": "6.24 Golf Score\nOn a given day, your golf score takes values from the numbers 1 through 10 with equal probability of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let \\(X_1, X_2\\) and \\(X_3\\) be the scores that you get, and let \\(X\\) be the minimum of these three scores.\n\n\nShow that for any discrete random variable \\(X\\) \\(p_X(k)=\\mathbb{P}(X &gt; k-1) - \\mathbb{P}(X&gt;k)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k-1) = P(X \\geq k) =P(X = k)+P(X &gt; k)\\), thus \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)\\).\n\n\n\n\nWhat is the probability that \\(\\mathbb{P}(X_1&gt;k)\\) for \\(k=1,\\ldots,10\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X_1&gt;k)=\\frac{10-k}{10}\\)\n\n\n\n\nUse (a) to determine the pmf \\(p_X(k)\\) for \\(k=1,\\ldots,10\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k) = P(X_1,X_2,X_3 &gt; k) = P(X_1 &gt; k)P(X_2&gt;k)P(X_3&gt;k)\\)\nThis means \\(P(X&gt;k) =\\frac{(10-k)^3}{10^3}\\), and \\(P(X&gt;k-1)=\\frac{(11-k)^3}{10^3}\\). From the previous result, \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)=\\frac{(11-k)^3-(10-k)^3}{10^3}\\)\n\n\n\n\nWhat is the average score improvement if you play just for one day compared with playing for three days and taking the minimum?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is asking to take the difference of the two expected values. It’s obvious that \\(E(X_1)=5.5\\); We need to find the expected value of \\(X\\).\n\nx &lt;- 1:10\npx &lt;- ((11-x)^3-(10-x)^3)/10^3\n#double check\nsum(px)\n\n[1] 1\n\n(EX &lt;- sum(x*px))\n\n[1] 3.025\n\n5.5-EX\n\n[1] 2.475\n\n\nThe average (expected) point improvement when going from a 1 day point to a minimum of 3 days is 2.475.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#functions-of-a-random-variable",
    "href": "rv_practice.html#functions-of-a-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.25 Functions of a random variable",
    "text": "6.25 Functions of a random variable\nLet \\(g(X) = \\begin{cases}1 & \\text{if }X&gt;10\\\\0 & \\text{otherwise}\\end{cases}\\) and \\(h(X) = \\begin{cases}X-10 & \\text{if }X-10&gt;0\\\\0 & \\text{otherwise}\\end{cases}\\)\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_k = p_1/k\\), find \\(\\mathbb{E}\\left[g(X)\\right]\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nk &lt;- 1:15\np1 &lt;- 1/(sum(1/k))\npk &lt;- p1/k\n\ng &lt;- function(x){\n  return(as.numeric(x&gt;10))\n}\n\nsum(g(k)*pk)\n\n[1] 0.1173098\n\n\n\n\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_{k+1} = p_k/2\\) (for \\(k&gt;1\\)), find \\(\\mathbb{E}\\left[h(X)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nh &lt;- function(x){\n  return(max(0, x-10))\n}\n\np1 &lt;- 1/(sum(1/2^(k-1)))\npk &lt;- p1*(1/2^(k-1))\n\nsum(h(k)*pk)\n\n[1] 5",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#voltage-ii",
    "href": "rv_practice.html#voltage-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.26 Voltage II",
    "text": "6.26 Voltage II\nA voltage \\(X\\) is uniformly distributed on the set \\(\\{-3,\\ldots,3,4\\}\\).\n\n\nFind the mean and variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- -3:4\npx &lt;- 1/length(x)\n\n(EX &lt;- sum(x*px))\n\n[1] 0.5\n\n(VarX &lt;- sum((x-EX)^2*px))\n\n[1] 5.25\n\n\n\n\n\n\nFind the mean and variance of \\(Y=-2X^2+3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny &lt;- -2*x^2+3\n\n(EY &lt;- sum(y*px))\n\n[1] -8\n\n(VarY &lt;- sum((y-EY)^2*px))\n\n[1] 105\n\n\n\n\n\n\nFind the mean and variance of \\(W=\\text{cos}(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- cos(pi*x/8)\n(EW &lt;- sum(w*px))\n\n[1] 0.6284174\n\n(VarW &lt;- sum((w-EW)^2*px))\n\n[1] 0.1050915\n\n\n\n\n\n\nFind the mean and variance of \\(Z=\\text{cos}^2(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz &lt;- w^2\n(EZ &lt;- sum(z*px))\n\n[1] 0.5\n\n(VarZ &lt;- sum((z-EZ)^2*px))\n\n[1] 0.125",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf",
    "href": "rv_practice.html#a-continuous-cdf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.27 A Continuous CDF",
    "text": "6.27 A Continuous CDF\nConsider a cdf\n\\(F_X(x)-\\begin{cases}0,&\\text{if }x &lt; -1\\\\ 0.5 & \\text{if }-1 \\leq x &lt; 0\\\\(1+x)/2 & \\text{if }0 \\leq x &lt; 1\\\\1&\\text{otherwise}\\end{cases}\\)\nFind \\(\\mathbb{P}(X &lt; -1)\\), \\(\\mathbb{P}(-0.5 &lt; X &lt; 0.5)\\), and \\(\\mathbb{P}(X&gt;0.5)\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X &lt; -1) = 0 because F(x) only goes up to .5 at x=-1, not when x &lt; -1\n\n#P(-.5 &lt; X &lt; .5) = F(.5) - F(-.5)\n(1+.5)/2 - .5\n\n[1] 0.25\n\n#P(X &gt; 0.5) = 1-P(X&lt;.5) = 1-F(.5) \n1- (1+.5)/2\n\n[1] 0.25",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf-ii",
    "href": "rv_practice.html#a-continuous-cdf-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.28 A Continuous CDF II",
    "text": "6.28 A Continuous CDF II\nLet \\(X\\) have PDF \\[f(x) = \\begin{cases} 1-x/2,&0\\leq x \\leq 2\\\\0,&\\text{otherwise}\\end{cases}\\]\n\nSketch the PDF and show geometrically that this is a valid PDF.\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X\\geq 0]\\)\nFind \\(Pr[X&gt;1]\\) and \\(Pr[X\\geq 1]\\)\nUse calculus or software to calculate the CDF from the PDF. Write the expression for \\(F(x)\\) and plot the PDF and CDF.\nUse calculus or software to calculate the expected value of \\(X\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#two-random-variables",
    "href": "rv_practice.html#two-random-variables",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.29 Two Random Variables",
    "text": "6.29 Two Random Variables\nSuppose that \\(X\\) and \\(Y\\) are random variables with \\(\\mathbb{E}(X)=12\\) and $(Y)=8.\n\nFind \\(\\mathbb{E}(X-Y)\\)\nFind \\(\\mathbb{E}(5X-6Y)\\)\nIs it possible to determine \\(\\mathbb{E}(X^2)\\) with the given information? Explain.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-formula",
    "href": "rv_practice.html#pmf-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.1 PMF Formula",
    "text": "7.1 PMF Formula\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=1,2,\\ldots\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis was done above; because \\(\\sum_{i=1}^\\infty 1/2^k = 1\\), the value of \\(c\\) must be \\(1\\).\n\n\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(6\\leq X \\leq 8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X&gt;4) = 1-P(X \\leq 3)=1-(\\frac12 + \\frac14 + \\frac18)\\)\n\n1-(1/2+1/4+1/8)\n\n[1] 0.125\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe expected value can be calculated by taking the sum \\(\\sum k p_k = \\sum_{k=1}^\\infty \\frac{k}{2^k}\\) which we can show using facts from calculus equals 2. Why? Well, as long as \\(|p|&lt;1, \\sum_{k=1}^{\\infty}p^k=\\frac{p}{1-p}\\) (this is a geometric series). If we take a derivative of both sides we get \\(\\sum_{k=1}^\\infty kp^{k-1}=\\frac{1}{(1-p)^2}\\). Multiply both sides by \\(p\\) to get \\(\\sum_{k=1}^\\infty kp^{k}=\\frac{p}{(1-p)^2}\\). In our case, \\(p=\\frac12\\). Plugging this in we get \\(\\frac{.5}{.5^2}=2\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#pmf-formula-ii",
    "href": "rv_practice.html#pmf-formula-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.2 PMF Formula II",
    "text": "7.2 PMF Formula II\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=-1,0,1,2,3,4,5\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sum of the probabilities are \\(c(2 + 1 + \\frac{1}{2}+\\frac{1}{4}+\\frac{1}8+\\frac{1}{16}+\\frac{1}{32})=c\\frac{127}{32}\\) so \\(c=\\frac{32}{127}\\).\n\n\n\n\nFind \\(\\mathbb{P}(1\\leq X &lt; 3)\\) and \\(\\mathbb{P}(1 &lt; X \\leq 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc=32/127\nk = seq(-1,5)\npk=c/2^k\nnames(pk) &lt;- k\nsum(pk[as.character(2)])\n\n[1] 0.06299213\n\nsum(pk[as.character(2:5)])\n\n[1] 0.1181102\n\n\n\n\n\n\nFind \\(\\mathbb{P}(X^3 &lt; 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(X^3 &lt; 5\\) that means \\(X^3 \\leq 4\\) and thus \\(X \\leq 4^{1/3}\\approx 1.587\\)\n\nsum(pk[k&lt;=4^(1/3)])\n\n[1] 0.8818898\n\nsum(pk[k^3&lt;5])\n\n[1] 0.8818898\n\n\n\n\n\n\nFind the pmf and the cdf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(height=pk, names=k, main='pmf of X')\n\n\n\n\n\n\n\nbarplot(height=cumsum(pk), names=k, main='cdf of X')",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-i",
    "href": "rv_practice.html#properties-of-expectation-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.3 Properties of Expectation I",
    "text": "7.3 Properties of Expectation I\nFor a continuous random variable \\(X\\) and constant \\(c\\), prove that \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-ii",
    "href": "rv_practice.html#properties-of-expectation-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.4 Properties of Expectation II",
    "text": "7.4 Properties of Expectation II\nFor a continuous random variable \\(X\\) and constants \\(a,b\\), prove that \\(\\mathbb{E}(aX+b)=a\\mathbb{E}(X)+b\\)",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "href": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.5 Variance of a RV from the pmf",
    "text": "7.5 Variance of a RV from the pmf\nLet \\(X\\) be a random variable with pmf \\(p_k = 1/2^k\\) for \\(k=1,2,\\ldots\\).\n\nFind \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance is \\(E(X^2)-(EX)^2=E(X^2)-4\\). The expected value of \\(X^2\\) can be derived, though it’s not so fun…\nStart by taking the equation \\(\\sum_k kp^k = \\frac{p}{(1-p)^2}\\) and take a derivative again. We get \\(\\sum_k k^2 p^{k-1} = \\frac{(1-p)^2+2p(1-p)}{(1-p)^4}\\). Multiply through by \\(p\\) to get \\(\\sum_k k^2 p^k = \\frac{p(1-p)^2+2p^2(1-p)}{(1-p)^4}\\). If we let \\(p=\\frac12\\) we have found \\(E(X^2)=\\sum_{k=1}^\\infty k^2(\\frac12)^k=\\dfrac{\\frac18-\\frac{2}{8}}{\\frac{1}{16}}=6\\). Thus \\(Var(X)=E(X^2)-E(X)^2 = 6-4=2\\). It should be noted that this random variable is actually a geometric random variable (well, according to the “number of trials until and including the first success definition). If we define \\(Y\\sim Geom(.5)\\) using our definition of “number of failures before the first success” then We can let \\(X=Y+1\\). \\(E(X)=E(Y+1)=\\frac{1-.5}{.5}+1=2\\) and \\(Var(X)=Var(Y)=\\frac{1-p}{p2}=\\frac{.5}{.25^2}=2\\).",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "rv_practice.html#variance-formula",
    "href": "rv_practice.html#variance-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.6 Variance Formula",
    "text": "7.6 Variance Formula\nShow that \\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2-\\mu^2\\). Hint: expand the quantity \\((X-\\mu)^2\\) and distribute the expectation over the resulting terms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe proof goes like this: We first FOIL \\((X-\\mu)^2\\):\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}(X^2 - 2\\mu X + \\mu^2)\\)\nWe next split the expected value into 3 expected values using the fact that \\(\\mathbb{E}\\) is a linear operator.\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mathbb{E}X + \\mathbb{E}\\mu^2\\)\nWe next observe that \\(\\mu^2\\) is constant and \\(\\mathbb{X}=\\mu\\)\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mu + \\mu^2\\)\nWe can simplify the expression and we’re done!",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Probability and Random Variables Practice</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html",
    "href": "R09_Multiple_Regression.html",
    "title": "29  R10 multiple regression further concepts",
    "section": "",
    "text": "29.1 R Squared and Correlation\nConsider a simple linear model on mtcars:\n\\[ hp = \\beta_0 + \\beta_1 wt  + \\epsilon\\]\nmt.fit &lt;- lm(hp ~ 1+wt, data=mtcars)\nsummary(mt.fit)\n\n\nCall:\nlm(formula = hp ~ 1 + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-83.430 -33.596 -13.587   7.913 172.030 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -1.821     32.325  -0.056    0.955    \nwt            46.160      9.625   4.796 4.15e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 52.44 on 30 degrees of freedom\nMultiple R-squared:  0.4339,    Adjusted R-squared:  0.4151 \nF-statistic:    23 on 1 and 30 DF,  p-value: 4.146e-05\n\\(R^2 = 0.4339\\)\nWhat is the sample correlation between wt and hp? We use “r” to refer to this statistic.\ncor(mtcars$wt, mtcars$hp)\n\n[1] 0.6587479\nAnd if we square that\ncor(mtcars$wt, mtcars$hp)^2\n\n[1] 0.4339488\nIn general, \\(R^2\\) is the correlation between \\(\\hat{y}_i\\) and \\(y_i\\). So for a multiple regression model:\nmt.fit2 &lt;- lm(qsec ~ 1+disp + wt + hp, data=mtcars)\nsummary(mt.fit2)$r.sq\n\n[1] 0.6808292\n\nplot(mtcars$qsec, predict(mt.fit2))\n\n\n\n\n\n\n\ncor(mtcars$qsec, predict(mt.fit2))^2\n\n[1] 0.6808292",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#interaction-plot-for-mtcars---looking-for-an-interaction",
    "href": "R09_Multiple_Regression.html#interaction-plot-for-mtcars---looking-for-an-interaction",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.2 Interaction plot for mtcars - looking for an interaction",
    "text": "29.2 Interaction plot for mtcars - looking for an interaction\n\nlibrary(interactions)\n#In the mtcars dataset, am and vs are simply numerical variable\n# I want to treat them as categorical variables - aka \"factor variables\"\nmtcars$am &lt;- factor(mtcars$am)\nmtcars$vs &lt;- factor(mtcars$vs)\n\nmt.fit.cat &lt;- lm(qsec ~ 1+ am * vs , data=mtcars)\ncat_plot(mt.fit.cat, pred=\"vs\", modx=\"am\", geom=\"line\", plot.points=TRUE, data=mtcars, interval=F)\n\n\n\n\n\n\n\n\nThe fact that these are parallel indicates that there is no measurable interaction - the effects of engine shape and transmission type seem to be additive. We also see that when we look at the model output:\n\nsummary(mt.fit.cat)$coefficients\n\n               Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept) 17.14250000  0.3049732 56.2098626 2.443930e-30\nam1         -1.34583333  0.5282290 -2.5478216 1.661442e-02\nvs1          2.82464286  0.5024460  5.6217837 5.087630e-06\nam1:vs1      0.07869048  0.7732481  0.1017661 9.196676e-01\n\n\nThe \\(p\\)-value on the am-vs interaction term is really high: that’s .917 indicating that we have no statistical evidence of an interaction. In other words, the inclusion of an interaction term in the model is not statistically justified.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#multiple-categorical-levels-the-iris-dataset",
    "href": "R09_Multiple_Regression.html#multiple-categorical-levels-the-iris-dataset",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.3 Multiple categorical levels : the iris dataset",
    "text": "29.3 Multiple categorical levels : the iris dataset\nThe iris dataset is somewhat famous - we have 50 samples from each of three iris species: Setosa, Versicolor and Virginica. Four characteristics are measured: Sepal length & width, and Petal length & width.\n\nhead(iris)\n\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9         3.0          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5          5.0         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n#fix the factor leveling just in case\niris$Species &lt;- factor(iris$Species, levels = c(\"setosa\", \"versicolor\", \"virginica\"))\n\nSpecies is a categorical (factor) variable with three values. In a case like this we still need a numerical encoding of the categorical variable. But using 0, 1 and 2 would be problematic for two reasons. First it implies a progression - an ordering - across the three categorical levels. Secondly, it implies that the effect moving from level 0 to 1 is the same effect of moving from level 1 to level 2.\nThe proper way to model such a situation is to identify one of the categorical levels (values) to be a “baseline”. R will do this for you alphabetically. In this case setosa is the first alphabetically. Then we create a dummy (indicator) variable for each of the remaining levels. Again - this is automatically done for us by R.\nIf we were to predict sepal length based only on species, the model would look like this: \\[Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\epsilon\\] The three cases are handled by the indicator variables as follows:\nSpecies       X_1     X2   \n-------------------------\nsetosa        0       0\nversicolor    1       0\nvirginica     0       1\nThe model fit is easy in R.\n\nsepal.fit &lt;- lm(Sepal.Length ~ 1 + Species, data=iris)\nsummary(sepal.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)         5.0060     0.0728  68.762  &lt; 2e-16 ***\nSpeciesversicolor   0.9300     0.1030   9.033 8.77e-16 ***\nSpeciesvirginica    1.5820     0.1030  15.366  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\n\\[\\hat{Y} = 5.0060 + .93 X_{vers} + 1.582 X_{vir}\\] - For a setosa variety, \\(X_1=0, X_2=0\\) so the predicted sepal length is just 5.006 - For versicolor, \\(X_1=1, X_2=0\\) so the predicted length is 5.006+.93=5.936 - For viginica, \\(X_1=0, X_2=1\\) so the predicted length is 5.006 + 1.582 = 6.588\n\naggregate(Sepal.Length ~ Species, data=iris, FUN=mean)\n\n     Species Sepal.Length\n1     setosa        5.006\n2 versicolor        5.936\n3  virginica        6.588\n\n\nIn other words, the intercept is the mean response for the baseline, the \\(\\beta_1, \\beta_2\\) coefficients are the mean difference between each other case and the baseline.\nIf you wanted versicolor to be the baseline instead you can refactor your variable\n\niris$Species &lt;- factor(iris$Species, \n                       levels = c(\"versicolor\",\"setosa\",\"virginica\"))\nsepal.fit &lt;- lm(Sepal.Length ~ 1+Species, data=iris)\nsummary(sepal.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species, data = iris)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6880 -0.3285 -0.0060  0.3120  1.3120 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        5.9360     0.0728  81.536  &lt; 2e-16 ***\nSpeciessetosa     -0.9300     0.1030  -9.033 8.77e-16 ***\nSpeciesvirginica   0.6520     0.1030   6.333 2.77e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5148 on 147 degrees of freedom\nMultiple R-squared:  0.6187,    Adjusted R-squared:  0.6135 \nF-statistic: 119.3 on 2 and 147 DF,  p-value: &lt; 2.2e-16\n\n\nIf you are curious about the dummy variable encoding that R is doing, you can look at the “model matrix”\n\nmodel.matrix(sepal.fit)\n\nBut I don’t want to refactor the variable, so I’m going to revert the dataset to its default factoring.\n\nrm(iris)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#a-model-with-categorical-and-quantitative-variables",
    "href": "R09_Multiple_Regression.html#a-model-with-categorical-and-quantitative-variables",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.4 A Model with categorical and quantitative variables",
    "text": "29.4 A Model with categorical and quantitative variables\nIf we were attempting to predict Sepal Length from Petal Length and Species, a model might look like this: \\[Sepal.L = \\beta_0 + \\beta_1 Species_{Vers} + \\beta_2 Species_{Vir} + \\beta_3 Petal.L + \\epsilon\\]\n\nplot(Sepal.Length ~ Petal.Length, data=iris, col=iris$Species, pch=16)\n\n\n\n\n\n\n\n\nA fitted model (without an interaction) would look like this:\n\nsepal.fit1 &lt;- lm(Sepal.Length ~ 1+Species+Petal.Length, data=iris)\nsummary(sepal.fit1)\n\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Species + Petal.Length, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.75310 -0.23142 -0.00081  0.23085  1.03100 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        3.68353    0.10610  34.719  &lt; 2e-16 ***\nSpeciesversicolor -1.60097    0.19347  -8.275 7.37e-14 ***\nSpeciesvirginica  -2.11767    0.27346  -7.744 1.48e-12 ***\nPetal.Length       0.90456    0.06479  13.962  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.338 on 146 degrees of freedom\nMultiple R-squared:  0.8367,    Adjusted R-squared:  0.8334 \nF-statistic: 249.4 on 3 and 146 DF,  p-value: &lt; 2.2e-16\n\n\nLet’s just interpret the coefficients for a moment:\nFirst off, setosa is the baseline case. The coefficient of Petal length is 0.90456; this means that controlling for species (i.e. holding species constant) a 1 cm increase in Petal Length is associated with a .90456 cm average increase in Sepal Length. \\[\\hat{sepal.L}_{set} = 3.68 + .906 petal.L\\]\nFor the versicolor coefficient, -1.60097 is interpreted as the average difference in sepal length between a versicolor compared to a setosa iris, controlling for petal length. \\[\\hat{sepal.L}_{vers} = (3.68 -1.6) + .906 petal.L\\]\nI’m not going to check the model assumptions at the moment because that’s not my primary focus. I want to plot the model with the quantitative and categorical predictor:\n\ninteract_plot( lm(Sepal.Length ~ Species+Petal.Length, data=iris), pred=Petal.Length, modx=Species, plot.points=TRUE)\n\n\n\n\n\n\n\n\nYou can see clearly that the model fit encodes three parallel lines. We can get the linear equations by writing out the cases. Take setosa first: \\[\\hat{Sepal.L}_{Set} = 3.68 + 0.90456 Petal.L\\] For versicolor we have \\[\\hat{Sepal.L}_{Vers} = 3.68 - 1.6 + 0.90456 Petal.L = 2.08 + .90456 Petal.L\\]\nFor virginica \\[\\hat{Sepal.L}_{Vers} = 3.68 - 2.12 + 0.90456 Petal.L = 1.56 + .90456 Petal.L\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#interactions-between-categorical-and-quantitative-variables",
    "href": "R09_Multiple_Regression.html#interactions-between-categorical-and-quantitative-variables",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.5 Interactions between categorical and quantitative variables",
    "text": "29.5 Interactions between categorical and quantitative variables\nAdding an interaction between the categorical predictor and the quantitative predictor allows more model flexibility. It allows for different intercepts and different slopes.\n\niris.fit&lt;-lm(Sepal.Length ~ 1+ Petal.Length + Species + Petal.Length:Species, data=iris)\n# or I can use 1 + Petal.Length * Species\n\ninteract_plot(iris.fit, pred=Petal.Length, modx=Species, plot.points=TRUE)\n\n\n\n\n\n\n\n\nLooking at the model summary tells us which of the interactions are statistically supported.\n\nsummary(iris.fit)\n\n\nCall:\nlm(formula = Sepal.Length ~ 1 + Petal.Length + Species + Petal.Length:Species, \n    data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73479 -0.22785 -0.03132  0.24375  0.93608 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      4.2132     0.4074  10.341  &lt; 2e-16 ***\nPetal.Length                     0.5423     0.2768   1.959  0.05200 .  \nSpeciesversicolor               -1.8056     0.5984  -3.017  0.00302 ** \nSpeciesvirginica                -3.1535     0.6341  -4.973 1.85e-06 ***\nPetal.Length:Speciesversicolor   0.2860     0.2951   0.969  0.33405    \nPetal.Length:Speciesvirginica    0.4534     0.2901   1.563  0.12029    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3365 on 144 degrees of freedom\nMultiple R-squared:  0.8405,    Adjusted R-squared:  0.8349 \nF-statistic: 151.7 on 5 and 144 DF,  p-value: &lt; 2.2e-16\n\n\nIn this case neither of them.\nThe interpretation of these interaction coefficients gives us an adjustment to the slope on Petal.Length. Here’s the full model with coefficient estimates (rounded):\n\\[\\hat{Sepal.L} = 4.2 + .54Petal.L + -1.8 Species_{Ver} - \\\\ 3.2 Species_{Vir}+0.29Petal.L\\cdot Species_{Ver} + .45 Petal.L \\cdot Species_{Vir}\\]\nThe indicator variables \\(Species_{Ver}\\) and \\(Species_{Vir}\\) are (0,0) for setosa, (1,0) for versicolor and (0,1) for virginica.\nFor instance, the model for Setosa would be:\n\\[\\hat{Sepal.L}_{Vers} = 4.2 + .54Petal.L\\] For the versicolor two more coefficients will play a part: \\[\\hat{Sepal.L}_{Ver} = 4.2 + .54Petal.L + -1.8(1) +0.29Petal.L(1)\\] Which simplifies to \\[\\hat{Sepal.L}_{Ver} = 2.4 + .83 Petal.L\\] This is the linear equation for the orange dotted line above. We could do the same for virginica but the idea is the same.\nSo the coefficients of the interaction terms can be interpreted as the difference in slope between the base case (versicolor) and the other cases (setosa or virginica).\nThat is how we use interactions between categorical variables and quantitative variables.\nYou can actually calculate these coefficients by subsetting the data into three species and fitting three linear models!\n\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"setosa\")))\n\n (Intercept) Petal.Length \n   4.2131682    0.5422926 \n\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"versicolor\")))\n\n (Intercept) Petal.Length \n    2.407523     0.828281 \n\ncoef(lm(Sepal.Length ~ 1 + Petal.Length, data=subset(iris, Species==\"virginica\")))\n\n (Intercept) Petal.Length \n   1.0596591    0.9957386 \n\n\nThe coefficient estimate for the interaction term Petal.Length:Speciessetosa is\n\n0.5422926 - 0.828281\n\n[1] -0.2859884",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#more-on-interactions---two-quantitative-predictors",
    "href": "R09_Multiple_Regression.html#more-on-interactions---two-quantitative-predictors",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.6 More on Interactions - two quantitative predictors",
    "text": "29.6 More on Interactions - two quantitative predictors\n\nlibrary(interactions)\n\nAn interaction of two quantitative variables \\(X1\\) and \\(X2\\) would allow for a model that looks something like this:\n\\[ Y = 10 + 5X1 + 3X2 - 2 X1 \\times X2 + \\epsilon\\]\nThis describes a hyperbolic paraboloid\n\n\n\n\n\n\n\n\n\nAn interaction between two quantitative variables is one form of a non-additive relationship with the response variable.\nHere is a dataset where the linear model includes an interaction between two continuous variables.\n\nstates &lt;- as.data.frame(state.x77)\nsummary(states)\n\n   Population        Income       Illiteracy       Life Exp    \n Min.   :  365   Min.   :3098   Min.   :0.500   Min.   :67.96  \n 1st Qu.: 1080   1st Qu.:3993   1st Qu.:0.625   1st Qu.:70.12  \n Median : 2838   Median :4519   Median :0.950   Median :70.67  \n Mean   : 4246   Mean   :4436   Mean   :1.170   Mean   :70.88  \n 3rd Qu.: 4968   3rd Qu.:4814   3rd Qu.:1.575   3rd Qu.:71.89  \n Max.   :21198   Max.   :6315   Max.   :2.800   Max.   :73.60  \n     Murder          HS Grad          Frost             Area       \n Min.   : 1.400   Min.   :37.80   Min.   :  0.00   Min.   :  1049  \n 1st Qu.: 4.350   1st Qu.:48.05   1st Qu.: 66.25   1st Qu.: 36985  \n Median : 6.850   Median :53.25   Median :114.50   Median : 54277  \n Mean   : 7.378   Mean   :53.11   Mean   :104.46   Mean   : 70736  \n 3rd Qu.:10.675   3rd Qu.:59.15   3rd Qu.:139.75   3rd Qu.: 81163  \n Max.   :15.100   Max.   :67.30   Max.   :188.00   Max.   :566432  \n\ncor(states$Income, states$Illiteracy)\n\n[1] -0.4370752\n\ncor(states$Income, states$Murder)\n\n[1] -0.2300776\n\n\nFirst I want to fit a model that does not include an interaction term.\n\nfit.noi &lt;- lm(Income ~ Illiteracy + Murder + `HS Grad`, data = states)\nsummary(fit.noi)\n\n\nCall:\nlm(formula = Income ~ Illiteracy + Murder + `HS Grad`, data = states)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1068.21  -274.66   -40.47   182.69  1203.82 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2017.96     750.10   2.690 0.009917 ** \nIlliteracy   -176.84     187.25  -0.944 0.349919    \nMurder         30.48      26.70   1.141 0.259591    \n`HS Grad`      45.19      11.51   3.925 0.000288 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 490.1 on 46 degrees of freedom\nMultiple R-squared:  0.4028,    Adjusted R-squared:  0.3638 \nF-statistic: 10.34 on 3 and 46 DF,  p-value: 2.551e-05\n\n\nThe interaction term can be added to the formula explicitly by including an addition Illiteracy:Murder term, or just use the asterisk to include the interaction and both single terms as well.\n\nfiti &lt;- lm(Income ~ Illiteracy * Murder + `HS Grad`, data = states)\nsummary(fiti)\n\n\nCall:\nlm(formula = Income ~ Illiteracy * Murder + `HS Grad`, data = states)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-916.27 -244.42   28.42  228.14 1221.16 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        1414.46     737.84   1.917  0.06160 .  \nIlliteracy          753.07     385.90   1.951  0.05724 .  \nMurder              130.60      44.67   2.923  0.00540 ** \n`HS Grad`            40.76      10.92   3.733  0.00053 ***\nIlliteracy:Murder   -97.04      35.86  -2.706  0.00958 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 459.5 on 45 degrees of freedom\nMultiple R-squared:  0.4864,    Adjusted R-squared:  0.4407 \nF-statistic: 10.65 on 4 and 45 DF,  p-value: 3.689e-06\n\n\nThe interactions between variables can be explored using interaction plots:\n\ninteract_plot(fiti, pred = Illiteracy, modx = Murder)\n\n\n\n\n\n\n\n\nThe interaction plot takes the other quantitative variable (HS Grad) and sets its value to be the mean (53.11). Three lines are given here indicating how the line changes as the Murder variable may vary from its average mean value.\n\\[\\hat{Income} = 1414.46+753 Ill + 130.60 Mur + 40.76(53.11) -97.04 Ill \\times Mur\\] You can see that as the murder rate increases the effect of illiteracy on income changes. This is reflected in the negative value for the interaction. In this case the Mur variable is continuous rather than categorical. The interpretation of the coefficient on the interaction term is a little more tricky to put into words. That’s beyond this course.\nThe important take-away here is that the interaction term is statistically significant, so we have strong evidence from the data that there is a real interaction in the model. The average effect of Illiteracy and Murder rate on per capita income is not purely additive.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#mt-cars-polynomial-model",
    "href": "R09_Multiple_Regression.html#mt-cars-polynomial-model",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.7 MT Cars polynomial model",
    "text": "29.7 MT Cars polynomial model\nLet’s briefly compare a linear model with a second order model\n\nplot(mpg~hp, data=mtcars)\n\n\n\n\n\n\n\nmtcars.pow1 &lt;- lm(mpg ~ 1 + hp, data=mtcars)\nsummary(mtcars.pow1)\n\n\nCall:\nlm(formula = mpg ~ 1 + hp, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7121 -2.1122 -0.8854  1.5819  8.2360 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 30.09886    1.63392  18.421  &lt; 2e-16 ***\nhp          -0.06823    0.01012  -6.742 1.79e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.863 on 30 degrees of freedom\nMultiple R-squared:  0.6024,    Adjusted R-squared:  0.5892 \nF-statistic: 45.46 on 1 and 30 DF,  p-value: 1.788e-07\n\nplot(mtcars.pow1, which=1)\n\n\n\n\n\n\n\n\n\nmtcars.pow2 &lt;- lm(mpg ~ 1 + hp + I(hp^2), data=mtcars)\nsummary(mtcars.pow2)\n\n\nCall:\nlm(formula = mpg ~ 1 + hp + I(hp^2), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.5512 -1.6027 -0.6977  1.5509  8.7213 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.041e+01  2.741e+00  14.744 5.23e-15 ***\nhp          -2.133e-01  3.488e-02  -6.115 1.16e-06 ***\nI(hp^2)      4.208e-04  9.844e-05   4.275 0.000189 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.077 on 29 degrees of freedom\nMultiple R-squared:  0.7561,    Adjusted R-squared:  0.7393 \nF-statistic: 44.95 on 2 and 29 DF,  p-value: 1.301e-09\n\nplot(mtcars.pow2, which=1)\n\n\n\n\n\n\n\n\nFor the moment let’s consider a third order polynomial just to see if the linearity is better satisfied.\n\nmtcars.pow3 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3), data=mtcars)\nsummary(mtcars.pow3)\n\n\nCall:\nlm(formula = mpg ~ 1 + hp + I(hp^2) + I(hp^3), data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.8605 -1.3972 -0.5736  1.6461  9.0738 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.422e+01  5.961e+00   7.419 4.43e-08 ***\nhp          -2.945e-01  1.178e-01  -2.500   0.0185 *  \nI(hp^2)      9.115e-04  6.863e-04   1.328   0.1949    \nI(hp^3)     -8.701e-07  1.204e-06  -0.722   0.4760    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.103 on 28 degrees of freedom\nMultiple R-squared:  0.7606,    Adjusted R-squared:  0.7349 \nF-statistic: 29.65 on 3 and 28 DF,  p-value: 7.769e-09\n\nplot(mtcars.pow3, which=1)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#variable-transformations",
    "href": "R09_Multiple_Regression.html#variable-transformations",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.8 Variable Transformations",
    "text": "29.8 Variable Transformations\nVariable transformations can be effective at remedying violated regression assumptions. Briefly here are the assumptions:\n\nThe error term has a normal distribution\nThe response variable has a linear relationship with the predictors. In other words, the errors have zero mean\nThe error term has constant variance\nThe errors are independent.\n\nThe fourth assumption is hard to check, but the first three can be checked with diagnostic plots of the residuals.\nTransformation of either a predictor variable or the response variable may be appropriate, often with theoretical justification. But first let’s explore the effect of variable transformation.\nHere is some left-skewed data\n\nset.seed(1)\nleft.skew.data &lt;- rbeta(60, 2, 1)*10\nhist(left.skew.data)\n\n\n\n\n\n\n\n\nAnd here are histograms of the data taken to different powers\n\npar(mfrow=c(2,2))\nhist(log(left.skew.data), main=\"Log transform\")\nhist((left.skew.data)^.5, main=\"square root transform\")\nhist((left.skew.data)^2, main=\"squaring\")\nhist((left.skew.data)^3, main=\"cubing\")\n\n\n\n\n\n\n\n\nYou can see that squaring the values give us a more symmetric distribution. Generally speaking, if you have left-skewed data, raising your values to a power \\(&gt;1\\) can do this for you.\nOn the other hand, here is some right-skewed data - an exponential population\n\nset.seed(1)\nright.skew.data &lt;- rexp(60, 2)\nhist(right.skew.data, breaks=25)\n\n\n\n\n\n\n\n\n\npar(mfrow=c(2,2))\nhist(log(right.skew.data), main=\"log\")\nhist((right.skew.data)^.5, main=\"sqrt\")\nhist((right.skew.data)^2, main=\"Square\", breaks=20)\nhist((right.skew.data)^3, main=\"cube\", breaks=20)\n\n\n\n\n\n\n\n\nA right skew is amplified by taking your data to a higher power, but square roots and log transformations can make your data more symmetric and sometimes more normally distributed.\nVariable transformations can often be effective means to remedy violated regression assumptions.\n\n29.8.1 Real Estate Air Conditioning\nThe real estate data is in the realestate.txt file.\n\nreal.estate &lt;- read.table(\"data/realestate.txt\", header=T)\nreal.estate$Air &lt;- factor(real.estate$Air)\n\nWhile there are many variables, we’ll focus only on three: - Y = sale price of the home (in 1000s of dollars) - X1 = square footage of the home (in 1000s of sqft) - X2 = whether or not the home has air conditioning\nThe interaction model is \\[Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\beta_3 X_{i,1}X_{i,2} + \\epsilon_i\\]\n\nreal.fit &lt;- lm(SalePrice ~ 1+SqFeet*Air, data=real.estate)\nlibrary(interactions)\ninteract_plot(real.fit, pred=SqFeet, modx=Air, plot.points = TRUE)\n\n\n\n\n\n\n\nsummary(real.fit)\n\n\nCall:\nlm(formula = SalePrice ~ 1 + SqFeet * Air, data = real.estate)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-248.01  -37.13   -7.80   22.25  381.92 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -3.218     30.085  -0.107 0.914871    \nSqFeet       104.902     15.748   6.661 6.96e-11 ***\nAir1         -78.868     32.663  -2.415 0.016100 *  \nSqFeet:Air1   55.888     16.580   3.371 0.000805 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 77.01 on 517 degrees of freedom\nMultiple R-squared:  0.6887,    Adjusted R-squared:  0.6869 \nF-statistic: 381.2 on 3 and 517 DF,  p-value: &lt; 2.2e-16\n\nplot(real.fit, which=1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is strong evidence supporting the interaction in the model, and both preditors are strong predictors of sale price. However there are two big problems. One is the residuals spread is not constant, and the residuals do not seem to be normally distributed. The non constant spread can be seen in the plot\n\ninteract_plot(real.fit, pred=SqFeet, modx=Air, plot.points = TRUE)\n\n\n\n\n\n\n\nhist(resid(real.fit))\n\n\n\n\n\n\n\n\nthe residuals are unimodal but the right tail is too long - this is shown in the QQ plot but more obviously emphasized in the histogram above.\nWhen large values of \\(y\\) have higher variance than low values of \\(y\\) a log transformation of the response variable will often fix the problem. This is because error is not in absolute term but rather relative to the magnitude of \\(y\\). Logs fix this mathematically.\nWe’ll transform Sale Price in the model: \\[\\ln Y_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\beta_3 X_{i,1}X_{i,2} + \\epsilon_i\\]\n\nreal.fit.log &lt;- lm(log(SalePrice) ~ 1+SqFeet*Air, data=real.estate)\nsummary(real.fit.log)\n\n\nCall:\nlm(formula = log(SalePrice) ~ 1 + SqFeet * Air, data = real.estate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.73816 -0.13955 -0.02114  0.11740  0.82106 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.32846    0.08813  49.113   &lt;2e-16 ***\nSqFeet       0.46808    0.04613  10.146   &lt;2e-16 ***\nAir1         0.11522    0.09569   1.204    0.229    \nSqFeet:Air1  0.02202    0.04857   0.453    0.650    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2256 on 517 degrees of freedom\nMultiple R-squared:  0.7274,    Adjusted R-squared:  0.7259 \nF-statistic: 459.9 on 3 and 517 DF,  p-value: &lt; 2.2e-16\n\nplot(real.fit.log, which=1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninteract_plot(real.fit.log, pred=SqFeet, modx=Air, plot.points = TRUE)\n\nUsing data real.estate from global environment. This could cause incorrect\nresults if real.estate has been altered since the model was fit. You can\nmanually provide the data to the \"data =\" argument.\n\n\n\n\n\n\n\n\nhist(resid(real.fit.log))\n\n\n\n\n\n\n\n\nThe predictor variable SqFeet also exhibits a right skew\n\nhist(real.estate$SqFeet)\n\n\n\n\n\n\n\n\nIn a case like this a linear model may possibly fit better if the predictor is log transformed too. This is worth checking\n\nreal.estate$logSqFeet &lt;- log(real.estate$SqFeet)\nreal.fit.log &lt;- lm(log(SalePrice) ~ 1+logSqFeet*Air, data=real.estate)\nsummary(real.fit.log)\n\n\nCall:\nlm(formula = log(SalePrice) ~ 1 + logSqFeet * Air, data = real.estate)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.58375 -0.13872 -0.01541  0.11524  0.76531 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     4.62997    0.05729  80.811  &lt; 2e-16 ***\nlogSqFeet       0.97321    0.09119  10.673  &lt; 2e-16 ***\nAir1           -0.04867    0.06549  -0.743  0.45770    \nlogSqFeet:Air1  0.27514    0.09838   2.797  0.00535 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2183 on 517 degrees of freedom\nMultiple R-squared:  0.7448,    Adjusted R-squared:  0.7434 \nF-statistic: 503.1 on 3 and 517 DF,  p-value: &lt; 2.2e-16\n\nplot(real.fit.log, which=1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ninteract_plot(real.fit.log, pred=logSqFeet, modx=Air, plot.points = TRUE)\n\nUsing data real.estate from global environment. This could cause incorrect\nresults if real.estate has been altered since the model was fit. You can\nmanually provide the data to the \"data =\" argument.\n\n\n\n\n\n\n\n\nhist(resid(real.fit.log))\n\n\n\n\n\n\n\n\nThe interaction of air conditioning and square feet is brought out more clearly when this transformation is done, as shown in the significance of the predictor. This model also has the strongest \\(R^2\\) of all three models considered.\nLet’s consider how this model would estimate / predict the sale price of a 1800 sqft house with air conditioning.\n\npredict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"))\n\n       1 \n5.315058 \n\n#to conver this to our units, we take e^&lt;this&gt;\nexp(predict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"))\n)\n\n       1 \n203.3764 \n\n#to convert a 95% confidence interval back to original units, do the same\n#this is my 95% prediction interval for ONE particular house that is 1800 sqft\nexp(predict(real.fit.log, newdata=data.frame(logSqFeet = log(1.800), Air=\"1\"), interval=\"prediction\")\n)\n\n       fit      lwr      upr\n1 203.3764 132.3514 312.5161",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "R09_Multiple_Regression.html#multicollinearity",
    "href": "R09_Multiple_Regression.html#multicollinearity",
    "title": "29  R10 multiple regression further concepts",
    "section": "29.9 Multicollinearity",
    "text": "29.9 Multicollinearity\nWhen predictor variables exhibit high correlation we call this multicollinearity. This can cause a number of problems which we’ll discuss. First let’s consider some synthetic data where the predictors are perfectly correlated - this is an extreme case.\n\\[ X_2 = 2.5 X_1-1\\] If the true model looks like this: \\[Y = 10 + 2X_1 +.3 X_2 + \\epsilon, \\quad \\epsilon \\sim N(0, 4^2)\\] We could actually write \\(X_2\\) in terms of \\(X_1\\) or vice versa. For instance, we could substitute in the expression \\(2.5X_1-1\\) and get \\[\\begin{aligned}\nY &= 10 + 2X_1 + .3(2.5X_1-1) + \\epsilon \\\\\n  &= 9.7 + 2.75X_1 + \\epsilon\n\\end{aligned}\\]\nOr we could substitute \\(X_1 = \\frac{X_2+1}{2.5}\\) and get \\[\\begin{aligned}\nY &= 10 + 2\\frac{X_2+1}{2.5}+.3X_2+\\epsilon\\\\\n  &= 10.8 + 1.1 X_2+ \\epsilon\n\\end{aligned}\\] Both of these models are perfectly true; both would fit the data just as well.\nIn fact, if your predictors exhibit perfect correlation then R will be unable to properly fit a linear model to the data.\nI’ll simulate a sample of size 30.\n\nX1 &lt;- runif(30, 10,20) \nX2 &lt;- 2.5 * X1 -1\nY &lt;- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nsummary(lm(Y~1+X1+X2))\n\n\nCall:\nlm(formula = Y ~ 1 + X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3413 -2.1553 -0.0532  2.5223  5.1597 \n\nCoefficients: (1 not defined because of singularities)\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  11.3450     3.3053   3.432  0.00188 ** \nX1            2.6857     0.2188  12.273 8.74e-13 ***\nX2                NA         NA      NA       NA    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.195 on 28 degrees of freedom\nMultiple R-squared:  0.8433,    Adjusted R-squared:  0.8377 \nF-statistic: 150.6 on 1 and 28 DF,  p-value: 8.738e-13\n\n\nThe case of perfect correlation is rare but a strong correlation could certainly arise. I’ll add a little bit of random spice in to the calculation of \\(X_2\\) and we’ll look at the correlation plots.\n\nset.seed(1)\nX1 &lt;- runif(30, 10,20) \nX2 &lt;- 2.5 * X1 -1 + rnorm(30,0,1)\nY &lt;- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nplot(data.frame(Y,X1,X2))\n\n\n\n\n\n\n\n\nWe can numerically measure the correlation using the cor function\n\ncor(data.frame(Y,X1,X2))\n\n           Y        X1        X2\nY  1.0000000 0.8710865 0.8966127\nX1 0.8710865 1.0000000 0.9944797\nX2 0.8966127 0.9944797 1.0000000\n\n\nIf we attempt to fit a linear model this is what we get:\n\nsummary(lm(Y~1+X1+X2))$coefficients\n\n             Estimate Std. Error   t value     Pr(&gt;|t|)\n(Intercept) 16.339897  3.4972494  4.672214 7.349847e-05\nX1          -5.446292  2.1220827 -2.566484 1.613689e-02\nX2           3.200272  0.8458235  3.783617 7.822802e-04\n\n\nIn the true model the coefficients of \\(X_1\\) and \\(X_2\\) are \\(2\\) and \\(.3\\). These quite far off! The intercept is way off as well.\nI’ll repeat again with newly generated data to see how it differs\n\nX1 &lt;- runif(30, 10,20) \nX2 &lt;- 2.5 * X1 -1 + rnorm(30,0,1)\nY &lt;- 10+2*X1+.3*X2 + rnorm(30, 0, 4)\nsummary(lm(Y~X1+X2))$coefficients\n\n              Estimate Std. Error    t value    Pr(&gt;|t|)\n(Intercept) 11.9418373  4.2513025  2.8089832 0.009126691\nX1           3.0977225  1.9737924  1.5694267 0.128195112\nX2          -0.2370767  0.7921835 -0.2992699 0.767025684\n\n\nNotice that the coefficient of \\(X_1\\) is now a lot lower (and negative) and the coefficient of \\(X_2\\) is positive now! Furthermore, the \\(p\\)-values for the coefficients are all \\(&gt;0.05\\). This is common when predictors are highly correlated. Typical problems that you will see when you have multicollinearity are:\n\nParameters of the model become indeterminate\nStandard errors of the estimates become large\naddition or removal of a predictor can drastically change the coefficients - even flip their signs\nit becomes difficult or impossible to interpret the coefficients\npredictions risk extrapolation\n\nThe first two problems are understandable if we think about the extreme case. We actually had three models that were equally valid when we had perfect correlation (ignoring \\(\\epsilon\\) for simplicity):\n\\[Y = 10 + 2X_1 +.3 X_2',\\\\ Y=9.7 + 2.75X_1,\\\\ Y=10.8 + 1.1 X_2\\] In fact, there are an infinite number of models you could fit that would fit equally well. This means we have no certainty of either of the coefficient values. In fact, the uncertainty of \\(\\beta_1\\) and \\(\\beta_2\\) affects our uncertainty of \\(\\beta_0\\) as well! This causes the standard errors to be large.\nWhat happens when we remove a predictor? I’ll refit the model with only \\(X_1\\) and only \\(X_2\\) to see\n\nsummary(lm(Y~X1+X2))$coefficients\n\n              Estimate Std. Error    t value    Pr(&gt;|t|)\n(Intercept) 11.9418373  4.2513025  2.8089832 0.009126691\nX1           3.0977225  1.9737924  1.5694267 0.128195112\nX2          -0.2370767  0.7921835 -0.2992699 0.767025684\n\nsummary(lm(Y~X1))$coefficients\n\n             Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 12.034186  4.1705844 2.885491 7.441841e-03\nX1           2.512754  0.2697257 9.315962 4.491075e-10\n\nsummary(lm(Y~X2))$coefficients\n\n              Estimate Std. Error  t value     Pr(&gt;|t|)\n(Intercept) 13.3338353  4.2649971 3.126341 4.099863e-03\nX2           0.9941401  0.1128976 8.805676 1.474748e-09\n\n\nNotice that in the models with only 1 predictor the \\(p\\)-value shoots down to close to zero. The standard error shrinks as well. When we have a model with only one of these correlated predictors we can focus on the predictive power of that one predictor and it is not muddied by the influence of the other one.\nThis leads to the fourth problem. Interpretation of coefficients in a multiple regression model requires us to say something like “holding other variables constant”. But if \\(X_1\\) and \\(X_2\\) exhibit strong correlation in the real world, then it’s not reasonable to hold one constant while changing the other.\nWhat about the fifth problem about extrapolations? Let’s look at the data summaries:\n\nsummary(data.frame(X1,X2))\n\n       X1              X2       \n Min.   :10.64   Min.   :25.31  \n 1st Qu.:13.36   1st Qu.:33.30  \n Median :15.15   Median :37.10  \n Mean   :15.26   Mean   :37.26  \n 3rd Qu.:17.37   3rd Qu.:42.42  \n Max.   :19.03   Max.   :46.84  \n\n\nOur dataset includes \\(X1\\) over the range 10 to 20 roughly, and \\(X2\\) over the range 23 to 50. So one might think that \\(Y\\) could be predicted for any pair of \\(X1,X2\\) values in the range \\([10,20]\\times[25,50]\\). For example, can we predict \\(\\hat{Y}|X_1=11, X_2=48\\) ?\n\nlmfit &lt;- lm(Y~X1+X2)\npredict(lmfit, newdata=data.frame(X1=11, X2=48))\n\n      1 \n34.6371 \n\n\nNo problem there. But let’s look again at the data range as a scatterplot\n\nplot(X1,X2)\n\n\n\n\n\n\n\n\nThe only data we’ve observed is over thin diagonal strip. Our model is not supported outside of this range, so predicting \\(Y\\) for \\(X1=11, X2=48\\) has a tremendous amount of uncertainty.\n\n29.9.1 Detecting Multicollinearity\nA simple statistic that measures multicollinearity is the Variance Inflation Factor or VIF. The idea is that we take each predictor and fit a linear model with this predictor as the response and the other predictors as predictors. We measure how strong the coefficient of determination (\\(R^2\\)) is, and calculate \\[VIF_j = \\frac{1}{1-R^2_j}\\] For instance, in this case we have\n\nrsq = summary(lm(X1~X2))$r.square\n1/(1-rsq)\n\n[1] 51.80865\n\n\nThis is very high - a VIF more than 10 is extremely problematic.\nThere’s a function in the car package to calculate VIF for all predictors in a model.\n\nlibrary(car)\n\nLoading required package: carData\n\nvif(lmfit)\n\n      X1       X2 \n51.80865 51.80865 \n\n\nNote that in this case both predictors have the same VIF. This is because when we regress \\(X1\\) on \\(X2\\) the correlation is the same as if we regress \\(X2\\) on \\(X1\\).\n\n\n29.9.2 How to handle Multicollinearity\nThere are some more advanced methods but probably the simplest method is to remove one of the problematic predictors. In this case we could choose arbitrarily to keep \\(X1\\) or \\(X2\\) in the model and drop the other one. Either choice would be fine.\nNote that in the case of multicollinearity this dropping of a redundant variable will not cause \\(R^2\\) to suffer much at all. Observe:\n\nsummary(lm(Y~X1+X2))\n\n\nCall:\nlm(formula = Y ~ X1 + X2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.4838 -2.3684 -0.4808  1.6046  9.7260 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  11.9418     4.2513   2.809  0.00913 **\nX1            3.0977     1.9738   1.569  0.12820   \nX2           -0.2371     0.7922  -0.299  0.76703   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.747 on 27 degrees of freedom\nMultiple R-squared:  0.7569,    Adjusted R-squared:  0.7389 \nF-statistic: 42.03 on 2 and 27 DF,  p-value: 5.113e-09\n\nsummary(lm(Y~X1))\n\n\nCall:\nlm(formula = Y ~ X1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.3143 -2.0682 -0.4356  1.2547  9.8196 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  12.0342     4.1706   2.885  0.00744 ** \nX1            2.5128     0.2697   9.316 4.49e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.686 on 28 degrees of freedom\nMultiple R-squared:  0.7561,    Adjusted R-squared:  0.7474 \nF-statistic: 86.79 on 1 and 28 DF,  p-value: 4.491e-10\n\n\n\n\n29.9.3 A Data Example\nThe datafile (wages85.txt) is downloaded from http://lib.stat.cmu.edu/datasets/. It contains 534 observations on 11 variables sampled from the Current Population Survey of 1985. The Current Population Survey (CPS) is used to supplement census information between census years. These data consist of a random sample of 534 persons from the CPS, with information on wages and other characteristics of the workers, including sex, number of years of education, years of work experience, occupational status, region of residence and union membership. We wish to determine whether wages are related to these characteristics. In particular, we are seeking for the following model:\n\\(wage = \\beta_0 + \\beta_1 south + \\beta_2 sex + \\beta_3 exp + \\beta_4 union + \\beta_5 age + \\beta_6 race_2 + \\beta_7 race_3 + \\cdots\\)\n\nwages &lt;- read.table(\"data/wages85.txt\", header=TRUE)\n\nBut we should understand how the data is encoded. In particular some of the variables are categorical variables: SOUTH, SEX, RACE, SECTOR, MARR, OCCUPATION, UNION. We should convert these to categorical factor type variables rather than numeric.\n\nwages$SOUTH &lt;- factor(wages$SOUTH)\nwages$SEX &lt;- factor(wages$SEX)\nwages$RACE &lt;- factor(wages$RACE)\nwages$SECTOR &lt;- factor(wages$SECTOR)\nwages$MARR &lt;- factor(wages$MARR)\nwages$OCCUPATION &lt;- factor(wages$OCCUPATION)\nwages$UNION &lt;- factor(wages$UNION)\n\nNow we can fit the model. Note that with all of these categorical variables we will have a lot of model coefficients. With a sample size of 500+ that’s not a huge concern right now. Anyway, the point is going to be about multicollinearity.\n\n# the \".\" indicates I want to use ALL predictors in the model. No interactions though.\nwage.lm &lt;- lm(WAGE ~ . , data=wages)\nsummary(wage.lm)\n\n\nCall:\nlm(formula = WAGE ~ ., data = wages)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-11.409  -2.486  -0.631   1.872  35.021 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.2781     6.6976   0.340  0.73390    \nEDUCATION     0.8128     1.0869   0.748  0.45491    \nSOUTH1       -0.5627     0.4198  -1.340  0.18070    \nSEX1         -1.9425     0.4194  -4.631 4.60e-06 ***\nEXPERIENCE    0.2448     1.0818   0.226  0.82103    \nUNION1        1.6017     0.5127   3.124  0.00188 ** \nAGE          -0.1580     1.0809  -0.146  0.88382    \nRACE2         0.2314     0.9915   0.233  0.81559    \nRACE3         0.8379     0.5745   1.458  0.14532    \nOCCUPATION2  -4.0638     0.9159  -4.437 1.12e-05 ***\nOCCUPATION3  -3.2682     0.7626  -4.286 2.17e-05 ***\nOCCUPATION4  -3.9754     0.8108  -4.903 1.26e-06 ***\nOCCUPATION5  -1.3336     0.7289  -1.829  0.06791 .  \nOCCUPATION6  -3.2905     0.8005  -4.111 4.59e-05 ***\nSECTOR1       1.0409     0.5492   1.895  0.05863 .  \nSECTOR2       0.4774     0.9661   0.494  0.62141    \nMARR1         0.3005     0.4112   0.731  0.46523    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.282 on 517 degrees of freedom\nMultiple R-squared:  0.3265,    Adjusted R-squared:  0.3056 \nF-statistic: 15.66 on 16 and 517 DF,  p-value: &lt; 2.2e-16\n\nplot(wage.lm, which=1:2)\n\nWarning: not plotting observations with leverage one:\n  444\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWithout going into the summary, it’s clear that variance increases with predicted wage; this is a good indication that a log transformation or square root transformation of the response variable could be useful. In fact that could even fix the non normality of residuals\n\nhist(resid(wage.lm), breaks=50)\n\n\n\n\n\n\n\n\n\nwage.log.lm &lt;- lm(log(WAGE) ~ ., data=wages)\nplot(wage.log.lm, which=1:2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhist(resid(wage.log.lm), breaks=50)\n\n\n\n\n\n\n\n\nThe log transform fixed both problems. So instead of predicting wage, we’ll predict log(wage). But let’s look at the summary output.\n\nsummary(wage.log.lm)\n\n\nCall:\nlm(formula = log(WAGE) ~ ., data = wages)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36167 -0.27926  0.00049  0.27957  1.79838 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.54415    0.66955   2.306 0.021491 *  \nEDUCATION    0.12525    0.10865   1.153 0.249530    \nSOUTH1      -0.09290    0.04197  -2.214 0.027291 *  \nSEX1        -0.21812    0.04193  -5.202 2.85e-07 ***\nEXPERIENCE   0.06799    0.10814   0.629 0.529813    \nUNION1       0.21178    0.05126   4.132 4.20e-05 ***\nAGE         -0.05858    0.10806  -0.542 0.587963    \nRACE2       -0.03345    0.09912  -0.338 0.735876    \nRACE3        0.07973    0.05743   1.388 0.165636    \nOCCUPATION2 -0.36440    0.09156  -3.980 7.88e-05 ***\nOCCUPATION3 -0.20964    0.07624  -2.750 0.006171 ** \nOCCUPATION4 -0.38345    0.08105  -4.731 2.89e-06 ***\nOCCUPATION5 -0.05278    0.07287  -0.724 0.469223    \nOCCUPATION6 -0.26555    0.08002  -3.318 0.000969 ***\nSECTOR1      0.11532    0.05491   2.100 0.036186 *  \nSECTOR2      0.09296    0.09658   0.962 0.336262    \nMARR1        0.06335    0.04111   1.541 0.123899    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4281 on 517 degrees of freedom\nMultiple R-squared:  0.3617,    Adjusted R-squared:  0.342 \nF-statistic: 18.31 on 16 and 517 DF,  p-value: &lt; 2.2e-16\n\n\nBy looking at the model summary, the R-squared value of 0.362 is not bad for a cross sectional data of 534 observations. The F-value is highly significant implying that all the explanatory variables together significantly explain the log of wages. However, coming to the individual regression coefficients, it is seen that as many as five variables (race, education, experience, age, marriage) are not statistically significant.\nFor further diagnosis of the problem, let us first look at the pair-wise correlation among the numerical variables.\n\nX&lt;-wages[,c(\"EDUCATION\",\"EXPERIENCE\",\"WAGE\",\"AGE\")]\npairs(X)\n\n\n\n\n\n\n\n\nThe high correlation between age and experience might be the root cause of multicollinearity. Again by looking at the partial correlation coefficient matrix among the variables, it is also clear that the partial correlation between experience – education, age – education and age – experience are quite high.\n\ncor(X)\n\n            EDUCATION  EXPERIENCE       WAGE        AGE\nEDUCATION   1.0000000 -0.35267645 0.38192207 -0.1500195\nEXPERIENCE -0.3526764  1.00000000 0.08705953  0.9779612\nWAGE        0.3819221  0.08705953 1.00000000  0.1769669\nAGE        -0.1500195  0.97796125 0.17696688  1.0000000\n\n\nIf we look at the VIF for the model we can get a sense of how bad it really is. Note that VIF is defined for quantitative variables, so if I use the vif function I will get a “generalized” vif since I have so many categorical predictors. That’s ok. It has the same general interpretation.\n\nvif(wage.log.lm)\n\n                  GVIF Df GVIF^(1/(2*Df))\nEDUCATION   234.855660  1       15.325001\nSOUTH         1.061343  1        1.030215\nSEX           1.272000  1        1.127830\nEXPERIENCE 5212.811724  1       72.199804\nUNION         1.128802  1        1.062451\nAGE        4669.808671  1       68.335998\nRACE          1.094538  2        1.022840\nOCCUPATION    3.020968  5        1.116901\nSECTOR        1.444859  2        1.096368\nMARR          1.111966  1        1.054498\n\n\nEducation, age and experience all have extremely high VIF measures. This is not surprising.\nThe simplest remedy for the problem is to remove one of these correlated predictors from the model. Since experience has the highest VIF, let’s remove EXPERIENCE. This can be done by adding -EXPERIENCE to the model.\n\nwage.log.fit2 &lt;- lm(log(WAGE) ~ . - EXPERIENCE, data=wages)\nsummary(wage.log.fit2)\n\n\nCall:\nlm(formula = log(WAGE) ~ . - EXPERIENCE, data = wages)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.36094 -0.28111  0.00376  0.27757  1.79529 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.139876   0.186526   6.111 1.95e-09 ***\nEDUCATION    0.057206   0.009541   5.996 3.80e-09 ***\nSOUTH1      -0.093477   0.041934  -2.229 0.026233 *  \nSEX1        -0.216721   0.041846  -5.179 3.20e-07 ***\nUNION1       0.211528   0.051224   4.129 4.24e-05 ***\nAGE          0.009347   0.001724   5.421 9.10e-08 ***\nRACE2       -0.034072   0.099060  -0.344 0.731019    \nRACE3        0.079864   0.057397   1.391 0.164692    \nOCCUPATION2 -0.364504   0.091508  -3.983 7.77e-05 ***\nOCCUPATION3 -0.210472   0.076181  -2.763 0.005935 ** \nOCCUPATION4 -0.384026   0.080997  -4.741 2.75e-06 ***\nOCCUPATION5 -0.050349   0.072726  -0.692 0.489054    \nOCCUPATION6 -0.265467   0.079977  -3.319 0.000966 ***\nSECTOR1      0.114832   0.054868   2.093 0.036846 *  \nSECTOR2      0.093247   0.096523   0.966 0.334462    \nMARR1        0.062131   0.041037   1.514 0.130629    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4278 on 518 degrees of freedom\nMultiple R-squared:  0.3613,    Adjusted R-squared:  0.3428 \nF-statistic: 19.53 on 15 and 518 DF,  p-value: &lt; 2.2e-16\n\n\nNotice that now age and education are statistically significant. Also the \\(R^2\\) is still about the same - it’s gone down from .362 to .361. Plus now we can be much more confident in the coefficient estimates.\nLet’s check the VIF values to see if that problem has been fixed\n\nvif(wage.log.fit2)\n\n               GVIF Df GVIF^(1/(2*Df))\nEDUCATION  1.813186  1        1.346546\nSOUTH      1.060840  1        1.029971\nSEX        1.268426  1        1.126244\nUNION      1.128733  1        1.062418\nAGE        1.190469  1        1.091086\nRACE       1.094327  2        1.022791\nOCCUPATION 2.999691  5        1.116112\nSECTOR     1.444471  2        1.096294\nMARR       1.109491  1        1.053324\n\n\nYes. We’d really only be concerned if a single VIF value was greater than 10, or if the average was much bigger than 2.5 or so. This is nothing to be concerned with.\nChecking other model diagnostics - to make sure our linear regression assumptions are still met\n\nplot(wage.log.fit2, which=1:2)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>29</span>  <span class='chapter-title'>R10 multiple regression further concepts</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html",
    "href": "mlr_practice.html",
    "title": "30  Multiple Linear Regression Practice",
    "section": "",
    "text": "31 Practice Problems",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#predicting-salary",
    "href": "mlr_practice.html#predicting-salary",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.1 Predicting salary",
    "text": "31.1 Predicting salary\nSuppose we have a data set with five predictors, \\(X_1 =\\) GPA, \\(X_2 =\\) IQ, $X_3 =$ Level ( \\(1\\) for College and \\(0\\) for High School), \\(X_4\\) = Interaction between GPA and IQ, and \\(X_5\\) = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\hat{\\beta}_0=50, \\hat{\\beta}_1=20, \\hat{\\beta}_2=0.07, \\hat{\\beta}_3=35, \\hat{\\beta}_4=0.01\\) and \\(\\hat{\\beta}_5 = -10\\).\n\n\nWhich answer is correct, and why?\n\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt helps to write out the estimated model with variable names:\n\\[\\widehat{Salary} = 50 + 20(GPA)+0.07(IQ) + 35(1_{college})+0.01(GPA \\times IQ) -10 (GPA \\times 1_{college})\\]\nIf we fix IQ and GPA, the models for salaries are:\n\\[\\widehat{Salary}_{HS} = 50 + 20(GPA)+0.07(IQ) +0.01(GPA \\times IQ)\\]\n\\[\\widehat{Salary}_{C} = 85 + 10(GPA)+0.07(IQ) +0.01(GPA \\times IQ)\\]\nIf we compare these two you can see that the effect of higher GPA is more substantial for high school students. What is the difference in salary?\n\\[ \\widehat{Salary}_{C}-\\widehat{Salary}_{HS} = 35-10(GPA)\\] If \\(GPA &lt; 3.5\\) this difference is positive (i.e. college grads earn a higher salary). If \\(GPA &gt; 3.5\\) then the difference is negative (i.e. high school grads earn higher salary)\nThus iii is the correct choice.\n\n\n\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n85+ 10*4 + .07*110 + 0.01*4*110\n\n[1] 137.1\n\n\nThe predicted salary is $137.1\n\n\n\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe magnitude of the coefficient has nothing to do with the level of evidence of the interaction. It’s all relative to the standard error of this coefficient. A small magnitude coefficient with a very small standard error would have very little uncertainty. If the standard error is big that would mean the uncertainty is great. This is why we take the ratio of \\(\\hat{\\beta}_j/se(\\hat{\\beta}_j)\\)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#cubic-model",
    "href": "mlr_practice.html#cubic-model",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.2 Cubic Model",
    "text": "31.2 Cubic Model\nI collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\\).\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is linear, i.e. \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn expectation (in the sense of expected value) we would expect \\(\\hat{\\beta}_2=\\hat{\\beta}_3=0\\) and then RSS would be the same for both the linear and the cubic model. However in practice the data will not exhibit a perfect straight line relationship and thus these coefficients will be nonzero. That would mean the cubic model will fit slightly better and RSS will be lower for the cubic model.\n\n\n\n\nAnswer (a) using test rather than training RSS.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA cubic model would presumably slightly overfit to the training data, and could actually perform worse on test data, data that is drawn from a truly linear model. So we would expect test RSS to be lower with the cubic model.\n\n\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWithout knowing what the relationship is, this would be hard to answer. However if there is any curve to the true relationship a cubic model will most likely fit the training data better and will give us a lower RSS than a linear model.\n\n\n\n\nAnswer (c) using test rather than training RSS.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#auto-i",
    "href": "mlr_practice.html#auto-i",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.3 Auto I",
    "text": "31.3 Auto I\nThis question involves the use of simple linear regression on the Auto data set.\n\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n\nFor example:\ni. Is there a relationship between the predictor and the response?\nii. How strong is the relationship between the predictor and the response?\niii. Is the relationship between the predictor and the response positive or negative?\niv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.2.3\n\nauto.fit &lt;- lm(mpg~1+horsepower, data=Auto)\nsummary(auto.fit)\n\n\nCall:\nlm(formula = mpg ~ 1 + horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a very strong relationship between the predictor and response. The p-value is close to zero meaning this data would be virtually impossible if the relationship was not real. The relationship is negative, clearly by the sign of the coefficient. Higher horsepower vehicles tend on average to have lower fuel efficiency.\n\npredict(auto.fit, newdata=data.frame(horsepower=98), interval=\"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\npredict(auto.fit, newdata=data.frame(horsepower=98), interval=\"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n\nA vehicle with 98 horsepower has a predicted fuel efficiency of 24.47 mpg. The 95% confidence interval says that we are 95% confident that the average mpg of cars with 98 horsepower is within 23.97 and 24.96 mpg. The 95% prediction interval tells us that if we pick one arbitrary car with a horsepower of 98, we are 95% confident that its mpg will be within 13.81 and 34.12 mpg. You can tell that there is quite a bit of variation among cars.\n\n\n\n\nPlot the response and the predictor. Use the abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#auto-ii",
    "href": "mlr_practice.html#auto-ii",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.4 Auto II",
    "text": "31.4 Auto II\nThis question involves the use of multiple linear regression on the Auto data set.\n\n\nProduce a scatterplot matrix which includes all of the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, cor() which is qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n\ni. Is there a relationship between the predictors and the response?\nii. Which predictors appear to have a statistically significant relationship to the response?\niii. What does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\nTry a few different transformations of the variables, such as \\(log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#carseats",
    "href": "mlr_practice.html#carseats",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.5 Carseats",
    "text": "31.5 Carseats\nThis question should be answered using the Carseats data set.\n\n\nFit a multiple regression model to predict Sales using Price, Urban, and US.\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nWrite out the model in equation form, being careful to handle the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\nOn the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\nHow well do the models in (a) and (e) fit the data?\nUsing the model from (e), obtain 95% confidence intervals for the coefficient(s).\nIs there evidence of outliers or high leverage observations in the model from (e)?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#simulated-data",
    "href": "mlr_practice.html#simulated-data",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.6 Simulated Data",
    "text": "31.6 Simulated Data\nIn this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.\n\n\nUsing the rnorm() function, create a vector, x, containing \\(100\\) observations drawn from a \\(N(0, 1)\\) distribution. This represents a feature, \\(X\\).\nUsing the rnorm() function, create a vector, eps, containing \\(100\\) observations drawn from a \\(N(0, 0.5^2)\\) distribution—a normal distribution with mean zero and variance 0.25.\nUsing x and eps, generate a vector y according to the model \\(Y = −1 + 0.5X + \\epsilon\\). What is the length of the vector y? What are the values of \\(\\beta_0\\) and \\(\\beta_1\\) in this linear model?\nCreate a scatterplot displaying the relationship between x and y. Comment on what you observe.\nFit a least squares linear model to predict y using x. Comment on the model obtained. How do \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) compare to \\(\\beta_0\\) and \\(\\beta_1\\)?\nDisplay the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.\nNow fit a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model fit? Explain your answer.\nRepeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results.\nRepeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term in (b). Describe your results.\nWhat are the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#collinearity",
    "href": "mlr_practice.html#collinearity",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.7 Collinearity",
    "text": "31.7 Collinearity\nThis problem focuses on the collinearity problem.\n\n\nPerform the following commands in R:\n\n\nset.seed (1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5 * x1 + rnorm(100) / 10\ny &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n\nThe last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?\n\nWhat is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.\nUsing this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)? How do these relate to the true \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\)? Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)? How about the null hypothesis \\(H_0 : \\beta_2 = 0\\)?\nNow fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)?\nNow fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)?\nDo the results obtained in (c)–(e) contradict each other? Explain your answer.\nNow suppose we obtain one additional observation, which was unfortunately mismeasured.\n\n\nx1 &lt;- c(x1 , 0.1)\nx2 &lt;- c(x2 , 0.8)\ny &lt;- c(y, 6)\n\nRe-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#boston",
    "href": "mlr_practice.html#boston",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.8 Boston",
    "text": "31.8 Boston\nThis problem involves the Boston data set. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nFor each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the \\(x\\)-axis, and the multiple regression coefficients from (b) on the \\(y\\)-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the \\(x\\)-axis, and its coefficient estimate in the multiple linear regression model is shown on the \\(y\\)-axis.\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form\n\n\\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon.\\)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#rock",
    "href": "mlr_practice.html#rock",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.9 Rock",
    "text": "31.9 Rock\nThe rock data set, which comes packaged with R, describes 48 rock samples taken at a petroleum reservoir. Each sample has four measurements, all related to measuring permeability of the rock (the geological details are not so important, here):\n\narea: the area of the pores (measured in a number of pixels out of a 256-by-256 image that were “pores”)\nperi: perimeter of the “pores” part of the sample\nshape: perimeter(peri) of the pores part divided by square root of the area (area)\nperm: a measurement of permeability (in milli-Darcies, a unit of permeability, naturally)\n\n\nplotting the data\n\nSuppose that our goal is to predict permeability (perm) from other characteristics of the rock.\nFor each of the three other variables area, peri and shape, fit a linear regression model that predicts perm from this variable and an intercept term. That is, you should fit three models, each using one of area, peri and shape.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\narea_lm &lt;- lm(perm ~ 1 + area, data=rock)\nperi_lm &lt;- lm(perm ~ 1 + peri, data=rock)\nshape_lm &lt;- lm(perm ~ 1 + shape, data=rock)\n\n\n\n\nPart b: comparing fits\nCompute the RSS for each of the three models fitted in Part a. Which is best?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_rss &lt;- function( fitted_model ) {\n  return( sum( (fitted_model$residuals)**2 ) )\n}\nmapply( my_rss, list(area_lm, peri_lm, shape_lm) )  \n\n[1] 7591852 4092864 6216896\n\n\nThe model using peri to predict perm achieves the smallest RSS.\n\n\n\nPart c: multiple regression\nNow, fit a model that predicts perm from the other three variables (you should not include any interaction terms).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# TODO: replace NA with model-fitting code\nfull_lm &lt;- lm( perm ~ 1 + area + peri + shape, data=rock )\n\nsummary(full_lm)\n\n\nCall:\nlm(formula = perm ~ 1 + area + peri + shape, data = rock)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-750.26  -59.57   10.66  100.25  620.91 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 485.61797  158.40826   3.066 0.003705 ** \narea          0.09133    0.02499   3.654 0.000684 ***\nperi         -0.34402    0.05111  -6.731 2.84e-08 ***\nshape       899.06926  506.95098   1.773 0.083070 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 246 on 44 degrees of freedom\nMultiple R-squared:  0.7044,    Adjusted R-squared:  0.6843 \nF-statistic: 34.95 on 3 and 44 DF,  p-value: 1.033e-11\n\n\n\n\n\nPart d: interpreting model fits\nConsider the coefficient of peri in the model from Part c.\n\nGive an interpretation of this estimated coefficient.\nIs this coefficient statistically significantly different from zero at the \\(\\alpha=0.01\\) level?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe estimated coefficient of peri is statistically significantly different from zero at the \\(0.01\\) level (the p-value is approximately \\(3*10^{-8}\\)).\nThe estimate of the coefficient indicates that holding area and shape constant, a unit increase in peri is associated with a decrease of \\(0.344\\) in perm.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#fitting-and-interpreting-a-linear-regression-model",
    "href": "mlr_practice.html#fitting-and-interpreting-a-linear-regression-model",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.10 Fitting and Interpreting a Linear Regression Model",
    "text": "31.10 Fitting and Interpreting a Linear Regression Model\nThe trees data set in R contains measurements of diameter (in inches), height (in feet) and the amount of timber (volume, in cubic feet) in each of 31 trees. See ?trees for additional information.\nThe code below loads the data set. Note that the column of the data set encoding tree diameter is mistakenly labeled Girth (girth is technically a measure of circumference, not a diameter).\n\ndata(trees)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nPart a: examining correlations\nIt stands to reason that the volume of timber in a tree should scale with both the height of the tree and the diameter. However, it also stands to reason that height and diameter are highly correlated. Use the cor function to compute the pairwise correlations among the three variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncor(trees)\n\n           Girth    Height    Volume\nGirth  1.0000000 0.5192801 0.9671194\nHeight 0.5192801 1.0000000 0.5982497\nVolume 0.9671194 0.5982497 1.0000000\n\n\n\n\n\nSuppose that we had to choose either tree diameter (labeled Girth in the data) or tree height with which to predict volume. Based on these correlations, Which would you choose? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGirth has a much higher correlation with Volume than does Height, so it is likely to be a better predictor.\n\n\n\nPart b: comparing model fits\nWell, let’s put the above to a test. Use lm to fit two linear regression models from this data set (both should, of course, include intercept terms):\n\nPredicting volume from height\nPredicting volume from diameter (labeled Girth in the data set)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nht_lm &lt;- lm( Volume ~ 1 + Height, data=trees );\ngr_lm &lt;- lm( Volume ~ 1 + Girth, data=trees );\n\nc( my_rss(ht_lm), my_rss(gr_lm) )\n\n[1] 5204.8950  524.3025\n\n\n\n\n\nCompare the sum of squared residuals of these two models. Which is better? Does this agree with your observations in Part a?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model predicting volume from girth is indeed a better model, achieving a factor of ten decrease in RSS compared to the model using height. This is in agreement with the prediction in part a.\n\n\n\nExamining the model outputs above (or extracting information again here), what do each of your fitted models conclude about the null hypothesis that the slope is equal to zero?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary( ht_lm )\n\n\nCall:\nlm(formula = Volume ~ 1 + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.274  -9.894  -2.894  12.068  29.852 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -87.1236    29.2731  -2.976 0.005835 ** \nHeight        1.5433     0.3839   4.021 0.000378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.4 on 29 degrees of freedom\nMultiple R-squared:  0.3579,    Adjusted R-squared:  0.3358 \nF-statistic: 16.16 on 1 and 29 DF,  p-value: 0.0003784\n\n\n\nsummary( gr_lm )\n\n\nCall:\nlm(formula = Volume ~ 1 + Girth, data = trees)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nBoth models conclude that the coefficient of the single predictor is statistically significantly different from zero at level \\(0.001\\).\n\n\n\nPart c: volume and diameter\nThinking back to fourth grade geometry with Mrs. Galvin, you remember that the area of a circle grows like the square of the diameter (that is, if we double the diameter, the area of the circle quadruples). It follows that timber volume, which is basically the volume of a cylinder, should scale linearly with the square of the diameter.\nCreate a scatter plot of volume as a function of diameter. Does the “geometric” intuition sketched above agree with what you see in the plot? Why or why not?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(trees$Girth, trees$Volume)\n\n\n\n\n\n\n\n\nAt least visually, this looks like a simple linear relationship. It doesn’t appear that using squared diameter should change anything in terms of prediction performance.\n\n\n\nPart d: incorporating non-linearities\nFit a linear regression model predicting volume from the squared diameter (and an intercept term, of course). Compute the residual sum of squares of this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndiam2_lm &lt;- lm( Volume ~ 1 + I(Girth^2), data=trees)\n\nmy_rss( diam2_lm )\n\n[1] 329.3191\n\n\n\n\n\nCompare the RSS of this model with that obtained in Part b using the linear diameter. Which is better, the quadratic model or the linear model? Does this surprise you? Why or why not? If you are surprised, what might explain the observation?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is indeed notably smaller than the RSS achieved by the linear model. This is surprising given that there is no clear nonlinearity in the scatterplot, but it is in keeping with our “geometric” intuition.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic.html",
    "href": "logistic.html",
    "title": "31  Logistic Regression",
    "section": "",
    "text": "31.1 Learning objectives\nThese notes concern the problem of logistic regression. This is a regression method that allows us to handle the situation where we have predictors and responses, but the predictors are binary rather than continuous. Fitting a linear regression model in this situation would be a bit silly, for reasons illustrated below. Logistic regression lets us keep all the “nice” things about linear regression (i.e., that we like linear functions), while being better suited to this different data setting.\nAfter this lecture, you will be able to",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#learning-objectives",
    "href": "logistic.html#learning-objectives",
    "title": "31  Logistic Regression",
    "section": "",
    "text": "Explain the motivation for logistic regression\nFit a logistic regression model to data with one or more predictors and a binary response\nInterpret estimated coefficients in a logistic regression model",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#logistic-regression-motivation",
    "href": "logistic.html#logistic-regression-motivation",
    "title": "31  Logistic Regression",
    "section": "31.2 Logistic regression: motivation",
    "text": "31.2 Logistic regression: motivation\nConsider the following data, which you’ll play with more in discussion section: the Pima Indian data set.\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nhead(Pima.te)\n\n  npreg glu bp skin  bmi   ped age type\n1     6 148 72   35 33.6 0.627  50  Yes\n2     1  85 66   29 26.6 0.351  31   No\n3     1  89 66   23 28.1 0.167  21   No\n4     3  78 50   32 31.0 0.248  26  Yes\n5     2 197 70   45 30.5 0.158  53  Yes\n6     5 166 72   19 25.8 0.587  51  Yes\n\n\nThis data was collection by the US National Institute of Diabetes and Digestive and Kidney Diseases. Each row of this data set corresponds to a woman of Pima Indian descent living near Phoenix Arizona. The columns include a number of biomarkers (e.g., glucose levels, blood pressure and age), as well as a column indicating whether or not each subject had diabetes (as measured according to measures laid out by the World Health Organization). See ?Pima.te for more information.\nCan we predict whether or not a given person has diabetes based on their biomarkers? It is natural to cast this as a prediction problem just like the linear regression problems we have discussed in recent weeks: we are given pairs \\((X_i, Y_i)\\), where \\(X_i\\) is a vector of predictors and \\(Y_i\\) is a response. The difference is that now, \\(Y_i\\) is more naturally thought of as a binary label (in this case, indicating whether or not a subject has diabetes), instead of a continuous number.\nExample: image classification Suppose that we have a collection of images, and our goal is to detect whether or not a given image has a cat in it. Our data takes the form of a collection of pairs \\((X_i, Y_i)\\), \\(i=1,2,\\dots,n\\) where \\(X_i\\) is an image (e.g., a bunch of a pixels) and \\[\nY_i = \\begin{cases} 1 &\\mbox{ if image } i \\text{ contains a cat }\\\\\n                    0 &\\mbox{ if image } i \\text{ does not contain a cat. }\n                    \\end{cases}\n\\] Example: fraud detection An online banking or credit card service might like to be able to detect whether or not a given transaction is fraudulent. Predictors \\(X_i\\) might take the form of things like transaction amount, location and time, and the binary label \\(Y_i\\) corresponds to whether or not the transaction is fraudulent.\nExample: brain imaging In own work in neuroscience, a common task is to detect whether or not a subject has a disease or disorder (e.g., Parkinson’s or schizophrenia) based on brain imaging data obtained via, for example, functional magnetic resonance imaging (fMRI). Our labels \\(Y_i\\) are given by \\[\nY_i = \\begin{cases} 1 &\\mbox{ if subject } i \\text{ has the disease/disorder }\\\\\n                    0 &\\mbox{ if subject } i \\text{ does not have the disease/disorder. }\n                    \\end{cases}\n\\] Our predictors \\(X_i\\) for subject \\(i \\in \\{1,2,\\dots,n\\}\\) might consist of a collection of features derived from the brain imagining (e.g., average pixel values or correlations between different locations in the brain). In my own research, \\(X_i\\) is a network that describes brain structure, constructed from the imaging data. In other studies, \\(X_i\\) might even be the brain imaging data itself. Our goal is to predict \\(Y_i\\) based on the observed imaging data (or image-derived features) \\(X_i\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#formulating-a-model",
    "href": "logistic.html#formulating-a-model",
    "title": "31  Logistic Regression",
    "section": "31.3 Formulating a model",
    "text": "31.3 Formulating a model\nA natural approach to this problem would be to just take our existing knowledge of linear regression and apply it here. If our predictor \\(X_i\\) has \\(p\\) dimensions, \\[\nX_i = (X_{i,1}, X_{i,2}, X_{i,3}, \\dots, X_{i,p} )^T \\in \\mathbb{R}^p,\n\\] then we might try to fit a model of the form \\[\nY_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_p X_{i,p}\n\\]\nThe obvious problem with this is that the right-hand side of this equation is an arbitrary real number, whereas the left-hand side (i.e., \\(Y_i\\)) is \\(0\\) or \\(1\\).\n\n31.3.1 Example: linear regression on the Pima data set\nFor easier coding later, let’s add a column to the Pima.te data set. The type column is Yes or No according to whether or not each subject has diabetes. Let’s add a column with a more straightforward name and which directly encodes this diabetes status as \\(0\\) or \\(1\\).\n\nPima.te$diabetes &lt;- ifelse( Pima.te$type=='Yes', 1, 0)\nhead(Pima.te)\n\n  npreg glu bp skin  bmi   ped age type diabetes\n1     6 148 72   35 33.6 0.627  50  Yes        1\n2     1  85 66   29 26.6 0.351  31   No        0\n3     1  89 66   23 28.1 0.167  21   No        0\n4     3  78 50   32 31.0 0.248  26  Yes        1\n5     2 197 70   45 30.5 0.158  53  Yes        1\n6     5 166 72   19 25.8 0.587  51  Yes        1\n\n\nNow, let’s pick a predictor and fit a simple linear regression model.\n\npima_lm &lt;- lm( diabetes ~ 1 + glu, data=Pima.te );\nsummary(pima_lm)\n\n\nCall:\nlm(formula = diabetes ~ 1 + glu, data = Pima.te)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9516 -0.2701 -0.1138  0.2408  1.0025 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.6278036  0.0892443  -7.035 1.16e-11 ***\nglu          0.0080171  0.0007251  11.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4023 on 330 degrees of freedom\nMultiple R-squared:  0.2703,    Adjusted R-squared:  0.2681 \nF-statistic: 122.3 on 1 and 330 DF,  p-value: &lt; 2.2e-16\n\n\nLooks like we managed to capture some signal there, at least according to the F-statistic! Let’s look at a plot.\n\nlibrary(ggplot2)\npp &lt;- ggplot(data=Pima.te, aes(x=glu, y=diabetes)) + geom_point()\npp &lt;- pp + geom_smooth(method='lm', se=FALSE, formula='y ~ 1 + x')\npp\n\n\n\n\n\n\n\n\nHmm… so our fitted model takes in a number (i.e., glu) and outputs a number that is our prediction for \\(Y_i\\). The trouble is that our responses are binary, but our outputs can be anything. If our goal is to predict \\(y\\), and we predict \\(y\\) to be \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\), then we can (potentially) predict arbitrary large or small values for \\(y\\), if \\(x\\) is suitably large or small. That’s a bit of an awkward fact– after all, we want our prediction to be \\(0\\) or \\(1\\). This isn’t the end of the world. We could do something like “clipping” our output \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) to whichever of \\(0\\) or \\(1\\) is closest or something like that. But this feels clumsy, at best. Is there a more appropriate approach?\nWell, there are many, but an especially simple one is logistic regression, which seeks to keep the “linear combination of predictors” idea that we like so much from linear regression, but modify how we make predictions to be better-suited to the “binary response” setting.\n\n\n31.3.2 Logistic regression\nThe core problem with applying linear regression to a binary response is that the output of linear regression can be any number, while we are really only interested in outputs in \\(\\{0,1\\}\\). How might we transform our linear prediction, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\), into something more sensible?\nLogistic regression solves this problem by taking our linear prediction \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) and turning it into a probability, i.e., a number between \\(0\\) and \\(1\\). We do that using the logistic function, \\[\n\\sigma( z ) = \\frac{ 1 }{1 + e^{-z}} = \\frac{ e^{z} }{ 1 + e^{z} }.\n\\]\nHere is a plot of the function:\n\nz&lt;- seq(-4,4,0.01);\nplot( z, 1/(1+exp(-z)), col='blue' );\nabline(h=0); abline(h=1);\nabline(v=0)\n\n\n\n\n\n\n\n\nThe logistic function is an example of a sigmoid function (fancy Greek for “S-shaped”).\nUsing the logistic function, we can modify the output of our model by passing our linear regression model’s prediction, which is a real number, as the input of the logistic function so that it outputs a number between zero and one. \\[\n\\sigma\\left( \\hat{y} \\right)\n=\n\\sigma\\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\right)\n=\n\\frac{ 1 }{1 + \\exp\\{ -(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\} }\n=\n\\frac{ \\exp\\{ \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\} }{1 + \\exp\\{ \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\} }\n\\]\nThe especially nice thing about this is that since this number is between \\(0\\) and \\(1\\), we can interpret this as a probability: \\[\n\\Pr[ Y_i = 1 ; X_i=x, \\beta_0, \\beta_1 ]\n=\n\\sigma\\left( \\beta_0 + \\beta_1 x \\right)\n=\n\\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n\\]\nThe semi-colon notation there is just to stress that we are treating \\(\\beta_0\\) and \\(\\beta_1\\) as parameters and the data \\(x\\) as an input, but not as things that we are conditioning on.\nAside: there is a whole branch of statistics called Bayesian statistics that seeks to treat parameters like our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) as variables that we can condition on. It’s a very cool area, especially useful in machine learning, but it will have to wait for later courses!\nNow, when it comes time to make a prediction on input \\(x\\), we can pass \\(\\beta_0 + \\beta_1 x\\) into the sigmoid function, and predict \\[\n\\hat{y} = \\begin{cases} 1 &\\mbox{ if } \\sigma( \\beta_0 + \\beta_1 x ) &gt; \\frac{1}{2} \\\\\n                        0 &\\mbox{ if } \\sigma( \\beta_0 + \\beta_1 x ) \\le \\frac{1}{2}. \\end{cases}\n\\]\nIn the multivariate setting, where our \\(i\\)-th predictor \\(X_i\\) takes the form \\[\nX_i = \\left( X_{i,1}, X_{i,2}, \\dots, X_{i,p} \\right),\n\\]\nmultiple logistic regression is the straight-forward extension of this idea. Given (estimated) coefficients \\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\), we output a probability \\[\n\\begin{aligned}\n\\Pr[ Y_i = 1 &; X_i=(x_1,x_2,\\dots,x_p), \\beta_0, \\beta_1, \\dots, \\beta_p ] \\\\\n&=\n\\sigma\\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p \\right) \\\\\n&=\n\\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p) \\} }\n\\end{aligned}\n\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#logistic-regression-in-r",
    "href": "logistic.html#logistic-regression-in-r",
    "title": "31  Logistic Regression",
    "section": "31.4 Logistic regression in R",
    "text": "31.4 Logistic regression in R\nLet’s ignore, for now, the question of how we fit this model (spoiler alert: least squares isn’t going to make a lot of sense anymore; we’ll need something more clever), let’s ask R to fit a logistic regression model to our problem of predicting diabetes from the glu variable in the Pima data set.\nThe function that we use in R is glm. “GLM” stands for generalized linear model. That is, we are generalizing linear regression. In particular, we are generalizing linear regression by doing linear regression, but then passing the linear regression prediction \\(\\beta_0 + \\beta_1 x\\) through another function.\nTo perform logistic regression, we need to specify to R that we are using a “binomial” family of responses– our response \\(Y\\) is binary, so it can be thought of as a Binomial random variable, with \\(\\Pr[ Y = 1] = \\sigma( \\beta_0 + \\beta_1 X)\\).\nOther than that, fitting a model with glm is basically the same as using plain old lm. Even the model summary output looks about the same:\n\npima_logistic &lt;- glm( diabetes ~ 1 + glu, data=Pima.te, family=binomial );\nsummary(pima_logistic)\n\n\nCall:\nglm(formula = diabetes ~ 1 + glu, family = binomial, data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2343  -0.7270  -0.4985   0.6663   2.3268  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.946808   0.659839  -9.013   &lt;2e-16 ***\nglu          0.042421   0.005165   8.213   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 325.99  on 330  degrees of freedom\nAIC: 329.99\n\nNumber of Fisher Scoring iterations: 4\n\n\nBut notice now that our model’s outputs are always between \\(0\\) and \\(1\\):\n\npp &lt;- ggplot( Pima.te, aes(x=glu, y=diabetes) ) + geom_point();\npp &lt;- pp + geom_smooth(formula='y ~ 1+x', se=FALSE,\n                       method='glm',\n                       method.args=list(family = \"binomial\") );\n# Note that geom_smooth needs an extra list() of arguments to specify the\n# extra arguments that we passed to the glm() function above.\n# In particular, we need to tell the GLM to use a binomial response.\npp\n\n\n\n\n\n\n\n\nIf we interpret our model’s output as a probability, namely the probability that we observe label \\(Y\\) for predictors \\(X\\), then it is clear that as glu increases, our model thinks it is more likely that a person has diabetes.\nJust as with linear regression, our logistic model object will make predictions for new, previously unseen predictors if we use the predict function.\n\n# Reminder: we pass a data frame into the predict function, and our model\n# will produce an output for ease row of that data frame.\n# So this is making a prediction for a subject with `glu=200`.\n# Looking at our scatter plot above, it's clear that `glu=200` is at the\n# far upper end of our data distribution, so we should expect our model to\n# produce an output close to 1.\npredict(pima_logistic, type='response', newdata = data.frame(glu=200) )\n\n        1 \n0.9267217 \n\n\nGenerally speaking, as glu increases, our model is more confident that the subject has diabetes.\n\nglu_vals &lt;- seq(50,200,10);\nlogistic_outputs &lt;- predict(pima_logistic, type='response',\n                            newdata = data.frame(glu=glu_vals) )\nplot( glu_vals, logistic_outputs)\n\n\n\n\n\n\n\n\nAgain, these are our model’s predicted outputs for given input values for glucose levels. As glu increases, our model assigns higher and higher probabilities to the possibility that a subject has diabetes.\nYou’ll play around with logistic regression and the Pima data set more in discussion section, where you’ll find that if we choose our predictors more carefully (and use more predictors), we can build a pretty good model!",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#interpreting-model-coefficients",
    "href": "logistic.html#interpreting-model-coefficients",
    "title": "31  Logistic Regression",
    "section": "31.5 Interpreting model coefficients",
    "text": "31.5 Interpreting model coefficients\nIn linear regression, our model coefficients had a simple interpretation: \\(\\beta_1\\) was the change in response associated with a unit change in the corresponding predictor. Is there an analogous interpretation for logistic regression?\nWell, there is, but it requires a bit of extra work to define some terms so that we can talk sensibly about what changes in response to a change in a predictor.\nThe odds of an event \\(E\\) with probability \\(p\\) are given by \\[\n\\operatorname{Odds}(E) = \\frac{ p }{ 1-p }.\n\\]\nThat is, the odds of an event is the ratio of the probability of the event happening to the probability that it doesn’t happen. So, for example, if we have an event that occurs with probability \\(1/2\\), the odds of that event are \\((1/2)/(1/2) = 1\\), which we usually say as “one to one odds”. Some of this kind of vocabulary may be familiar to you from sports betting, if you’ve been to Las Vegas or watched the Kentucky Derby.\nSo, let’s suppose that we have a logistic regression model with a single predictor, that takes predictor \\(x\\) and outputs a probability \\[\np(x) = \\frac{ 1 }{ 1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }.\n\\] and note that \\[\n1-p(x) = \\frac{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}}\n\\] The odds associated with this probability are \\[\n\\operatorname{Odds}(x)\n= \\frac{ p(x) }{1-p(x)}\n= \\frac{1 }{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n= \\exp\\{ \\beta_0 + \\beta_1 x \\}.\n\\]\nSince the odds can get very large or very small, especially once we start working with small probabilities, it is often much easier to work with the logarithm of the odds, usually called the “log-odds” for short. This quantity is especially important in statistics associated with biology (e.g., drug trials and studies of risks associated with diseases). If we look at the log odds associated with our logistic regression model, \\[\n\\operatorname{Log-Odds}(x)\n= \\log \\operatorname{Odds}(x)\n= \\log \\exp\\{ \\beta_0 + \\beta_1 x \\}\n= \\beta_0 + \\beta_1 x.\n\\]\nIn other words, under our logistic regression model, the “slope” \\(\\beta_1\\) is the increase in the log-odds of the response being \\(1\\) associated with a unit increase in \\(x\\).\nSaid yet another way, logistic regression is what happens if we use a linear regression model to predict the log-odds of the event that the response is \\(1\\), i.e., the log-odds of the event \\(Y_i = 1\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#fitting-the-model",
    "href": "logistic.html#fitting-the-model",
    "title": "31  Logistic Regression",
    "section": "31.6 Fitting the model",
    "text": "31.6 Fitting the model\nWe said previously that we were going to ignore, for the time being, the matter of how we fit out logistic regression model. Well, let’s come back to that question.\nLet’s start by recalling ordinary least squares regression, where we had the following loss function:\n\\[\n\\ell( \\beta_0 ,\\beta_1 )\n= \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\n= \\sum_{i=1}^n (\\hat{y}_i - y_i)^2,\n\\]\nwhere \\(\\hat{y_i}=\\beta_0+\\beta_1 x_i\\) is the predicted \\(y_i\\) based on the coefficients \\(\\beta_0,\\beta_1\\). This function measures how “wrong” our model is if we use coefficients \\(\\beta_0\\) and \\(\\beta_1\\), and we choose these coefficients in such a way as to make thie loss function as small as possible. We used the square of the loss because it makes a lot of the math easier.\nNow that we’re working with logistic regression instead of linear, how do we decide what makes a good choice of coefficient? We could try and force a way to use least squares, but things would get a bit complicated.\nLet’s try something different. Our logistic regression model takes a predictor and outputs a probability that the response is \\(1\\). That is, for our \\(i\\)-th predictor, our model outputs (we’re sticking with the case of one predictor for simplicity, but the idea extends to multiple predictors in the natural way): \\[\n\\Pr[ Y_i=1; X_i=x, \\beta_0, \\beta_1 ]\n= \\frac{1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}}.\n\\]\nIn other words, this is the probability that our model gives to the event that the \\(i\\)-th response is \\(1\\). On the other hand, our model assigns probability to the event that the \\(i\\)-th response is \\(0\\) given by \\[\n\\begin{aligned}\n\\Pr[ Y_i=0; X_i=x, \\beta_0, \\beta_1 ]\n&=\n1-\\Pr[ Y_i=1; X_i=x, \\beta_0, \\beta_1 ] \\\\\n&= \\frac{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\\\\n&= \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}}\n.\n\\end{aligned}\n\\]\nNow, if the \\(i\\)-th response really is \\(0\\), then we want this number to be big. And if the \\(i\\)-th response really is \\(1\\), we want \\(1/(1+\\exp\\{ -(\\beta_0 + \\beta_1 x )\\})\\) to be big.\nFurther, we want this pattern to hold for all of our data. That is, we want our model to give a “high probability” to all of our data. If our \\(i\\)-th observation takes the form \\((x_i,y_i)\\), then the probability of our data under the model, assuming the observations are independent, is \\[\n\\prod_{i=1}^n \\Pr[ Y_i=y_i; X_i=x_i, \\beta_0, \\beta_1 ].\n\\]\nFor one of our data points, using the fact that \\(y_i\\) is either \\(0\\) or \\(1\\), we can write \\[\n\\Pr[ Y_i=y_i; X_i=x_i, \\beta_0, \\beta_1 ]\n=\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}.\n\\] So the probability of our whole data set is \\[\n\\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}.\n\\] So this is the probability of all of our data under our model, for a particular choice of parameters \\(\\beta_0\\) and \\(\\beta_1\\). We call this the likelihood of the data. One way to pick our model coefficients is to choose them so that this quantity is large– if this model is large, that means our data agrees with the model.\nSo we want to choose \\(\\beta_0\\) and \\(\\beta_1\\) to make this probability, the likelihood, large. How do we do that? Well, our likelihood is a product of a bunch of things, and products are hard to work with. Let’s take a logarithm. Remember, logs turn products into sums, and sums are easy to work with. \\[\n\\begin{aligned}\n\\log\n& \\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\sum_{i=1}^n \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i} \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}\n\\end{aligned}\n\\]\nNow, let’s notice that an individual term in this sum is a log of a product, and that \\(\\log a^b = b \\log a\\), so that: \\[\n\\begin{aligned}\n\\log &\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i} \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n+ \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= y_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)\n\\end{aligned}\n\\]\nSo the logarithm of our likelihood is \\[\n\\begin{aligned}\n\\log\n& \\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\sum_{i=1}^n\ny_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right).\n\\end{aligned}\n\\] Notice that since the logarithm function is monotone, making the likelihood large is the same as making the likelihood large and vice versa.\nLogistic regression chooses the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to make this log-likelihood large. Equivalently, we minimize the negative log likelihood, \\[\n\\ell(\\beta_0, \\beta_1)\n=\n-\\sum_{i=1}^n \\left[\ny_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)\n\\right].\n\\]\nThat is, while our loss function for linear regression is the sum of squared errors, our loss function for logistic regression is the negative log likelihood.\nUnlike in the least squares case, these optimized coefficients do not have nice closed form solutions. Still, when we conduct logistic regression in R, this minimization problem is solved for us using tools from optimization.\n\n31.6.1 A bit of philosophy: maximum likelihood\nIt turns out that this idea of choosing our parameters to maximize the probability of the data is so popular in statistics that it has a name: maximum likelihood estimation. When we need to estimate the value of a parameter, we choose the one that maximizes the likelihood of the data.\nInterestingly, in many situations, the least squares estimate and the maximum likelihood estimate of a parameter are, in fact, the same. In many situations, the sample mean is the least squares estimate of our parameter, and it is also the estimate that we would obtain if we maximize the likelihood. Examples of this kind of situation include: linear regression, estimating the mean of a normal, and estimating the rate parameter of the Poisson.\nYou’ll see many more connections between these ideas, and establish some of the interesting properties of maximum likelihood estimation in your more advanced statistics classes. For now, let’s very quickly establish that the sample mean of the normal is both the least squares estimator and the maximum likelihood estimator.\n\n\n31.6.2 Example: mean of a normal\nConsider the setting in which we observe data \\(X_1,X_2,\\dots,X_n\\) drawn independently and identically distributed according to a normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\) (assuming the variance is known is just for the sake of making some things simpler– a similar story is true if we have to estimate the variance, as well).\nSo let’s suppose that we observe \\(X_1=x_1, X_2=x_2, \\dots, X_n=x_n\\). Let’s consider two different ways to estimate the mean \\(\\mu\\).\nLeast squares estimation. In least squares, we want to choose the number that minimizes the sum of squares between our estimate and the data: \\[\n\\min_{m} \\sum_{i=1}^n ( x_i - m )^2.\n\\]\nLet’s dust off our calculus and solve this. We’ll take the derivative and set it equal to zero. First, let’s find the derivative: \\[\n\\frac{d}{d m} \\sum_{i=1}^n ( x_i - m )^2\n= \\sum_{i=1}^n \\frac{d}{d m} ( x_i - m )^2\n= 2 \\sum_{i=1}^n (x_i - m),\n\\]\nwhere we used the fact that the derivative is linear (so the derivative of a sum is the sum of the derivatives) to get the first equality.\nNow, let’s set that derivative equal to zero and solve for \\(m\\). \\[\n\\begin{aligned}\n0 &= 2 \\sum_{i=1}^n (x_i - m) \\\\\n0 &= \\sum_{i=1}^n x_i - nm \\\\\nn m &= \\sum_{i=1}^n x_i \\\\\nm &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\nSo the least squares estimate of \\(\\mu\\) is just the sample mean, \\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i.\n\\]\nMaximum likelihood estimation. Okay, now let’s try a maximum likelihood approach. Remember, maximum likelihood says that we should choose our estimate to be the number that makes our data as “likely” as possible– that is, the number that makes the probability of the data large.\nUnder the normal, the “probability” (it’s actually a density, not a probability, but that’s okay) of our data is \\[\n\\begin{aligned}\n\\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n&= \\prod_{i=1}^n \\Pr[ X_i = x_i; \\mu, \\sigma^2 ] \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\},\n\\end{aligned}\n\\] where we used the fact that our data is independent to write the “probability” as a product.\nOkay, now how are we going to choose \\(\\mu\\) to minimize this? Well, we have a product of things, and products are annoying. Let’s take the log of this, instead, to make it a sum: \\[\n\\begin{aligned}\n\\log \\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n&= \\log \\prod_{i=1}^n \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\} \\\\\n&= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n            \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\} \\\\\n&= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  + \\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\},\n\\end{aligned}\n\\]\nwhere we made repeated use of the fact that the logarithm of a product is the sum of the logarithms.\nTaking the log to simplify the likelihood is so common that we have a name for this quantity: the log likelihood. Notice that because the logarithm is a monotone function, maximizing the likelihood and maximizing the log-likelihood are equivalent. The maximizer of one will be the maximizer of the other, so there’s no problem here.\nSo we have \\[\n\\log \\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  + \\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\}.\n\\]\nWe’re almost there!\nNotice that the first of these two sums doesn’t depend on \\(\\mu\\). So for the purposes of finding which value of \\(\\mu\\) makes the log-likelihood large, we can ignore it, and just concentrate on maximizing \\[\n\\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\}.\n\\]\nSince \\(\\log \\exp x = x\\), this simplifies to \\[\n\\sum_{i=1}^n \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 }\n= \\frac{-1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]\nNow, maximizing \\(-f(\\mu)\\) is the same as minimizing \\(f(\\mu)\\). In other words, the maximum likelihood estimate for the mean of our normal distribution is the value of \\(\\mu\\) that minimizes \\[\n\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nIf we take derivatives and set the result equal to zero, we’re going to get the very same least squares problem we had before, and we know that the minimizer is obtained by taking \\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i,\n\\]\nthe sample mean!\nSo, to recap, two different ways to measure what is a “good” estimate give us the same answer: least squares and maximum likelihood both lead us to estimate the mean of the normal as the sample mean!\nThis “specialness” of the sample mean is actually very common in the “nice” distributions that we see a lot in our early statistics courses, but it’s worth bearing in mind that this isn’t generally the case: in more “interesting” settings, least squares and maximum likelihood estimators can be very different! Stay tuned in your later courses to learn more.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#prediction-versus-feature-selection-the-two-cultures",
    "href": "logistic.html#prediction-versus-feature-selection-the-two-cultures",
    "title": "31  Logistic Regression",
    "section": "31.7 Prediction versus feature selection: the two cultures",
    "text": "31.7 Prediction versus feature selection: the two cultures\nWe have motivated most of our discussion of regression the last few weeks by talking about prediction. We see data \\(X_i\\), and our goal is to predict (i.e., guess) an associated response or label \\(Y_i\\). A “good” model is one that does well at predicting these labels or responses.\nExample: image recognition Consider once again the problem of detecting whether or not an image contains a cat. We observe \\((X_i, Y_i)\\) pairs in which each image \\(X_i\\) has an associated label \\[\nY_i = \\begin{cases} 1 &\\mbox{ if image } i \\text{ contains a cat }\\\\\n                    0 &\\mbox{ if image } i \\text{ does not contain a cat. }\n                    \\end{cases}\n\\]\nOur goal is then to build a model that takes in an image \\(x\\) and produces a prediction (i.e., a best guess) \\(\\hat{y}\\) based on \\(X\\) as to the true label \\(y\\). A good model is one that correctly guesses \\(y\\) most of the time. Something like \\[\n\\Pr[ \\hat{y} = y ] \\text{ is close to } 1\n\\]\nExample: housing prices Consider once again the problem of predicting how much a house will sell for. We get predictors in the form of a vector \\(x\\) containing the house’s age, square footage, proximity to parks, quality of the local school system, and so on, and our goal is to predict the price \\(y\\) of that house. A “good” model is one whose prediction \\(\\hat{y}\\) is “close” to the true value \\(y\\) “on average”. Something like \\[\n\\mathbb{E} \\left( \\hat{y} - y \\right)^2 \\text{ is small. }\n\\]\nNow, if our goal is only to detect whether or not a picture contains a cat, or only to predict how much a house will sell for, then that’s kind of the end of the story.\nBut suppose that we have a really good predictor for housing price, and someone asks us “what is it that makes a house expensive?”. Just because we have a good predictive model doesn’t mean that we can answer this question easily.\nFor example, large complicated neural nets are very good at image detection, but it’s very hard to take a trained neural net and figure out what it is about a particular image that causes the neural net to “believe” that there is or isn’t a cat in the image. There is no easy way to determine what the “meaningful” or “predictive” features of the image are. Indeed, this problem is a major area of research currently in machine learning and statistics. See here if you’re curious to learn more.\nOne of the good things about linear and logistic regression is that we can very easily determine which features were “useful” or “meaningful” for prediction– the estimated coefficients and associated p-values tell us quite a lot about which predictors (i.e., “features” in the language of machine learning) are (probably) useful.\nIndeed, sometimes the determination of which predictors are “useful” or “meaningful” is the important part of the statistical question.\nExample: fMRI data and schizophrenia. In my own research, I work a lot with functional magnetic resonance imagine (fMRI) data obtained from studies in which the subjects are either healthy controls or are diagnosed with schizophrenia. It’s tempting to want to build a model that can predict, given an fMRI scan of a person’s brain, whether or not this person has schizophrenia.\nThat’s all fine and good, but doctors already have a very good way of telling whether or not a person has schizophrenia, and it’s way cheaper than doing an MRI: just talk to them for five minutes! If you doubt this, search around for videos of interviews with schizophrenics.\nSo being able to predict whether or not someone has schizophrenia based on an fMRI scan is interesting and perhaps impressive, but it isn’t at all useful scientifically or clinically: we have a cheaper simpler way of telling if someone is schizophrenic.\nWhat is important to my colleagues in neuroscience and neurology is a more subtle question: what is it that is different between the brains of schizophrenic and healthy controls?\nIn other words, oftentimes we don’t just want a model that can perform prediction well, but we want a model that explains what it is in our data that generally explains the differences we see in responses or labels. Further, we usually want to be able to use that model to make inferences about the world– to estimate things like how levels of coffee consumption correlate with health outcomes.\nThis tension, between prediction and modeling, was discussed extensively in a famous paper from 2001 by Leo Breiman, titled Statistical Modeling: The Two Cultures.\nNow, it’s not as though there’s some huge fight between “prediction people” and “modeling people”. We work together all the time! But it’s useful to understand the distinction between these two different kinds of problems and to be able to distinguish when one or the other is the more appropriate “hat” to be wearing. Keep an eye out for these kinds of things as you progress in your data science career!",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#review",
    "href": "logistic.html#review",
    "title": "31  Logistic Regression",
    "section": "31.8 Review",
    "text": "31.8 Review\n\nRelationship between probability, odds and log-odds\nSigmoid function \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\nSimple logistic model \\(\\text{log-odds}(Y=1)=\\beta_0 + \\beta_1 X_1\\)\nPredicting \\(y\\) from a logistic model\ninterpreting the coefficients of a logistic model\nZ tests of significance of logistic coefficients\nfitting a logistic model (maximize log-likelihood / minimize residual deviance)\nlikelihood / log likelihood / residual deviance\nconfusion matrix for a logistic model, prediction statistics",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html",
    "href": "R10_LogisticReg_Extended.html",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "",
    "text": "32.1 The Pima dataset - predicting diabetes\nlibrary(MASS)\nhead(Pima.te)\n\n  npreg glu bp skin  bmi   ped age type\n1     6 148 72   35 33.6 0.627  50  Yes\n2     1  85 66   29 26.6 0.351  31   No\n3     1  89 66   23 28.1 0.167  21   No\n4     3  78 50   32 31.0 0.248  26  Yes\n5     2 197 70   45 30.5 0.158  53  Yes\n6     5 166 72   19 25.8 0.587  51  Yes\n\nPima.te$glu_bin &lt;- (round(Pima.te$glu/10))*10\nPima.te$typeTF &lt;- as.numeric(Pima.te$type==\"Yes\")\nPima.glm &lt;- glm(typeTF ~ glu, data=Pima.te, family=\"binomial\")\n\naggr.props &lt;- aggregate(typeTF ~ glu_bin, data=Pima.te, FUN=mean)\n\nxs &lt;- seq(60,200,10)\nzs &lt;- predict(Pima.glm, newdata=data.frame(glu=xs))\nps &lt;- exp(zs)/(1+exp(zs))\n\nplot(x=Pima.te$glu, y=Pima.te$typeTF, pch=16, col=rgb(0,0,1,.2), xlim=c(50,200))\n#points(aggr.props, ylim=c(0,1))\nlines(x=xs,y=ps, col=\"red\")\ntext(x=xs-2, y=as.vector(aggr.props$typeTF*.8+.5*.2), label=round(aggr.props$typeTF,2))\nsegments(x0=xs-5, aggr.props$typeTF, xs+5,aggr.props$typeTF)\nabline(v=seq(55,195,10), col=\"gray\")\nModel summary\nsummary(Pima.glm)\n\n\nCall:\nglm(formula = typeTF ~ glu, family = \"binomial\", data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2343  -0.7270  -0.4985   0.6663   2.3268  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.946808   0.659839  -9.013   &lt;2e-16 ***\nglu          0.042421   0.005165   8.213   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 325.99  on 330  degrees of freedom\nAIC: 329.99\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#odds-ratios",
    "href": "R10_LogisticReg_Extended.html#odds-ratios",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.2 Odds Ratios",
    "text": "32.2 Odds Ratios\nOdds Ratio = Odds(Event | Case A) / Odds(Event | Case B) We look at odds ratio when the cases differe by an increase of 1 unit in the predictor \\(x\\). \\(\\beta_1\\) is the log of the odds ratio associated with a 1 unit increase in \\(x\\).\nThe log(odds ratio)=0.042421\nThe odds ratio = exp(0.042421)\n\nexp(0.042421)\n\n[1] 1.043334\n\n\n\n32.2.1 Interpret the slope\n\nPima.glm$coefficients[2]\n\n       glu \n0.04242098 \n\nexp(Pima.glm$coefficients[2])\n\n     glu \n1.043334 \n\n\nOdd ratio for a 1 unit increase in blood glucose is 1.0433 so if we hold all other variables constant and increase blood glucose by 1 unit, we expect an increase in the Odds by 4.33%\n\n\n32.2.2 odds ratio vs risk ratio\nRisk is Prob(Event | case A) / Pr(Event | Case B) which is not the same as odds ratio. Consider this example:\nPredict probability for glu 200 vs 201\n\npredict(Pima.glm, type=\"response\", newdata=data.frame(glu=c(200,201)))\n\n        1         2 \n0.9267217 0.9295508 \n\n#The risk ratio is\n0.9295508 / 0.9267217\n\n[1] 1.003053\n\n#The Odds ratio is\n(0.9295508/(1-0.9295508)) / (0.9267217/(1-0.9267217))\n\n[1] 1.043333\n\n\nCompare glu 100 to 101\n\npredict(Pima.glm, type=\"response\", newdata=data.frame(glu=c(100,101)))\n\n        1         2 \n0.1538511 0.1594550 \n\n#The risk ratio is\n0.1594550 / 0.1538511 \n\n[1] 1.036424\n\n#The Odds ratio is\n(0.1594550/(1-0.1594550)) / (0.1538511 /(1-0.1538511 ))\n\n[1] 1.043334\n\n\nThe risk ratios are different but the odds ratio is a constant 1.04333",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#log-likelihood",
    "href": "R10_LogisticReg_Extended.html#log-likelihood",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.3 Log Likelihood",
    "text": "32.3 Log Likelihood\nlog likelihood calculation Let’s compare the true Y values and the predicted Y values\n\ny &lt;- Pima.te$typeTF\n\ny.hat &lt;- predict(Pima.glm, type=\"response\")\npY &lt;- y * y.hat + (1-y)*(1-y.hat)\nhead(data.frame(y,y.hat,pY),10)\n\n   y      y.hat         pY\n1  1 0.58212363 0.58212363\n2  0 0.08778183 0.91221817\n3  0 0.10235379 0.89764621\n4  1 0.06673426 0.06673426\n5  1 0.91759616 0.91759616\n6  1 0.74933615 0.74933615\n7  1 0.28067169 0.28067169\n8  0 0.17115736 0.82884264\n9  0 0.35394014 0.64605986\n10 1 0.28931541 0.28931541\n\n\nThe likelihood is the product of predicted probabilities (of the actual label) For actual 1s, we use y.hat For actual 0s we use 1-y.hat\n\nL &lt;- prod(y * y.hat + (1-y)*(1-y.hat))\nL\n\n[1] 1.629075e-71\n\n\nMaximizing likelihood is equivalent to maximising the log of likelihood, and because logarithms turn products into sums, the log likelihood is easier to work with mathematically. Because likelihood &lt; 1, log likelihood is going to be negative.\n\nlog(L)\n\n[1] -162.9955\n\nl &lt;- sum(log(y * y.hat + (1-y)*(1-y.hat)))\nl\n\n[1] -162.9955\n\n\nThe closer to zero the log likelihood is, the better; The closer to -infinity, the “worse”.\nThe residual deviance is defined as -2 times the log likelihood. It turns a big negative log likelihood into a large positive number (easier to interpret)\n\n#residual deviance\n-2*l\n\n[1] 325.9911\n\n\nFor each observation, we can translate \\(P(Y=y | model)\\) into a log-likelihood by just taking the log of that decimal. Furthermore, we could multiply each of these by -2 to see which data points contribute more (or less) to the total “residual deviance”\n\nll &lt;- log(pY)\nhead(data.frame(y,y.hat,pY, ll, -2*ll),10)\n\n   y      y.hat         pY         ll  X.2...ll\n1  1 0.58212363 0.58212363 -0.5410724 1.0821449\n2  0 0.08778183 0.91221817 -0.0918761 0.1837522\n3  0 0.10235379 0.89764621 -0.1079793 0.2159585\n4  1 0.06673426 0.06673426 -2.7070368 5.4140735\n5  1 0.91759616 0.91759616 -0.0859979 0.1719958\n6  1 0.74933615 0.74933615 -0.2885676 0.5771352\n7  1 0.28067169 0.28067169 -1.2705696 2.5411393\n8  0 0.17115736 0.82884264 -0.1877250 0.3754499\n9  0 0.35394014 0.64605986 -0.4368631 0.8737262\n10 1 0.28931541 0.28931541 -1.2402378 2.4804756",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#prediction",
    "href": "R10_LogisticReg_Extended.html#prediction",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.4 Prediction",
    "text": "32.4 Prediction\nSuppose someone has glucose level 150; what is the estimated probability of diabetes? \\[\\hat{logodds}(Y+i=1) = \\hat{\\beta_0}+\\hat{\\beta_1}X_i\\]\n\n#Predict log odds\npredict(Pima.glm, newdata=data.frame(glu=150))\n\n        1 \n0.4163392 \n\n\n\n#Predicts probability\npredict(Pima.glm, newdata=data.frame(glu=150), type=\"response\")\n\n        1 \n0.6026069 \n\n\n\n#calculation\nlog.odds.150 &lt;- sum(Pima.glm$coefficients * c(1, 150))\n\n#Estimated probability = e^log-odds / (1+e^log-odds)\n#or\n#                      = 1/ (1 + e^ -logodds)\nexp(log.odds.150) / (1+ exp(log.odds.150))\n\n[1] 0.6026069\n\n#or\n1 / (1+ exp(-log.odds.150))\n\n[1] 0.6026069",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#the-null-model",
    "href": "R10_LogisticReg_Extended.html#the-null-model",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.5 The null model",
    "text": "32.5 The null model\nThe null model would be what we compare our “better” model to - it is a logistic model that does not have any predictors. Just like the null model for linear regression predicts \\(\\hat{y}=\\bar{y}\\) the null model predicts \\(\\hat{y}=\\) the proportion of 1s in the data set.\n\npima.glm.null &lt;- glm(typeTF ~ 1, data=Pima.te, family=\"binomial\")\nsummary(pima.glm.null)\n\n\nCall:\nglm(formula = typeTF ~ 1, family = \"binomial\", data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.8921  -0.8921  -0.8921   1.4925   1.4925  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7158     0.1169  -6.125 9.07e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.3  on 331  degrees of freedom\nResidual deviance: 420.3  on 331  degrees of freedom\nAIC: 422.3\n\nNumber of Fisher Scoring iterations: 4\n\nlog.odds &lt;- pima.glm.null$coefficients[1]\nexp(log.odds) / (1+ exp(log.odds))\n\n(Intercept) \n  0.3283133 \n\nmean(Pima.te$typeTF)\n\n[1] 0.3283133\n\n\nLet’s just double check that. We can calculate the coefficient manually\n\n#What is the proportion of 1s in the data?\n(prop1 &lt;- mean(Pima.te$typeTF))\n\n[1] 0.3283133\n\n#convert this to a log-odds\nlog(prop1/(1-prop1))\n\n[1] -0.7158239\n\n\nThis is the same as the constant in the null model.\n\n#null deviance calculation\n\n#log likelihood = sum [y * y.hat + (1-y)*(1-yhat)]\n\n#deviance = -2*log likelihood\n\nn &lt;- nrow(Pima.te)\nm &lt;- sum(Pima.te$typeTF)\n-2*(m*log(m/n) + (n-m)*log((n-m)/n))\n\n[1] 420.2972",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#results-of-logistic-regression-confusion-matrix",
    "href": "R10_LogisticReg_Extended.html#results-of-logistic-regression-confusion-matrix",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.6 Results of logistic regression: confusion matrix",
    "text": "32.6 Results of logistic regression: confusion matrix\nSee https://en.wikipedia.org/wiki/Confusion_matrix\n\npima_logit &lt;- glm(typeTF ~ glu, data=Pima.te, family=\"binomial\")\n\nconfusion.table &lt;- table(Pima.te$typeTF, factor(predict(pima_logit, type=\"response\") &gt;= .5, levels=c(FALSE, TRUE)))\n\ncolnames(confusion.table) &lt;- c(\"Predict 0\",\"Predict 1\")\nrownames(confusion.table) &lt;- c(\"No diabetes\",\"Yes diabetes\")\n\n#confusion.table\nTP &lt;- confusion.table[2,2]\nTN &lt;- confusion.table[1,1]\nFP &lt;- confusion.table[1,2]\nFN &lt;- confusion.table[2,1]\n\nt(addmargins(confusion.table))\n\n           \n            No diabetes Yes diabetes Sum\n  Predict 0         200           53 253\n  Predict 1          23           56  79\n  Sum               223          109 332\n\n\nStatistics for a predictor:\nAccuracy of a classifier - the total # correct predictions / total predictions\n\n(accuracy &lt;- (TP+TN)/(TP+TN+FP+FN))  \n\n[1] 0.7710843\n\n\nSensitivity (aka recall, hit rate, true positive rate, aka POWER) - what proportion of actual positive cases are classified as positive\n\n(sensitivity &lt;- TP / (TP+FN))\n\n[1] 0.5137615\n\n\nSpecificity (selectivity, true negative rate, \\(1-\\alpha\\)) - what proportion of actual negative cases are classified as negative\n\n(specificity &lt;- TN / (TN+FP))\n\n[1] 0.896861\n\n\nPrecision (positive predictive value) - what proportion of positive predictions are correct\n\n(ppv &lt;- TP/(TP+FP))\n\n[1] 0.7088608\n\n\nNegative Predictive Value (NPV) - what proportion of negative predictions are correct\n\n(npv &lt;- TN/(TN+FN))\n\n[1] 0.7905138\n\n\nWhat was the Type 1 error rate? The False positive rate\n\nFP / (FP + TN)\n\n[1] 0.103139\n\n\nLet’s just change the theshold from \\(p=.5\\) to some other number to see what happens to this matrix.\n\nconfusion.table &lt;- table(Pima.te$typeTF, factor(predict(pima_logit, type=\"response\") &gt;= .6, levels=c(FALSE, TRUE)))\ncolnames(confusion.table) &lt;- c(\"Predict 0\",\"Predict 1\")\nrownames(confusion.table) &lt;- c(\"No diabetes\",\"Yes diabetes\")\n#confusion.table\nt(addmargins(confusion.table))\n\n           \n            No diabetes Yes diabetes Sum\n  Predict 0         210           61 271\n  Predict 1          13           48  61\n  Sum               223          109 332\n\n#column proportions\nprop.table(t(confusion.table), 2)\n\n           \n            No diabetes Yes diabetes\n  Predict 0  0.94170404   0.55963303\n  Predict 1  0.05829596   0.44036697\n\n\nIf I look at column proportions, the first column (bottom row) is the alpha, type 1 error rate i.e. FPR. Column 2, in row 2 gives power.\nThe accuracy is (210 + 48 ) / 332\n\n(210 + 48  ) / 332\n\n[1] 0.7771084\n\n\nBy adjusting the positive prediction threshold we can improve/worsen statistics such as accuracy, specificity, sensitivity, etc.\n\npima_logit &lt;- glm(typeTF ~ glu, data=Pima.te, family=\"binomial\")\nacc &lt;- 0\nthresh &lt;- seq(0.01, 0.99, .01)\nfor(i in 1:length(thresh)){\n  confusion.table &lt;- table(Pima.te$typeTF, factor(predict(pima_logit, type=\"response\")&gt;= thresh[i], levels=c(FALSE, TRUE)))\n  colnames(confusion.table) &lt;- c(\"Predict 0\",\"Predict 1\")\n  rownames(confusion.table) &lt;- c(\"No diabetes\",\"Yes diabetes\")\n  #confusion.table\n  TP &lt;- confusion.table[2,2]\n  TN &lt;- confusion.table[1,1]\n  FP &lt;- confusion.table[1,2]\n  FN &lt;- confusion.table[2,1]\n  acc[i] = (TP+TN)/(TP+FP+TN+FN)\n}\nplot(thresh, acc, type=\"l\", main=\"Accuracy by threshold\", ylim=c(0,1))\n\n\n\n\n\n\n\n\nIn particular, we can look at the relationship between True positive rate and False Positive Rate (what proportion of actual negatives are incorrectly predicted to be positive) (FP / (FP+TN))\n\nTPR &lt;- 0; FPR &lt;- 0; \nthresh &lt;- seq(0, 1, .001)\nfor(i in 1:length(thresh)){\n  confusion.table &lt;- table(Pima.te$typeTF, factor(predict(pima_logit, type=\"response\")&gt;= thresh[i], levels=c(FALSE, TRUE)))\n  colnames(confusion.table) &lt;- c(\"Predict 0\",\"Predict 1\")\n  rownames(confusion.table) &lt;- c(\"No diabetes\",\"Yes diabetes\")\n  #confusion.table\n  TP &lt;- confusion.table[2,2]\n  TN &lt;- confusion.table[1,1]\n  FP &lt;- confusion.table[1,2]\n  FN &lt;- confusion.table[2,1]\n  \n  TPR[i] &lt;- TP/(TP+FN) #power\n  FPR[i] &lt;- FP/(FP+TN) #alpha / significance / type 1 error\n}\nplot(thresh, TPR, type=\"n\", main=\"TPR and FPR by threshold\", xlim=c(0,1), ylim=c(0,1))\nlines(thresh, TPR, lty=1)\nlines(thresh, FPR, lty=2)\nabline(h=0.05, col=\"red\", lty=3)\nlegend(x=.8, y=1, legend = c(\"TPR\", \"FPR\"), lty=c(1,2))\n\n\n\n\n\n\n\n\nTo put things into the language of hypothesis testing we could ask this: What threshold would we use to achieve a 5% significance level (false positive rate) and what would the power of the classifier be?\n\noptimalThresh &lt;- min(which(FPR&lt;=0.05))\nthresh[optimalThresh]\n\n[1] 0.613\n\nas.numeric(FPR[optimalThresh])\n\n[1] 0.04484305\n\nas.numeric(TPR[optimalThresh])\n\n[1] 0.4311927\n\n\n\n#data.frame(TPR, FPR)\n\nA scatterplot of these rates is called the ROC curve. We start at (0,0) and go to (1,1)\n\nplot(c(1,FPR,0),c(1, TPR,0), type=\"s\", xlab=\"False Positive Rate\", ylab=\"True Positive Rate\")\n\n\n\n\n\n\n\n\nThe PRROC package produces pretty ROC curves.\n\n#install.packages(\"PRROC\")\nlibrary(PRROC)\n\nLoading required package: rlang\n\nPRROC_obj &lt;- roc.curve(scores.class0 = predict(pima_logit, type=\"response\"), \n                       weights.class0=Pima.te$typeTF,\n                       curve=TRUE)\nplot(PRROC_obj)\n\n\n\n\n\n\n\n\nThe area under the curve (AUC) is a common metric measuring how good your classifier is. An AUC = 0.5 (where the ROC curve is a diagonal line) is a “dumb” predictor. AUC closer to 1 represents a discerning “good” predictor.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#comparing-using-aic-and-bic",
    "href": "R10_LogisticReg_Extended.html#comparing-using-aic-and-bic",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.7 Comparing using AIC and BIC",
    "text": "32.7 Comparing using AIC and BIC\nAs we’ll see next week, the AIC (Akaike Information Criterion) is a useful statistic to compare logistic models. \\[AIC = \\text{residual deviance} + 2k\\text{ (k is the number of coefficients fit)}\\] \\[BIC = \\text{residual deviance} + ln(n)*k\\] When sample size is large then BIC will give a larger penalty per predictor. \\(ln(8) &gt; 2\\) so when \\(n \\geq 8\\) BIC gives a heftier penalty per coefficient in the model than AIC does and thus will favor simpler models.\nWhen using either of these comparative statistics, the lower the value the better.\n\npima.logit1 &lt;- glm(typeTF ~ 1 + glu,data=Pima.te, family=\"binomial\")\npima.logit2 &lt;- glm(typeTF ~ 1 + glu + bmi, data=Pima.te, family=\"binomial\")\nsummary(pima.logit1)\n\n\nCall:\nglm(formula = typeTF ~ 1 + glu, family = \"binomial\", data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2343  -0.7270  -0.4985   0.6663   2.3268  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.946808   0.659839  -9.013   &lt;2e-16 ***\nglu          0.042421   0.005165   8.213   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 325.99  on 330  degrees of freedom\nAIC: 329.99\n\nNumber of Fisher Scoring iterations: 4\n\nsummary(pima.logit2)\n\n\nCall:\nglm(formula = typeTF ~ 1 + glu + bmi, family = \"binomial\", data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2240  -0.7216  -0.4421   0.6158   2.3544  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -8.025302   0.942896  -8.511  &lt; 2e-16 ***\nglu          0.039311   0.005257   7.478 7.57e-14 ***\nbmi          0.072645   0.020621   3.523 0.000427 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 312.49  on 329  degrees of freedom\nAIC: 318.49\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#example-2-a-logistic-model-with-a-categorical-predictor",
    "href": "R10_LogisticReg_Extended.html#example-2-a-logistic-model-with-a-categorical-predictor",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.8 Example 2: A Logistic model with a categorical predictor",
    "text": "32.8 Example 2: A Logistic model with a categorical predictor\n\n#Let's use age as a predictor\n#Only for age &gt; 50\nPima.te$age50 &lt;- as.numeric(Pima.te$age &gt; 50)\n\ndiabetes.aggr &lt;- aggregate(typeTF ~ age50, data=Pima.te, FUN=mean)\ndiabetes.aggr\n\n  age50    typeTF\n1     0 0.3129032\n2     1 0.5454545\n\n\nAmong those 50 or younger, proportion with diabetes is .3129032 among those 51+ it is .545454\n\n#Log-odds for the two groups\nodds &lt;- diabetes.aggr$typeTF / (1-diabetes.aggr$typeTF)\nodds\n\n[1] 0.4553991 1.2000000\n\nlogodds &lt;- log(odds)\nlogodds\n\n[1] -0.7865812  0.1823216\n\ndiff(logodds)\n\n[1] 0.9689027\n\n\nNotice the log-odds are -.7865 and .1823 respectively. If we were to build a model we would say that \\[ \\hat{LO} = -.7865812 + .9689 \\times age_{&gt;50}\\]\n\nPima50 &lt;- glm(typeTF ~ age50, data=Pima.te, family=\"binomial\")\nsummary(Pima50)\n\n\nCall:\nglm(formula = typeTF ~ age50, family = \"binomial\", data = Pima.te)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.2557  -0.8663  -0.8663   1.5244   1.5244  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -0.7866     0.1225  -6.422 1.35e-10 ***\nage50         0.9689     0.4454   2.176   0.0296 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 415.59  on 330  degrees of freedom\nAIC: 419.59\n\nNumber of Fisher Scoring iterations: 4",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "R10_LogisticReg_Extended.html#example-3-iris-dataset",
    "href": "R10_LogisticReg_Extended.html#example-3-iris-dataset",
    "title": "32  Logistic Regression - Extended Examples",
    "section": "32.9 Example 3: Iris dataset",
    "text": "32.9 Example 3: Iris dataset\nAnother classic example is the iris dataset. This famous dataset gives measurements in centimeters of the sepal and petal width and length for 3 species of iris: setosa, versicolor and virginica. The dataset has 150 rows, with 50 samples of each.\nBecause the logistic model handles binary response variables, what we will do is create a logistic model to predict whether an iris is virginica Let’s build a full model with all 4 predictors.\n\nvirginica.model &lt;- glm(Species==\"virginica\" ~ 1 + Sepal.Length + Sepal.Width+Petal.Length+Petal.Width, data=iris, family=\"binomial\")\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(virginica.model)\n\n\nCall:\nglm(formula = Species == \"virginica\" ~ 1 + Sepal.Length + Sepal.Width + \n    Petal.Length + Petal.Width, family = \"binomial\", data = iris)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.01105  -0.00065   0.00000   0.00048   1.78065  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -42.638     25.708  -1.659   0.0972 .\nSepal.Length   -2.465      2.394  -1.030   0.3032  \nSepal.Width    -6.681      4.480  -1.491   0.1359  \nPetal.Length    9.429      4.737   1.990   0.0465 *\nPetal.Width    18.286      9.743   1.877   0.0605 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 190.954  on 149  degrees of freedom\nResidual deviance:  11.899  on 145  degrees of freedom\nAIC: 21.899\n\nNumber of Fisher Scoring iterations: 12\n\n\n\nplot(predict(virginica.model), iris$Species==\"virginica\", col=iris$Species)\nrng &lt;- range(predict(virginica.model))\nxs &lt;- seq(rng[1],rng[2], length.out=100)\nys &lt;- 1/(1+exp(-xs))\nlines(xs,ys)\n\n\n\n\n\n\n\n\nLet’s just contrast this plot with a model that only uses sepal length.\n\nvirginica.model.simple &lt;- glm(Species==\"virginica\" ~ 1 + Sepal.Length, data=iris, family=\"binomial\")\nsummary(virginica.model.simple)\n\n\nCall:\nglm(formula = Species == \"virginica\" ~ 1 + Sepal.Length, family = \"binomial\", \n    data = iris)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.9870  -0.5520  -0.2614   0.5832   2.7001  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -16.3198     2.6581  -6.140 8.27e-10 ***\nSepal.Length   2.5921     0.4316   6.006 1.90e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 190.95  on 149  degrees of freedom\nResidual deviance: 117.35  on 148  degrees of freedom\nAIC: 121.35\n\nNumber of Fisher Scoring iterations: 5\n\nplot(jitter(predict(virginica.model.simple)), iris$Species==\"virginica\", col=iris$Species)\nrng &lt;- range(predict(virginica.model.simple))\nxs &lt;- seq(rng[1],rng[2], length.out=100)\nys &lt;- 1/(1+exp(-xs))\nlines(xs,ys)\n\n\n\n\n\n\n\n\nHow well we did? TRUE represents virginica\n\ntable(predict = predict(virginica.model, type=\"response\")&gt;.5, actual = iris$Species==\"virginica\")\n\n       actual\npredict FALSE TRUE\n  FALSE    99    1\n  TRUE      1   49\n\n\ncolumn prop table will show us type 1&2 error rates and power:\n\nprop.table(table(predict = predict(virginica.model, type=\"response\")&gt;.5, actual = iris$Species==\"virginica\"), margin=2)\n\n       actual\npredict FALSE TRUE\n  FALSE  0.99 0.02\n  TRUE   0.01 0.98\n\n\nCan we achieve a 5% Type 1 error rate? Adjust the threshold. We can find the 5th percentile for the predictions for the “other”\n\nquantile(predict(virginica.model, type=\"response\")[iris$Species!=\"virginica\"], .95)\n\n       95% \n0.00504067 \n\n\n\nprop.table(table(predict = predict(virginica.model, type=\"response\")&gt;0.00504067 , actual = iris$Species==\"virginica\"), margin=2)\n\n       actual\npredict FALSE TRUE\n  FALSE  0.95 0.00\n  TRUE   0.05 1.00\n\n\nWe require very little evidence to claim the flower is virginica, a probability of 0.00504 is enough. In this case we have a 5% type 1 error rate. But note that the power has increased to 100%.\n\n32.9.1 When a logistic model cannot converge\nIf your model is so good that it correctly labels everyhing as 1 or zero this is what the output looks like\n\nsetosa.model &lt;- glm(Species==\"setosa\" ~ 1 + Sepal.Length + Sepal.Width+Petal.Length+Petal.Width, data=iris, family=\"binomial\")\n\nWarning: glm.fit: algorithm did not converge\n\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(setosa.model)\n\n\nCall:\nglm(formula = Species == \"setosa\" ~ 1 + Sepal.Length + Sepal.Width + \n    Petal.Length + Petal.Width, family = \"binomial\", data = iris)\n\nDeviance Residuals: \n       Min          1Q      Median          3Q         Max  \n-3.185e-05  -2.100e-08  -2.100e-08   2.100e-08   3.173e-05  \n\nCoefficients:\n               Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)     -16.946 457457.106       0        1\nSepal.Length     11.759 130504.043       0        1\nSepal.Width       7.842  59415.386       0        1\nPetal.Length    -20.088 107724.596       0        1\nPetal.Width     -21.608 154350.619       0        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.9095e+02  on 149  degrees of freedom\nResidual deviance: 3.2940e-09  on 145  degrees of freedom\nAIC: 10\n\nNumber of Fisher Scoring iterations: 25\n\n\nIt means that the coefficients are able to separate the predicted log odds so far that you have no misclassifications. Note above that after 25 fisher iterations it stopped. You can set the max iterations to be a lot lower, like 3, and then the model summary will be far more interpretable. But the standard errors of the coefficients will still be high so you can’t say much about the significance of individual predictors.\n\nsetosa.model.maxit &lt;- glm(Species==\"setosa\" ~ 1 + Sepal.Length + Sepal.Width+Petal.Length+Petal.Width, data=iris, family=\"binomial\", control = list(maxit = 2))\n\nWarning: glm.fit: algorithm did not converge\n\nsummary(setosa.model.maxit)\n\n\nCall:\nglm(formula = Species == \"setosa\" ~ 1 + Sepal.Length + Sepal.Width + \n    Petal.Length + Petal.Width, family = \"binomial\", data = iris, \n    control = list(maxit = 2))\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-0.7003  -0.2994  -0.1592   0.2188   0.7140  \n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept)   -2.4428     3.1506  -0.775   0.4381  \nSepal.Length   0.4038     0.8871   0.455   0.6490  \nSepal.Width    1.8157     0.8944   2.030   0.0424 *\nPetal.Length  -1.7249     0.8993  -1.918   0.0551 .\nPetal.Width   -0.2378     1.5787  -0.151   0.8803  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 190.954  on 149  degrees of freedom\nResidual deviance:  14.313  on 145  degrees of freedom\nAIC: 24.313\n\nNumber of Fisher Scoring iterations: 2\n\n\nHow correlated are flower measurements?\n\ncor(iris[,1:4])\n\n             Sepal.Length Sepal.Width Petal.Length Petal.Width\nSepal.Length    1.0000000  -0.1175698    0.8717538   0.8179411\nSepal.Width    -0.1175698   1.0000000   -0.4284401  -0.3661259\nPetal.Length    0.8717538  -0.4284401    1.0000000   0.9628654\nPetal.Width     0.8179411  -0.3661259    0.9628654   1.0000000",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>32</span>  <span class='chapter-title'>Logistic Regression - Extended Examples</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html",
    "href": "logistic_practice.html",
    "title": "33  Logistic Regression Practice",
    "section": "",
    "text": "34 Practice Problems",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#getting-an-a",
    "href": "logistic_practice.html#getting-an-a",
    "title": "33  Logistic Regression Practice",
    "section": "34.1 Getting an A",
    "text": "34.1 Getting an A\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0 = −6\\), \\(\\hat{\\beta}_1 = 0.05\\), \\(\\hat{\\beta}_2 = 1\\).\n\n\nEstimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#mens-basketball",
    "href": "logistic_practice.html#mens-basketball",
    "title": "33  Logistic Regression Practice",
    "section": "34.2 Men’s Basketball",
    "text": "34.2 Men’s Basketball\nThe code below reads the Xavier men’s basketball 20-21 team game results and creates a dummy variable “win” that takes the numeric value of 1 if XU outscored their opponent and 0 otherwise. In this application we’ll use logistic regression to model wins/losses.\n\nxub &lt;- read.csv(\"https://remiller1450.github.io/data/xubball2021.csv\") \nxub$win &lt;- ifelse(xub$Margin &gt; 0, 1, 0)\n\n\nFit the logistic regression model win ~ X3P, which predicts whether a game was won or lost based upon the number of made 3-point shots. How does the number of made 3-point shots impact the odds of Xavier winning? Is the effect of “3XP” statistically meaningful?\nFit the logistic regression model win ~ X3P + X3P., which adjusts for the percentage of three point attempts that are made. How does the effect of “3XP” in this model differ from the effect you described in the previous question? Briefly explain why the adjusted effect is so different from the unadjusted effect.\nUse a likelihood ratio test to compare the two models described in (a) and (b). Does the test provide statistically compelling evidence that the larger model provides a better fit?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#spam-filter",
    "href": "logistic_practice.html#spam-filter",
    "title": "33  Logistic Regression Practice",
    "section": "34.3 Spam Filter",
    "text": "34.3 Spam Filter\nClassifying emails as “spam” is a classical data-science application that involves modeling a binary outcome. The following questions will use the spam dataset contained in the kernlab package, which contains a sample of 4601 emails that were manually labeled as either “spam” or “nonspam” (recorded as the variable “type”), along with various predictors that describe the normalized frequencies of certain words/symbols within each email message. We will focus on the following predictors:\n\n“capitalAve” - The average number of capitalized letters (per sentence) contained in the email.\n“charDollar” - The normalized frequency of the dollar sign character\n“charExclamation” - The normalized frequency of the exclamation point character\n\n\n#install.packages(\"kernlab\")\nlibrary(kernlab) \ndata(\"spam\") \ntable(spam$type)\n\n\nnonspam    spam \n   2788    1813 \n\n\n\nUse boxplots to show the distribution of each of the three variables listed above in “spam” and “nonspam” emails. Based upon a visual inspection of these plots, which variable appears to be the strongest predictor of an email being “spam”? (Hint: Your choie of predictor is somewhat subjective, but try “zooming in” on your boxplots using the “xlim” or “ylim” arguments to help you make your selection)\nFit a logistic regression model that uses the variable you chose in (a) to predict “spam”. Then, use this model to intpret the effect of this variable on the odds of an email being “spam”.\nCreate a graph that displays the expected probability, along with 90% confidence intervals, of an email being spam in response to the predictor you identified in Question #13. (Hint: this graph should resemble the ones from the “Confidence and Prediction Intervals” section)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#paying-dividentds",
    "href": "logistic_practice.html#paying-dividentds",
    "title": "33  Logistic Regression Practice",
    "section": "34.4 Paying Dividentds",
    "text": "34.4 Paying Dividentds\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#odds",
    "href": "logistic_practice.html#odds",
    "title": "33  Logistic Regression Practice",
    "section": "34.5 Odds",
    "text": "34.5 Odds\nThis problem has to do with odds.\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#weekly",
    "href": "logistic_practice.html#weekly",
    "title": "33  Logistic Regression Practice",
    "section": "34.6 Weekly",
    "text": "34.6 Weekly\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\nExperiment with different combinations of predictors, including possible transformations and interactions. Report the variables and associated confusion matrix that appears to provide the best results on the held out data.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#logistic-auto",
    "href": "logistic_practice.html#logistic-auto",
    "title": "33  Logistic Regression Practice",
    "section": "34.7 Logistic Auto",
    "text": "34.7 Logistic Auto\nIn this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n\n\nCreate a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\nSplit the data into a training set and a test set.\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#boston-logistic",
    "href": "logistic_practice.html#boston-logistic",
    "title": "33  Logistic Regression Practice",
    "section": "34.8 Boston Logistic",
    "text": "34.8 Boston Logistic\nUsing the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression and describe your findings.\nHint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set. # Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#function-practice",
    "href": "logistic_practice.html#function-practice",
    "title": "33  Logistic Regression Practice",
    "section": "34.9 Function Practice",
    "text": "34.9 Function Practice\nThis problem involves writing functions.\n\n\nWrite a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results.\n\nHint: Recall that x^a raises x to the power a. Use the print() function to output the result.\n\nCreate a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line\n\nPower2 &lt;- function(x, a){}\nYou should be able to call your function by entering, for instance, Power2(3,8) on the command line. This should output the value of \\(3^8\\), namely, 6,561.\n\nUsing the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\nNow create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n\nreturn(result)\nThe line above should be the last line in your function, before the } symbol.\n\nNow using the Power3() function, create a plot of \\(f(x) = x^2\\). The \\(x\\)-axis should display a range of integers from 1 to 10, and the y-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the \\(x\\)-axis, the \\(y\\)-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function.\nCreate a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call PlotPower(1:10, 3) then a plot should be created with an \\(x\\)-axis taking on values \\(1, 2, \\ldots , 10\\), and a \\(y\\)-axis taking on values \\(1^3, 2^3, \\ldots , 10^3\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "34  Model Selection and Cross Validation",
    "section": "",
    "text": "34.1 Learning objectives\nIn our discussions of regression the past few weeks, we have encountered many situations in which we needed to make a choice about the model that we fit to data. For example, suppose we have a data set with hundreds or even thousands of predictor variables. This is frequently the case in applications to genetics, where we have thousands of genes, and we want to predict some outcome (e.g., disease status). How do we decide which variables to include in linear regression (or any other prediction model)?\nThis is an example of model selection, our subject for this week.\nAfter this lecture, you will be able to",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#learning-objectives",
    "href": "cv.html#learning-objectives",
    "title": "34  Model Selection and Cross Validation",
    "section": "",
    "text": "Explain the problem of variable selection in the context of linear regression\nExplain and apply cross-validation methods, including leave-one-out cross-validation and \\(K\\)-fold cross-validation.\nExplain subset selection methods, including forward and backward stepwise selection.\nExplain and apply regularization and shrinkage methods, including ridge regression and the LASSO.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#model-selection-overview",
    "href": "cv.html#model-selection-overview",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.2 Model selection: overview",
    "text": "34.2 Model selection: overview\nOur focus this week will be on model selection for regression problems. Still, we should note that similar ideas apply in many other situations. For example, when clustering data, we use model selection to choose how many clusters to group the data into.\nThe unifying idea is that we have to choose among many different similar ways of describing the data. That’s what model selection helps us do.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#variable-selection",
    "href": "cv.html#variable-selection",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.3 Variable selection",
    "text": "34.3 Variable selection\nSuppose that we have a collection of \\(p\\) predictors, and that \\(p\\) is very large (say, \\(p \\approx n\\)). If we try to fit a model using all of these predictors, we will end up over-fitting to the data, a point that we have discussed briefly a few times this semester.\nIf you’ve taken linear algebra (skip this paragraph if not!): in linear regression (a similar story holds for other prediction models), we have \\(n\\) equations in \\(p\\) unknowns. When \\(p\\) is of a similar size to the number of observations \\(n\\), the system is over-determined (or close to it)..\nIn situations like this, we would like to choose just a few of these predictors for inclusion in a statistical model (e.g., linear regression). This is an example of model selection: we have a bunch of different models under consideration (i.e., a different possible model for each set of variables we might choose), and we want to pick the best one.\nThe natural question, then, is: how do we compare models?\n\n34.3.1 Example: mtcars\nLet’s consider a very simple example, adapted from Section 3.3.2 and Section 5.1 in ISLR, revisiting our old friend the mtcars data set.\n\ndata('mtcars');\nhead(mtcars);\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nLet’s confine our attention to the mpg (miles per gallon) and hp (horsepower) variables. Can we predict gas mileage from the horsepower?\nLet’s try fitting linear regression.\n\nmodel1 &lt;- lm(mpg ~ 1 + hp, mtcars);\nintercept1 &lt;- model1$coefficients[1];\nslope1 &lt;- model1$coefficients[2];\n\n# Plot the data itself\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\npp &lt;- pp + geom_abline(intercept=intercept1, slope=slope1, colour='blue' );\npp\n\n\n\n\n\n\n\n\nOkay, it looks reasonable, but you might notice that the residuals have a bit of a weird behavior. Let’s plot them to see what I mean.\n\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg-(slope1*hp+intercept1))) + geom_point()\npp\n\n\n\n\n\n\n\n\nThe residuals have a kind of U-shape. This suggests that there is a non-linearity in the data that we are failing to capture. Let’s try adding another predictor: the squared horsepower.\n\nmodel2 &lt;- lm(mpg ~ 1 + hp + I(hp^2), mtcars);\nintercept2 &lt;- model2$coefficients[1];\nslope2_1 &lt;- model2$coefficients[2];\nslope2_2 &lt;- model2$coefficients[3];\n\n# Plot the data itself\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\n# As usual, there are cleaner ways to do this plot, but this is the quick and easy way to make itt.\n# If we were doing this more carefully, we would evaluate the cureve in the plot at more x-values than just the ones in the data to smooth things out.\npp &lt;- pp + geom_line( aes(x=hp, y=intercept2 + slope2_1*hp + slope2_2*I(hp^2) ), colour='red' );\npp\n\n\n\n\n\n\n\n\nThat looks like quite an improvement! Just for comparison:\n\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\npp &lt;- pp + geom_abline(intercept=intercept1, slope=slope1, colour='blue' );\npp &lt;- pp + geom_line( aes(x=hp, y=intercept2 + slope2_1*hp + slope2_2*I(hp^2) ), colour='red' );\npp\n\n\n\n\n\n\n\n\nWe can also compare the squared residuals to confirm that adding the feature hp^2 actually decreased our error:\n\nc( sum( model1$residuals^2 ), sum( model2$residuals^2 ) );\n\n[1] 447.6743 274.6317\n\n\nWhy stop there? Why not add hp^3 as well, or even hp^4? Well, funny enough, that is precisely the idea behind polynomial regression, which you can learn more about in ISLR (Section 3.3.2; more substantial discussion in Chapter 7) or in a regression course.\nBut that raises the question: how do we know when to stop?\n-You’ll find that if you add hp^3 to the model above, that the sum of squared residuals does indeed improve.\n-But how do we know if that improvement is worth it?\nOne approach to this problem would be to examine the \\(p\\)-values associated to the coefficients (see ISLR Chapter 3 for a discussion of that approach). In these notes, we will see a different, arguably more principled approach.\n\n\n34.3.2 Overfitting and Unseen Data\nIf we keep adding more predictors to our model, the residuals will continue to decrease, but this will not actually mean that our model is better. Instead, what we will be doing is over-fitting to the data. That is, our model will really just be “memorizing” the data itself rather than learning a model.\nThe true test of model quality is how well it does at predicting for data that we didn’t see.\nThat is, if we fit our model on data \\((X_i,Y_i)\\) for \\(i=1,2,\\dots,n\\), how well does our model do on a previously unseen data point \\((X_{n+1},Y_{n+1})\\)?\nSpecifically, in the case of regression, we want our model to minimize \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2,\n\\]\nwhere \\(\\hat{Y}_{n+1}\\) is our model’s prediction based on coefficients estimated from our \\(n\\) training observations.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#validation-sets",
    "href": "cv.html#validation-sets",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.4 Validation Sets",
    "text": "34.4 Validation Sets\nSo rather than focusing on how well our model fits our training data, we should be trying to determine how well our model does when it gets applied to data that we haven’t seen before.\nSpecifically, we would like to know the mean squared error (MSE), \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2.\n\\]\nNote: The name is hopefully clear– it is the expectation (mean) of the squared error between our prediction and the truth.\nIf you’ve taken an ML course, this idea should already be quite familiar. We always train (“fit” in the language of statistics) our model on a training set, and then assess how well the model performs on a test set that our model hasn’t seen before.\nThe trouble is that in most statistical problems, we have at most a few hundred data points to work with. As a result, we can’t really afford to set aside some of our data just to use as a test set.\nNote that this is in contrast to many machine learning settings (e.g., training a neural net), where we often have tens or hundreds of thousands of data points to work with.\nFollowing the logic of the train/test split idea in ML, though, a natural approach is to do the following:\n\nSplit our data into two parts, say \\(S_1,S_2\\), such that \\(S_1 \\cup S_2 = \\{1,2,\\dots,n\\}\\) and \\(S_1 \\cap S_2 = \\emptyset\\).\nObtain estimate \\(\\hat{\\beta}_1\\) by fitting a model on the observations in \\(S_1\\)\nEvaluate the error of our fitted model on \\(S_2\\), \\[\n\\hat{E}_1\n=\n\\frac{1}{|S_2|} \\sum_{i \\in S_2} \\left( Y_i - \\hat{\\beta}_1 X_i \\right)^2.\n\\]\n\nTypically, we call \\(S_2\\), the set that we make predictions for, the validation set, because it is validating our model’s performance.\n\n34.4.1 Example: mtcars revisited\nLet’s see this in action on the mtcars data set.\nWe randomly split the data set into two groups. For each model order 1, 2, 3, 4 and 5, we fit the model to the training set and then measure the sum of squared residuals of that model when applied to the validation set.\nOne run of this experiment is summarized in resids_onerun. For details, refer to mtcars_poly.R, which is included among the supplementary files for this lecture.\n\nsource('r_scripts/mtcars_poly.R');\n\nhead(resids_onerun);\n\n  Order       Error\n1     1   23.305619\n2     2    9.875918\n3     3   13.684748\n4     4    8.643560\n5     5 4720.668710\n\n\n\n# Plot these results\npp &lt;- ggplot(resids_onerun, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\n\n\n\n\nLet’s pause to make sure that we understand what this plot actually shows. We split the mtcars dataset randomly into two sets, a “training” set and a “validation” set. For each order (1, 2, 3, 4, 5), we fit a model of that order to the training set. Then we use that model to try and predict the outcomes (mpg) on the validation set. So this is the performance of five different models, each trained on the same data \\(S_1\\) and evaluated on the same data \\(S_2\\), different from the training data.\nLooking at the plot, we see that as we add higher-order powers of hp, we don’t really gain much in terms of the error (i.e., sum of squared residuals) beyond order 2. Indeed, past the order-3 model, the error gets worse again!\nAside: this deteriorating performance is due largely to the fact that the mtcars data set is so small. Once we split it in half, we are fitting our model to just 16 observations. Estimating four or five coefficients from only about 15 observations is asking for trouble! This is a tell-tale sign of over-fitting of a model. This would be a good occasion for some kind of regularization, but we’ll come back to that.\n\n\n34.4.2 Variance in the residuals\nThere’s one problem, though, beyond matters of sample size. That plot shows the residuals as a function of model order for one particular random set \\(S_1\\). Let’s plot the same residuals for a few different random sets.\nNote: the data frame resids contains multiple replicates of the above experiment. Once again, refer to the code in mtcars_poly.R for details.\n\nhead(resids)\n\n  Rep Order     Error\n1   1     1 23.305619\n2   2     1 21.228027\n3   3     1 15.148903\n4   4     1 21.662602\n5   5     1 11.521220\n6   6     1  9.732585\n\n\n\npp &lt;- ggplot(resids, aes(x=Order, y=log(Error), color=as.factor(Rep) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n\n\n\nHmm. There’s quite a lot of variance among our different estimates of the prediction error. Note that the y-axis is on a log scale, so an increase from, say, 2 to 3 is an order of magnitude increase in error.\nEach of these is supposed to be estimating the error \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2,\n\\]\nbut there’s so much variation among our estimates that it’s hard to know if we can trust any one of them in particular!\nIndeed, the variance is so high that we needed to plot the error on a log scale! Once in a while, we get unlucky and pick an especially bad train/validate split, and the error is truly awful!\nQuestion: what explains the variance among the different lines in that plot?\nQuestion: How might we reduce that variance?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#reducing-the-variance-leave-one-out-cross-validation",
    "href": "cv.html#reducing-the-variance-leave-one-out-cross-validation",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.5 Reducing the variance: Leave-one-out cross-validation",
    "text": "34.5 Reducing the variance: Leave-one-out cross-validation\nOne source of variance in our cross-validation plots above was the fact that each replicate involved splitting the data in half and training on only one of the two halves.\nThat means that on average, from one replicate to another, the data used to train the model changes quite a lot, and hence our estimated model changes a lot. That’s where the variance comes from in the plot we just looked at!\nThere is also the related problem that we are training on only half of the available data. As statisticians and/or machine learners, we don’t like not using all of our data!\nSo, here’s one possible solution: instead of training on half the data and validating (i.e., evaluating the model) on the other half, let’s train on all of our data except for one observation, then evaluate our learned model on that one held-out data point.\nThat is, instead of splitting our data into two halves, we\n\nTake one observation and set it aside (i.e., hold it out)\nTrain our model on the other \\(n-1\\) observations\nEvaluate our model on the held-out observation.\n\nThis is called leave-one-out cross-validation (LOO-CV).\n\n# This R file implements the same experiment as we saw above,\n# but this time doing LOO-CV instead of a naive two-set split.\nsource('r_scripts/mtcars_poly_loocv.R');\n\npp &lt;- ggplot(resids_onerun, aes(x=Order, y=Error ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n\n\n\nBut once again, that’s just one run. Let’s display several of them in one plot.\n\npp &lt;- ggplot(resids, aes(x=Order, y=log(Error), color=as.factor(Rep) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n\n\n\nFor each of our replicates, we are estimating our model based on \\(n-1\\) of the observations, and then evaluating our prediction on the one held-out observation.\nBut now we have a different kind of variance: our estimate of the error is at the mercy of the one observation that we chose to hold out. If we chose an especially “bad” or “challenging” observation to hold out, then our error might be especially high.\nLeave-one-out cross-validation (LOO-CV) tries to bridge this gap (i.e., balancing the better stability of leaving one observation out with the variability induced by evaluating on a single point) by:\nFor each \\(i=1,2,\\dots,n\\):\n\nTrain the model on \\(\\{ (X_j, Y_j) : i \\neq i \\}\\).\nEvaluate on \\((X_i, Y_i)\\).\nAverage the model error over all \\(i =1,2,\\dots,n\\).\n\nThis illustration from ISLR should give you the general idea.\n\n\n\nSchematic of LOO-CV (Credit: ISLR2e fig. 5.3)\n\n\nLet’s see that in action. As we have done many times this semester, this code is optimized for clarity and readability, not for concision or “cleverness”. There are much more “graceful” ways of doing this, and shortly we’ll see R’s built-in CV tools, which are what we would normally use for this. But here the goal is to illustrate the core ideas in a really obvious way, hence the “clumsy” code.\n\ndata('mtcars'); # Still using mtcars data; reloading it just to remind us.\n\nnrows &lt;- nrow(mtcars); # Number of observations in the data\nnorder &lt;- 5;\n# For each choice of observation to hold out, we need to record the score\n# (i.e., squared erro) for each of the five model orders.\nerrors &lt;- data.frame( 'Row'=rep(1:nrows, each=norder),\n                     'Order'=rep(1:norder, times=nrows),\n                                 'Error'=rep(NA, nrows*norder));\n\nfor ( i in 1:nrow(mtcars) ) {\n  train_data &lt;- mtcars[-c(i),]; # Leave out the i-th observation\n  leftout &lt;- mtcars[c(i),]; # the row containing the left-out sample.\n  \n  # Remember, we are fitting five different models\n  # So that we can compare them.\n  \n  # Fit the linear model, then evaluate.\n  m1 &lt;- lm(mpg ~ 1 + hp, train_data );\n  m1.pred &lt;- predict( m1, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==1); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m1.pred - leftout$mpg)^2 ; \n  \n  # Fit the quadratic model, then evaluate.\n  m2 &lt;- lm(mpg ~ 1 + hp + I(hp^2), train_data );\n  m2.pred &lt;- predict( m2, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==2); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m2.pred - leftout$mpg)^2; \n\n  # Fit the cubic model, then evaluate.\n  m3 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3), train_data );\n  m3.pred &lt;- predict( m3, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==3); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m3.pred - leftout$mpg)^2; \n  \n  # Fit the 4-th order model, then evaluate.\n  m4 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3) + I(hp^4), train_data );\n  m4.pred &lt;- predict( m4, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==4); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m4.pred - leftout$mpg)^2; \n  \n  # Fit the 5-th order model, then evaluate.\n  m5 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5), train_data );\n  m5.pred &lt;- predict( m5, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==5); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m5.pred - leftout$mpg)^2;\n}\n\nOkay, so let’s make sure that we understand what is going on, here.\nThe data frame errors now has nrows*norders rows. So for each observation in the cars data set, there are five entries in the table errors, recording the squared error for the models of order 1, 2, 3, 4 and 5 when that data point was held out.\nWe said that when we do CV, we want to average across the \\(n\\) observations, so let’s do that. We’re going to use the aggregate function, which is one of the ways to perform “group-by” operations in R.\nGroup-by operations are where we pool our observations into subsets according to some criterion, and then compute a summary statistic over all of the observations in the same subset (i.e., the same “group”).\nUsing that language, we want to group the rows of errors according to model order, and take the average squared error within each order.\n\n# Error ~ Order tells R to group the data according to the Order column\n# and that we want to summarize the Error column within observations\n# of the same Order.\n# Passing the FUN=mean argument tells R that the summary statistic we want to use\n# is the function mean().\n# We could pass other summary statistic functions in this argument.\n# For example, we could use median, sd, var, max, etc.,\n# though those would be a bit silly here.\nerr_agg &lt;- aggregate(Error ~ Order, data=errors, FUN=mean);\n\nhead(err_agg)\n\n  Order     Error\n1     1  17.25330\n2     2  10.56143\n3     3  10.57458\n4     4  61.21760\n5     5 641.19551\n\n\nAnd we can plot that just to drive the point home.\n\npp &lt;- ggplot(err_agg, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n\n\n\n\n34.5.1 Recap: single split versus LOO-CV\nSo far we have seen two different ways of estimating a model’s performance on unseen data.\nThe first was to randomly split the data into two sets, train on one and evaluate on the other.\nPro: Only have to fit a model once (or just a few times, if we are going to repeat the operation and average)\nCon: Only have half of the data available to fit the model, which leads to less accurate prediction (and thus high variance in estimated model).\nThe second is leave-one-out cross-validation.\nPro: Use all but one observation to fit the model, so model fit is almost as good as if we had used all of the data\nCon: Have to fit the model anew for each held-out data point, results in fitting the model \\(n\\) different times, which can be expensive.\nCon: Because any two training sets overlap in all but one of their elements, our fitted models are very highly correlated with one another, so we’re doing a lot of work (\\(n\\) model fits) to get a bunch of highly correlated measurements.\nSo, the natural question is: can we bridge the gap between these two extremes.\n\n\n34.5.2 The happy medium: \\(K\\)-fold cross validation\nWell, there are a few different ways to bridge this gap, for example using Monte Carlo methods. Let’s discuss the most popular one here.\nWe’ll borrow a bit from the LOO-CV idea, while lessening the correlatedness of the models fits.\n\\(K\\)-fold CV randomly divides the data into \\(K\\) subsets, called folds. Then, one at a time, we hold out one of the folds, train our model on the \\(K-1\\) remaining folds, and evaluate our model’s prediction error on the held-out fold. Then, we can average the errors across the \\(K\\) folds.\nThat is, the “recipe” for \\(K\\)-fold cross-validation is\n\nRandomly partition the data into \\(K\\) (approximately) same-sized subsets, \\(S_1,S_2,\\dots,S_K\\) such that \\(\\cup_k S_k = \\{1,2,\\dots,n\\}\\) and \\(S_k \\cap S_\\ell = \\emptyset\\) for all \\(k \\neq \\ell\\)\nFor each \\(k=1,2,\\dots,K\\), train a model on the observations indexed by \\(i \\in \\cup_{\\ell \\neq k} S_\\ell\\) and compute the prediction error \\[\n\\hat{E}_k =  \\frac{1}{|S_k|} \\sum_{i \\in S_k} (\\hat{y}_i - y_i)^2\n\\]\nEstimate the true error \\(\\mathbb{E} (\\hat{y}_{n+1} - y_{n+1})^2\\) as \\[\n\\frac{1}{K} \\sum_{k=1}^K \\hat{E}_k,\n\\] Schematically, this looks something like this (with \\(K=5\\)):\n\n\n\n\nSchematic of \\(K\\)-fold CV (Credit: ISLR2e fig. 5.5)\n\n\nLet’s implement this in R, just for the practice. Once again, R has built-in tools for making this easier, which we will discuss later, but this is a good opportunity to practice our R a bit.\n\ndata('mtcars'); # We'll continue to use the mtcars data set\nK &lt;- 5; # 5-fold regularization. K between 5 and 10 is a fairly standard choice\n\n# The first thing we need to do is partition the data into K folds.\n# There are many different ways to do this,\n# including using functions from other packages\n# (e.g., https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/trainControl)\n# But here's an approach using the R function split() that I like\nn &lt;- nrow(mtcars);\n# sample(n,n,replace=FALSE) really just randomly permutes the data.\n# Then, passing that into the split function assigns these to the K different\n# factors defined by as.factor(1:K).\n# See ?split for more information.\nKfolds &lt;- split( sample(1:n, n,replace=FALSE), as.factor(1:K));\n\nWarning in split.default(sample(1:n, n, replace = FALSE), as.factor(1:K)): data\nlength is not a multiple of split variable\n\n# Note that this will throw a warning in the event that K does not divide n\n# evenly. That's totally fine!\n\nKfolds\n\n$`1`\n[1] 29 12  6  9  2  1 11\n\n$`2`\n[1]  5 24 21 10 19  3 23\n\n$`3`\n[1] 14 13  4 30 28 22\n\n$`4`\n[1] 32 26  8 25 15 18\n\n$`5`\n[1] 27 17 16 20 31  7\n\n\nNow, for each of these \\(K=5\\) folds, we’ll set it aside, train on the remaining data, and evaluate on the fold.\n\n# The file mtcars_Kfold.R defines a function that trains the five different-order\n# models and evaluates each one according to the given holdout set.\n# It largely repeats the structure of the LOO-CV code implemented above,\n# hence why it is relegated to a file for your later perusal.\nsource('r_scripts/mtcars_Kfold.R');\n\n# Set up a data frame to hold our residuals.\nnorder &lt;- 5;\nKfold_resids &lt;- data.frame( 'Order'=rep(1:norder, each=K),\n                            'Fold'=rep(1:K, norder ),\n                            'Error'=rep(NA, K*norder) );\n\nfor (k in 1:K ) {\n  heldout_idxs &lt;- Kfolds[[k]]; # The indices of the k-th hold-out set.\n  \n  # Now train the 5 different models and store their residuals.\n  idx &lt;- (Kfold_resids$Fold==k);\n  Kfold_resids[idx, ]$Error &lt;- mtcars_fit_models( heldout_idxs );\n  \n}\n\nhead(Kfold_resids)\n\n  Order Fold     Error\n1     1    1 11.930364\n2     1    2  7.338068\n3     1    3 15.919574\n4     1    4 16.645293\n5     1    5 32.325581\n6     2    1  7.572326\n\n\nNow, we need to aggregate over the \\(K=5\\) folds, and then we can plot the errors. Once again, we need to use a log scale for the errors, because the higher-order models cause some really bad prediction errors on a handful of “bad” examples.\n\nKF_agg &lt;- aggregate(Error ~ Order, data=Kfold_resids, FUN=mean);\n\npp &lt;- ggplot(KF_agg, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n\n\n\nOnce again, the order-2 model, mpg ~ 1 + hp + hp^2, does best (usually, anyway– occasionally the order-3 model is slightly better due to randomness on this small data set).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#aside-the-bias-variance-decomposition",
    "href": "cv.html#aside-the-bias-variance-decomposition",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.6 Aside: the bias-variance decomposition",
    "text": "34.6 Aside: the bias-variance decomposition\nNote: This subsection includes a lot of math, including a lot of expectation and variance terms and taking expectations with respect to some variables but not others. You are not responsible for these details on an exam. The important thing to take away from this subsection is the concept of the bias-variance decomposition of the means squared error (MSE), in which we can write the MSE as a variance term plus a squared bias.\nSuppose that we have a quantity \\(\\theta\\) that we want to estimate, and we have an estimator \\(\\hat{\\theta}\\), the mean squared error is defined as \\[\n\\operatorname{MSE}(\\hat{\\theta}, \\theta)\n= \\mathbb{E} \\left( \\hat{\\theta} - \\theta \\right)^2.\n\\]\nFor example, in our CV examples above, we wanted to estimate the squared error on a previously unseen data point, \\(\\mathbb{E}( \\hat{Y}_{n+1} - Y_{n+1} )^2\\). Note that even though this looks kind of like MSE, it is not. This quantity is \\(\\theta\\) in our MSE expression above. It is a thing we want to estimate. Our love of squared errors has caused us to have a whole mess of colliding notation. Such is life.\nImportant point: we are taking expectation here with respect to the random variable \\(\\hat{theta}\\). Its randomness comes from the data itself (which we usually assume to depend on the true parameter \\(\\theta\\) in some way).\nNow, let’s expand the MSE by adding and subtracting \\(\\mathbb{E} \\hat{\\theta}\\) inside the square: \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\begin{aligned}\n\\operatorname{MSE}\n&= \\E \\left( \\thetahat - \\theta \\right)^2 \\\\\n&= \\E \\left( \\thetahat - \\E \\thetahat + \\E \\thetahat - \\theta \\right)^2 \\\\\n&= \\E\\left[  \\left( \\thetahat - \\E \\thetahat \\right)^2\n           + 2\\left( \\thetahat - \\E \\thetahat \\right)\\left( \\E \\thetahat - \\theta \\right)\n           + \\left( \\E \\thetahat - \\theta \\right)^2 \\right] \\\\\n&= \\E \\left( \\thetahat - \\E \\thetahat \\right)^2\n  +  \\E 2\\left( \\thetahat - \\E \\thetahat \\right)\n                \\left( \\E \\thetahat - \\theta \\right)\n  + \\E \\left( \\E \\thetahat - \\theta \\right)^2.\n\\end{aligned}\n\\] Now, let’s notice that \\(\\theta\\) and \\(\\mathbb{E} \\hat{\\theta}\\) are not random, so they can get pulled out of the expectation (along with the factor of \\(2\\), which is also not random!). we can write (again, remember that the expectation is over \\(\\hat{\\theta}\\), while \\(\\theta\\) is non-random) \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\E 2\\left( \\thetahat - \\E \\thetahat \\right) \\left( \\E \\thetahat - \\theta \\right)\n= 2 \\left( \\E \\thetahat - \\theta \\right) \\E \\left( \\thetahat - \\E \\thetahat \\right)\n    = 0,\n\\] because \\[\n\\mathbb{E}\\left(\\hat{\\theta} - \\mathbb{E} \\hat{\\theta} \\right)\n= \\mathbb{E} \\hat{\\theta} - \\mathbb{E} \\hat{\\theta}\n= 0.\n\\]\nPlugging this into our equation above, we conclude that \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\operatorname{MSE}\n= \\E \\left( \\thetahat - \\E \\thetahat \\right)^2\n+ \\E \\left( \\E \\thetahat - \\theta \\right)^2.\n\\]\nThe first term on the right is just a variance– like \\(\\mathbb{E}(X - \\mathbb{E}X)^2\\).\nThe second term on the right is the expectation of \\((\\mathbb{E} \\hat{\\theta} - \\theta)^2\\). But this term isn’t random at all– \\(\\theta\\) is a fixed parameter, and \\(\\mathbb{E} \\hat{\\theta}\\) is just an expected value (i.e., not random!), so \\[\n\\E \\left( \\E \\thetahat - \\theta \\right)^2\n= \\left( \\E \\thetahat - \\theta \\right)^2,\n\\] and notice that this is just the squared bias– the square of the difference between the expectation of our estimator and the thing it is supposed to estimate.\nSo, to recap, we have shown that we can decompose the MSE as \\[\n\\operatorname{MSE}(\\hat{\\theta}, \\theta)\n= \\operatorname{Var} \\hat{\\theta} + \\operatorname{Bias}^2(\\hat{\\theta}, \\theta).\n\\]\nIn general, there will be many different estimators (i.e., many different choices of \\(\\hat{\\theta}\\)) that all obtain (approximately) the same MSE. The above equation means that once we are choosing among these different “similar” estimators (i.e., estimators that have similar MSE), we are really just trading off between bias and variance. That is, an estimator with smaller bias will have to “pay” for it with more variance. This is often referred to as the bias-variance tradeoff.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#cv-and-the-bias-variance-tradeoff",
    "href": "cv.html#cv-and-the-bias-variance-tradeoff",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.7 CV and the bias-variance tradeoff",
    "text": "34.7 CV and the bias-variance tradeoff\nNow, the purpose of cross-validation is to estimate the model error \\(\\E(\\hat{Y}_{n+1}-Y_{n+1})^2\\). The bias-variance tradeoff says that, roughly speaking, different “reasonable” ways of estimating this quantity will all have about the same MSE, but will involve balancing bias against variance.\n\n34.7.1 Bias in CV\nLet’s think back to the “naive” cross-validation approach, in which we split the data into two sets of similar sizes, train on one and evaluate on the other. When we do that, we train our model on a much smaller data set than if we used the full data. The result is that we (accidentally) over-estimate the error of our model, because models trained on less data simply tend to be less accurate.\nThat is to say, the “naive” cross-validation approach tends to yield a biased estimate of the true error of the model. Specifically, our estimate is biased upward.\nOn the other hand, LOOCV should be approximately unbiased as an estimate of the model error, because the difference between training on \\(n\\) and \\(n-1\\) data points should not be especially large (at least once \\(n\\) is reasonably large).\nIt stands to reason that \\(K\\)-fold CV should sit at a kind of “happy medium” level of bias between LOOCV and “naive” CV.\n\n\n34.7.2 Variance in CV\nSo LOOCV is the least biased estimate of model error, but the bias-variance trade-off predicts that we must “pay” for this in variance. It turns out that LOOCV has the most variance out of the three methods LOOCV, \\(K\\)-fold CV (for \\(K &lt; n\\)) and “naive” CV.\nIntuitively, the variance in LOOCV comes from the following fact: recall that for each \\(i=1,2,\\dots,n\\), we hold out the \\(i\\)-th data point and train a model on the rest.\nThis means that we have \\(n\\) different trained models, each trained on \\(n-1\\) data points, but each pair of training sets overlap in \\(n-2\\) of their data points. The result is that the trained models are highly correlated with one another. Changing just one data point in our data set doesn’t change the fitted model much!\nThe result is that these estimated model errors are highly correlated with one another, with the result that our overall estimate of the model error has high variance.\nThe \\(K\\) models trained in \\(K\\)-fold CV are less correlated with one another, and hence we have (comparatively) less variance. It turns out in this case that \\(K\\) less-correlated error estimates have smaller correlation than \\(n\\) highly-correlated ones.\n\n\n34.7.3 \\(K\\)-fold CV: the happy medium\nThus, \\(K\\)-fold CV is a popular choice both because it is computationally cheaper than LOOCV (\\(K\\) model fits compared to \\(n\\) of them) and because it strikes a good balance between bias and variance.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#variable-selection-for-real-this-time",
    "href": "cv.html#variable-selection-for-real-this-time",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.8 Variable selection, for real this time",
    "text": "34.8 Variable selection, for real this time\nIn our examples above, we concentrated on choosing among a family of linear regression models that varied in their orders, in the sense that they included as predictors all powers of the horsepower variable hp, up to some maximum power, to predict gas mileage. Hopefully it is clear how we could modify our approach to, say, choose which variables we do and don’t include in a model (e.g., as in the Pima diabetes data set that we’ve seen a few times this semester).\nUltimately, our goal was to choose, from among a set of predictors that we could include in our model (e.g., powers of hp, in the case of the mtcars example), which predictors to actually include in the model. Again, this task is variable selection.\nOne thing that might be bugging us so far is that any way we slice it, cross-validation doesn’t use all of the available data: we are always holding something out of our fitted model for the sake of estimating our error on unseen data.\nLet’s look at a few different approaches to variable selection that do not rely on cross-validation. These alternative methods have the advantage of not trying to estimate the unknown model error on unseen data. On the other hand, these methods can be more computationally intensive and tend to come with fewer theoretical guarantees.\nThis is not to suggest, however, that these methods are at odds with cross-validation. In actual research papers and in industry applications, you’ll often see both CV and some of the methods presented below used in tandem to select the best model for the job.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#setup-linear-regression-and-fitting",
    "href": "cv.html#setup-linear-regression-and-fitting",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.9 Setup: linear regression and fitting",
    "text": "34.9 Setup: linear regression and fitting\nLet’s continue to focus on linear regression, bearing in mind that the ideas introduced here apply equally well to other regression and prediction methods (e.g., logistic regression). Let’s recall that multiple linear regression models a response \\(Y \\in \\mathbb{R}\\) as a linear (again, technically affine– linear plus an intercept!) function of a set of \\(p\\) predictors plus normal noise: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\nHere, \\(\\epsilon\\) is mean-zero normal with unknown variance \\(\\sigma^2 &gt; 0\\), and the variables \\(X_1,X_2,\\dots,X_p\\) are the predictors. We often refer to \\(p\\), the number of predictors, as the dimension of the problem, because the data (well, the vector of predictors, anyway), lies in \\(p\\)-dimensional space. Collecting the coefficients into a vector \\((\\beta_0,\\beta_1,\\dots,\\beta_p) \\in \\mathbb{R}^{p+1}\\) and creating a vector \\(X=(1,X_1,X_2,\\dots,X_p) \\in \\mathbb{R}^p\\), we can write this more succinctly as (if you have not taken linear algebra, you can safely ignore this, we’re just including it because it’s a common notation) \\[\nY = \\beta^T X + \\epsilon.\n\\]\nIn multiple linear regression, we observe a collection of predictor-response pairs \\((X_i,Y_i)\\) for \\(i=1,2,\\dots,n\\), with \\[\nX_i = (1,X_{i,1},X_{i,2},\\dots,X_{i,p}) \\in \\mathbb{R}^{p+1}.\n\\]\nNote that here we are including the intercept term \\(1\\) in the vector of predictors for ease of notation. This is a common notational choice, so we’re including it here to get you used to seeing this. Of course, this is not universal– it’s one of those conventions that you have to be careful of and check what you are reading.\n\n34.9.1 Recap: variable selection\nSo we have \\(p\\) variables (plus an intercept term), and we want to select which ones to include in our model. There are many reasons to want to do this, but let’s just highlight three of them:\n\nIf there are many “useless” variables (i.e., ones that are not good predictors of the response), then including them in the model can make our predictions less accurate. Thus, we would like to proactively identify which variables are not useful, and avoid including them in the model in the first place.\nA model with fewer variables is simpler, and we like simple models! Explaining, say, heart attack risk as a function of two or three factors is a lot easier to use than a model that uses ten or twenty factors.\nIf the number of predictors \\(p\\) is too large (say, larger than \\(n\\)– a common occurrence in genomic studies, for example), our estimates of the coefficients are very unstable. Variable selection and related tools give us a way to introduce stability in the form of regularization, which we will talk about below.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#best-subset-selection",
    "href": "cv.html#best-subset-selection",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.10 Best subset selection",
    "text": "34.10 Best subset selection\nSo, we have \\(p\\) predictor variables available to us, and we want to choose which of them to actually include in our model.\nWell, the most obvious solution is to just try all possible combinations of features, train a model using each combination, and keep the best one (measured by, say, residual sum of squares).\nThis would have an obvious drawback: we have already seen that we can trivially improve the RSS of our model by adding variables. So the models that include more variables would do better, even if those variables did not actually lead to better model error on unseen data.\nThe solution to this is to do the following:\n\nFor each \\(k=1,2,\\dots,p\\), for every set of \\(k\\) different variables, fit a model and keep the model that best fits the data (measured by RSS). Call this model \\(M_k\\).\nUse CV (or some other tool like AIC or adjusted \\(R^2\\), which we’ll discuss below) to select among the models \\(M_1,M_2,\\dots,M_p\\).\n\nThis is called best subset selection. It is implemented in R in, for example, the leaps library the function regsubsets, which gets called in more or less the same way as lm. See (here)[https://cran.r-project.org/web/packages/leaps/index.html] for documentation if you’re interested.\nThere is one rather glaring problem with best subset selection, though:\nQuestion: if there are \\(p\\) predictors, how many models does best subset selection fit before it makes a decision?\nSo once \\(p\\) is even moderately large, best subset selection is computationally expensive, and we need to do something a little more clever.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#stepwise-selection",
    "href": "cv.html#stepwise-selection",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.11 Stepwise selection",
    "text": "34.11 Stepwise selection\nSo best subset selection is expensive because we have to try every possible model, and then choose among the best “size-\\(k\\)” model for each \\(k=1,2,\\dots,p\\). How might we cut down on the computational expense?\nStepwise selection methods avoid exhaustively checking all \\(2^p\\) possible models by starting with a particular model and adding or removing one variable at a time (i.e., in “steps”).\nThe important part is in how we decide which predictor to add or remove from the model at a particular time.\n\n34.11.1 Forward stepwise selection\nThe most obvious (to me, anyway) way to avoid checking every possible model is to start with a “null” model (i.e., no predictors, just an intercept term), then repeatedly add the “best” predictor not already in the model. That is,\n\nStart by fitting the “null” model, with just an intercept term. Call it \\(M_0\\).\nFor each \\(k=1,2,\\dots,p\\), among the \\(p-k\\) predictors not already in the model, add the one that yields the biggest improvement in RSS. Call this model, which includes \\(k\\) predictors and the intercept term, \\(M_k\\).\nUse CV or some other method (e.g., an information criterion; see ISLR Section 6.1) to choose among \\(M_0,M_1,M_2,\\dots,M_p\\).\n\nThe important thing is that in Step 2 above, for each \\(k=1,2,\\dots,p\\), we need to fit \\(p-k\\) different models. Thus, in total (i.e., summing over \\(k=0,1,2,\\dots,p\\)), we end up fitting \\[\n1+ \\sum_{k=0}^{p-1} (p-k)\n= 1+p^2 - \\frac{(p-1)p}{2} = 1+\\frac{ 2p^2 - p^2 + p }{2}\n= 1+ \\frac{ p(p+1)}{2}\n\\] different models.\nTo get a sense of what a big improvement this is, when \\(p\\) is large, this right-hand side is approximately \\(p^2/2\\). Compare that with \\(2^p\\), which is a MUCH larger number. For example, when \\(p=10\\), \\(2^{10} \\approx 1000\\), while \\(10^2/2 \\approx 50\\). When \\(p=20\\), \\(2^{20} \\approx 1,000,000\\) while \\(20^2/2 \\approx 200\\).\nOf course, the drawback is that forward stepwise selection might “miss” the optimal model, since it does not exhaustively fit every possible model the way that best subset selection does.\n\n\n34.11.2 Backward stepwise selection\nWell, if we can do forward stepwise selection, why not go backwards?\nIn backward stepwise selection, we start with the full model (i.e., a model with all \\(p\\) predictors), and iteratively remove one predictor at a time, always removing the predictor that decreases RSS the least.\nJust like forward stepwise regression, this decreases the number of models we have to fit from \\(2^p\\) to something more like (approximately) \\(p^2/2\\).\nCautionary note: backward selection will only work if the number of observations \\(n\\) is larger than \\(p\\). If \\(n &lt; p\\), the “full” model cannot be fit, because we have an overdetermined system of linear equations– \\(n\\) equations in \\(p\\) unknowns, and \\(p &gt; n\\). This is a setting where regularization can help a lot (see below), but the details are best left to your regression course(s).\n\n\n34.11.3 Hybrid approaches: the best of both worlds?\nIt is outside the scope of this course, but there do exist stepwise selection methods that try to combine forward and backward stepwise selection. For example, we can alternately add and remove variables as needed. This can be helpful when, for example, a predictor is useful “early” in the selection process, but becomes a less useful predictor once other predictors have been included.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#model-comparison-statistics-adjusted-r2-aic-and-bic",
    "href": "cv.html#model-comparison-statistics-adjusted-r2-aic-and-bic",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.12 Model Comparison Statistics: Adjusted \\(R^2\\), AIC and BIC",
    "text": "34.12 Model Comparison Statistics: Adjusted \\(R^2\\), AIC and BIC\nRather than comparing the RSS of two models, which only compares the reduction to residuals with no regards to the number of predictors (the complexity) in the model, there are some statistics that are often used. Note that these statistics are relevant when comparing models of different complexity - two models with the same number of predictors would just as well be compared using \\(RSS\\). These statistics are useful to balance the benefit of reduced \\(RSS\\) with the cost of additional model complexity.\nIn each case, \\(k\\) is the number of parameters being estimated - including the intercept.\n\n34.12.1 Adjusted \\(R^2\\)\n\\[R^2_{adj} = 1 − \\frac{RSS/(n-k)}{TSS/(n-1)}=1-\\frac{RSS}{TSS}\\left(\\frac{n-1}{n-k}\\right)\\] The fraction multiplied will be \\(&gt;1\\), and grows with model complexity. In an extreme case, this penalty can result in a negative \\(R^2_{adj}\\), so it’s important to remember that this statistic is not meaningful by itself, only when used to compare models. Thus \\(R^2_{adj} &lt; R^2\\), and applies a penalty that grows with the number of predictors. When comparing two models using \\(R^2_{adj}\\) we prefer the model that has the higher value.\n\n\n34.12.2 Akaike information criterion (AIC)\n\\[AIC = −2 \\ln(L) + 2k\\] If you work out the math (we won’t here) for a linear model this can be expressed in terms of \\(RSS\\) \\[AIC = n\\ln(RSS/n) + 2k\\]\n\n\n34.12.3 Bayesian information criterion (BIC)\n\\[BIC = -2 \\ln(L)+\\ln(n)k =  n\\ln(RSS/n) + \\ln(n)k\\] The first term in AIC and BIC is the residual deviance, which we want to be as low as possible. While a more complex model will reduce residual deviance, both AIC and BIC add a penalty. BIC adds a more severe penalty per predictor (if \\(n &gt; e^2\\approx7.4\\)).\nThe bottom line: between these three model comparison statistics, BIC more heavily favors simpler models, \\(R^2_{adj}\\) allows for more complex models and AIC is somewhere in the middle.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#shrinkage-and-regularization",
    "href": "cv.html#shrinkage-and-regularization",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.13 Shrinkage and Regularization",
    "text": "34.13 Shrinkage and Regularization\nThe variable selection methods we just discussed involved trying out different subsets of the predictors and seeing how the model performance changed as a result.\nLet’s consider an alternative approach. What if instead of trying lots of different models with different numbers of predictors, we went ahead and fit a model with all \\(p\\) available predictors, but we modify our loss function in such a way that we will set the coefficients of “unhelpful” predictors to zero? This is usually called “shrinkage”, because we shrink the coefficients toward zero. You will also often hear the term regularization, which is popular in machine learning, and means more or less the same thing.\nLet’s briefly discuss two such methods, undoubtedly two of the most important tools in the statistical toolbox: ridge regression and the LASSO.\n\n34.13.1 Ridge regression\nBy now you are bored to death of seeing the linear regression least squares objective, but here it is again: \\[\n\\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j X_{i,j} \\right)^2\n\\]\nHere we are assuming that we have \\(p\\) predictors, so each \\((X_i,Y_i)\\) pair has a vector of predictors \\(X_i = (X_{i,1},X_{i,2},\\dots,X_{i,p}) \\in \\mathbb{R}^p\\) and response \\(Y_i \\in \\mathbb{R}\\).\nRemember, we’re trying to minimize this RSS by choosing the coefficients \\(\\beta_j\\), \\(j=0,1,2,\\dots,p\\) in a clever way.\nRidge regression shrinks these estimated coefficients toward zero by changing the loss slightly. Instead of minimizing the RSS alone, we add a penalty term: \\[\n\\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\operatorname{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] where \\(\\lambda \\ge 0\\) is a tuning parameter (which we have to choose– more on that soon).\nOur cost function now has two different terms:\n\nOur old friend RSS, which encourages us to choose coefficients that reproduce the observed responses accurately\nThe shrinkage penalty \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\), which encourages us to choose all our coefficients (other than \\(\\beta_0\\)) equal to zero. That is, it shrinks our solution toward the origin.\n\nThe tuning parameter \\(\\lambda\\) controls how much we care about this shrinkage penalty compared to the RSS term. When \\(\\lambda\\) is big, we “pay” more for large coefficients, so we will prefer coefficients closer to zero. When \\(\\lambda=0\\), we recover plain old least squares regression.\nFor each value of \\(\\lambda\\) that we choose, we get a different solution to our (regularized) regression, say, \\(\\hat{\\beta}^{(\\lambda)}\\). In this sense, whereas least squares linear regression gives us just one solution \\(\\hat{\\beta}\\), shrinkage methods give us a whole family of solutions, corresponding to different choices of \\(\\lambda\\).\nFor this reason, choosing the tuning parameter \\(\\lambda\\) is crucial, but we will have only a little to say about this matter, owing to time constraints. Luckily, you already know a family of methods for choosing \\(\\lambda\\)– cross validation is a very common appraoch!\n\n34.13.1.1 Ridge regression on the mtcars data set\nLet’s try this out on the mtcars data set, trying to predict mpg from all the of the available predictors, this time. One thing to bear in mind: the data set is only 32 observations, so our fits are going to be a little unstable (but this is precisely why we use regularization!).\n\nnames(mtcars);\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nRidge regression is available in the MASS library in R.\n\nlibrary(MASS);\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nlambda_vals &lt;- c(0,1,2,5,10,20,50,100,200,500); # Choose lambdas to try.\n# lm.ridge needs:\n# 1) a model (mpg~. says to model mpg as an intercept\n#         plus a coefficient for every other variable in the data frame)\n# 2) a data set (mtcars, of course)\n# 3) a value for lambda. lambda=0 is the default,\n#         and recovers classic linear regression.\n#         But we can also pass a whole vector of lambdas, like we are about to do,\n#         and lm.ridge will fit a separate model for each.\n# See ?lm.ridge for details.\nridge_models &lt;- lm.ridge(mpg~., mtcars, lambda=lambda_vals);\n\n# Naively plotting this object shows us how the different coefficients\n# change as lambda changes.\nplot( ridge_models );\n\n\n\n\n\n\n\n\nEach line in the above plot represents the coefficient of one of our predictors. The x-axis is our choice of \\(\\lambda\\) (lambda in the code) and the y-axis is the actual value of the coefficients.\nActually extracting those predictor labels to make a legend for this plot is annoying, and beside the point– refer to the documentation in ?lm.ridge). The important point is that as we change \\(\\lambda\\), the coefficients change. Generally speaking, as \\(\\lambda\\) gets bigger, more coefficients are closer to zero.\nIndeed, if we make \\(\\lambda\\) big enough, all of the coefficients will be zero (except the intercept, because it isn’t multiplied by \\(\\lambda\\) in the loss). That’s shrinkage!\nJust as a sanity check, let’s fit plain old linear regression and verify that the coefficients with \\(\\lambda=0\\) match.\n\nlm_sanity_check &lt;- lm(mpg~., mtcars);\nlm_sanity_check$coefficients\n\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925 \n\n\nAnd compare that with\n\nhead( coef( ridge_models), 1 ); # the first row is the lambda=0.0 setting.\n\n                    cyl       disp          hp     drat        wt      qsec\n  0 12.30337 -0.1114405 0.01333524 -0.02148212 0.787111 -3.715304 0.8210407\n           vs       am     gear       carb\n  0 0.3177628 2.520227 0.655413 -0.1994193\n\n\nThey’re the same, up to several digits of precision, anyway. Good!\n\n\n34.13.1.2 Shrinkage and RSS\nNow, for each value of \\(\\lambda\\), we get a different fitted model. How do these different models do in terms of their fit (as measured by RSS)?\nWell, annoyingly, the object returned by lm.ridge does not include a residuals attribute the same way that the lm object does:\n\nmean( lm_sanity_check$residuals^2 );\n\n[1] 4.609201\n\n\nMore annoyingly still, the object returned by lm.ridge also does not include a predict method, so we can just call something like predict( model, data) the way we would with the output of lm:\n\nmean( (predict( lm_sanity_check, mtcars) - mtcars$mpg )^2 )\n\n[1] 4.609201\n\n\nSo, we have to roll our own predict/residuals computation. This is going to be a bit complicated, but it’s worth the detour to get some programming practice.\nOur ridge regression model has coefficients: one set of coefficients for each valu of lambda that we passed in.\n\nlength( lambda_vals )\n\n[1] 10\n\n\nThose estimated coefficients are stored in a matrix. Each column of this matrix corresponds to a coefficient (including the intercept, the first column. Each row corresponds to one \\(\\lambda\\) value.\n\ncoef( ridge_models )\n\n                    cyl          disp           hp      drat         wt\n  0 12.30337 -0.1114405  0.0133352399 -0.021482119 0.7871110 -3.7153039\n  1 16.53766 -0.1624028  0.0023330776 -0.014934856 0.9246313 -2.4611460\n  2 18.55460 -0.2212878 -0.0007347273 -0.013481440 0.9597173 -2.0619305\n  5 20.72198 -0.3079969 -0.0036206803 -0.012460649 1.0060841 -1.6219933\n 10 21.27391 -0.3563891 -0.0048700433 -0.011966312 1.0409643 -1.3618221\n 20 20.88322 -0.3792044 -0.0054324669 -0.011248991 1.0545238 -1.1389693\n 50 19.85752 -0.3614183 -0.0052401829 -0.009609992 0.9828882 -0.8673112\n100 19.37410 -0.3092845 -0.0044742424 -0.007814946 0.8353896 -0.6656981\n200 19.29829 -0.2337711 -0.0033702943 -0.005732613 0.6286426 -0.4706006\n500 19.54099 -0.1331434 -0.0019134597 -0.003201881 0.3567082 -0.2559721\n          qsec        vs        am      gear        carb\n  0 0.82104075 0.3177628 2.5202269 0.6554130 -0.19941925\n  1 0.49258752 0.3746517 2.3083758 0.6857159 -0.57579125\n  2 0.36772540 0.4389536 2.1835666 0.6545252 -0.64772938\n  5 0.23220486 0.5749017 1.9622086 0.5933014 -0.65548632\n 10 0.17615173 0.7007040 1.7537869 0.5573491 -0.59737336\n 20 0.15680180 0.8158626 1.5168704 0.5362044 -0.50497523\n 50 0.15120048 0.8706103 1.1764833 0.4942313 -0.36858373\n100 0.13669747 0.7898063 0.9064651 0.4225302 -0.27224119\n200 0.10798351 0.6187171 0.6407444 0.3195920 -0.18719020\n500 0.06363418 0.3611011 0.3479026 0.1819390 -0.09983672\n\n\nSo we can pick out the coefficients associated to a particular lambda value by taking the corresponding row of this matrix. For example, \\(\\lambda = 5\\) is in the 4-th row of the matrix:\n\ncat(paste0(\"The 4-th lambda value is: \", lambda_vals[4]) );\n\nThe 4-th lambda value is: 5\n\ncoef( ridge_models )[4,]; # Pick out the 4-th row. these are coefs when lambda=5.\n\n                    cyl        disp          hp        drat          wt \n20.72198423 -0.30799694 -0.00362068 -0.01246065  1.00608409 -1.62199325 \n       qsec          vs          am        gear        carb \n 0.23220486  0.57490168  1.96220860  0.59330141 -0.65548632 \n\n\nNow, to get our prediction from these coefficients, we have to multiply each predictor by its coefficient and add the intercept term. Equivalently, we can think of adding an extra predictor that is just \\(1\\) for every observation. Something like \\[\n\\beta_0 + \\sum_{j=1}^p \\beta_j X_{i,j}\n= \\sum_{j=0}^p \\beta_j X_{i,j},\n\\]\nAs an aside, for those that have taken linear algebra, you should be looking at that and thinking “that’s just an inner product!” \\[\n\\beta^T X_i = \\sum_{j=0}^p \\beta_j X_{i,j}.\n\\]\nSo let’s modify the mtcars data to make that all easy.\n\n# The mpg column of mtcars needs to get removed (it is the outcome,\n# not a predictor), so we drop it-- it's the column numbered 1.\n# And we're using cbind to add a column of 1s with column name const.\nmtc_predictors &lt;- cbind(const=1,mtcars[,-c(1)]);\nhead(mtc_predictors);\n\n                  const cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4             1   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag         1   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710            1   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive        1   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout     1   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant               1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nNow, to make a prediction on, say, the Datsun 710 observation, we need to multiply each predictor (including the const column) by its coefficient, and sum up the total. Again, something like \\[\n\\sum_{j=0}^p \\beta_j X_{i,j},\n\\] where \\(X_{i,0}=1\\) is the extra constant term that we tacked on.\nSo to get the prediction for a particular observation (again, say, the Datsun 710 row in mtcars), we need to make this sum (i.e., inner product) between that row of the predictors matrix and the vector of coefficients.\n\nbeta5 &lt;- coef( ridge_models )[4,]; # 4th row was for lambda=5.\ndatsun &lt;- mtc_predictors['Datsun 710',]\nsum( beta5*datsun )\n\n[1] 26.62668\n\n\nAs a sanity check, let’s verify that the 1-th row, which is \\(\\lambda=0\\), agrees with our linear model’s prediction.\n\nbeta0 &lt;- coef( ridge_models )[1,]; # 1st row was for lambda=0, i.e., plain old LR.\ndatsun &lt;- mtc_predictors['Datsun 710',]\nsum( beta0*datsun );\n\n[1] 26.25064\n\n\nand compare with\n\npredict( lm_sanity_check, datsun )\n\nDatsun 710 \n  26.25064 \n\n\nOkay, but to compute the RSS of our model we want to compute predictions for all 32 of our observations in the mtcars data set. And we want to compute those predictions for each of our different choices of \\(\\lambda\\).\nWe’re going to get those predictions in a matrix. If you haven’t taken linear algebra, don’t let the word scare you. In this context, it’s enough to just think of a matrix as a big box of numbers.\nNow, we currently have two boxes of numbers. One is mtc_predictors. Each row is an observation (so there are 32 rows), and each row has 11 entries, corresponding to the intercept term and ten additional predictors.\n\ndim( mtc_predictors )\n\n[1] 32 11\n\n\nThe other box of numbers is our collection of coefficients. One row for each of the models we fit (i.e., \\(\\lambda\\) values), and one column for each predictor.\n\nbeta_matrix &lt;- coef( ridge_models )\ndim(beta_matrix);\n\n[1] 10 11\n\n\n\nbeta_matrix\n\n                    cyl          disp           hp      drat         wt\n  0 12.30337 -0.1114405  0.0133352399 -0.021482119 0.7871110 -3.7153039\n  1 16.53766 -0.1624028  0.0023330776 -0.014934856 0.9246313 -2.4611460\n  2 18.55460 -0.2212878 -0.0007347273 -0.013481440 0.9597173 -2.0619305\n  5 20.72198 -0.3079969 -0.0036206803 -0.012460649 1.0060841 -1.6219933\n 10 21.27391 -0.3563891 -0.0048700433 -0.011966312 1.0409643 -1.3618221\n 20 20.88322 -0.3792044 -0.0054324669 -0.011248991 1.0545238 -1.1389693\n 50 19.85752 -0.3614183 -0.0052401829 -0.009609992 0.9828882 -0.8673112\n100 19.37410 -0.3092845 -0.0044742424 -0.007814946 0.8353896 -0.6656981\n200 19.29829 -0.2337711 -0.0033702943 -0.005732613 0.6286426 -0.4706006\n500 19.54099 -0.1331434 -0.0019134597 -0.003201881 0.3567082 -0.2559721\n          qsec        vs        am      gear        carb\n  0 0.82104075 0.3177628 2.5202269 0.6554130 -0.19941925\n  1 0.49258752 0.3746517 2.3083758 0.6857159 -0.57579125\n  2 0.36772540 0.4389536 2.1835666 0.6545252 -0.64772938\n  5 0.23220486 0.5749017 1.9622086 0.5933014 -0.65548632\n 10 0.17615173 0.7007040 1.7537869 0.5573491 -0.59737336\n 20 0.15680180 0.8158626 1.5168704 0.5362044 -0.50497523\n 50 0.15120048 0.8706103 1.1764833 0.4942313 -0.36858373\n100 0.13669747 0.7898063 0.9064651 0.4225302 -0.27224119\n200 0.10798351 0.6187171 0.6407444 0.3195920 -0.18719020\n500 0.06363418 0.3611011 0.3479026 0.1819390 -0.09983672\n\n\nOnce again, each column corresponds to one of 11 predictors (the intercept term and ten non-trivial predictors), and the rows correspond to the different choice of \\(\\lambda\\).\nSo, for each value of \\(\\lambda\\) (i.e., each row of \\(\\beta\\)), and each row of mtc_predictors (i.e., each observation in the data set), we want to sum up the products of the coefficients with their corresponding predictors.\nWe are going to make a new matrix, whose rows correspond to the 32 data observations and whose columns correspond to different choices of \\(\\lambda\\). We need to use some basic matrix algebra to construct that. Let’s do the computation, then unpack it.\n\nmtc_mx &lt;- as.matrix( mtc_predictors );\ncat('Dimensions of the predictors matrix: ');\n\nDimensions of the predictors matrix: \n\ncat(dim(mtc_mx)) \n\n32 11\n\nbeta_mx &lt;- coef( ridge_models );\ncat('Dimensions of the coefficients matrix: ');\n\nDimensions of the coefficients matrix: \n\ncat( dim(beta_mx) );\n\n10 11\n\n# Now compute the appropriate matrix product.\n# We want to rows indexed by observations\n# and the columns indexed by lambdas.\n# That requires transposing the coefficients matrix, whose original form\n# has rows indexed by lambda and columns indexed by the predictors.\n# We transpose a matrix in R with t( ).\nobs_by_lambda_predictions &lt;- as.matrix( mtc_mx ) %*% t( beta_mx );\nobs_by_lambda_predictions\n\n                           0        1        2        5       10       20\nMazda RX4           22.59951 22.30763 22.23051 22.13369 22.02501 21.85270\nMazda RX4 Wag       22.11189 21.95588 21.91065 21.85012 21.77639 21.65007\nDatsun 710          26.25064 26.61821 26.68382 26.62668 26.42251 26.04170\nHornet 4 Drive      21.23740 20.88953 20.78456 20.66661 20.59049 20.52290\nHornet Sportabout   17.69343 17.20040 17.01742 16.78452 16.64525 16.59645\nValiant             20.38304 20.37257 20.35076 20.31018 20.28168 20.26423\nDuster 360          14.38626 14.15765 14.13388 14.17680 14.29042 14.52925\nMerc 240D           22.49601 22.68287 22.80572 23.00581 23.14734 23.21667\nMerc 230            24.41909 23.91587 23.73479 23.58564 23.58592 23.62033\nMerc 280            18.69903 19.10420 19.31008 19.67422 20.00760 20.33982\nMerc 280C           19.19165 19.39975 19.53072 19.81354 20.11329 20.43390\nMerc 450SE          14.17216 14.91618 15.12809 15.35748 15.52382 15.75031\nMerc 450SL          15.59957 15.85149 15.90269 15.95540 16.02207 16.16892\nMerc 450SLC         15.74222 15.92546 15.94668 15.96718 16.02444 16.17470\nCadillac Fleetwood  12.03401 11.67687 11.64501 11.75998 12.02127 12.49759\nLincoln Continental 10.93644 11.05719 11.16858 11.42987 11.76777 12.30084\nChrysler Imperial   10.49363 10.99657 11.21759 11.58203 11.96222 12.51055\nFiat 128            27.77291 27.88472 27.85376 27.69494 27.44262 27.01867\nHonda Civic         29.89674 29.26876 29.06962 28.80821 28.54153 28.10115\nToyota Corolla      29.51237 29.12150 28.91791 28.56765 28.21014 27.70198\nToyota Corona       23.64310 23.78667 23.85479 23.91651 23.89758 23.77409\nDodge Challenger    16.94305 16.84439 16.79091 16.69114 16.60761 16.57993\nAMC Javelin         17.73218 17.59335 17.50887 17.37192 17.27326 17.23149\nCamaro Z28          13.30602 13.73881 13.92543 14.19839 14.43699 14.75698\nPontiac Firebird    16.69168 16.24701 16.09680 15.91932 15.83133 15.84875\nFiat X1-9           28.29347 28.25684 28.19035 27.99133 27.70163 27.22949\nPorsche 914-2       26.15295 26.45050 26.49502 26.40196 26.15825 25.72990\nLotus Europa        27.63627 27.46919 27.38886 27.19338 26.92047 26.48469\nFord Pantera L      18.87004 18.79097 18.67829 18.47524 18.33544 18.26423\nFerrari Dino        19.69383 19.73505 19.79328 19.91247 20.01800 20.11261\nMaserati Bora       13.94112 13.74682 13.72632 13.83990 14.10487 14.56029\nVolvo 142E          24.36827 24.93714 25.10820 25.23790 25.21281 25.03481\n                          50      100      200      500\nMazda RX4           21.52222 21.21442 20.89232 20.52801\nMazda RX4 Wag       21.38573 21.12122 20.83279 20.49838\nDatsun 710          25.19341 24.25687 23.15477 21.80461\nHornet 4 Drive      20.44289 20.37973 20.30828 20.21609\nHornet Sportabout   16.82946 17.31265 17.99524 18.89895\nValiant             20.25478 20.24265 20.21593 20.16802\nDuster 360          15.18740 16.02340 17.06870 18.38818\nMerc 240D           23.04180 22.62447 22.01421 21.19076\nMerc 230            23.45483 23.00816 22.32148 21.37321\nMerc 280            20.63826 20.68452 20.59553 20.39937\nMerc 280C           20.72898 20.76654 20.66032 20.43755\nMerc 450SE          16.28647 16.94379 17.75743 18.77860\nMerc 450SL          16.61160 17.19747 17.93903 18.87836\nMerc 450SLC         16.62871 17.21886 17.95869 18.89101\nCadillac Fleetwood  13.57618 14.77514 16.18498 17.90822\nLincoln Continental 13.43666 14.67145 16.11294 17.86941\nChrysler Imperial   13.63141 14.83376 16.23293 17.93646\nFiat 128            26.06659 24.98855 23.70223 22.11461\nHonda Civic         27.04746 25.80880 24.31247 22.45784\nToyota Corolla      26.63521 25.44909 24.03979 22.30309\nToyota Corona       23.35933 22.81202 22.11937 21.23728\nDodge Challenger    16.81441 17.29638 17.98109 18.89022\nAMC Javelin         17.40983 17.80019 18.35988 19.10525\nCamaro Z28          15.45171 16.26403 17.25580 18.49633\nPontiac Firebird    16.20433 16.80970 17.62907 18.69568\nFiat X1-9           26.20867 25.08570 23.76438 22.14560\nPorsche 914-2       24.84061 23.92121 22.88154 21.63991\nLotus Europa        25.55719 24.54521 23.36083 21.91658\nFord Pantera L      18.35395 18.59356 18.94851 19.43374\nFerrari Dino        20.18278 20.18662 20.16519 20.13150\nMaserati Bora       15.45915 16.35277 17.35633 18.56415\nVolvo 142E          24.45802 23.71104 22.77794 21.60303\n\n\nSo this matrix has rows indexed by observations (i.e., cars) and columns indexed by choices of \\(\\lambda\\). So the \\((i,j)\\) entry of this matrix is the prediction made for the \\(i\\)-th car by the model with the \\(j\\)-th lambda value.\nWe are now ready (finally!) to compute the mean squared residuals for these different choices of \\(\\lambda\\). We just need to\n\nCompute the errors between these predictions and the true mpg values for the cars\nSquare those errors.\nSum along the columns (because each column corresponds to a different choice of \\(\\lambda\\), and hence a different fitted model).\n\n\nerrors &lt;- mtcars$mpg - obs_by_lambda_predictions;\n# Just to check, each column of errors should be length-32, because we have\n# 32 data points in the mtcars data set.\n# And there should be 32 columns, one for each of our ten lambda values.\ndim( errors );\n\n[1] 32 10\n\n\nSo we’re going to square those errors and take a mean along each column\n\n# We're going to squares the entries of errors,\n# then take a mean along the columns (that's the 2 argument to apply)\nRSS_by_model &lt;- apply( errors^2, 2, FUN=mean);\nRSS_by_model\n\n        0         1         2         5        10        20        50       100 \n 4.609201  4.724319  4.816983  4.986377  5.192462  5.590303  6.937583  9.407707 \n      200       500 \n13.821594 21.567053 \n\n\nThis is easier to see in a plot– and we’ll put the \\(\\lambda\\) values on a log-scale, because the lambda_vals vector spans multiple orders of magnitude.\n\nplot( log(lambda_vals), RSS_by_model, type='b', lwd=2)\n\n\n\n\n\n\n\n\nLet’s unpack this. We have the smallest RSS when \\(\\lambda = 0\\), and RSS increases as \\(\\lambda\\) increases. This is exactly what we expect. Recall that our loss function is \\[\n\\sum_{i=1}^n \\left( Y_i - \\sum_{j=0}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] As \\(\\lambda\\) gets bigger, we pay a bigger and bigger penalty for making coefficients non-zero. Thus, as \\(\\lambda\\) get bigger, it becomes “harder” or “more expensive” to make the coefficients take the values that would make the RSS term smaller. As a result, for larger values of \\(\\lambda\\), the RSS of our solution is larger.\n\n\n34.13.1.3 Why is ridge regression helpful?\nWell, the short answer is that ridge regression (and other shrinkage methods) prevents over-fitting. \\(\\lambda\\) makes it more expensive to simply choose whatever coefficients we please, which in turn prevents us from over-fitting to the data.\nIn essence, this is the bias-variance tradeoff again! As \\(\\lambda\\) increases, our freedom to choose the coefficients becomes more constrained, and the variance decreases (and the bias increases).\nHere’s an example from ISLR.\n\n\n\nMSE (pink), squared bias (black) and variance (green), estimated from performance on previously unseen data, as a function of \\(\\lambda\\) (ISLR fig. 6.5)\n\n\nNotice that the variance decreases as \\(\\lambda\\) increases, while squares bias increases, but there is a “sweet spot” that minimizes the MSE. The whole point of model selection (CV, AIC, ridge regression, etc.) is to find this sweet spot (or spot close to it).\n\n\n\n34.13.2 The LASSO\nNow, there’s one issue with ridge regression, which becomes evident when we compare it with subset selection methods. Except when \\(\\lambda\\) is truly huge (i.e., infinite), ridge regression fits a model that still has all of the coefficients in it (i.e., all of the coefficients are nonzero, though perhaps small). Said another way, we haven’t simplified the model in the sense of reducing the number of predictors or only keeping the “useful” predictors around.\nThis isn’t a problem for prediction. After all, more predictors often make prediction better, especially when we have regularization to prevent us from over-fitting.\nBut this is a problem if our goal is to simplify our model by selecting only some predictors to include in our model. One way to do this would be to make it so that coefficients that aren’t “pulling their weight” in the sense of helping our prediction error will be set to zero. This is precisely the goal of the LASSO.\nThe LASSO looks a lot like ridge regression, except that the penalty is slightly different: \\[\n\\sum_{i=1}^n \\left( Y_i - \\sum_{j=0}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right|\n= \\operatorname{RSS} + \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right|\n\\]\nThe penalty term now involves a sum of absolute values of the coefficients instead of a sum of squares.\nThe interesting thing is that this small change has a big effect on what our estimated coefficients look like. The LASSO penalty encourages coefficients to be set precisely equal to zero if they aren’t useful predictors (i.e., if they do not help to decrease the RSS).\nThere is an interesting geometric reason for this, though it is outside the scope of the course. See the end of Section 6.6.2 in ISLR.\nThe important point is that the LASSO performs variable selection for us by setting many coefficients to zero.\nThe glmnet package has a very good LASSO implementation. This is generally a very useful package for doing all kinds of different penalized regression, and you’ll likely use it extensively in your regression course(s). You’ll get a bit of practice with it in discussion section.\n\n\n34.13.3 How to choose \\(\\lambda\\)? CV to the rescue!\nA natural question in both ridge and the LASSO concerns how we should choose the term \\(\\lambda\\) that controls the “strength” of the penalty term.\nWe said a few lectures ago that CV was useful beyond just variable selection, and here’s the payoff.\nCV is also a great way to choose \\(\\lambda\\) in these kinds of penalized problems. We choose different values of \\(\\lambda\\), fit the corresponding models, and use CV to select among them!\nSection 6.2.3 has a more detailed discussion of this, using \\(10\\)-fold CV to compare different choices of \\(\\lambda\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "cv.html#review",
    "href": "cv.html#review",
    "title": "34  Model Selection and Cross Validation",
    "section": "34.14 Review",
    "text": "34.14 Review\nIn these notes we covered\n\nThe concept of over-fitting to training data\nTraining / Validation data splits\nSingle Split Validation\nLeave One Out Cross Validation\nK-Fold CV\nBias - Variance Tradeoff\nBias & Variance of CV Methods\nBias & Variance of Model Error by Model Complexity\nModel Selection - Best Subset, Forward Stepwise, Backwards Stepwise\nModel Comparison Statistics (Adjusted \\(R^2\\), AIC, BIC)\nRidge Regression - shrinkage of coefficients\nLASSO regression - shrinkage and variable selection",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>34</span>  <span class='chapter-title'>Model Selection and Cross Validation</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html",
    "href": "R11_cv-MSEcomparison.html",
    "title": "35  cv-extra",
    "section": "",
    "text": "35.1 Set parameters\nSimulate a data set with second order predictor with specified slope1, slope2, and sigma.\n# set parameters\nformulas = c(y~x, \n             y~x+I(x^2), \n             y~x+I(x^2)+I(x^3),\n             y~x+I(x^2)+I(x^3)+I(x^4))\nM  = 500\nN  = 40\nB1 = 4\nB2 = 3\nS  = 1",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#estimate-true-model-error",
    "href": "R11_cv-MSEcomparison.html#estimate-true-model-error",
    "title": "35  cv-extra",
    "section": "35.2 Estimate true model error",
    "text": "35.2 Estimate true model error\nThis is to say what is the actual root mean square error on out of sample data for models of order 1, 2, 3 and 4. We’re going to estimate these using monte carlo - 500 replicates, each time we generate sample data of size N, fit the model of the specified order, and then predict on 1000 out of sample data, averaging the rmse for each of the 500 estimates. These are the targets that the cross validation methods will attempt to estimate.\n\nestimate_error = function(n,b1,b2,s,M){\n  function(O){\n    df = sim_data(n,b1,b2,s)\n    outData = sim_data(M,b1,b2,s)\n    lm.fit = lm(formulas[[O]],data=df)\n    sqrt(mean((predict(lm.fit,newdata=outData)-outData$y)^2))\n  }\n}\n\nresults &lt;- sapply(1:M,function(i){sapply(1:4, estimate_error(N,B1,B2,S,1000))})\nresults &lt;- setNames(as.data.frame(t(results)),1:4)\nmodelErrors &lt;- unlist(lapply(results,\"mean\"))\nmodelErrors\n\n       1        2        3        4 \n1.052831 1.038969 1.058259 1.076323 \n\n\nModel error are all a little bit higher than 1. 1 is the standard deviation of the error term, but the model error is going to be higher than 1 because a polynomial regression based on a sample size of 40 will have some added uncertainty due to sample size.\nFrom this (with the magic ability to generate an unlimited amount of out-of-sample data) we can see that the order 2 model has the lowest model error - and of course it does because that is the population model. The next question is how well do the different cross validation methods estimate the model error of the different order models.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#train-on-half-test-on-half",
    "href": "R11_cv-MSEcomparison.html#train-on-half-test-on-half",
    "title": "35  cv-extra",
    "section": "35.3 Train on half, test on half",
    "text": "35.3 Train on half, test on half\nIn order to compare apples to apples, we will double the sample size \\(N\\). This way the training data will have size \\(N\\).\n\nrun_m1 = function(n,b1,b2,s){\n  function(O){\n    df = sim_data(n,b1,b2,s)\n    \n    train = sample(1:n)[1:floor(n/2)]\n    lm.fit = lm(formulas[[O]],data=df[train,])\n    sqrt(mean((predict(lm.fit,newdata=df[-train,])-df[-train,]$y)^2))\n  }\n}\n\nm1 = sapply(1:M,function(i){sapply(1:4, run_m1(N*2,B1,B2,S))})",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#leave-one-out-cv",
    "href": "R11_cv-MSEcomparison.html#leave-one-out-cv",
    "title": "35  cv-extra",
    "section": "35.4 Leave one out CV",
    "text": "35.4 Leave one out CV\nParallelized computation to get more iterations.\nThis time we’ll just add 1 to the sample size, this will ensure that the training set has size \\(N\\).\n\nlibrary(caret)\nlibrary(parallel)\n\nrun_m2 = function(n,b1,b2,s){\n  function(O){\n    df = sim_data(n,b1,b2,s)\n    \n    train = sample(1:n)[1:floor(n/2)]\n    lm.fit = train(formulas[[O]],data=df,method=\"lm\",\n                   trControl=trainControl(method=\"LOOCV\"))\n    \n    lm.fit$results$RMSE\n  }\n}\n\ncl = makeCluster(detectCores()-1)\ninvisible(clusterEvalQ(cl,\"library(caret)\"))\nclusterExport(cl,c(\"N\",\"B1\",\"B2\",\"S\",\"formulas\",\"sim_data\",\"run_m2\",\"train\",\"trainControl\"))\nclusterSetRNGStream(cl,1)\n\nm2 = parSapply(cl,1:M,function(i){sapply(1:4, run_m2(N+1,B1,B2,S))})",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#k-fold-cv",
    "href": "R11_cv-MSEcomparison.html#k-fold-cv",
    "title": "35  cv-extra",
    "section": "35.5 K-fold CV",
    "text": "35.5 K-fold CV\nWe’ll use \\(K=5\\) and thus we’ll scale up \\(N\\) by 5/4. This will ensure that training size will be approximately \\(N\\).\n\nrun_m3 = function(n,b1,b2,s){\n  function(O){\n    df = sim_data(n,b1,b2,s)\n    \n    train = sample(1:n)[1:floor(n/2)]\n    lm.fit = train(formulas[[O]],data=df,method=\"lm\",\n                   trControl=trainControl(method=\"CV\",number=5))\n    \n    lm.fit$results$RMSE\n  }\n}\n\nclusterExport(cl,c(\"run_m3\"))\nm3 = parSapply(cl,1:M,function(i){sapply(1:4, run_m3(N*5/4,B1,B2,S))})\n\nstopCluster(cl)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#putting-it-all-together",
    "href": "R11_cv-MSEcomparison.html#putting-it-all-together",
    "title": "35  cv-extra",
    "section": "35.6 Putting it all together",
    "text": "35.6 Putting it all together\nCombine all runs together\n\ndf.rmse = bind_rows(\n  setNames(as.data.frame(t(m1)),1:4) %&gt;% mutate(method=\"split half\"),\n  setNames(as.data.frame(t(m2)),1:4) %&gt;% mutate(method=\"LOOCV\"),\n  setNames(as.data.frame(t(m3)),1:4) %&gt;% mutate(method=\"5-fold CV\")) %&gt;% \n  pivot_longer(1:4,names_to=\"order\",values_to=\"rmse\") %&gt;% \n  arrange(desc(method),order)\n\nPlotting the resultant RMSE computations\n\ndf.rmse %&gt;% group_by(method,order) %&gt;% summarise(rmse=mean(rmse)) %&gt;% \n  ggplot(aes(x=order,y=rmse,color=method,group=method))+geom_line() + \n  ggtitle(\"RMSE for different orders/methods (order 2 is correct)\")\n\n\n\n\n\n\n\n\nTable showing the final estimations of RMSE\n\ndf.rmse %&gt;% group_by(method,order) %&gt;% summarise(mean_rmse = mean(rmse)) %&gt;% pivot_wider(names_from=order,values_from=mean_rmse)\n\n# A tibble: 3 × 5\n# Groups:   method [3]\n  method       `1`   `2`   `3`   `4`\n  &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 5-fold CV   1.02 1.000  1.02  1.04\n2 LOOCV       1.04 1.03   1.05  1.07\n3 split half  1.05 1.03   1.05  1.07\n\n\nVariance of Error Estimates\n\ndf.rmse %&gt;% group_by(method,order) %&gt;% summarise(var_rmse = var(rmse)) %&gt;% pivot_wider(names_from=order,values_from=var_rmse)\n\n# A tibble: 3 × 5\n# Groups:   method [3]\n  method        `1`    `2`    `3`    `4`\n  &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 5-fold CV  0.0115 0.0109 0.0142 0.0148\n2 LOOCV      0.0118 0.0142 0.0164 0.0154\n3 split half 0.0123 0.0136 0.0151 0.0172\n\n\nSquared Bias of Error Estimates\n\noptions(scipen=999)\ndf.rmse$me &lt;- modelErrors[df.rmse$order]\ndf.rmse %&gt;% group_by(method,order) %&gt;% summarise(bias2_rmse = mean(rmse-me)^2) %&gt;% pivot_wider(names_from=order,values_from=bias2_rmse)\n\n# A tibble: 3 × 5\n# Groups:   method [3]\n  method            `1`      `2`      `3`       `4`\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 5-fold CV  0.00109    0.00155  0.00147  0.00137  \n2 LOOCV      0.0000982  0.000130 0.000131 0.000104 \n3 split half 0.00000816 0.000110 0.000113 0.0000348",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_cv-MSEcomparison.html#a-few-key-observations",
    "href": "R11_cv-MSEcomparison.html#a-few-key-observations",
    "title": "35  cv-extra",
    "section": "35.7 A few key observations:",
    "text": "35.7 A few key observations:\n\nOrder 2 correctly found by both CV methods to be best order for fit (though k-fold tends to be more reliable on reruns)\nsplit half and LOOCV have higher variance in error estimates, 5-fold has lower variance.\n5-fold tends to under-estimate model error, split in half over-estimates model error. The squared bias of split half is lower than either of the other two.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>35</span>  <span class='chapter-title'>cv-extra</span>"
    ]
  },
  {
    "objectID": "R11_Bias_Variance.html",
    "href": "R11_Bias_Variance.html",
    "title": "36  Bias Variance Tradeoff",
    "section": "",
    "text": "36.1 Bias and Variance of an Linear Model\nWe will sample from a population, fit a linear model, then predict 1000 data points. We will actually repeat this over and over and in the end we will have 1000 model fits and for each one we will have 1000 predictions.\nThe true model is\n\\[Y_i = 10 + 2X_i + \\epsilon_i, \\text{ where }\\epsilon\\sim N(0,2^2)\\] A simple function will be used to generate an unlimited number of data so we can precisely estimate out of sample error for our model.\ngenerate_data &lt;- function(n){\n  X &lt;- rnorm(n, 5, 4)\n  eps &lt;- rnorm(n, 0, 2)\n  Y &lt;- 10 + 2*X + eps\n  return(data.frame(X,Y))\n}\nThe question is this: What is the expected model error of this linear model based on a sample size of 32, for this population? We’ll start with 1000 out of sample data points, which we can take as being representative of the population of out of sample data.\nFor 1000 Monte Carlo replicates we’ll\nThis will give us a great estimate of model error. We can then use this as a benchmark for K-fold cross validation in the next phase of this example.\nNote: This code can take a little bit to run\nNMC &lt;- 1000\nnOut &lt;- 1000\noutData &lt;- generate_data(nOut)\n\nresults &lt;- data.frame('rep' = rep(1:NMC, nOut),\n                      'i' = rep(1:nOut, each=NMC),\n                      'X' = rep(outData$X, each=NMC),\n                      'Y' = rep(outData$Y, each=NMC),\n                      'beta0' = rep(0,nOut*NMC),\n                      'beta1' = rep(0,nOut*NMC),\n                      'Yhat' = rep(0,nOut*NMC),\n                      'error'= rep(0,nOut*NMC))\n\nfor(k in 1:NMC){\n  sampleData &lt;- generate_data(32)\n  modelFit &lt;- lm(Y~X, data=sampleData)\n  yhat &lt;- predict(modelFit, newdata=outData)\n  results[results$rep==k, \"beta0\"] &lt;- coef(modelFit)[1]\n  results[results$rep==k, \"beta1\"] &lt;- coef(modelFit)[2]\n  results[results$rep==k,\"Yhat\"] &lt;- yhat\n}\nresults$error = results$Y-results$Yhat\n\npredVar &lt;- as.numeric(mean(aggregate(error~rep, results, FUN=\"var\")$error))\npredBias2 &lt;- mean(aggregate(error~rep, results, FUN=\"mean\")$error)^2\npredErr &lt;- mean(results$error^2)\nVariance of predictions is 3.883823.\nSquared Bias of predictions is 0.0104627.\nMean Square Error is 4.0225088\nVar + Bias^2 = 3.8942857.\nA great explanation of the Bias Variance Tradeoff\nNMC &lt;- 500\nks &lt;- c(2,4,8,16,32)\ncrossValidationResults &lt;- data.frame('k' = ks,\n                                     trainingError = 0,\n                                     testingError = 0,\n                                     testingVariance = 0,\n                                     testingBias2 = 0,\n                                     estimationError=0)\n\nfor(K in ks){\n  trnErr &lt;- 0\n  tstErr &lt;- 0\n\n  for(j in 1:NMC){\n    set.seed(j) #same data, repeated once for each k\n    sampleData &lt;- generate_data(32)\n\n    #idx is a shuffled vector of row numbers\n    idx &lt;- sample(1:nrow(sampleData))\n    #folds partitions the row indices\n    folds &lt;- split(idx, as.factor(1:K))\n  \n    trainingErrors &lt;- 0\n    testingErrors &lt;- 0\n    \n    for(k in 1:K){ \n      training &lt;- sampleData[-folds[[k]],]\n      testing &lt;- sampleData[folds[[k]],]\n      #estimate MSE\n      #fit the model to the training data\n      modelFit &lt;- lm(Y~X, data=training)\n      #calculate the average squared error on the testing data\n      trainingErrors[k] &lt;- mean(resid(modelFit)^2)\n      testingErrors[k] &lt;- mean((predict(modelFit, newdata=testing) - testing$Y)^2)\n    }\n    \n    trnErr[j] &lt;- mean(trainingErrors)\n    tstErr[j] &lt;- mean(testingErrors)\n  }\n  crossValidationResults[crossValidationResults$k==K, \"trainingError\"] &lt;- mean(trnErr)\n  crossValidationResults[crossValidationResults$k==K, \"testingError\"] &lt;- mean(tstErr)\n  \n  crossValidationResults[crossValidationResults$k==K, \"testingVariance\"] &lt;- var(tstErr)\n  crossValidationResults[crossValidationResults$k==K, \"testingBias2\"] &lt;- mean((tstErr-4))^2\n  crossValidationResults[crossValidationResults$k==K, \"estimationError\"] &lt;- mean((tstErr-4)^2)\n}\ncrossValidationResults\n\n   k trainingError testingError testingVariance testingBias2 estimationError\n1  2      3.477456     4.498287        1.992548   0.24829035        2.236853\n2  4      3.635586     4.312318        1.441599   0.09754247        1.536259\n3  8      3.680593     4.274024        1.316525   0.07508933        1.388982\n4 16      3.699758     4.249614        1.250197   0.06230698        1.310004\n5 32      3.708385     4.234627        1.229465   0.05505002        1.282056\nplot(testingVariance~k, data=crossValidationResults, type=\"l\", main=\"Bias Variance Decomposition - Estimation of MSE\", ylim=c(0, max(crossValidationResults$estimationError)), ylab=\"\", xlab=\"k (folds)\")\nlines(testingBias2~k, data=crossValidationResults, col=\"blue\")\nlines(estimationError~k, data=crossValidationResults, col=\"red\")\nlegend(0,1,legend=c(\"Variance\",\"Squared Bias\",\"Error\"), col=c(\"black\",\"blue\",\"red\"), lty=1)\n\n\n\n\n\n\n\nplot(trainingError~k, data=crossValidationResults, type=\"l\", ylim=c(3,4.6))\nlines(testingError~k, data=crossValidationResults, col=\"blue\")\nabline(h=2^2, col=\"red\")",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "R11_Bias_Variance.html#bias-and-variance-of-an-linear-model",
    "href": "R11_Bias_Variance.html#bias-and-variance-of-an-linear-model",
    "title": "36  Bias Variance Tradeoff",
    "section": "",
    "text": "Generate a random sample of size 32\nFit a linear model to the data\nMake predictions for all 1000 out of sample data\nCalculate the average squared error on those predictions",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "R11_Bias_Variance.html#another-example---a-multiple-regression-model",
    "href": "R11_Bias_Variance.html#another-example---a-multiple-regression-model",
    "title": "36  Bias Variance Tradeoff",
    "section": "36.2 Another Example - A Multiple Regression Model",
    "text": "36.2 Another Example - A Multiple Regression Model\nWe will sample from a population, fit a linear model, then predict 1000 data points. We will actually repeat this over and over and in the end we will have 1000 model fits and for each one we will have 1000 predictions.\nThe true model is\n\\[Y_i = 10 + 2X_{1,i} + 5X_{2,i} -3X_{3,i} + \\epsilon_i, \\text{ where }\\epsilon\\sim N(0,2^2)\\] This model fitting will be based on a larger sample, a sample of size 128.\n\ngenerate_data &lt;- function(n){\n  X1 &lt;- rnorm(n, 5, 4)\n  X2 &lt;- rnorm(n, 1, 4)\n  X3 &lt;- rnorm(n, 3, 4)\n  eps &lt;- rnorm(n, 0, 2)\n  Y &lt;- 10 + 2*X1 +5*X2 -3*X3+ eps\n  return(data.frame(X1,X2,X3,Y))\n}\n\nNMC &lt;- 1000\nnOut &lt;- 1000\noutData &lt;- generate_data(nOut)\nn &lt;- 128\n\nresults &lt;- data.frame('rep' = rep(1:NMC, nOut),\n                      'i' = rep(1:nOut, each=NMC),\n                      'Y' = rep(outData$Y, each=NMC),\n                      'Yhat' = rep(0,nOut*NMC),\n                      'error'= rep(0,nOut*NMC))\n\nfor(k in 1:NMC){\n  sampleData &lt;- generate_data(128)\n  modelFit &lt;- lm(Y~., data=sampleData)\n  yhat &lt;- predict(modelFit, newdata=outData)\n  results[results$rep==k,\"Yhat\"] &lt;- yhat\n}\nresults$error = results$Y-results$Yhat\n\n\npredVar &lt;- mean(aggregate(Yhat~rep, results, FUN=\"var\")$Yhat)\npredBias2 &lt;- mean(aggregate(error~i, results, FUN=\"mean\")$error)^2\npredErr &lt;- mean(results$error^2)\n\nprint(paste(\"Variance =\",predVar))\n\n[1] \"Variance = 580.633211183573\"\n\nprint(paste(\"Squared Bias = \", predBias2))\n\n[1] \"Squared Bias =  0.00160131358719143\"\n\nprint(paste(\"Mean Square Error = \",predErr))\n\n[1] \"Mean Square Error =  4.36704941289589\"\n\nprint(paste(\"Var + Bias^2 = \",predVar+predBias2))\n\n[1] \"Var + Bias^2 =  580.634812497161\"\n\nprint(paste(\"estimate of sigma = \",sqrt(predErr)))\n\n[1] \"estimate of sigma =  2.08974864825794\"\n\n\nNow we will estimate the testing errror and training error for various numbers of folds\n\nNMC &lt;- 250\nks &lt;- c(2,4,8,16,32,64,128)\ncrossValidationResults &lt;- data.frame('k' = ks,\n                                     trainingError = 0,\n                                     testingError = 0,\n                                     testingVariance = 0,\n                                     testingBias2 = 0,\n                                     estimationError=0)\n\nfor(K in ks){\n  trnErr &lt;- 0\n  tstErr &lt;- 0\n\n  for(j in 1:NMC){\n    set.seed(j) #same data, repeated once for each k\n    sampleData &lt;- generate_data(n)\n\n    #idx is a shuffled vector of row numbers\n    idx &lt;- sample(1:nrow(sampleData))\n    #folds partitions the row indices\n    folds &lt;- split(idx, as.factor(1:K))\n  \n    trainingErrors &lt;- 0\n    testingErrors &lt;- 0\n    \n    for(k in 1:K){ \n      training &lt;- sampleData[-folds[[k]],]\n      testing &lt;- sampleData[folds[[k]],]\n      #estimate MSE\n      #fit the model to the training data\n      modelFit &lt;- lm(Y~., data=training)\n      #calculate the average squared error on the testing data\n      trainingErrors[k] &lt;- mean(resid(modelFit)^2)\n      testingErrors[k] &lt;- mean((predict(modelFit, newdata=testing) - testing$Y)^2)\n    }\n    \n    trnErr[j] &lt;- mean(trainingErrors)\n    tstErr[j] &lt;- mean(testingErrors)\n  }\n  crossValidationResults[crossValidationResults$k==K, \"trainingError\"] &lt;- mean(trnErr)\n  crossValidationResults[crossValidationResults$k==K, \"testingError\"] &lt;- mean(tstErr)\n  crossValidationResults[crossValidationResults$k==K, \"testingVariance\"] &lt;- var(tstErr)\n  crossValidationResults[crossValidationResults$k==K, \"testingBias2\"] &lt;- mean((tstErr-4))^2\n  crossValidationResults[crossValidationResults$k==K, \"estimationError\"] &lt;- mean((tstErr-4)^2)\n}\ncrossValidationResults\n\n    k trainingError testingError testingVariance testingBias2 estimationError\n1   2      3.750027     4.286584       0.4030953   0.08213047       0.4836134\n2   4      3.836878     4.183352       0.3323707   0.03361782       0.3646590\n3   8      3.861163     4.154025       0.3210524   0.02372356       0.3434918\n4  16      3.870692     4.144120       0.3167117   0.02077066       0.3362155\n5  32      3.875014     4.139478       0.3156883   0.01945414       0.3338797\n6  64      3.877074     4.137198       0.3139791   0.01882324       0.3315464\n7 128      3.878085     4.134802       0.3129656   0.01817168       0.3298854\n\n\n\nplot(testingVariance~k, data=crossValidationResults, type=\"l\", main=\"Bias Variance Decomposition - Estimation of MSE\", ylim=c(0, max(crossValidationResults$estimationError)), ylab=\"\", xlab=\"k (folds)\")\nlines(testingBias2~k, data=crossValidationResults, col=\"blue\")\nlines(estimationError~k, data=crossValidationResults, col=\"red\")\nlegend(0,1,legend=c(\"Variance\",\"Squared Bias\",\"Error\"), col=c(\"black\",\"blue\",\"red\"), lty=1)\n\n\n\n\n\n\n\nplot(trainingError~k, data=crossValidationResults, type=\"l\", ylim=range(crossValidationResults[,2:3]), main=\"Error vs K\", ylab=\"mean sq err\",xlab=\"k (folds)\")\nlines(testingError~k, data=crossValidationResults, col=\"blue\")\nabline(h=predErr, col=\"red\")\ntext(2, crossValidationResults$testingError[1],\"Validation\",adj=c(0,1))\ntext(2, crossValidationResults$trainingError[1],\"Training\",adj=c(0,0))\ntext(2, 4,\"Model Error\",adj=c(0,0))",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>36</span>  <span class='chapter-title'>Bias Variance Tradeoff</span>"
    ]
  },
  {
    "objectID": "R11_Model_Selection.html",
    "href": "R11_Model_Selection.html",
    "title": "37  R11 Model Selection",
    "section": "",
    "text": "37.1 Polynomial Models predicting mpg\nmodels &lt;- list()\nmodels[[1]] &lt;- lm(mpg ~ hp, data=mtcars)\nmodels[[2]] &lt;- lm(mpg ~ hp + I(hp^2), data=mtcars)\nmodels[[3]] &lt;- lm(mpg ~ hp + I(hp^2)+ I(hp^3), data=mtcars)\nmodels[[4]] &lt;- lm(mpg ~ hp + I(hp^2)+ I(hp^3)+ I(hp^4), data=mtcars)\nmodels[[5]] &lt;- lm(mpg ~ hp + I(hp^2)+ I(hp^3)+ I(hp^4)+ I(hp^5), data=mtcars)\n\nxrng &lt;- c(10, 400)\nxpred &lt;- seq(xrng[1], xrng[2], length.out=50)\nplot(mpg~hp, data=mtcars, xlim=xrng, ylim=c(0, 60))\nfor(i in 1:5){\n  lines(x=xpred, y=predict(models[[i]], newdata=data.frame(hp=xpred)), col=i, lwd=2)\n}\nlegend(300,60, legend=1:5, col=1:5, lty=1, title=\"Order\", lwd=2)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>R11 Model Selection</span>"
    ]
  },
  {
    "objectID": "R11_Model_Selection.html#model-selection-in-mtcars",
    "href": "R11_Model_Selection.html#model-selection-in-mtcars",
    "title": "37  R11 Model Selection",
    "section": "37.2 Model Selection in mtcars",
    "text": "37.2 Model Selection in mtcars\n\n37.2.1 Look at AIC as we add predictors\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\n# I'll consider models to predict mpg based on all other predictors. I'll look at AIC for each model\nmodel &lt;- lm(mpg ~ 1 + cyl, data=mtcars)\nL &lt;- logLik(model, data=mtcars)[1]\n-2*L + 2*(length(coef(model)))\n\n[1] 167.3064\n\nmodel &lt;- lm(mpg ~ 1 + cyl+disp, data=mtcars)\nL &lt;- logLik(model, data=mtcars)[1]\n-2*L + 2*(length(coef(model)))\n\n[1] 165.1456\n\nmodel &lt;- lm(mpg ~ 1 + cyl+disp+wt, data=mtcars)\nL &lt;- logLik(model, data=mtcars)[1]\n-2*L + 2*(length(coef(model)))\n\n[1] 155.5584\n\nmodel &lt;- lm(mpg ~ 1 + cyl+disp+wt+qsec, data=mtcars)\nL &lt;- logLik(model, data=mtcars)[1]\n-2*L + 2*(length(coef(model)))\n\n[1] 155.3037\n\n# Continue until no decrease in AIC is observed\n\nIn this code we will compare different multiple regression models to predict miles per gallon from the various predictor variables in the mtcars dataset.\nWe implement a forward stepwise selection algorithm, and a backwards stepwise selection algorithm. We also have a function that automates K fold validation and gives the formula resulting in the lowest MSE\n\n\n37.2.2 K fold validation on the models produced\n\nkfoldCV &lt;- function(K, formulas, dataset, responseVar, reps=1){\n  m &lt;- length(formulas)\n\n  #an empty data frame to store the results of each validation\n  results &lt;- data.frame(fold = rep(rep(1:K, each=m),times=reps),\n                        model = rep(1:m, K*reps),\n                        error = 0,\n                        repl = rep(1:reps, each=m*K))    \n  for(r in 1:reps){\n    #idx is a shuffled vector of row numbers\n    idx &lt;- sample(1:nrow(dataset))\n    #folds partitions the row indices\n    folds &lt;- split(idx, as.factor(1:K))\n    for(k in 1:K){\n      #split the data into training and testing sets\n      training &lt;- dataset[-folds[[k]],]\n      testing &lt;- dataset[folds[[k]],]\n      #go through each model and estimate MSE\n      for(f in 1:m){\n        #fit the model to the training data\n        fit &lt;- lm(formula = formulas[[f]], data=training)\n        #calculate the average squared error on the testing data\n        results[results$fold == k & results$model == f & results$repl==r, \"error\"] &lt;- mean((predict(fit, newdata=testing) - testing[,responseVar])^2)\n      }\n    }\n  }\n  #aggregate over each model & replicate, averaging the error\n  aggregated &lt;- aggregate(error~model, data=results, FUN=\"mean\")\n  #produces a simple line & dot plot\n  plot(sqrt(error) ~ model, type=\"b\", data=aggregated, ylab=\"RMSE\")\n#  lines(error ~ model, data=aggregated)\n  print(which(aggregated$error == min(aggregated$error)))\n  print(formulas[[which(aggregated$error == min(aggregated$error))]])\n  return(aggregated)\n}\n\n\n\n37.2.3 Model Statistics\nA function to take in one or more models and return a data frame giving some comparitive model statistics.\n\ncreateModelStats &lt;- function(formulas, dataset){\n  n &lt;- nrow(dataset)\n  model.stats &lt;- data.frame('model' = gsub(\" \", \"\", as.character(formulas), fixed = TRUE))\n  for(m in 1:length(formulas)){\n    modelsum &lt;- summary(lm(formulas[[m]], data=dataset))\n    #I am using k = p+1, the number of predictors + the intercept\n    model.stats$k[m] &lt;- nrow(modelsum$coefficients)\n    model.stats$Rsq[m] &lt;- modelsum$r.squared\n    model.stats$Rsq.adj[m] &lt;- modelsum$adj.r.squared\n    L &lt;- logLik(lm(formulas[[m]], data=mtcars))[1]\n    model.stats$AIC[m] &lt;- -2*L + 2*model.stats$k[m]\n    model.stats$BIC[m] &lt;- -2*L + model.stats$k[m] * log(n)\n  }\n  return(model.stats)  \n}\n\n\n\n37.2.4 Best Subset Selection\n\nget_model_formula &lt;- function(id, object, outcome){\n  # get models data\n  models &lt;- summary(object)$which[id,-1]\n  # Get model predictors\n  predictors &lt;- names(which(models == TRUE))\n  predictors &lt;- paste(predictors, collapse = \"+\")\n  # Build model formula\n  as.formula(paste0(outcome, \"~\", predictors))\n}  \n\nbestSubsetSelection &lt;- function(dataset, responseVar, maxModelSize){\n  library(leaps)\n  models &lt;- regsubsets(reformulate(\".\",responseVar), data = dataset, nvmax = maxModelSize);\n  modelList &lt;- list(\"formula\")\n  nModels &lt;- models$last-models$first+1\n  for(i in 1:nModels){\n    modelList[[i]] &lt;- get_model_formula(i, models, responseVar)\n  }\n  return(modelList)  \n}\n\nmodelList &lt;- bestSubsetSelection(mtcars, \"mpg\", 10)\nkfoldCV(32, modelList, mtcars, \"mpg\",1)\n\n\n\n\n\n\n\n\n[1] 4\nmpg ~ hp + wt + qsec + am\n&lt;environment: 0x000001646666e878&gt;\n\n\n   model     error\n1      1 10.250712\n2      2  7.376451\n3      3  7.228234\n4      4  6.963568\n5      5  7.092947\n6      6  7.548634\n7      7  8.376991\n8      8  9.444218\n9      9 10.433151\n10    10 12.181558\n\ncreateModelStats(modelList, mtcars)\n\n                                          model  k       Rsq   Rsq.adj      AIC\n1                                        mpg~wt  2 0.7528328 0.7445939 164.0294\n2                                    mpg~cyl+wt  3 0.8302274 0.8185189 154.0101\n3                                mpg~wt+qsec+am  4 0.8496636 0.8335561 152.1194\n4                             mpg~hp+wt+qsec+am  5 0.8578510 0.8367919 152.3274\n5                        mpg~disp+hp+wt+qsec+am  6 0.8637377 0.8375334 152.9740\n6                   mpg~disp+hp+drat+wt+qsec+am  7 0.8667078 0.8347177 154.2687\n7              mpg~disp+hp+drat+wt+qsec+am+gear  8 0.8680976 0.8296261 155.9333\n8         mpg~disp+hp+drat+wt+qsec+am+gear+carb  9 0.8687064 0.8230390 157.7853\n9      mpg~disp+hp+drat+wt+qsec+vs+am+gear+carb 10 0.8689448 0.8153314 159.7271\n10 mpg~cyl+disp+hp+drat+wt+qsec+vs+am+gear+carb 11 0.8690158 0.8066423 161.7098\n        BIC\n1  166.9609\n2  158.4073\n3  157.9823\n4  159.6560\n5  161.7684\n6  164.5289\n7  167.6592\n8  170.9769\n9  174.3845\n10 177.8329\n\n\n\n\n37.2.5 Forward Stepwise Selection\n\nforwardStepwiseSelection &lt;- function(dataset, responseVar){\n  predictors = c(\"1\",names(dataset[,-which(names(dataset)==responseVar)]))\n  used = 1\n  M &lt;- list() #empty list to hold all of the models, except M0\n  \n  #the null model and its RSS\n  M0 &lt;- lm(reformulate(predictors[used], responseVar), data=dataset)\n  RSS &lt;- sum(M0$residuals^2)\n  \n  #the list of formulas to return\n  formulas &lt;- list()\n  \n  for(model in 1:(length(predictors)-1)){\n    RSS.best &lt;- RSS\n    for(try in predictors[-used]){\n      fitModel &lt;- lm(reformulate(c(predictors[used],try), responseVar), data=dataset)\n      RSS.new &lt;- sum(fitModel$residuals^2)\n      if(RSS.new &lt;= RSS.best){\n        new.pred &lt;- try\n        RSS.best &lt;- RSS.new\n      }\n    }\n    formulas[[model]] &lt;- reformulate(c(predictors[used],new.pred), responseVar)\n    M[[model]] &lt;- lm(formulas[[model]], data=dataset) \n    RSS &lt;- sum(M[[model]]$residuals^2)\n    print(paste(\"adding\", new.pred, \"; RSS = \", RSS))\n    used &lt;- c(used, which(predictors==new.pred))\n  }\n  return(formulas)  \n}\n\nformulas &lt;- forwardStepwiseSelection(mtcars, \"mpg\")\n\n[1] \"adding wt ; RSS =  278.321937543344\"\n[1] \"adding cyl ; RSS =  191.171966255962\"\n[1] \"adding hp ; RSS =  176.620520198822\"\n[1] \"adding am ; RSS =  169.997769193248\"\n[1] \"adding qsec ; RSS =  159.817481197347\"\n[1] \"adding disp ; RSS =  150.991113368961\"\n[1] \"adding drat ; RSS =  149.089856415591\"\n[1] \"adding gear ; RSS =  148.113856122815\"\n[1] \"adding carb ; RSS =  147.654555723284\"\n[1] \"adding vs ; RSS =  147.494430016651\"\n\nkfoldCV(32, formulas, mtcars, \"mpg\")\n\n\n\n\n\n\n\n\n[1] 3\nmpg ~ 1 + wt + cyl + hp\n&lt;environment: 0x00000164655be8a0&gt;\n\n\n   model     error\n1      1 10.250712\n2      2  7.376451\n3      3  7.189898\n4      4  7.399795\n5      5  7.559137\n6      6  7.561309\n7      7  8.150201\n8      8  9.186201\n9      9 10.978155\n10    10 12.181558\n\ncreateModelStats(formulas, mtcars)\n\n                                            model  k       Rsq   Rsq.adj\n1                                        mpg~1+wt  2 0.7528328 0.7445939\n2                                    mpg~1+wt+cyl  3 0.8302274 0.8185189\n3                                 mpg~1+wt+cyl+hp  4 0.8431500 0.8263446\n4                              mpg~1+wt+cyl+hp+am  5 0.8490314 0.8266657\n5                         mpg~1+wt+cyl+hp+am+qsec  6 0.8580721 0.8307783\n6                    mpg~1+wt+cyl+hp+am+qsec+disp  7 0.8659105 0.8337290\n7               mpg~1+wt+cyl+hp+am+qsec+disp+drat  8 0.8675989 0.8289819\n8          mpg~1+wt+cyl+hp+am+qsec+disp+drat+gear  9 0.8684657 0.8227146\n9     mpg~1+wt+cyl+hp+am+qsec+disp+drat+gear+carb 10 0.8688736 0.8152309\n10 mpg~1+wt+cyl+hp+am+qsec+disp+drat+gear+carb+vs 11 0.8690158 0.8066423\n        AIC      BIC\n1  164.0294 166.9609\n2  154.0101 158.4073\n3  153.4766 159.3396\n4  154.2536 161.5823\n5  154.2776 163.0720\n6  154.4596 164.7197\n7  156.0541 167.7800\n8  157.8439 171.0355\n9  159.7445 174.4019\n10 161.7098 177.8329\n\n\n\n\n37.2.6 Backwards Stepwise\n\nbackwardsStepwiseSelection &lt;- function(dataset, responseVar){\n  predictors = c(\"1\",names(dataset[,-which(names(dataset)==responseVar)]))\n  used = (1:ncol(dataset))[-which(names(dataset)==responseVar)]\n  M &lt;- list()\n  Mfull &lt;- lm(reformulate(predictors[c(1,used)], responseVar), data=dataset)\n  RSS &lt;- sum(Mfull$residuals^2)\n  formulas &lt;- list()\n  formulas[[length(used)]] &lt;- reformulate(predictors[used], responseVar)\n  \n  RSS.best &lt;- RSS\n  RSS.worst &lt;- sum(lm(reformulate(\"1\",response=responseVar), data=dataset)$residuals^2)\n  print(paste(\"Full Model RSS: \", RSS))\n  for(model in (length(used)-1):1){\n    RSS.best &lt;- RSS.worst\n    for(try in used){\n      modelFit &lt;- lm(reformulate(predictors[used[-which(used==try)]], responseVar), data=dataset)\n      RSS.new &lt;- sum(modelFit$residuals^2)\n      if(RSS.new &lt;= RSS.best){\n        new.pred &lt;- try\n        RSS.best &lt;- RSS.new\n      }\n    }\n    formulas[[model]] &lt;- reformulate(predictors[used[-which(used==try)]], responseVar)\n    M[[model]] &lt;- lm(formulas[[model]], data=dataset) \n    RSS &lt;- sum(M[[model]]$residuals^2)\n    print(paste(\"removing\", predictors[new.pred], \"; RSS = \", RSS))\n    used &lt;- used[-which(used==new.pred)]\n  }\n  return(formulas)  \n}\n\nformulas &lt;- backwardsStepwiseSelection(mtcars, \"mpg\")\n\n[1] \"Full Model RSS:  147.494430016651\"\n[1] \"removing cyl ; RSS =  147.901098768614\"\n[1] \"removing vs ; RSS =  148.094443018952\"\n[1] \"removing carb ; RSS =  148.528284803963\"\n[1] \"removing gear ; RSS =  150.093255330781\"\n[1] \"removing drat ; RSS =  170.129133515847\"\n[1] \"removing disp ; RSS =  185.635324942769\"\n[1] \"removing hp ; RSS =  186.05929721548\"\n[1] \"removing am ; RSS =  195.463631604656\"\n[1] \"removing qsec ; RSS =  278.321937543344\"\n\nkfoldCV(32, formulas, mtcars, \"mpg\")\n\n\n\n\n\n\n\n\n[1] 6\nmpg ~ disp + hp + drat + wt + qsec + am\n&lt;environment: 0x00000164664ab3f0&gt;\n\n\n   model     error\n1      1 10.250712\n2      2  7.792151\n3      3  7.671356\n4      4  8.057231\n5      5  8.212356\n6      6  7.548634\n7      7  8.376991\n8      8  9.433235\n9      9 10.257359\n10    10 12.181558\n\ncreateModelStats(formulas, mtcars)\n\n                                          model  k       Rsq   Rsq.adj      AIC\n1                                        mpg~wt  2 0.7528328 0.7445939 164.0294\n2                                   mpg~wt+qsec  3 0.8264161 0.8144448 154.7205\n3                                mpg~hp+wt+qsec  4 0.8347678 0.8170643 155.1426\n4                           mpg~disp+hp+wt+qsec  5 0.8351443 0.8107212 157.0696\n5                      mpg~disp+hp+drat+wt+qsec  6 0.8489147 0.8198599 156.2784\n6                   mpg~disp+hp+drat+wt+qsec+am  7 0.8667078 0.8347177 154.2687\n7              mpg~disp+hp+drat+wt+qsec+am+gear  8 0.8680976 0.8296261 155.9333\n8           mpg~disp+hp+drat+wt+qsec+vs+am+gear  9 0.8684829 0.8227378 157.8397\n9       mpg~cyl+disp+hp+drat+wt+qsec+vs+am+gear 10 0.8686546 0.8149224 159.7979\n10 mpg~cyl+disp+hp+drat+wt+qsec+vs+am+gear+carb 11 0.8690158 0.8066423 161.7098\n        BIC\n1  166.9609\n2  159.1177\n3  161.0056\n4  164.3983\n5  165.0728\n6  164.5289\n7  167.6592\n8  171.0313\n9  174.4553\n10 177.8329\n\n\n\n\n37.2.7 Visualization of Models Selection Methods\n\nresponseVar &lt;- \"mpg\"\npredictors &lt;- names(mtcars)[-which(names(mtcars)==responseVar)]\nnFormulas &lt;- 2^length(predictors)\n\nnumber2binary = function(number, noBits) {\n       binary_vector = rev(as.numeric(intToBits(number)))\n       if(missing(noBits)) {\n          return(binary_vector)\n       } else {\n          binary_vector[-(1:(length(binary_vector) - noBits))]\n       }\n}\n\nallModels &lt;- data.frame(i=0:(nFormulas-1))\nfor(predictor in predictors){\n  newdf &lt;- data.frame(x=numeric(nFormulas)); names(newdf) &lt;- predictor\n  allModels &lt;- cbind(allModels, newdf)\n}\nallModels$size &lt;- 0\nallModels$aic &lt;- 0\nallModels$bic &lt;- 0\nallModels$adjr2 &lt;- 0\nfor(i in allModels$i){\n  allModels[i+1, 2:11] &lt;- number2binary(i, length(predictors))\n  if(i==0){\n    f &lt;- reformulate(\"1\", responseVar)\n  } else {\n    f &lt;- reformulate(predictors[allModels[i+1,2:11]==1], responseVar)\n  }\n  allModels[i+1, \"size\"] &lt;- sum(allModels[i+1, 2:11])+1\n  fit &lt;- lm(f, data=mtcars)\n  allModels[i+1, \"adjr2\"] &lt;- summary(fit)$adj.r.squared\n  allModels[i+1, \"aic\"] &lt;- AIC(fit)\n  allModels[i+1, \"bic\"] &lt;- -2*as.numeric(logLik(fit)) + log(nrow(mtcars))*(allModels[i+1, \"size\"]+1) \n}\n\nedges &lt;- data.frame(from=numeric(), to=numeric())\nfor(i in allModels[-nrow(allModels),\"i\"]){\n  to &lt;- i + 2^(which(rev(allModels[i,2:11]==0))-1)\n  from &lt;- rep(i, length(to))\n  edges &lt;- rbind(edges, data.frame(from,to))\n}\n\n\ncriterion &lt;- \"bic\" #or \"bic\" or \"adjr2\"\n\nwhich.best &lt;- function(x){\n  if(criterion==\"adjr2\") return (which.max(x)) else return (which.min(x))\n}\n\nbestSubset &lt;- numeric()\nfor(size in unique(allModels$size)){\n    bestSubset[size] &lt;- allModels[allModels$size==size, \"i\"][which.best(allModels[allModels$size==size, criterion])]+1\n}\n\nbestForward &lt;- numeric()\nfor(size in unique(allModels$size)){\n  if(size==1) bestForward[size] &lt;- 1\n  else {\n    bestForward[size] &lt;- allModels[edges[edges$from==bestForward[size-1],\"to\"] , \"i\"][which.best(allModels[edges[edges$from==bestForward[size-1],\"to\"], criterion])]+1\n  }\n}\n\nbestBackward &lt;- numeric(max(allModels$size))\nfor(size in rev(unique(allModels$size))){\n  if(size==11) {bestBackward[size] &lt;- 1024}\n  else {\n      bestBackward[size] &lt;- allModels[edges[edges$to==bestBackward[size+1],\"from\"] , \"i\"][which.best(allModels[edges[edges$to==bestBackward[size+1],\"from\"], criterion])]+1\n  }\n}\n\nallModels$compare &lt;- allModels[,criterion]\n\nplot(compare~size, data=allModels, main=paste(\"Best Subset by\",criterion), ylab=criterion)\npoints(compare~size, data=allModels[bestSubset,], col=\"magenta\", pch=16)\npoints(compare~size, data=allModels[bestSubset[which.best(allModels[bestSubset,criterion])],], col=\"magenta\", cex=2, lwd=2)\n\n\n\n\n\n\n\nplot(compare~size, data=allModels, main=paste(\"Forward Stepwise by\",criterion), ylab=criterion)\nsegments(allModels[edges$from,\"size\"],allModels[edges$from,criterion],allModels[edges$to,\"size\"],allModels[edges$to,criterion], col=\"gray\")\npoints(compare~size, data=allModels)\nedgesForward &lt;- subset(edges, from %in% bestForward)\nsegments(allModels[edgesForward$from,\"size\"],allModels[edgesForward$from,criterion],allModels[edgesForward$to,\"size\"],allModels[edgesForward$to,criterion], col=\"red\")\npoints(compare~size, data=allModels[bestForward,], col=\"red\", pch=16)\npoints(compare~size, data=allModels[bestForward[which.best(allModels[bestForward,criterion])],], col=\"red\", cex=2, lwd=2)\n\n\n\n\n\n\n\nplot(compare~size, data=allModels, main=paste(\"Backward Stepwise by\",criterion), ylab=criterion)\nsegments(allModels[edges$from,\"size\"],allModels[edges$from,criterion],allModels[edges$to,\"size\"],allModels[edges$to,criterion], col=\"gray\")\npoints(compare~size, data=allModels)\nedgesBackward &lt;- subset(edges, to %in% bestBackward)\nsegments(allModels[edgesBackward$from,\"size\"],allModels[edgesBackward$from,criterion],allModels[edgesBackward$to,\"size\"],allModels[edgesBackward$to,criterion], col=\"blue\")\npoints(compare~size, data=allModels[bestBackward,], col=\"blue\", pch=16)\npoints(compare~size, data=allModels[bestBackward[which.best(allModels[bestBackward,criterion])],], col=\"blue\", cex=2, lwd=2)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>37</span>  <span class='chapter-title'>R11 Model Selection</span>"
    ]
  },
  {
    "objectID": "R11_Ridge_LASSO.html",
    "href": "R11_Ridge_LASSO.html",
    "title": "38  R11: Ridge and Lasso",
    "section": "",
    "text": "38.1 K Fold Validation for Ridge Regression\nkfoldCV.ridge &lt;- function(K, lambdas, dataset, responseVar){\n  m &lt;- length(lambdas)\n  \n  #idx is a shuffled vector of row numbers\n  idx &lt;- sample(1:nrow(dataset))\n  #folds partitions the row indices\n  folds &lt;- split(idx, as.factor(1:K))\n\n  #an empty data frame to store the results of each validation\n  results &lt;- data.frame(fold = rep(1:K, rep(m,K)),\n                        model = rep(1:m, K),\n                        error = 0)    \n  for(k in 1:K){\n    #split the data into training and testing sets\n    training &lt;- dataset[-folds[[k]],]\n    testing &lt;- dataset[folds[[k]],]\n    #go through each model and estimate MSE\n    ridge_models &lt;- lm.ridge(reformulate(\".\",responseVar), training, lambda=lambdas);\n\n    for(f in 1:m){\n      coeff &lt;- coef(ridge_models)[f,]\n      \n      Y &lt;- testing[,c(responseVar)] \n      X &lt;- cbind( 1, testing[,names(dataset) != responseVar])\n\n      Y.hat &lt;- as.numeric(coeff) %*% as.matrix(t(X))\n            \n      #calculate the average squared error on the testing data\n      results[results$fold == k & results$model == f, \"error\"] &lt;- mean((Y-Y.hat)^2)\n    }\n  }\n  #aggregate over each model, averaging the error\n  aggregated &lt;- aggregate(error~model, data=results, FUN=\"mean\")\n  #produces a simple line & dot plot\n  plot(error ~ sqrt(lambdas), type=\"b\", data=aggregated, ylab=\"MSE\")\n#  lines(error ~ model, data=aggregated)\n  print(which(aggregated$error == min(aggregated$error)))\n  print(lambdas[[which(aggregated$error == min(aggregated$error))]])\n  return(aggregated)\n}",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>R11: Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "R11_Ridge_LASSO.html#ridge-regression",
    "href": "R11_Ridge_LASSO.html#ridge-regression",
    "title": "38  R11: Ridge and Lasso",
    "section": "38.2 Ridge Regression",
    "text": "38.2 Ridge Regression\n\nlibrary(MASS);\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nlambda_vals &lt;- seq(0,10,1)^2; # Choose lambdas to try.\n# lm.ridge needs:\n# 1) a model (mpg~. says to model mpg as an intercept\n#         plus a coefficient for every other variable in the data frame)\n# 2) a data set (mtcars, of course)\n# 3) a value for lambda. lambda=0 is the default,\n#         and recovers classic linear regression.\n#         But we can also pass a whole vector of lambdas, like we are about to do,\n#         and lm.ridge will fit a separate model for each.\n# See ?lm.ridge for details.\nridge_models &lt;- lm.ridge(mpg~., mtcars, lambda=lambda_vals);\n\n# Naively plotting this object shows us how the different coefficients\n# change as lambda changes.\nplot( ridge_models );\n\n\n\n\n\n\n\nkfoldCV.ridge(32, lambda_vals, mtcars, \"mpg\")\n\n\n\n\n\n\n\n\n[1] 5\n[1] 16\n\n\n   model     error\n1      1 12.181558\n2      2  9.452096\n3      3  7.936502\n4      4  7.393522\n5      5  7.283203\n6      6  7.441021\n7      7  7.809000\n8      8  8.361948\n9      9  9.080131\n10    10  9.940278\n11    11 10.914771\n\nridge_models$coef\n\n              0          1          4          9         16         25\ncyl  -0.1958895 -0.2854708 -0.5066139 -0.6161717 -0.6591445 -0.6683601\ndisp  1.6267230  0.2846046 -0.3725236 -0.5766167 -0.6490976 -0.6685263\nhp   -1.4496794 -1.0078499 -0.8528542 -0.8129980 -0.7775318 -0.7375242\ndrat  0.4142235  0.4865947  0.5232729  0.5453579  0.5549330  0.5519234\nwt   -3.5780149 -2.3702010 -1.6545051 -1.3466497 -1.1634754 -1.0318544\nqsec  1.4440470  0.8663632  0.4563819  0.3199939  0.2816562  0.2727392\nvs    0.1576353  0.1858566  0.2665591  0.3379935  0.3882118  0.4182555\nam    1.2377648  1.1337179  0.9930509  0.8779217  0.7836540  0.7053996\ngear  0.4759507  0.4979561  0.4415798  0.4078556  0.3938022  0.3845387\ncarb -0.3170292 -0.9153711 -1.0545303 -0.9681697 -0.8536815 -0.7501713\n             36         49         64         81        100\ncyl  -0.6584602 -0.6371174 -0.6089930 -0.5771490 -0.5436589\ndisp -0.6618424 -0.6410597 -0.6125306 -0.5799961 -0.5457984\nhp   -0.6949141 -0.6515838 -0.6087390 -0.5671623 -0.5273766\ndrat  0.5389018  0.5188717  0.4944121  0.4675133  0.4396305\nwt   -0.9277904 -0.8409367 -0.7659920 -0.6999756 -0.6410990\nqsec  0.2700141  0.2662930  0.2599270  0.2510841  0.2404236\nvs    0.4315668  0.4322241  0.4239420  0.4097144  0.3918058\nam    0.6390688  0.5816137  0.5309703  0.4857926  0.4451943\ngear  0.3737460  0.3599879  0.3436658  0.3256615  0.3068348\ncarb -0.6631350 -0.5906396 -0.5296824 -0.4777057 -0.4327987",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>R11: Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "R11_Ridge_LASSO.html#ridge-regression-with-standardized-data",
    "href": "R11_Ridge_LASSO.html#ridge-regression-with-standardized-data",
    "title": "38  R11: Ridge and Lasso",
    "section": "38.3 Ridge Regression With Standardized Data",
    "text": "38.3 Ridge Regression With Standardized Data\nOne problem that we will encounter with Ridge regression (and LASSO) is that it assumes large betas are large because the predictor is important. This is not necessarily true - it could be due to the units of the variable.\nConsider a model of mpg based on weight in pounds.\n\nlm(mpg~ I(1000*wt), data=mtcars)\n\n\nCall:\nlm(formula = mpg ~ I(1000 * wt), data = mtcars)\n\nCoefficients:\n (Intercept)  I(1000 * wt)  \n   37.285126     -0.005344  \n\n\nWhen we convert weight to pounds (multiply by 1000) then the coefficient is -.005. But if we have wt in 1000s of pounds\n\nlm(mpg~ wt, data=mtcars)\n\n\nCall:\nlm(formula = mpg ~ wt, data = mtcars)\n\nCoefficients:\n(Intercept)           wt  \n     37.285       -5.344  \n\n\nThe coefficient is 1000x as large! For this reason it is a good idea to standardize your data before you apply a shrinkage method. Standardized data requires subtracting the mean and dividing by standard deviation (of that column): \\[ Z_{i,j} = \\frac{X_{i,j} - \\bar{X}_j}{S_{j}}\\] No need to standardize the response variable.\n\nmtcars.std &lt;- mtcars\n#let's standardize the quantitative predictors\nstdcols &lt;- c(\"cyl\",\"disp\",\"hp\",\"drat\",\"wt\",\"qsec\",\"gear\",\"carb\")\nfor(col in stdcols){\n  xbar &lt;- mean(mtcars.std[,col])\n  sd &lt;- sd(mtcars.std[,col])\n  mtcars.std[,col] &lt;- (mtcars.std[,col]-xbar)/sd\n}\n\nlambda_vals &lt;- seq(0,10,1)^2; # Choose lambdas to try.\nridge_models &lt;- lm.ridge(mpg~., mtcars.std, lambda=lambda_vals);\nplot( ridge_models );\n\n\n\n\n\n\n\nkfoldCV.ridge(32, lambda_vals, mtcars.std, \"mpg\")\n\n\n\n\n\n\n\n\n[1] 5\n[1] 16\n\n\n   model     error\n1      1 12.181558\n2      2  9.452096\n3      3  7.936502\n4      4  7.393522\n5      5  7.283203\n6      6  7.441021\n7      7  7.809000\n8      8  8.361948\n9      9  9.080131\n10    10  9.940278\n11    11 10.914771",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>R11: Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "R11_Ridge_LASSO.html#lasso-regression",
    "href": "R11_Ridge_LASSO.html#lasso-regression",
    "title": "38  R11: Ridge and Lasso",
    "section": "38.4 LASSO Regression",
    "text": "38.4 LASSO Regression\n\nlibrary(glmnet)\n\nLoading required package: Matrix\n\n\nLoaded glmnet 4.1-10\n\nkfoldCV.LASSO &lt;- function(K, lambdas, dataset, responseVar){\n  m &lt;- length(lambdas)\n  \n  #idx is a shuffled vector of row numbers\n  idx &lt;- sample(1:nrow(dataset))\n  #folds partitions the row indices\n  folds &lt;- split(idx, as.factor(1:K))\n\n  #an empty data frame to store the results of each validation\n  results &lt;- data.frame(fold = rep(1:K, rep(m,K)),\n                        model = rep(1:m, K),\n                        error = 0)    \n  for(k in 1:K){\n    #split the data into training and testing sets\n    training &lt;- dataset[-folds[[k]],]\n    testing &lt;- dataset[folds[[k]],]\n    #go through each model and estimate MSE\n\n      for(f in 1:m){\n      mtc_lasso_lambda &lt;- glmnet(training[,names(dataset) != responseVar], training[,c(responseVar)], alpha = 1, lambda=lambdas[f]);\n      coeffs &lt;- as.vector(coef(mtc_lasso_lambda))\n      y.mtc.predict &lt;- coeffs %*% t(cbind(1,testing[,names(dataset) != responseVar]))\n\n      results[results$fold == k & results$model == f, \"error\"] &lt;- mean((y.mtc.predict-testing[,c(responseVar)])^2)\n    }\n  }\n  #aggregate over each model, averaging the error\n  aggregated &lt;- aggregate(error~model, data=results, FUN=\"mean\")\n  #produces a simple line & dot plot\n  plot(error ~ lambdas, type=\"b\", data=aggregated, ylab=\"MSE\")\n#  print(which(aggregated$error == min(aggregated$error)))\n  print(paste(\"best lambdas:\", paste(lambdas[which(aggregated$error == min(aggregated$error))], collapse=\",\")))\n  return(aggregated)\n}\n\n#LASSO mpg on mtcars\n\nlambda_vals &lt;- c(0,.1,.2,.3,.5,.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 5)\nkfoldCV.LASSO(32, lambda_vals, mtcars.std, \"mpg\")\n\n\n\n\n\n\n\n\n[1] \"best lambdas: 0.7\"\n\n\n   model     error\n1      1 12.139522\n2      2  9.401635\n3      3  9.208783\n4      4  8.945935\n5      5  8.314812\n6      6  8.013806\n7      7  8.357635\n8      8  9.960995\n9      9 12.241522\n10    10 14.956380\n11    11 18.398062\n12    12 22.475571\n13    13 27.129122\n14    14 36.870189\n\nLASSO.fits &lt;- glmnet(mtcars[,-1], mtcars[,1], alpha=1, lambda=lambda_vals)\nplot(LASSO.fits, label=TRUE, xvar=\"lambda\")\n\n\n\n\n\n\n\nLASSO_coeff&lt;- t(coef(LASSO.fits))\n#colnames(LASSO_coeff) &lt;- c(\"intercept\",names(mtcars)[-1])\nLASSO_coeff\n\n14 x 11 sparse Matrix of class \"dgCMatrix\"\n\n\n  [[ suppressing 11 column names '(Intercept)', 'cyl', 'disp' ... ]]\n\n\n                                                                     \ns0  20.58164  .         .           .           .          -0.1526208\ns1  24.29182 -0.2320135 .           .           .          -0.8596190\ns2  26.21643 -0.3916771 .           .           .          -1.1507654\ns3  28.14090 -0.5512556 .           .           .          -1.4420334\ns4  30.06537 -0.7108337 .           .           .          -1.7333021\ns5  31.87369 -0.8000158 .          -0.002226427 .          -2.0223414\ns6  33.59349 -0.8355844 .          -0.006175353 .          -2.3084431\ns7  35.31356 -0.8713207 .          -0.010120873 .          -2.5944626\ns8  36.34612 -0.8930789 .          -0.012481712 .          -2.7659212\ns9  35.87185 -0.8564098 .          -0.014070252 0.07915497 -2.6716570\ns10 32.27520 -0.7185199 .          -0.014111007 0.40765131 -2.5704279\ns11 26.92701 -0.5030523 .          -0.013211244 0.60633782 -2.6290418\ns12 20.12195 -0.2164116 .          -0.013112947 0.77468208 -2.6356369\ns13 12.32700 -0.1194297 0.01369736 -0.021693022 0.78186836 -3.7504495\n                                                         \ns0  .          .           .         .          .        \ns1  .          .           .         .          .        \ns2  .          .           .         .          .        \ns3  .          .           .         .          .        \ns4  .          .           .         .          .        \ns5  .          .           .         .          .        \ns6  .          .           .         .          .        \ns7  .          .           .         .          .        \ns8  .          .           .         .          .        \ns9  .          .           0.4869054 .         -0.1085726\ns10 0.08195105 0.003182207 1.1364003 .         -0.2812566\ns11 0.26537365 0.068664832 1.6929202 .         -0.3422762\ns12 0.45898572 0.123968843 2.1142688 0.3046746 -0.4637967\ns13 0.82638838 0.316450552 2.5192826 0.6476344 -0.1854400",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>R11: Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "R11_Ridge_LASSO.html#boston-dataset-example",
    "href": "R11_Ridge_LASSO.html#boston-dataset-example",
    "title": "38  R11: Ridge and Lasso",
    "section": "38.5 Boston Dataset Example",
    "text": "38.5 Boston Dataset Example\n\ndata(\"Boston\")\nlibrary(glmnet)\n\n#Standardize Variables\nBoston.std &lt;- Boston\nfor(i in 1:(ncol(Boston.std)-1)){\n  Boston.std[,i] &lt;- (Boston.std[,i]-mean(Boston.std[,i]))/sd(Boston.std[,i])\n}\n\nlambda_vals &lt;- seq(0,5,length.out=10)^2; # Choose lambdas to try.\n\nridge_models &lt;- lm.ridge(medv ~ . , Boston.std, lambda=lambda_vals);\nplot( ridge_models );\n\n\n\n\n\n\n\nkfoldCV.ridge(4, lambda_vals, Boston.std, \"medv\")\n\n\n\n\n\n\n\n\n[1] 4\n[1] 2.777778\n\n\n   model    error\n1      1 23.22624\n2      2 23.22228\n3      3 23.21316\n4      4 23.20575\n5      5 23.20800\n6      6 23.22627\n7      7 23.26433\n8      8 23.32364\n9      9 23.40416\n10    10 23.50516\n\n\n\n#LASSO \nlambda_vals &lt;- seq(0,.05,length.out=10)\n\nLASSO.fits &lt;- glmnet(Boston.std[,names(Boston.std)!=\"medv\"], Boston[,\"medv\"], alpha=1, lambda=lambda_vals)\nplot(LASSO.fits, label=TRUE, xvar=\"lambda\")\n\n\n\n\n\n\n\nkfoldCV.LASSO(10, lambda_vals, Boston.std, \"medv\")\n\nWarning in split.default(idx, as.factor(1:K)): data length is not a multiple of\nsplit variable\n\n\n\n\n\n\n\n\n\n[1] \"best lambdas: 0.0166666666666667\"\n\n\n   model    error\n1      1 23.80285\n2      2 23.79381\n3      3 23.78668\n4      4 23.78288\n5      5 23.78492\n6      6 23.79383\n7      7 23.80548\n8      8 23.82536\n9      9 23.85166\n10    10 23.88365\n\nresultsTable &lt;- t(as.matrix(coef(LASSO.fits)))\nrow.names(resultsTable) &lt;- sort(lambda_vals,decreasing=TRUE)\nround(resultsTable,3)\n\n                    (Intercept)   crim    zn indus  chas    nox    rm   age\n0.05                     22.533 -0.783 0.891 0.000 0.674 -1.799 2.747 0.000\n0.0444444444444444       22.533 -0.799 0.907 0.000 0.677 -1.814 2.744 0.000\n0.0388888888888889       22.533 -0.816 0.927 0.000 0.678 -1.838 2.735 0.000\n0.0333333333333333       22.533 -0.832 0.947 0.000 0.680 -1.863 2.726 0.000\n0.0277777777777778       22.533 -0.849 0.967 0.000 0.682 -1.887 2.718 0.000\n0.0222222222222222       22.533 -0.865 0.987 0.000 0.684 -1.911 2.708 0.000\n0.0166666666666667       22.533 -0.882 1.007 0.000 0.686 -1.935 2.700 0.000\n0.0111111111111111       22.533 -0.898 1.029 0.031 0.686 -1.968 2.693 0.000\n0.00555555555555556      22.533 -0.913 1.053 0.084 0.684 -2.006 2.687 0.000\n0                        22.533 -0.928 1.078 0.138 0.683 -2.049 2.680 0.017\n                       dis   rad    tax ptratio black  lstat\n0.05                -2.789 1.914 -1.424  -1.988 0.806 -3.732\n0.0444444444444444  -2.818 1.971 -1.471  -1.992 0.810 -3.730\n0.0388888888888889  -2.858 2.052 -1.537  -2.000 0.815 -3.730\n0.0333333333333333  -2.898 2.131 -1.601  -2.007 0.820 -3.730\n0.0277777777777778  -2.937 2.207 -1.662  -2.013 0.824 -3.730\n0.0222222222222222  -2.978 2.287 -1.727  -2.020 0.829 -3.731\n0.0166666666666667  -3.017 2.363 -1.788  -2.027 0.834 -3.730\n0.0111111111111111  -3.050 2.456 -1.874  -2.037 0.839 -3.733\n0.00555555555555556 -3.079 2.556 -1.974  -2.049 0.845 -3.737\n0                   -3.100 2.658 -2.074  -2.061 0.851 -3.745",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>38</span>  <span class='chapter-title'>R11: Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "cv_practice.html",
    "href": "cv_practice.html",
    "title": "39  Cross Validation Practice",
    "section": "",
    "text": "40 Practice Problems",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#boston-polynomial-regression",
    "href": "cv_practice.html#boston-polynomial-regression",
    "title": "39  Cross Validation Practice",
    "section": "40.1 Boston Polynomial Regression",
    "text": "40.1 Boston Polynomial Regression\nUse 5-fold validation to fit polynomials of order 1-5 on the Boston dataset, predicting MDEV from LSTAT",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#logistic-cv-on-mtcars",
    "href": "cv_practice.html#logistic-cv-on-mtcars",
    "title": "39  Cross Validation Practice",
    "section": "40.2 Logistic CV on mtcars",
    "text": "40.2 Logistic CV on mtcars\nLet’s revisit our dear old friend the mtcars data set once more. The am column of this data frame indicates whether a particular model of car has an automatic (0) or manual (1) transmission. This problem will consider how to go about predicting this trait based on the other available variables.\nPart a: fitting logistic regression\nFit a logistic regression model to predict am based on the read axle ratio (drat) and an intercept term.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndrat_lr &lt;- glm( am ~ 1 + drat, data=mtcars, family='binomial')\n\nsummary(drat_lr)\n\n\nCall:\nglm(formula = am ~ 1 + drat, family = \"binomial\", data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5495  -0.2609  -0.1505   0.5382   1.7453  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -21.021      7.838  -2.682  0.00732 **\ndrat           5.577      2.063   2.704  0.00685 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.23  on 31  degrees of freedom\nResidual deviance: 21.65  on 30  degrees of freedom\nAIC: 25.65\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nPart b: interpreting logistic regression\nLooking at your fitted model from Part a, is the estimated coefficient for drat statistically significantly different from zero?\nGive a 95% confidence interval for the fitted coefficient.\nGive an interpretation of what this coefficient means.\nDoes an increase in drat tend to result in a higher or lower probability of a car having a manual transmission?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nconfint( drat_lr )\n\nWaiting for profiling to be done...\n\n\n                 2.5 %    97.5 %\n(Intercept) -42.327246 -9.555062\ndrat          2.526815 11.158615\n\n\nA unit increase in drat is associated with an increase of 5.577 in the log-odds of a car having a manual transmission.\nThus, an increase in drat is associated with a higher probability of a car having a manual transmission.\n\n\n\nPart c: model comparison with LOOCV\nNow, suppose we are considering adding another variable to our model from Part a. As discussed in lecture and your readings, adding additional variables to a model will always increase our performance when measured on the training data, so to make a fair comparison between this new model and the model from Part a, we need to compare their performances on data not seen in the training procedure.\nUse leave-one-out cross validation (LOOCV) to compare the performance of your model from Part a against a model that predicts am from drat and mpg (and an intercept term).\nNote: in our discussion of LOOCV, we mostly discussed linear regression, where the natural way to assess a model’s performance was its squared error in predicting the held-out observations. In logistic regression, squared error is not such a natural choice. For this problem, you should fit the model using the standard maximum likelihood approach as implemented in glm, but you should assess the model’s performance on held-out data by making a prediction with the model (i.e., predicting 0 or 1), and then checking whether or not this prediction matches the true value of am in the held-out observation. Recall that our trained logistic regression model outputs a probability based on the given predictor(s). You should turn this probability into a prediction by rounding the probability to 0 or 1 (you may break a tie at probability \\(0.5\\) as you see fit).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- nrow(mtcars);\nsinglevar_performances &lt;- rep(NA, n);\ntwovar_performances &lt;- rep(NA, n);\nfor( i in 1:n ) {\n  heldout &lt;- mtcars[i,];\n  heldout_correct_answer &lt;- heldout$am\n  trainset &lt;- mtcars[-c(i),]\n  singlevar_model &lt;- glm( am ~ 1 + drat, data=trainset, family='binomial')\n  twovar_model &lt;- glm( am ~ 1 + drat + mpg, data=trainset, family='binomial')\n  # Make predictions on the held-out point.\n  # Need to turn a probability into a 0 or 1, which we do with round().\n  singlevar_pred &lt;- round( predict( singlevar_model,\n                                        type='response', newdata=heldout ) )\n  twovar_pred &lt;- round( predict( twovar_model,                            \n                                 type='response', newdata=heldout ) )\n  # Record whether we predicted correctly.\n  singlevar_performances[i] &lt;- singlevar_pred==heldout_correct_answer;\n  twovar_performances[i] &lt;- twovar_pred==heldout_correct_answer;\n}\n\n# Now, average over the n different held-out data points.\nc( mean(singlevar_performances), mean(twovar_performances) )\n\n[1] 0.8125 0.8125\n\n\n\n\n\nWhich model is better according to your implementation of leave-one-out cross validation?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBoth models perform identically, according to LOOCV. In light of this, we should prefer the model that uses only drat, since we prefer simpler models (i.e., those with fewer variables), all else being equal.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#defaulting-on-loans",
    "href": "cv_practice.html#defaulting-on-loans",
    "title": "39  Cross Validation Practice",
    "section": "40.3 Defaulting on loans",
    "text": "40.3 Defaulting on loans\nwe used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n\n\nFit a logistic regression model that uses income and balance to predict default.\nUsing the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n\n\n\nSplit the sample set into a training set and a validation set.\nFit a multiple logistic regression model using only the training observations.\nObtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\nCompute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n\n\n\nRepeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\nNow consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#defaulting-on-loans-ii",
    "href": "cv_practice.html#defaulting-on-loans-ii",
    "title": "39  Cross Validation Practice",
    "section": "40.4 Defaulting on loans II",
    "text": "40.4 Defaulting on loans II\nWe continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nWrite a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\nComment on the estimated standard errors obtained using the glm() function and using your bootstrap function.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#looping-loocv-logistic",
    "href": "cv_practice.html#looping-loocv-logistic",
    "title": "39  Cross Validation Practice",
    "section": "40.5 Looping LOOCV Logistic",
    "text": "40.5 Looping LOOCV Logistic\nWe saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).\n\n\nFit a logistic regression model that predicts Direction using Lag1 and Lag2.\nFit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation.\nUse the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if P(Direction = “Up”|Lag1, Lag2) &gt; 0.5. Was this observation correctly classified?\nWrite a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps:\n\n\n\nFit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\nCompute the posterior probability of the market moving up for the ith observation.\nUse the posterior probability for the ith observation in order to predict whether or not the market moves up.\nDetermine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.\n\n\n\nTake the average of the n numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#simulated-data-cv",
    "href": "cv_practice.html#simulated-data-cv",
    "title": "39  Cross Validation Practice",
    "section": "40.6 Simulated Data CV",
    "text": "40.6 Simulated Data CV\n\nGenerate a simulated data set as follows:\n\n\nset.seed (1)\nx &lt;- rnorm (100)\ny &lt;- x - 2 * x^2 + rnorm (100)\n\nIn this data set, what is n and what is p? Write out the model used to generate the data in equation form. (b) Create a scatterplot of X against Y . Comment on what you find. (c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares: i. Y = β0 + β1X + ϵ ii. Y = β0 + β1X + β2X2 + ϵ iii. Y = β0 + β1X + β2X2 + β3X3 + ϵ iv. Y = β0 + β1X + β2X2 + β3X3 + β4X4 + ϵ. Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y . (d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why? (e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer. (f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#simulated-data-best-subset",
    "href": "cv_practice.html#simulated-data-best-subset",
    "title": "39  Cross Validation Practice",
    "section": "40.7 Simulated Data Best Subset",
    "text": "40.7 Simulated Data Best Subset\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n\n\nUse the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ϵ of length n = 100.\nGenerate a response vector Y of length n = 100 according to the model Y = β0 + β1X + β2X2 + β3X3 + ϵ, where β0, β1, β2, and β3 are constants of your choice.\nUse the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2, . . . ,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .\nRepeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\nNow fit a lasso model to the simulated data, again using X,X2, . . . , X10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained.\nNow generate a response vector Y according to the model Y = β0 + β7X7 + ϵ, and perform best subset selection and the lasso. Discuss the results obtained.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#college-applications",
    "href": "cv_practice.html#college-applications",
    "title": "39  Cross Validation Practice",
    "section": "40.8 College Applications",
    "text": "40.8 College Applications\nIn this exercise, we will predict the number of applications received using the other variables in the College data set.\n\n\nSplit the data set into a training set and a test set.\nFit a linear model using least squares on the training set, and report the test error obtained.\nFit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\nFit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\nComment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these three approaches?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html#features-and-error",
    "href": "cv_practice.html#features-and-error",
    "title": "39  Cross Validation Practice",
    "section": "40.9 Features and Error",
    "text": "40.9 Features and Error\nWe have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n\n\nGenerate a data set with p = 20 features, n = 1,000 observations, and an associated quantitative response vector generated according to the model Y = Xβ + ϵ, where β has some elements that are exactly equal to zero.\nSplit your data set into a training set containing 100 observations and a test set containing 900 observations.\nPerform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\nPlot the test set MSE associated with the best model of each size.\nFor which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\nHow does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\nCreate a plot displaying \\(\\sqrt{\\sum_{j=1}^p(\\beta_j − \\hat{\\beta}_j^r)^2}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_j^r\\) \\(j\\) is the \\(j\\)th coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>39</span>  <span class='chapter-title'>Cross Validation Practice</span>"
    ]
  },
  {
    "objectID": "bootstrap.html",
    "href": "bootstrap.html",
    "title": "40  Bootstrapping",
    "section": "",
    "text": "40.1 Learning objectives\nReadings: ISLR Section 5.2\nIn previous lectures throughout the semester, we have looked at different ways of quantifying uncertainty in our data and our resulting estimates. Among the most fundamental tools in statistics for quantifying uncertainty is the bootstrap. Ultimately, the bootstrap amounts to resampling our data as though it were the population itself. Rather surprisingly, this can actually help us estimate certain quantities related to variances (i.e., uncertainty).\nAfter this lecture, you will be able to",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#learning-objectives",
    "href": "bootstrap.html#learning-objectives",
    "title": "40  Bootstrapping",
    "section": "",
    "text": "Explain the bootstrap and its applicability.\nImplement and apply the bootstrap to estimate variance in simple models.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#estimating-variance",
    "href": "bootstrap.html#estimating-variance",
    "title": "40  Bootstrapping",
    "section": "40.2 Estimating Variance",
    "text": "40.2 Estimating Variance\nIn a wide range of applications, we need to estimate the variance of our data.\nExample: confidence intervals\nGiven data \\(X_1,X_2,\\dots,X_n\\) drawn from some distribution, we are often interested in constructing a confidence interval for some parameter \\(\\theta\\). For example, \\(\\theta\\) might be the mean of our distribution. If an estimator \\(\\hat{\\theta}\\) is approximately normal about \\(\\theta\\), we could construct an approximate CI for \\(\\theta\\) if only we knew the standard deviation of \\(\\hat{\\theta}\\), \\[\n\\sigma_{\\hat{\\theta}} = \\sqrt{ \\operatorname{Var} \\hat{\\theta} }.\n\\] We have seen in our lectures on estimation and confidence intervals that there are ways to estimate this variance, but sometimes it is a lot more complicated (e.g., more computationally expensive or more mathematically complicated).\nExample: errors in linear regression\nRecall that in linear regression we observe \\((X_i,Y_i)\\) pairs where \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,~~~i=1,2,\\dots,n\n\\] where \\(\\epsilon_1,\\epsilon_2,\\dots,\\epsilon_n\\) are drawn iid according to a normal with mean zero and unknown variance \\(\\sigma^2 &gt; 0\\).\nDuring our discussions of linear regression in the last few weeks, we saw situations where we were interested in the sampling distribution of the estimated coefficients \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) (e.g., in adjusted \\(R^2\\) or in interpreting the p-values associated to different coefficients). Often, we need to know \\(\\sigma^2\\) or at least have a good estimate of it. How might we estimate \\(\\sigma^2\\)?\n\n40.2.1 Tricky variance computations\nIn situations like the above, we had central limit theorems or similar results that allowed us to estimate the variance of the quantities we cared about. Unfortunately, this isn’t always the case. Here’s a simple example, adapted from ISLR (beginning of Section 5.2):\nExample: allocating investments\nSuppose that we have two different stocks we can invest in, which yield returns \\(X\\) and \\(Y\\). So, we invest a single dollar (just to keep things simple!), and we need to split between these two different stocks. Let’s say we put \\(\\alpha\\) dollars into the first stock (with return \\(X\\)) and \\(1-\\alpha\\) (i.e., the rest) into the second stock (with return \\(Y\\)).\nThen we get back an amount \\(\\alpha X+ (1-\\alpha) Y\\).\nNow, this amount of money that we get back on our investment is random, because it is based on the (random) stock yields \\(X\\) and \\(Y\\). An obvious thing to do is to try and maximize this, but this can result in us making risky bets.\nMost investment strategies instead aim to minimize the “risk” (note: in statistics, risk means something very different from this– this is the “finance” meaning of the term “risk”): \\[\n\\begin{aligned}\n  \\operatorname{Var}\\left( \\alpha X + (1-\\alpha) Y \\right)\n  &= \\alpha^2 \\operatorname{Var} X\n      + (1-\\alpha)^2 \\operatorname{Var} Y\n      + 2\\alpha(1-\\alpha) \\operatorname{Cov}(X,Y) \\\\\n  &= \\alpha^2 \\sigma_X^2\n    + (1-\\alpha)^2 \\sigma_Y^2\n    + 2\\alpha(1-\\alpha) \\sigma_{XY}\n    \\end{aligned}\n\\] Where \\[\n\\begin{aligned}\n\\sigma_X^2 &= \\operatorname{Var} X \\\\\n\\sigma_Y^2 &= \\operatorname{Var} Y \\\\\n\\sigma_{XY} &= \\operatorname{Cov}( X, Y ).\n\\end{aligned}\n\\] One can prove that \\(\\operatorname{Var}\\left( \\alpha X + (1-\\alpha) Y \\right)\\) is minimized by taking \\[\n\\alpha = \\frac{ \\sigma_Y^2 - \\sigma_{XY} }{ \\sigma_X^2 + \\sigma_Y^2 -2\\sigma_{XY} }.\n\\] Unfortunately, we don’t know the variance and covariance terms (i.e., \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), \\(\\sigma_{XY}\\))…\nBut suppose we have observations \\((X_i,Y_i)\\), \\(i=1,2,\\dots,n\\) of, say, the performance of these two stocks on prior days. We could use this data to estimate \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\) and \\(\\sigma_{XY}\\).\nIf we denote the estimates \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\) and \\(\\hat{\\sigma}_{XY}\\), then we could plug these into the above equation and obtain an estimator for \\(\\alpha\\), \\[\n\\hat{\\alpha}\n= \\frac{ \\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY} }\n        { \\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 -2\\hat{\\sigma}_{XY} }.\n\\]\nNow, here’s where things get tricky. What if we want to quantify our uncertainty about \\(\\alpha\\)?\nAfter all, \\(\\hat{\\alpha}\\) is a function of the (random) data, so it is itself a random variable, and it is reasonable to ask about, for example, its variance.\nBut \\(\\hat{\\alpha}\\) depends on the data \\(\\{ (X_i,Y_i) : i=1,2,\\dots,n \\}\\) in a fairly complicated way, so it’s not obvious what \\(\\operatorname{Var} \\hat{ \\alpha}\\) should actually be.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#refresher-simulation-based-methods",
    "href": "bootstrap.html#refresher-simulation-based-methods",
    "title": "40  Bootstrapping",
    "section": "40.3 Refresher: simulation-based methods",
    "text": "40.3 Refresher: simulation-based methods\nIn our discussion of estimation and confidence intervals earlier in the semester, we obtained simulation-based confidence intervals by\n\nAssuming a model for our data (e.g., assuming the data came from a Poisson distribution)\nEstimating the parameter(s) of that model from the data (e.g., estimating \\(\\lambda\\) under the Poisson)\nGenerating new random samples from the model with the estimated parameter(s) (e.g., drawing from \\(\\operatorname{Pois}(\\hat{\\lambda})\\))\nUsing these “fake” data samples to approximate the sampling distribution of our statistic of interest (e.g., \\(\\hat{\\lambda}(X_1,X_2,\\dots,X_n)\\)).\n\nIf we had a model for our \\((X_i,Y_i)\\) pairs, we could use the observed pairs to estimate the parameter(s) of that model and generate new samples \\((X'_i,Y'_i), i=1,2,\\dots,n\\) and compute \\[\n\\hat{\\alpha}' =\n\\hat{\\alpha}\\left( (X'_1,Y'_1),(X'_2,Y'_2),\\dots,(X'_n,Y'_n) \\right).\n\\] Doing this many times, we would get multiple random variables that approximate the distribution of \\(\\hat{\\alpha}\\) (i.e., the statistic computed on our original data).",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#what-if-we-dont-have-a-model",
    "href": "bootstrap.html#what-if-we-dont-have-a-model",
    "title": "40  Bootstrapping",
    "section": "40.4 What if we don’t have a model?",
    "text": "40.4 What if we don’t have a model?\nSimulation-based approaches like the one discussed above work because we have made a model assumption about our data. If we assumed our data came from a Poisson distribution, then we could just estimate the Poisson parameters and generate new samples.\nBut often we don’t want to make such assumptions about our data. Because, for example\n\nOur parameter(s) may be expensive to estimate\nThe distribution may be expensive to draw from\nWe don’t want to make model assumptions in the first place!\n\nThis last concern gives rise to what we call non-parametric statistics. That is, doing statistics while avoiding assumptions of the form “We assume that the data are generated according to a normal (or Poisson or Binomial or…)”.\nThe details of non-parametric statistics will have to wait for your later courses, but these concerns lead us to try and come up with a different way to “resample” copies of our statistic.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#introducing-the-bootstrap",
    "href": "bootstrap.html#introducing-the-bootstrap",
    "title": "40  Bootstrapping",
    "section": "40.5 Introducing the Bootstrap",
    "text": "40.5 Introducing the Bootstrap\nSo, let’s try something a little weird.\nWhat we want to do is to draw new samples from the same population (i.e., distribution) as our data came from. In the simulation-based approach, we estimate the parameter(s) to get an approximation to that true distribution.\nThe bootstrap takes a different tack. The data, \\(X_1,X_2,\\dots,X_n\\) is a sample from the actual population that we care about (i.e., not an approximation!). The bootstrap says, “let’s just sample from \\(X_1,X_2,\\dots,X_n\\).”\nSaid another way, in the bootstrap, we sample with replacement from the observed data \\[\nX_1, X_2, \\dots, X_n,\n\\] obtaining the sample \\[\nX^*_1, X^*_2, \\dots, X^*_n.\n\\] The \\(X^*_i\\) notation is convention in statistics– the asterisk (\\(*\\)) denotes that the variable is resampled from the original data.\nImportant note: \\(X^*_i\\) does not necessarily equal \\(X_i\\). It is just the \\(i\\)-th resampled data point. \\(X^*_i\\) is equal to some \\(X_j\\), \\(j=1,2,\\dots,n\\), with each of the \\(n\\) different data points being equally likely. Each \\(X_i^*\\) is a sample with replacement from the data \\(X_1,X_2,\\dots,X_n\\).\nNow, having resampled our data, we can compute our estimator on the bootstrap sample \\(X^*_1,X^*_2,\\dots,X^*_n\\), say, \\[\n\\hat{\\theta}\\left(X^*_1,X^*_2,\\dots,X^*_n \\right).\n\\] Repeating this many times (say, \\(B\\) times), we can use these resampled replicates of our original estimator, say \\[\n\\hat{\\theta}_1, \\hat{\\theta}_2, \\dots, \\hat{\\theta}_B.\n\\] The intuition is that these \\(B\\) random variables, each based on a sample with replacement from the original data, are a good approximation to the true distribution of our estimate \\(\\hat{\\theta}(X_1,X_2,\\dots,X_n)\\), and we can use them to do things like estimating the variance.\n\n40.5.1 Wait, how can this possibly work?!\nYes, this is just as crazy as it sounds, and yet, it works (for certain problems, anyway).\nThe intuition is something like this: when \\(n\\) is suitably large, the “point cloud” formed by \\(X_1,X_2,\\dots,X_n\\) looks a lot like the true population distribution.\nAs a result, resampling from the observed sample \\(X_1,X_2,\\dots,X_n\\) and resampling from the true population distribution are not actually so different!\nThe careful mathematical proof of this intuition is beyond the scope of this course. But I agree that on a first glance this should not work at all. And yet it does!\nI have been studying statistics for more than 15 years, and I am still pretty sure the bootstrap is magic…\n\n\n40.5.2 Example: estimating the Poisson rate parameter\nLet’s start by illustrating on a simple data example that we have seen multiple times before. Let’s suppose that \\(X_1,X_2,\\dots,X_n\\) are drawn iid from a Poisson distribution with rate parameter \\(\\lambda = 5\\).\nOur goal is to construct a confidence interval for the rate parameter \\(\\lambda\\).\nNow, we’ve already seen ways to do this– we could construct a CLT-based confidence interval or use a simulation-based approach. The point here is just to illustrate how the bootstrap applies in a setting that we are already familiar with. The claim is not that this the bootstrap is the best solution to this particular problem.\n\nn = 25\nlambdatrue = 5\ndata = rpois(n=n, lambda=5); # Generate a sample of 25 iid RVs\nlambdahat = mean(data); # Estimate lambda.\n\n# Now, let's do the bootstrap.\n# We'll repeatedly (B times)  resample n=25 observations from the data.\n# On each resample, compute lambdahat (i.e., take the mean).\nB = 200; # Number of bootstrap replicates.\nreplicates = rep(NA,B); # We'll store\nfor( i in 1:B ) {\n  # Sample WITH REPLACEMENT from the data sample itself.\n  resample = sample(data, n, replace=TRUE)\n  # Compute our statistic on the resample data.\n  # This is a *bootstrap replicate* of our statistic.\n  replicates[i] = mean(resample)\n}\n\nNow, replicates is a vector of (approximate) replicates of our estimate for \\(\\lambda\\). Let’s visualize the replicates– we’ll indicate the true \\(\\lambda\\) in blue and our estimate (lambdahat) in red.\n\nhist(replicates)\nabline(v=lambdahat, lwd=2, col='red')\nabline(v=lambdatrue, lwd=2, col='blue')\n\n\n\n\n\n\n\n\nNow, we’re going to use those bootstrap replicates to estimate the standard deviation of \\(\\hat{\\lambda} = n^{-1} \\sum_i X_i\\), and then we’ll use that to get a 95% CI for \\(\\lambda\\).\n\nsd_lambda = sd( replicates )\nCI = c( lambdahat-1.96*sd_lambda, lambdahat+1.96*sd_lambda)\n# And check if our CI contains lambda = 5.\n(CI[1] &lt; lambdatrue) & (lambdatrue &lt; CI[2])\n\n[1] TRUE\n\n\nWell, about 95% of the time, we should catch \\(\\lambda=5\\). Let’s run that same experiment a bunch of times, just to verify. That is, we are going to:\n\nGenerate data \\(X_1,X_2,\\dots,X_n\\) iid from Poisson with \\(\\lambda=5\\).\nCompute \\(\\hat{\\lambda} = \\bar{X}\\)\nRepeatedly resample from \\(X_1,X_2,\\dots,X_n\\) with replacement, compute the mean of each resample (i.e., compute our estimator on each resample)\nCompute the standard deviation of these resampled copies of \\(\\hat{\\lambda}\\).\nUse that SD to compute a 95% CI, under the assumption that \\(\\hat{\\lambda}\\) is approximately normal about its expectation \\(\\lambda\\).\n\nOkay, let’s do that in code. It will be useful to have a command that runs all of the bootstrap machinery for us.\n\nrun_pois_bootstrap_expt = function( lambdatrue, n, B ) {\n  # lambdatrue is the true value of lambda to use in generating our data.\n  # n is the sample size.\n  # B is the number of bootstrap replicates to use.\n  \n  # First things first, generate data.\n  data = rpois( n=n, lambda=lambdatrue )\n  lambdahat = mean( data ); # Our point estimate for lambda.\n  \n  # Generate B bootstrap replicates.\n  replicates = rep(NA,B)\n  # Each replicate draws n data points, with replacement, from data\n  # and computes its mean (i.e., estimates lambda from the resampled data)\n  for( i in 1:B ) {\n    # resample n elements of the data, with replacement.\n    resampled_data = sample( data, n, replace=TRUE )\n    replicates[i] = mean( resampled_data )\n  }\n  # Now use those replicates to estimate SD of and construct a 95% CI.\n  sd_boot = sd( replicates ); # estimate of the std dev of alphahat\n  CI = c(lambdahat-1.96*sd_boot, lambdahat+1.96*sd_boot)\n  # Finally, check if this CI caught lambda successfully.\n  # Return TRUE/FALSE accordingly.\n  \n  return( (CI[1] &lt; lambdatrue) & (lambdatrue &lt; CI[2]) )\n}\n\nNow, let’s repeat this experiment a few hundred times, and see how often our CI contains the true \\(\\lambda\\).\n\nN_expt = 500; # number of CIs to construct.\nsuccesses = rep(NA, N_expt); # Keep track of whether or not each CI caught alpha.\nfor( i in 1:N_expt ) {\n  # Repeat the experiment N_expt times.\n  # lambdatrue=5, B=200 bootstrap replicates, n=25 observations in the data sample.\n  successes[i] = run_pois_bootstrap_expt( 5, 200, 25 )\n}\nmean(successes)\n\n[1] 0.928\n\n\nThat number should be between 0.93 and 0.97 (of course, we are always subject to randomness in our experiments…).\n\n\n40.5.3 Recap: general recipe for the bootstrap\nJust to drive things home, let’s walk through the “recipe” for the bootstrap again, this time at a slightly more abstract level. Then we’ll come back and do a more complicated example.\nSuppose that we have data \\(X_1,X_2,\\dots,X_n\\) drawn iid from some unknown distribution. We wish to estimate a parameter \\(\\theta\\), and we have an estimator \\(\\hat{\\theta} = \\hat{\\theta}(X_1,X_2,\\dots,X_n)\\) for \\(\\theta\\).\nThe basic idea behind the bootstrap is to resample from the data \\(X_1,X_2,\\dots,X_n\\) with replacement, evaluate \\(\\hat{\\theta}\\) on each resample, and use those replicates of \\(\\hat{\\theta}(X_1,X_2,\\dots,X_n)\\) to approximate its true distribution (usually we are specifically interested in variance, but we’ll come back to this point).\nSo, given data \\(X_1,X_2,\\dots,X_n\\), we:\n\nRepeatedly (\\(B\\) times) sample \\(n\\) observations with replacement from \\(X_1,X_2,\\dots,X_n\\), to obtain samples \\(X_{b,1}^*,X_{b,2}^*,\\dots,X_{b,n}^*\\), for \\(b=1,2,\\dots,B\\). Note that putting the asterisk (*) on the resampled data points is a common notation in statistics to indicate bootstrap samples.\nFor each of these \\(B\\) resamples, compute the estimator on that sample, to obtain \\[\n\\hat{\\theta}_b = \\hat{\\theta}(X_{b,1}^*,X_{b,2}^*,\\dots,X_{b,n}^*) ~~~ \\text{ for } b=1,2,\\dots,B.\n\\]\nCompute the standard deviation of our bootstrap replicates, \\[\n\\operatorname{SE}_{\\text{boot}}\n= \\sqrt{ \\frac{1}{B-1} \\sum_{b=1}^B \\left( \\hat{\\theta}_b - \\frac{1}{B}\\sum_{r=1}^B \\hat{\\theta}_r \\right)^2 }.\n\\]\nUse this standard deviation estimate to construct an (approximate) confidence interval, under the assumption that \\(\\hat{\\theta}\\) is normally distributed about its mean \\(\\theta\\). \\[\n(\\hat{\\theta} - 1.96\\operatorname{SE}_{\\text{boot}},\n\\hat{\\theta} + 1.96\\operatorname{SE}_{\\text{boot}} )\n\\]",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#bootstrapping-for-more-complicated-statistics",
    "href": "bootstrap.html#bootstrapping-for-more-complicated-statistics",
    "title": "40  Bootstrapping",
    "section": "40.6 Bootstrapping for more complicated statistics",
    "text": "40.6 Bootstrapping for more complicated statistics\nNow, our example above is kind of silly– we already know multiple different ways to construct confidence intervals for the Poisson parameter!\nBut what about more complicated functions of our data? That is often where the bootstrap really shines.\nTo see this, let’s return to our example of financial returns \\((X_i,Y_i)\\)\nThe bootstrap says that we should sample with replacement from the observed data \\[\n(X_1,Y_1), (X_2,Y_2), \\dots, (X_n, Y_n),\n\\] obtaining the sample \\[\n(X^*_1,Y^*_1), (X^*_2,Y^*_2), \\dots, (X^*_n, Y^*_n).\n\\] Again, the \\(X^*_i\\) notation is the conventional way to show that the variable is resampled from the original data.\nHaving resampled from our data in this way, we could then compute \\[\n\\alpha^* = \\hat{\\alpha}\\left( (X^*_1,Y^*_1), (X^*_2,Y^*_2), \\dots, (X^*_n, Y^*_n)  \\right).\n\\] Repeating this, say, \\(B\\) times, we would obtain bootstrap replicates \\[\n\\alpha^*_1, \\alpha^*_2, \\dots, \\alpha^*_B,\n\\] from which we can estimate the variance of \\(\\hat{\\alpha}\\) as \\[\n\\hat{\\sigma}^2_{\\alpha}\n=\n\\frac{1}{B-1}\\sum_{r=1}^B \\left( \\alpha^*_r - \\frac{1}{B} \\sum_{b=1}^B \\alpha^*_b \\right)^2.\n\\]\nNotice that this looks like a variance, except that we are using \\(\\hat{\\alpha}\\), the statistic computed on the original data sample, as our estimate of the mean.\n\n40.6.1 Demo: applying the bootstrap to financial data\nThe following chunk contains code for generating \\((X_i,Y_i)\\) pairs like those discussed in our financial example above.\n\nrequire(MASS)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.2.3\n\ngenerate_pairs = function( n ) {\n  # Generate n pairs of financial returns.\n  muX = 2; muY = -1\n  CovMx = matrix( c(1,-.25,-.25,2), nrow = 2)\n  data = mvrnorm(n=100, mu=c(muX,muY), Sigma=CovMx)\n  return( data.frame( 'X'=data[,1], 'Y'=data[,2]) )\n}\n\n\nfin_pairs = generate_pairs( 100 ); # Generate 100 (X,Y) pairs.\nhead(fin_pairs)\n\n          X          Y\n1 1.8516953  0.1067379\n2 1.9822517 -1.0541823\n3 1.5664094 -1.5491879\n4 1.8915520 -0.5044080\n5 0.4696023 -2.1840953\n6 1.3919421 -2.2014030\n\n\nAlways look at your data first. Here is a scatter plot.\n\npp = ggplot(data=fin_pairs, aes(x=X, y=Y)) + geom_point()\npp\n\n\n\n\n\n\n\n\nNow, let’s compute \\(\\hat{\\alpha}\\) on this data. Remember, \\(\\hat{\\alpha}\\) is just a function of the (estimated) variances of \\(X\\) and \\(Y\\), along with their covariance: \\[\n\\hat{\\alpha}\n= \\frac{ \\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY} }\n        { \\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 -2\\hat{\\sigma}_{XY} }.\n\\] So, let’s just compute those three quantities and plug them in. The cov function gets us the whole sample covariance matrix of our data:\n\nSigmahat = cov( fin_pairs ); #Sigma is the common symbol for a covariance matrix.\nSigmahat\n\n           X          Y\nX  0.9891055 -0.4064777\nY -0.4064777  2.5544449\n\n\nNow we can just pluck our three estimates out of there\n\nsigma2hatXX = Sigmahat[1,1]\nsigma2hatYY = Sigmahat[2,2]\nsigmahatXY = Sigmahat[1,2]\n\nand we can plug these into our formula for \\(\\hat{\\alpha}\\) above.\n\nalphahat = (sigma2hatYY - sigmahatXY)/(sigma2hatXX + sigma2hatYY -2*sigmahatXY)\nalphahat\n\n[1] 0.6796554\n\n\nNow, in truth, the covariances that generated our data are: \\[\n\\sigma^2_X = 1,~~~\\sigma^2_Y = 2, ~~~\\sigma_{XY} = -0.25,\n\\] so the true optimal choice of \\(\\alpha\\) is \\[\n\\alpha = \\frac{ \\sigma_Y^2 - \\sigma_{XY} }\n        { \\sigma_X^2 + \\sigma_Y^2 -2\\sigma_{XY} }\n        = \\frac{2 - (-0.25) }{ 1 + 2 + 2*0.25  }\n        = \\frac{ 2.25 }{3.5 } \\approx 0.64\n\\] Let’s store the true value of alpha while we’re thinking of it.\n\nsigma2XX = 1\nsigma2YY = 2\nsigmaXY = -0.25\nalpha_true =(sigma2YY - sigmaXY)/(sigma2XX + sigma2YY -2*sigmaXY)\nalpha_true\n\n[1] 0.6428571\n\n\nNow, again, we’re going to resample with replacement from our data, and compute our statistic \\(\\hat{\\alpha}\\) on each resample. The hope is that these resampled versions of the statistic will resemble the distribution of the statistic evaluated on the original data.\nIt will be convenient to just have a function to compute alphahat from a given data set.\n\ncompute_alphahat = function( data ) {\n  # We're assuming that data is a data frame with two columns.\n  Sigmahat = cov( data )\n  # Extract the variance and covariance estimates from the sample covariance\n  sigma2hatXX = Sigmahat[1,1]\n  sigma2hatYY = Sigmahat[2,2]\n  sigmahatXY = Sigmahat[1,2]\n  # plug these into the definition of alpha.\n  alphahat = (sigma2hatYY - sigmahatXY)/(sigma2hatXX + sigma2hatYY -2*sigmahatXY)\n  return(alphahat)\n}\n\nalphahat = compute_alphahat( fin_pairs )\n\nOkay, we’re ready to go. Let’s resample the data \\(B=200\\) times, evaluating \\(\\hat{\\alpha}\\) on each resample. Then, we’ll use those resampled values to estimate the variance.\n\nB = 200\nreplicates = rep(NA,B)\nn = nrow( fin_pairs ); # number of observations in our data set.\nfor( i in 1:B ) {\n  # To resample the data, we will sample indices, and then grab those rows.\n  resample_indices = sample( 1:n, n, replace=TRUE )\n  resampled_data = fin_pairs[resample_indices,]\n  replicates[i] = compute_alphahat( resampled_data )\n}\nhist( replicates )\nabline( v=alphahat, col='red', lwd=2); # alpha of true data.\nabline( v=0.64, col='blue', lwd=2); # True alpha\n\n\n\n\n\n\n\n\nSo the red line indicates the value of \\(\\alpha\\) estimated from the original data in the data frame fin_pairs, while the blue line indicates the true value of \\(\\alpha\\) computed from the true covariance structure of \\((X,Y)\\).\nClearly, our resampled data is centering about our estimate of \\(\\alpha\\), not its true value.\nBut that is okay! We just want to use our bootstrap replicates to estimate the variance, so that we can center a confidence interval at our estimate…\n\n# Estimate the variance of alphahat from our bootstrap replicates.\nsd_alphahat = sd( replicates ); # estimate of the std dev of alphahat\nCI = c(alphahat-1.96*sd_alphahat, alphahat+1.96*sd_alphahat)\nCI\n\n[1] 0.6150652 0.7442455\n\n\n…and we can verify that the CI contains the true value of \\(\\alpha\\) (about 95% of the time you run this code, anyway):\n\n(CI[1] &lt; alpha_true) & (alpha_true &lt; CI[2])\n\n[1] TRUE\n\n\nJust to verify, let’s rerun this whole machinery a few times, and check that our bootstrap-based CI catches \\(\\alpha\\) about 95% of the time.\nImportantly, that “95% of the time” means “if we generate new data, then 95% of the time, our bootstrap-based CI will catch the true value of \\(\\alpha\\).”\nTo do this experiment, we want a function to run the above procedure once for us on a given data frame and a given number of bootstrap replicates.\n\nrun_bootstrap_expt = function( B ) {\n  # B is the number of bootstrap replicates to use.\n  \n  # Generate new data: 1-- X-Y pairs.\n  data = generate_pairs( 100 )\n  # Generate B bootstrap replicates.\n  replicates = rep(NA,B)\n  # Each replicate draws n data points, with replacement, from fin_pairs.\n  n = nrow( data ); # number of observations in our data set.\n  for( i in 1:B ) {\n    # To resample the data, we will sample indices, and then grab those rows.\n    resample_indices = sample( 1:n, n, replace=TRUE )\n    resampled_data = data[resample_indices,]\n    replicates[i] = compute_alphahat( resampled_data )\n  }\n  # Now use those replicates to estimate SD of alphahat and construct a 95% CI.\n  alphahat = compute_alphahat( data )\n  sd_alphahat = sd( replicates ); # estimate of the std dev of alphahat\n  CI = c(alphahat-1.96*sd_alphahat, alphahat+1.96*sd_alphahat)\n  # Finally, check if this CI caught alpha successfully.\n  # Return TRUE/FALSE accordingly.\n  return( (CI[1] &lt; alpha_true) & (alpha_true &lt; CI[2]) )\n}\n\n\nN_expt = 200; # number of CIs to construct.\nsuccesses = rep(NA,N_expt); # Keep track of whether or not each CI caught alpha.\nfor( i in 1:N_expt ) {\n  successes[i] = run_bootstrap_expt( 200 ); # Using B=200 replicates.\n}\nmean(successes)\n\n[1] 0.93\n\n\nOnce again, this should be approximately 0.95, up to the randomness in our experiment.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#limitations-and-extensions-of-the-bootstrap",
    "href": "bootstrap.html#limitations-and-extensions-of-the-bootstrap",
    "title": "40  Bootstrapping",
    "section": "40.7 Limitations and extensions of the bootstrap",
    "text": "40.7 Limitations and extensions of the bootstrap\nNow, the bootstrap is magic, at least when it works.\nBut the set of problems for which the “classical” bootstrap (i.e., the version of the bootstrap outlined above) works, is pretty limited.\nHere’s a simple example of where the bootstrap can fail.\n\n40.7.1 Example: estimating the distribution of the maximum\nSuppose that we see data \\(X_1,X_2,\\dots,X_n\\) drawn iid from some distribution, and we are interested in the distribution of \\[\nM = \\max\\{ X_1, X_2, \\dots, X_n \\}.\n\\] In particular, suppose we want to estimate \\(\\mathbb{E} M\\).\nThe bootstrap would suggest that we sample \\(X_1^*,X_2,^*,\\dots,X_n^*\\) with replacement from \\(X_1,X_2,\\dots,X_n\\) and compute \\[\nM^* = \\max\\{ X_1^*, X_2^*, \\dots, X_n^* \\}.\n\\] Repeating this \\(B\\) times, we get replicates \\(M_1^*,M_2^*,\\dots,M_B^*\\), and we can compute their standard deviation to estimate the standard deviation of our variable of interest \\(M\\).\nNow let’s implement that and see how well it works.\nWe’ll draw our data from a standard normal. First, let’s generate a bunch of copies of \\(M\\), so we can see its distribution (and its variance, in particular).\n\nn_samp = 25; # Number of data samples to take\nn_rep = 2000; # Number of replicates of M to create.\nM_reps = rep( NA, n_rep )\nfor( i in 1:n_rep ) {\n  data = rnorm(n=n_samp)\n  M_reps[i] = max(data)\n}\nhist( M_reps )\n\n\n\n\n\n\n\n\nThe true value of \\(\\mathbb{E} M\\) is actually very hard to compute, but we can use Monte Carlo estimation to approximate it:\n\nExpecM = mean(M_reps)\nExpecM\n\n[1] 1.973678\n\n\nIn particular, the (sample) standard deviation is\n\nsd(M_reps)\n\n[1] 0.5149964\n\n\nNow, let’s generate a new sample and run the bootstrap on it.\n\ndata = rnorm(n=n_samp)\nB = 200\nMboot_reps = rep( NA, B )\nfor( b in 1:B ){\n  resamp = sample( data, n_samp, replace=TRUE)\n  Mboot_reps[b] = max(resamp)\n}\nhist(Mboot_reps)\n\n\n\n\n\n\n\n\nYikes! That looks very different from the true distribution of \\(M\\) we saw above…\nLet’s just check the variance? Maybe it’s close to the observed \\(\\sigma_M \\approx 0.483\\)?\n\nsd(Mboot_reps)\n\n[1] 0.1923805\n\n\nCompare with\n\nsd(M_reps)\n\n[1] 0.5149964\n\n\nAnd if we try to construct a confidence interval, we get a CI\n\nM = max(data)\nseMboot = sd(Mboot_reps)\nCI = c( M-1.96*seMboot, M+1.96*seMboot )\nCI\n\n[1] 1.156103 1.910234\n\n\n\n(CI[1] &lt; ExpecM) & (ExpecM &lt; CI[2])\n\n[1] FALSE\n\n\nOkay, but we really have to run the experiment multiple times, so let’s package it up in a function…\n\n# A function to repeat the same experiment easily without having to repeat ourselves.\nrun_max_boot_expt = function( n, B ) {\n  data = rnorm(n)\n  M = max(data)\n  Mboot_reps = rep(NA,B)\n  for(b in 1:B) {\n    resamp = sample( data, n, replace=TRUE)\n    Mboot_reps[b] = max(resamp)\n  }\n  seMboot = sd(Mboot_reps)\n  CI = c( M-1.96*seMboot, M+1.96*seMboot )\n  return( CI[1] &lt; ExpecM & ExpecM &lt; CI[2] )\n}\n\n…and, just like we’ve done above, we’ll generate data samples a bunch of times and see how often our bootstrap CI catches the (estimated!) true value of \\(\\mathbb{E}M\\).\n\nNexpt = 500\nsuccesses = rep(NA,Nexpt)\nfor(i in 1:Nexpt) {\n  successes[i] = run_max_boot_expt( n_samp, 200 )\n}\nmean(successes)\n\n[1] 0.75\n\n\nHmm… Not ideal.\nThe problem is that the bootstrap tends (usually! it’s possible that we got lucky when knitting, but unlikely) to vastly underestimate the variance of “challenging” functions like the maximum.\nGenerally speaking, the bootstrap only works well when the statistic we are trying to work with is approximately normal about its expectation. The maximum of a collection of variables does obey a central limit theorem, but its limiting distribution is not a normal!\nSee here if you’re curious to learn more.\n\n\n40.7.2 A cautionary tale\nThere are, in fact, ways to modify the bootstrap to handle issues like resampling “challenging” functions like the maximum, but those are going to have to wait for your more advanced courses.\nThe above example is just a cautionary tale of how important it is to know the limitations of the methods that you are using. The bootstrap works amazingly for the problems that it is “right” for, but it is also demonstrably bad for certain other problems.\nGenerally speaking, it’s important to know what tool is right for what job!",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "bootstrap.html#review",
    "href": "bootstrap.html#review",
    "title": "40  Bootstrapping",
    "section": "40.8 Review",
    "text": "40.8 Review\nIn these notes we covered\n\nSituations where Parametric Estimation is inadequate\nA General method for bootstrap estimation\nBootstrap estimation for univariate data\nBootstrap estimation for bivariate data\nLimitations of bootstrapping",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>40</span>  <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html",
    "href": "R12_Bootstrap_Examples.html",
    "title": "41  R13_Bootstrap",
    "section": "",
    "text": "41.1 Simple Example - Estimating the mean of a Weibull Distribution\nThe Weibull distribution is a flexible random variable distribution which takes a shape and scale parameter. The Expected value of the distribution is not a pretty expression, but it’s true that the expected value of the sample mean is the mean of the RV. Suppose we have some data which comes from a Weibull shaped population\nplot(x=seq(0.1,20,.1), y=dweibull(seq(0.1,20,.1), 4,10), type=\"l\", main=\"Population Shape\")\n\n\n\n\n\n\n\nmyData &lt;- rweibull(15, shape=4, scale=10)\nmyData\n\n [1]  6.179585 12.353713  7.249329 10.732078 12.466363  8.658189  9.306232\n [8]  9.499164  7.633360  5.434905  9.739487  9.864806  6.277671 10.836992\n[15] 13.762623\n\nhist(myData)\nWe wish to come up with a 95% confidence interval for the population mean. We’ll use 10000 bootstrapped resamples\nB &lt;- 10000\nbootstrap.mean &lt;- vector(\"numeric\")\nfor(i in 1:B){\n  b.sample &lt;- sample(myData, size=length(myData), replace=TRUE)\n  bootstrap.mean[i]&lt;-mean(b.sample)  \n}\n\nhist(bootstrap.mean)\n\n\n\n\n\n\n\nquantile(bootstrap.mean, c(.025, .975))\n\n     2.5%     97.5% \n 8.143363 10.523373 \n\n#True mean?\n10*gamma(1+1/4)\n\n[1] 9.064025\nWhat if we wanted to use a MC estimate? Well we’d need to parameterize the weibull distribution. Finding estimates for the shape and scale are… annoying to do by hand. You would have to use software to fit these parameters using some method. The MASS library has a function fitdistr that can be used.\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nparam.hat &lt;- fitdistr(myData,\"weibull\")    \nNMC &lt;- 10000\nshape.hat &lt;- param.hat$estimate[1]\nscale.hat &lt;- param.hat$estimate[2]\nmc.mean &lt;- replicate(NMC, mean(rweibull(length(myData), shape=shape.hat, scale=scale.hat)))\nhist(mc.mean)\n\n\n\n\n\n\n\nquantile(mc.mean, c(.025, .975))\n\n     2.5%     97.5% \n 8.096548 10.577235",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#simple-example---estimating-the-mean-of-a-weibull-distribution",
    "href": "R12_Bootstrap_Examples.html#simple-example---estimating-the-mean-of-a-weibull-distribution",
    "title": "41  R13_Bootstrap",
    "section": "",
    "text": "41.1.1 A bootstrap estimate for the standard deviation of the population\nLet’s look at the standard deviation of the distribution\n\nbootstrap.sd &lt;- vector(\"numeric\")\nfor(i in 1:B){\n  b.sample &lt;- sample(myData, size=length(myData), replace=TRUE)\n  bootstrap.sd[i]&lt;-sd(b.sample)  \n}\n\nhist(bootstrap.sd)\n\n\n\n\n\n\n\nquantile(bootstrap.sd, c(.025, .975))\n\n    2.5%    97.5% \n1.665017 3.023119",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#another-bootstrap-estimation-of-the-standard-deviation",
    "href": "R12_Bootstrap_Examples.html#another-bootstrap-estimation-of-the-standard-deviation",
    "title": "41  R13_Bootstrap",
    "section": "41.2 Another Bootstrap estimation of the standard deviation",
    "text": "41.2 Another Bootstrap estimation of the standard deviation\nStart with some data. Where did it come from?\n\nmysteryData &lt;- c(0.26,10.87,5.05,0.25,4.03,19.66,1.64,4.85,6.64,4.01,0.76,4.82,9.53,4.11,2.05,0.47)\nhist(mysteryData)\n\n\n\n\n\n\n\n\nSuppose we want to estimate the population standard deviation. A good point estimate is of course the sample standard deviation\n\nsd(mysteryData)\n\n[1] 5.025347\n\n\nA 95% bootstrapped confidence interval can be produced easily because we don’t have to make any population assumptions.\n\nsd.boot &lt;- 0\nfor(i in 1:10000){\n  sd.boot[i] &lt;- sd(sample(mysteryData, replace=TRUE))\n}\nhist(sd.boot, breaks=50)\n\n\n\n\n\n\n\nquantile(sd.boot, c(0.025, 0.975))\n\n    2.5%    97.5% \n2.207912 7.161173",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#mc-vs-bootstrap-intervals",
    "href": "R12_Bootstrap_Examples.html#mc-vs-bootstrap-intervals",
    "title": "41  R13_Bootstrap",
    "section": "41.3 MC vs Bootstrap Intervals",
    "text": "41.3 MC vs Bootstrap Intervals\nLet’s compare MC estimation vs bootstrapping methods. The difference will become apparent when we make the correct decision about the model to use for the population.\n\n41.3.1 Correctly Identifying the Model\nSuppose the data is drawn from a normal distribution. We can actually compare 3 methods of estimating the variance of the population.\n\nThe classical statistical approach is to use a chi squared distribution for the sampling distribution of \\(S^2\\)\nThe Monte Carlo approach requires us to simulate new data from \\(N(\\hat{\\mu}, \\hat{\\sigma^2})\\)\nThe bootstrap approach has us re-sample from the initial data.\n\nWe’ll simulate using all three of these methods over and over to get coverage estimates as well as precision estimates (average width of the intervals)\n\nMC.coverage &lt;- FALSE; boot.coverage &lt;- FALSE; param.coverage &lt;- FALSE\nMC.width &lt;- 0; boot.width &lt;- 0; param.width &lt;- 0;\n\nfor(i in 1:500){\n  myData &lt;- rnorm(15, 37, 2) #The data is actually coming from a normal distr.\n  trueSigma2 &lt;- 2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- unname(quantile(MC.var, c(0.025, 0.975)))\n  Boot.ci &lt;- unname(quantile(Boot.var, c(0.025, 0.975)))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage    width\n1   Classic    0.942 7.577618\n2        MC    0.894 5.647650\n3 Bootstrap    0.808 4.664877\n\n\n\n\n41.3.2 Incorrectly identifying the model - a slight skew.\nWhat if the population is skewed. Take t population is skewed\n\nplot(density(rgamma(1000, 2, scale=2)), main=\"Population Shape\")\n\n\n\n\n\n\n\n\n\nMC.coverage &lt;- FALSE\nboot.coverage &lt;- FALSE\nparam.coverage &lt;- FALSE\n\nfor(i in 1:500){\n  myData &lt;- rgamma(15, 2, scale=2) #data is simulated from a gamma\n  trueSigma2 &lt;- 2*2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n  Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage    width\n1   Classic    0.850 15.37387\n2        MC    0.760 11.46172\n3 Bootstrap    0.724 11.29158\n\n\n\n\n41.3.3 Misidentifying the population - A very skewed population\n\nplot(density(rgamma(1000, .5, scale=2)), main=\"Population Shape - very skewed\")\n\n\n\n\n\n\n\n\n\nMC.coverage &lt;- FALSE\nboot.coverage &lt;- FALSE\nparam.coverage &lt;- FALSE\n\nfor(i in 1:200){\n  myData &lt;- rgamma(15, .5, scale=2)\n  trueSigma2 &lt;- .5*2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n  Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage     width\n1   Classic    0.620 10.838677\n2        MC    0.555  8.085137\n3 Bootstrap    0.615  8.264097\n\n\nBottom line - if we misidentify the model MC methods can underperform bootstrap. Why do all three of these do so bad though? Small sample size is the answer. Let’s dig further into the effect of sample size on the performance of bootstrap estimation.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#sensitivity-to-sample-size",
    "href": "R12_Bootstrap_Examples.html#sensitivity-to-sample-size",
    "title": "41  R13_Bootstrap",
    "section": "41.4 Sensitivity to sample size",
    "text": "41.4 Sensitivity to sample size\nWe will look at samples of size 10, 20, 30,…, 100 and how the three methods perform on this same population.\n\nn &lt;- seq(10, 100, 10)\nMC.rate &lt;- 0\nboot.rate &lt;- 0\nparam.rate &lt;- 0\nresults &lt;- data.frame(n, MC.rate, boot.rate, param.rate)\n\nfor(samplesize in n){\n  MC.coverage &lt;- FALSE\n  boot.coverage &lt;- FALSE\n  param.coverage &lt;- FALSE\n  \n  for(i in 1:200){\n    myData &lt;- rgamma(samplesize, .5, scale=2)\n    trueSigma2 &lt;- .5*2^2\n    MC.var &lt;- 0\n    Boot.var &lt;- 0\n    xbar &lt;- mean(myData)\n    sd &lt;- sd(myData)\n    for(j in 1:1000){\n      #Assuming the data was drawn from a normal population\n      MC.var[j] &lt;- var(rnorm(samplesize, xbar, sd))\n      Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n    }\n    MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n    Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n    param.ci &lt;- (samplesize-1)*var(myData)/qchisq(c(0.975, 0.025), samplesize-1)\n    MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n    boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n    param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  }\n  results[results$n==samplesize, 2:4]=c(mean(MC.coverage),\n                                        mean(boot.coverage),\n                                        mean(param.coverage))\n    \n}\n\nplot(x=results$n, y=results$MC.rate, type=\"l\", ylim=c(0,1), main=\"'95%' CI Coverage Rate vs Sample Size (skewed population)\",\n     xlab=\"Sample Size\", ylab=\"Coverage Rate\")\nabline(h=.95, lty=2)\nlines(x=results$n, y=results$boot.rate, col=\"blue\")\nlines(x=results$n, y=results$param.rate, col=\"red\")\nlegend(x=10, y=.4, legend=c(\"MC\",\"Boot\",\"Parametric\"), col=c(\"black\",\"blue\",\"red\"), lwd=2)\n\n\n\n\n\n\n\n\nThe larger sample size allows bootstrap confidence intervals to improve and improve in terms of coverage rate, approaching 95%. This improvement is not seen in the MC or parametric (classical) method. They are both suffering from mis-identifying the population shape, thinking the population is a normal distribution. The bootstrap does not make any population assumptions, and the sample better represents the population as the sample size grows so naturally resampling will provide more representative simulated samples as \\(n\\) increases.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#confidence-interval-for-the-difference-of-two-population-means",
    "href": "R12_Bootstrap_Examples.html#confidence-interval-for-the-difference-of-two-population-means",
    "title": "41  R13_Bootstrap",
    "section": "41.5 Confidence Interval for the difference of two population means",
    "text": "41.5 Confidence Interval for the difference of two population means\nExample: A mean experiment on crickets\nIn this experiment the treatment group of crickets were starved and we observed the time it took for the starved females to mate with males (they get to eat them after mating - do hungry females mate faster on average?)\n\nStarved&lt;-c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)\nFed&lt;-c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)\nboxplot(Starved, Fed)\n\n\n\n\n\n\n\n\nWe want to estimate \\(\\mu_{starved}-\\mu_{fed}\\) using bootstrapping\n\nB &lt;- 1000\ndiff.mean.boot &lt;- 0 #empty vector to store the estimated difference of means\nfor(i in 1:B){\n  #bootstrap both samples at the same time\n  boot.fed &lt;- sample(Fed, replace=TRUE)\n  boot.starved &lt;- sample(Starved, replace=TRUE)\n  #calculate the difference of means\n  diff.mean.boot[i] &lt;- mean(boot.starved) - mean(boot.fed)\n}\nhist(diff.mean.boot, breaks=50)\nabline(v=mean(Starved)-mean(Fed), col=\"red\")\n\n\n\n\n\n\n\nquantile(diff.mean.boot, c(0.025,.975))\n\n     2.5%     97.5% \n-38.81519   2.27542",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-estimation-of-correlation",
    "href": "R12_Bootstrap_Examples.html#bootstrap-estimation-of-correlation",
    "title": "41  R13_Bootstrap",
    "section": "41.6 Bootstrap estimation of Correlation",
    "text": "41.6 Bootstrap estimation of Correlation\nGenerate some data with a certain correlation matrix The variables each have a standard deviation of 1, and a correlation of .6 Thus covariance = \\(1\\times 1 \\times .6\\)\n\nlibrary(MASS)\nSigma &lt;- matrix(c(1, .6,\n                  .6, 1), byrow=TRUE, nrow=2)\ndf&lt;-as.data.frame(mvrnorm(n=15, mu=c(5,10), Sigma=Sigma))\nplot(df)\n\n\n\n\n\n\n\n\nGet a 95% confidence interval estimate of the correlation\n\nboot.correlations &lt;- 0\nB &lt;- 10000\n\nfor(i in 1:B){\n  #A bootstrapped sample\n  boot.df &lt;- df[sample(1:nrow(df), replace=TRUE),]\n  boot.correlations[i] = cor(boot.df$V1, boot.df$V2)\n}\nhist(boot.correlations)\n\n\n\n\n\n\n\n\nWe can use two methods to construct a 95% confidence interval.\n\nWe can take the 2.5th and 97.5th quantiles from the simulated correlations\nWe could find the standard deviation and take \\(\\hat\\rho \\pm 1.96 sd(\\hat{\\rho})\\)\n\n\nquantile(boot.correlations, c(.025, .975), names=FALSE)\n\n[1] 0.4133182 0.8799668\n\n#alternative\ncor(df$V1, df$V2) + c(-1,1)*1.96*sd(boot.correlations)\n\n[1] 0.4701656 0.9378815\n\n\nWhich method do we prefer? Let’s repeat the experiment to see how good each method’s coverage rate is. Our aim is 95%.\n\na.coverage &lt;- FALSE\nb.coverage &lt;- FALSE\nboot.correlations &lt;- 0\nB &lt;- 1000 #lowering to 1000\n\nfor(j in 1:1000){\n  Sigma &lt;- matrix(c(1,.6,.6,1), byrow=TRUE, nrow=2)\n  df&lt;-as.data.frame(mvrnorm(n=15, mu=c(5,10), Sigma=Sigma))\n  for(i in 1:B){\n    #A bootstrapped sample\n    boot.df &lt;- df[sample(1:nrow(df), replace=TRUE),]\n    boot.correlations[i] = cor(boot.df$V1, boot.df$V2)\n  }\n  a.CI &lt;- quantile(boot.correlations, c(.025, .975))\n  #alternative\n  b.CI &lt;- cor(df$V1, df$V2) + c(-1,1)*1.96*sd(boot.correlations)\n  \n  a.coverage[j] &lt;- a.CI[1] &lt;= .6 & a.CI[2] &gt;= .6\n  b.coverage[j] &lt;- b.CI[1] &lt;= .6 & b.CI[2] &gt;= .6\n}\nmean(a.coverage)\n\n[1] 0.916\n\nmean(b.coverage)\n\n[1] 0.882\n\n\nThey both under-cover, but the second method’s under-coverage is worse.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-linear-regression",
    "href": "R12_Bootstrap_Examples.html#bootstrap-linear-regression",
    "title": "41  R13_Bootstrap",
    "section": "41.7 Bootstrap Linear Regression",
    "text": "41.7 Bootstrap Linear Regression\nNow let’s bootstrap a linear regression model. Suppose the population has the relationship \\[Y = 10 + 4X + \\epsilon\\] Where \\(\\epsilon \\sim exp(.1)\\). We can generate a small sample from such a population.\n\n#Generate data from a model: y = 10 + 4x + e\nset.seed(2024)\nx &lt;- runif(25, 0, 10)\ne &lt;- rexp(25, .1)\ny &lt;- 10 + 4*x + e\n\nAnd look at the scatter plot:\n\nplot(x,y)\n\n\n\n\n\n\n\n\nNote here that the assumptions of the linear model are not true; the errors are not normally distributed but actually exponentially distributed. This becomes a little clearer when we look at the residual QQ plot:\n\nplot(lm(y~x), which=2)\n\n\n\n\n\n\n\n\nNow we will pretend we don’t know the true parameter values. What can we do with bootstrapping to help us with our estimate of the model? \\[ y = \\beta_0 + \\beta_1 X + \\epsilon\\] Fit a linear model first - save those coefficients. Fitting the model will be done with least squares estimation. Bootstrapping will be used\n\nas.vector(lm(y ~ 1 + x)$coeff)\n\n[1] 20.926612  4.256114\n\n\nNow let’s bootstrap the data, fit the coefficients and repeat, keeping track of all of them\n\nnB &lt;- 1000\nslopes &lt;- rep(0, nB)\nintercepts &lt;- rep(0,nB)\nn &lt;- length(x)\n\nfor(i in 1:nB){\n  boot.index &lt;- sample(1:n, n, replace=TRUE)\n  boot.x &lt;- x[boot.index]\n  boot.y &lt;- y[boot.index]\n  boot.coeffs &lt;- as.vector(lm(boot.y~1+boot.x)$coeff)\n  intercepts[i] &lt;- boot.coeffs[1]\n  slopes[i] &lt;- boot.coeffs[2]\n}\n\nPlot the data, and all of the bootstrap lines\n\nplot(x,y)\nfor(i in 1:nB){\n  abline(a=intercepts[i], b=slopes[i], col=rgb(.5,.5,.5,.05))\n}\npoints(x,y)\n\n\n\n\n\n\n\n\naverage the bootstrapped intercepts and slopes - let this be the bootstrapped linear model.\n\nmean(intercepts)\n\n[1] 20.73885\n\nmean(slopes)\n\n[1] 4.307377\n\nas.vector(lm(y ~ 1 + x)$coeff)\n\n[1] 20.926612  4.256114\n\n\nHow well does it match the original linear model? How well does it match the population model? Plot it all on the same plot.\n\nplot(x,y, col=\"gray\")\nabline(a=10+10, b=4, col=\"black\") #E(e)=10 so\nabline(a=mean(intercepts), b=mean(slopes), col=\"red\")\nabline(lm(y ~ 1 + x), col=\"blue\")\n\n\n\n\n\n\n\n\nNow construct a bootstrapped confidence interval for the intercept, and a bootstrap interval for the slope.\n\nquantile(intercepts, c(0.025, 0.975))\n\n    2.5%    97.5% \n13.54361 28.33806 \n\nquantile(slopes, c(0.025, 0.975))\n\n    2.5%    97.5% \n2.961448 6.113239 \n\n#compared to the parametric method\nconfint(lm(y ~ 1 + x))\n\n               2.5 %    97.5 %\n(Intercept) 6.577624 35.275601\nx           1.939825  6.572403\n\n\nCome up with a bootstrap confidence interval for E(Y|x=5.8)\n\n# This is the parametric model fit\nxy.fit &lt;- lm(y ~ 1 + x)\npredict(xy.fit, newdata = data.frame(x=5.8))\n\n       1 \n45.61207 \n\n#Using the bootstrapped linear model\nb.intercept &lt;- mean(intercepts)\nb.slope &lt;- mean(slopes)\n\n#point estimate\nb.intercept + b.slope * 5.8\n\n[1] 45.72164\n\n#Let's estimate y-hat for every single bootstrapped model fit and use that as a distribution of means / expected values of y\nb.y.hats &lt;- intercepts + slopes * 5.8\n\nquantile(b.y.hats, c(0.025, .975))\n\n    2.5%    97.5% \n40.08676 53.50911 \n\n#compared to a parametric estimate\npredict(xy.fit, newdata = data.frame(x=5.8), interval=\"confidence\")\n\n       fit      lwr      upr\n1 45.61207 38.81064 52.41351\n\n\nCome up with a bootstrap prediction interval for Y|x=5.8\n\n# NOW BY BOOTSTRAP 95%-PREDICTION INTERVAL\nB &lt;- 1000\npred &lt;- numeric(B)\ndata &lt;- data.frame(x,y)\nfor (i in 1:B) {\n  boot &lt;- sample(n, n, replace = TRUE)\n  fit.b &lt;- lm(y ~ x, data = data[boot,])\n  pred[i] &lt;- predict(fit.b, list(x = 5.8)) + sample(resid(fit.b), size = 1)\n}\nquantile(pred, c(0.025, 0.975))\n\n     2.5%     97.5% \n 32.55693 102.30715 \n\n#compared to a parametric estimate\npredict(xy.fit, newdata = data.frame(x=5.8), interval=\"prediction\")\n\n       fit      lwr      upr\n1 45.61207 11.14919 80.07496\n\n#What is the true interval for 95% likelihood?\n10+4*5.8 + qexp(c(.025, .975), .1)\n\n[1] 33.45318 70.08879",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-interval-for-sigma-in-linear-model",
    "href": "R12_Bootstrap_Examples.html#bootstrap-interval-for-sigma-in-linear-model",
    "title": "41  R13_Bootstrap",
    "section": "41.8 Bootstrap Interval for sigma in linear model",
    "text": "41.8 Bootstrap Interval for sigma in linear model\nWe can also use bootstrapping to estimate the sigma. Here however I will go back to a linear model with normal errors. Let’s suppose our population can be described by \\[Y = 10 + 4X + e\\] Where \\(\\epsilon \\sim N(0, 10^2)\\)\n\n#Generate data from a model: y = 10 + 4x + e\nset.seed(2024)\nx &lt;- runif(25, 0, 10)\ne &lt;- rnorm(25, 10)\ny &lt;- 10 + 4*x + e\n\nA point estimate is obtained from the residual standard error. The summary output from linear regression calls it sigma\n\nxy.fit &lt;- lm(y~x)\nsummary(xy.fit)$sigma   \n\n[1] 0.8869751\n\n\nWe can create bootstrap samples from our residuals to come up with a bootstrap distribution for the residual standard error\n\nresid.se &lt;- 0\nfor(i in 1:1000){\n  resid.se[i] &lt;- sqrt(sum(sample(resid(xy.fit), replace=TRUE)^2)/summary(xy.fit)$df[2])\n}\nquantile(resid.se, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6116832 1.1408085",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#draft-data",
    "href": "R12_Bootstrap_Examples.html#draft-data",
    "title": "41  R13_Bootstrap",
    "section": "41.9 Draft Data",
    "text": "41.9 Draft Data\n\ndraft &lt;- read.csv(\"data/Celtics_Heat_Game_6_Actual.csv\")\n\nThis dataset is the last post-season game between two NBA teams – Celtics and Heat – at DraftKings. The dataset contains four variables: Player, Roster Position, %Drafted, and FPTS.\n\nPlayer: NBA players’ names\nRoster Position: the position the player held in the DraftKings lineups, whether as Captain (CPT) or Utility (UTIL). DraftKings has a 1.5 multiplication power over the captain.\n%Drafted: the percentage of people drafted this player\nFPTS: Fantasy Points\n\nThis project focuses on the last two variables – %Drafted and FPTS – and tries to understand how strongly they correlate with repeated samplings.\nNow let’s repeat this on the draft model; come up with a bootstrapped linear regression model to predict fantasy points (FPTS) from % drafted (X.Drafted). Compare it to the parametric model\n\ndraft.intercepts &lt;- 0\ndraft.slopes &lt;- 0\n\nfor(i in 1:1000){\n  draft.boot &lt;- draft[sample(nrow(draft), replace=TRUE),]\n  draft.boot.fit &lt;- lm(FPTS ~ X.Drafted, data=draft.boot)\n  draft.intercepts[i] &lt;- coef(draft.boot.fit)[1]\n  draft.slopes[i] &lt;- coef(draft.boot.fit)[2]\n}\n#Bootstrap linear model\nmean(draft.intercepts)\n\n[1] -0.5601898\n\nmean(draft.slopes)\n\n[1] 89.73204\n\n#Parametric linear model\nlm(FPTS ~ X.Drafted, data=draft)\n\n\nCall:\nlm(formula = FPTS ~ X.Drafted, data = draft)\n\nCoefficients:\n(Intercept)    X.Drafted  \n    -0.6888      90.8991",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-hypothesis-test",
    "href": "R12_Bootstrap_Examples.html#bootstrap-hypothesis-test",
    "title": "41  R13_Bootstrap",
    "section": "41.10 Bootstrap Hypothesis Test",
    "text": "41.10 Bootstrap Hypothesis Test\nBootstrapped One Sample Test function: What it does is comes up with a bootstraped estimate of the sampling distribution of a test statistic without relying on the central limit theorem or a population model. The t.hat returned is the bootstraped test statistic distribution.\n\nbootstrap=function(x,n.boot){\n  n&lt;-length(x)\n  x.bar&lt;-mean(x)\n  t.hat&lt;-rep(0, n.boot) \n  #create vector that we will fill with \"t\" values\n  for (i in 1:n.boot){\n    x.star&lt;-sample(x, size=n, replace=TRUE)\n    x.bar.star&lt;-mean(x.star)\n    s.star&lt;-sd(x.star)\n    t.hat[i]&lt;-(x.bar.star-x.bar)/(s.star/sqrt(n))\n  }\n  return(t.hat)\n}\n\n\n41.10.1 Example: Steel Conduits\nSteel conduits are buried for 2 years and the maximum depth of corrosion is measured on each conduit. If the average max penetration exceeds 50 micrometers then the conduits have to be re-engineered.\nIs there evidence that the mean maxPen &gt; 50?\n\\(H_0: \\mu \\leq 50\\) \\(H_1: \\mu &gt; 50\\)\n\nmaxPen&lt;-c(53, 61.1, 49.6, 59.3, 57.2, 57.4, 46.2, 55.1, 63, 54, 61.9, 62.2, 52.4, 45.4, 57.7, 45.1)\n\nx.bar&lt;-mean(maxPen)\nsd.maxPen&lt;-sd(maxPen)\nt.obs = (x.bar - 50)/(sd.maxPen/sqrt(length(maxPen)))\nt.obs\n\n[1] 3.341184\n\nset.seed(1)\nn.boot&lt;-10000\nMaxPen.boot&lt;-bootstrap(x=maxPen, n.boot)\nhist(MaxPen.boot, breaks=50)\nabline(v=t.obs)\n\n\n\n\n\n\n\n#pvalue\nsum(MaxPen.boot&gt;=t.obs) \n\n[1] 59\n\nsum(MaxPen.boot&lt;=t.obs) \n\n[1] 9941\n\n#Right-tailed test p-value - mean calculates the proportion for me\nmean(MaxPen.boot&gt;=t.obs) \n\n[1] 0.0059\n\n#if it were a 2 tailed test\np_upper = mean(MaxPen.boot &gt;= t.obs)\np_lower = mean(MaxPen.boot &lt;= t.obs)\n2*min(p_upper, p_lower)\n\n[1] 0.0118\n\n\n\n\n41.10.2 Two Sample Bootstrap Test\nYou can use bootstrapping as opposed to permutation testing. The nice thing about this method is the null hypothesis does not have to assume that the populations are equal to each others - we can directly test whether the population means are unequal. We calculate the bootstrapped test statistic distribution by resampling from each sample with the appropriate sample size. This function calculates a t-type statistic, but you can modify it to use any 2 sample test statistic that is useful for quantifying evidence for the alternative hypothesis.\nBootstrap Function from 2 independent samples\n\nboottwo = function(dat1, dat2, nboot) {\n  bootstat = numeric(nboot)     #Make Empty Vector for t* to fill\n  obsdiff = mean(dat1) - mean(dat2)\n  n1 = length(dat1)\n  n2 = length(dat2)\n  for(i in 1:nboot) {\n    samp1 = sample(dat1, size = n1, replace = T)    #Sample From Sample Data\n    samp2 = sample(dat2, size = n2, replace = T)\n    bootmean1 = mean(samp1)\n    bootmean2 = mean(samp2)\n    bootvar1 = var(samp1)\n    bootvar2 = var(samp2)\n    bootstat[i] = ((bootmean1 - bootmean2) - obsdiff)/sqrt((bootvar1/n1) + (bootvar2/n2))         \n    #Compute and Save “bootstrap t” value\n  }\n  return(bootstat)\n}\n\n\n\n41.10.3 Example: A mean experiment on crickets\nIn this experiment the treatmetn group of crickets were starved and we observed the time it took for the starved females to mate with males (they get to eat them after mating - do hungry females mate faster on average?)\n\nStarved&lt;-c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)\nFed&lt;-c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)\nboxplot(Starved, Fed)\n\n\n\n\n\n\n\n\nA permutation test would make the null hypothesis assumption that the two populations are identically shaped, and we can see that really is not the case. No need to test that.\n\nt.Obs &lt;- (mean(Starved)-mean(Fed))/sqrt(var(Starved)/length(Starved)+var(Fed)/length(Fed))\n\nboot.t &lt;- boottwo(Starved, Fed, 10000)\nhist(boot.t)\nabline(v=t.Obs)\n\n\n\n\n\n\n\n#2-tailed p-value\nmean(boot.t&lt;t.Obs)*2\n\n[1] 0.1194",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>41</span>  <span class='chapter-title'>R13_Bootstrap</span>"
    ]
  },
  {
    "objectID": "bootstrap_practice.html",
    "href": "bootstrap_practice.html",
    "title": "42  Bootstrap Practice",
    "section": "",
    "text": "43 Practice Problems\n\nA researcher is studying salamanders in western Massachusetts, and wishes to estimate the average length, measured in centimeters, of adult female salamanders living in wetlands there. She and her team collect an independent sample of 36 salamanders, measuring each one (in centimeters) from head to tail and recording the measurement. The data is reproduced below.\n\n\nsalamander_lengths &lt;- c( 14.5, 13.9, 14.1, 14.2, 14.7, 13.8, 14.6, 16.0, 14.7,\n                         14.8, 15.1, 14.6, 14.4, 13.7, 12.1, 14.8, 11.0, 12.0,\n                         13.8, 14.3, 13.9, 13.4, 14.6, 14.5, 15.6, 15.2, 16.1,\n                         15.2, 13.8, 16.6, 13.6, 15.4, 12.5, 12.8, 14.1, 15.2);\n\nUse the bootstrap to construct a 95% confidence interval for the population mean height. You should use at least 200 bootstrap replicates.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- length(salamander_lengths)\nB &lt;- 200;\nbootreps &lt;- rep( NA, B );\nfor(bb in 1:B ) {\n  bootsamp &lt;- sample(salamander_lengths, size=n, replace=TRUE)\n  bootreps[bb] &lt;- mean(bootsamp)\n}\n\nSEboot &lt;- sd(bootreps)\n\nmuhat &lt;- mean(salamander_lengths)\nc( muhat - 1.96*SEboot, muhat+1.96*SEboot )\n\n[1] 13.87555 14.65779\n\n\n\n\n\n\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of \\(n\\) observations.\n\n\n\nWhat is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nWhat is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample?\nArgue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1 − 1/n)^n\\).\nWhen \\(n = 5\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 100\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 10,000\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nCreate a plot that displays, for each integer value of \\(n\\) from 1 to 100,000, the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe.\nWe will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\n\nstore &lt;- rep(NA, 10000)\nfor(i in 1:10000){\nstore[i] &lt;- sum(sample (1:100 , rep=TRUE) == 4) &gt; 0\n}\nmean(store)\n\n[1] 0.6377\n\n\nComment on the results obtained.\n\nWe will now consider the Boston housing data set, from the ISLR2 library.\n\n\n\nBased on this data set, provide an estimate for the population mean of medv. Call this estimate ˆμ.\nProvide an estimate of the standard error of ˆμ. Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\nNow estimate the standard error of ˆμ using the bootstrap. How does this compare to your answer from (b)?\nBased on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95% confidence interval using the formula [ˆμ − 2SE(ˆμ), ˆμ + 2SE(ˆμ)].\nBased on this data set, provide an estimate, ˆμmed, for the median value of medv in the population.\nWe now would like to estimate the standard error of ˆμmed. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\nBased on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity ˆμ0.1. (You can use the quantile() function.)\nUse the bootstrap to estimate the standard error of ˆμ0.1. Comment on your findings.https://ritsokiguess.site/pasias/the-bootstrap.html\n\nhttps://ritsokiguess.site/pasias/the-bootstrap.html\n\n\n44 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Bootstrapping",
      "<span class='chapter-number'>42</span>  <span class='chapter-title'>Bootstrap Practice</span>"
    ]
  },
  {
    "objectID": "RV_summary.html",
    "href": "RV_summary.html",
    "title": "Appendix A — Random Variable Summary",
    "section": "",
    "text": "A.1 RV summary",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Variable Summary</span>"
    ]
  },
  {
    "objectID": "RV_summary.html#rv-summary",
    "href": "RV_summary.html#rv-summary",
    "title": "Appendix A — Random Variable Summary",
    "section": "",
    "text": "\\(X\\)\nBinomial\nGeometric\nPoisson\nDiscrete Uniform\nNormal\n(Continuous) Uniform\nExponential\n\n\n\n\nType\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nContinuous\nContinuous\nContinuous\n\n\nParameters\n\\(n\\), \\(p\\)\n\\(p\\)\n\\(\\lambda\\)\n\\(a\\), \\(b\\)\n\\(\\mu\\), \\(\\sigma^2\\)\n\\(a\\), \\(b\\)\n\\(\\lambda\\)\n\n\nDescription\nNumber of successes in \\(n\\) independent trials with \\(p\\) probability of success for each trial  (Note: Bernoulli is just Binomial with \\(n\\!=\\!1\\))\nNumber of failures BEFORE the first success while independently repeating trial with \\(p\\) probability of success\nCount of number of occurrences of an event with constant mean rate \\(\\lambda\\) that’s independent of previous occurrences\n\\(n\\)-sided fair die\nNormal distributions usually arise from CLT (i.e. they’re processes that are the sum of many smaller independent processes)\nGeneralizing \\(n\\)-sided fair die to a continuous interval\nWaiting time between Poisson events\n\n\nOutcomes\n\\(0,1,\\ldots,n\\)\n\\(0,1,\\ldots\\)\n\\(0,1,\\ldots\\)\n\\(a,a\\!+\\!1,\\ldots,b\\)\n\\((-\\infty,\\infty)\\)\n\\([a,b]\\)\n\\([0,\\infty)\\)\n\n\nPDF/PMF at \\(k\\)\n\\({n\\choose k}p^k(n-p)^{n-k}\\)\n\\(p(1-p)^k\\)\n\\(\\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\n\\(\\frac1{b-(a-1)}\\)\n\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac12\\left(\\frac{x-\\mu}\\sigma\\right)^2}\\)\n\\(\\frac1{b-a}\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(P(X\\le k)\\)\n\n\\(1-(1-p)^{\\lfloor k\\rfloor+1}\\)\n\n\\(\\frac{\\lfloor k\\rfloor-(a-1)}{b-(a-1)}\\)\n\n\\(\\frac{x-a}{b-a}\\)\n\\(1-e^{-\\lambda x}\\)\n\n\nMean\n\\(np\\)\n\\(\\frac{1-p}p\\)\n\\(\\lambda\\)\n\\(\\frac{a+b}2\\)\n\\(\\mu\\)\n\\(\\frac{a+b}2\\)\n\\(\\frac1\\lambda\\)\n\n\nVariance\n\\(np(1-p)\\)\n\\(\\frac{1-p}{p^2}\\)\n\\(\\lambda\\)\n\\(\\frac{(b-(a-1))^2-1}{12}\\)\n\\(\\sigma^2\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\\(\\frac1{\\lambda^2}\\)\n\n\nR functions\ndbinom, pbinom, qbinom, rbinom\ndgeom, pgeom, qgeom, rgeom\ndpois, ppois, qpois, rpois\nsample\ndnorm, pnorm, qnorm, rnorm\ndunif, punif, qunif, runif\ndexp, pexp, qexp, rexp",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Random Variable Summary</span>"
    ]
  }
]