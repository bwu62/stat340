[
  {
    "objectID": "index.html#collection-of-shiny-apps",
    "href": "index.html#collection-of-shiny-apps",
    "title": "STAT 340: Data Science II",
    "section": "0.1 Collection of Shiny Apps",
    "text": "0.1 Collection of Shiny Apps\n\nNormal PDF CDF Inverse CDF \nBinomial One/Two Tail Test Power\nLogistic Model ROC Plot\nModel Selection Visualization"
  },
  {
    "objectID": "rv_practice.html",
    "href": "rv_practice.html",
    "title": "5  Probability and Random Variables Practice",
    "section": "",
    "text": "6 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "rv_practice.html#phone-repair",
    "href": "rv_practice.html#phone-repair",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.1 Phone Repair",
    "text": "6.1 Phone Repair\n(From Introduction to Probability and Statistics for Data Science) The repair of a broken cell phone is either completed on time or it is late and the repair is either done satisfactoily or unsatisfactorily. What is the sample space for a cell phone repair?"
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-i",
    "href": "rv_practice.html#dice-probabilities-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.2 Dice Probabilities I",
    "text": "6.2 Dice Probabilities I\nWhat is the probability that at least one of the following events occurs on a single throw of two fair six-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7."
  },
  {
    "objectID": "rv_practice.html#dice-probabilities-ii",
    "href": "rv_practice.html#dice-probabilities-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.3 Dice Probabilities II",
    "text": "6.3 Dice Probabilities II\nWhat is the probability that at least one of the following events occurs on a single throw of two fair four-sided dice?\n\nThe dice total 5.\nThe dice total 6.\nThe dice total 7."
  },
  {
    "objectID": "rv_practice.html#using-wikipedia-in-college",
    "href": "rv_practice.html#using-wikipedia-in-college",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 Using Wikipedia in college",
    "text": "6.4 Using Wikipedia in college\nA recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \\(X\\) equal the number of students in a random sample of size \\(n = 31\\) who have used Wikipedia as a source.\n\nHow is \\(X\\) distributed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming independence in sampling, and a representative sample, we can use a Binomial distribution with \\(n=31\\) and \\(p=0.447\\).\n\n\n\n\nSketch the probability mass function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(dbinom(0:31, 31,.447), names=0:31, ylab=\"probability\", main=\"PMF of Binomial(31,.447)\")\n\n\n\n\n\n\n\n\nSketch the cumulative distribution function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(pbinom(0:31, 31, .447), type=\"s\", ylab=\"cumulative prob.\", main=\"CDF of Binomial(31, .447)\")\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\) is equal to 17.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(17, 31, .447)\n\n[1] 0.07532248\n\n\n\n\n\n\nFind the probability that \\(X\\) is at most 13.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npbinom(13, 31, .447)\n\n[1] 0.451357\n\n\n\n\n\n\nFind the probability that \\(X\\) is bigger than 11.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(12:31, 31, .447))\n\n[1] 0.8020339\n\n#or\npbinom(11, 31, .447, lower.tail=FALSE)\n\n[1] 0.8020339\n\n#or\n1-pbinom(11, 31, .447)\n\n[1] 0.8020339\n\n\n\n\n\n\nFind the probability that \\(X\\) is at least 15.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X at least 15)\nsum(dbinom(15:31,31,.447))\n\n[1] 0.406024\n\n\n\n\n\n\nFind the probability that \\(X\\) is between 16 and 19, inclusive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(16:19, 31, .447))\n\n[1] 0.2544758\n\n\n\n\n\n\nGive the mean of \\(X\\), denoted \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(X)=n*p\n31*.447\n\n[1] 13.857\n\n#or you can also do this (but it's too much work)\nsum( (0:31) * dbinom(0:31, 31, .447))\n\n[1] 13.857\n\n\n\n\n\n\nGive the variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Var(X) = n * p * (1-p)\n31 * .447 * (1-.447)\n\n[1] 7.662921\n\n#or - if you want (but why would you want to?)\nsum((0:31 - 31*.447)^2 * dbinom(0:31, 31, .447))\n\n[1] 7.662921\n\n\n\n\n\n\nGive the standard deviation of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#SD(X) = sqrt(n*p*(1-p))\nsqrt(31*.447*(1-.447))\n\n[1] 2.768198\n\n\n\n\n\n\nFind \\(\\mathbb{E}(4X+51.324)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(4X+51.324) = 4*E(X)+51.324\n4*(31*.447) + 51.324\n\n[1] 106.752"
  },
  {
    "objectID": "rv_practice.html#choose-the-distribution",
    "href": "rv_practice.html#choose-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 Choose the distribution",
    "text": "6.5 Choose the distribution\n\nFor the following situations, decide what the distribution of \\(X\\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)\n\n\nWe shoot basketballs at a basketball hoop, and count the number of shots until we make a basket. Let X denote the number of missed shots. On a normal day we would typically make about 37% of the shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of missed shots before the first basket, assuming independence, can be modeled by a Geometric random variable with parameter \\(p=.37\\).\n\n\n\n\nIn a local lottery in which a three digit number is selected randomly, let X be the number selected.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming that all 3 digit numbers are equally likely (A reasonable assumption) the number selected can be modeled by a discrete uniform distribution with minimum 100 and maximum 999.\n\n\n\n\nWe drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let X be the number of times the cup lands perfectly right side up.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we drop the cup 20 times, and the result each time is independent with a constant probability of landing right side up, the number of times it does can be modeled by a Binomial random variable with parameters \\(n=20\\) and \\(p\\) (unknown).\n\n\n\n\nWe toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let X denote the number of missed shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGeometric random variable (unknown parameter value for \\(p\\)).\n\n\n\n\nWorking for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let X count the number of containers that illegally contain contraband.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTechnically we should use a hypergeometric random variable for this situation (since it is a small population size of 557), but since we do not cover the hypergeometric the closest random variable we have is the binomial.\n\n\n\n\nAt the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let X denote the number of birds in the bush.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a discrete random variable, but without other information it’s hard to say. The distribution is likely unimodal and bell-curved. You could probably model this using a normal distribution rounded off to the nearest integer.\n\n\n\n\nWe count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe observation window is the circular area, and the 10 minutes during observation. Assuming the rate of rainfall is constant, the number of raindrops in the circle can be modeled using a Poisson random variable.\n\n\n\n\nWe count the number of moth eggs on our window screen.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCounting indicates a discrete random variable. A binomial or a rounded normal distribution may be appropriate, but we lack enough details to be sure.\n\n\n\n\nWe count the number of blades of grass in a one square foot patch of land.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe location of the sprouting grass could be modeled well by a Poisson random variable - the \\(\\lambda\\) parameter would likely be very large, in the range of 1000 or 10000, and as such the distribution would look very much like a normal distribution.\n\n\n\n\nWe count the number of pats on a baby’s back until (s)he burps.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we define a geometric random variable, we let \\(X\\) be the number of failures before the first success. The last pat (that causes the burp) is the success in this context. So we could use a geometric random variable, but we would have to add 1 to it in order to count all burps (the failures + 1 success)."
  },
  {
    "objectID": "rv_practice.html#valid-pmf",
    "href": "rv_practice.html#valid-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.19 Valid PMF",
    "text": "6.19 Valid PMF\n(From Introduction to Probability and Statistics for Data Science) Let \\(X\\) have pmf \\[p(x)=\\begin{cases}0.2,&x=1\\\\0.3,&x=2\\\\0.5,&x=4\\\\0,&\\text{otherwise}\\end{cases}\\] Show that this is a valid pmf."
  },
  {
    "objectID": "rv_practice.html#two-dice-pmf",
    "href": "rv_practice.html#two-dice-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.20 Two Dice PMF",
    "text": "6.20 Two Dice PMF\nLet \\(X\\) denote the sum of two fair, six-sided dice. Specify the domain that \\(X\\) can take on and then write out the equation for the PMF and show that it is valid."
  },
  {
    "objectID": "rv_practice.html#a-uniform-pmf",
    "href": "rv_practice.html#a-uniform-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 A Uniform PMF",
    "text": "6.8 A Uniform PMF\nLet \\(X\\) have discrete uniform PMF on the values \\(x\\in\\left\\{-1,0,1\\right\\}\\).\n\nWrite the equation for its PMF.\nFind \\(Pr[X&lt;-1]\\) and \\(Pr[X\\leq -1]\\)\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X \\geq 0]\\)\nCalculate the CDF from the PDF. Write out an expression for \\(F(x)\\) and plot the PMF and CDF. ## PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that."
  },
  {
    "objectID": "rv_practice.html#dice-difference",
    "href": "rv_practice.html#dice-difference",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.22 Dice Difference",
    "text": "6.22 Dice Difference\nTwo dice are tossed. Let \\(X\\) be the absolute difference in the number of dots facing up.\n\n\nFind and plot the pmf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#It may be simplest to calculate all possible values of X. \nx &lt;- vector(\"numeric\")\nfor(i in 1:6){\n  for(j in 1:6){\nx[length(x)+1] = abs(i-j)\n  }\n}\n#now that we have all equally likely values, we can just calculate the pmf in a prop.table\npX &lt;- prop.table(table(x))\n#And create a barplot.\nbarplot(pX)\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\leq 2\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The probability that X &lt;= 2 is easy to find using the pmf\n#The columns are named with strings, so we can convert 0 1 and 2 to strings to pull out the proper probabilities.\nsum(pX[as.character(0:2)])\n\n[1] 0.6666667\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\) and \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The expected value and variance can be calculated from the pmf.\n\n(EX &lt;- sum(0:5 * pX))\n\n[1] 1.944444\n\n(VarX &lt;- sum((0:5 - EX)^2 * pX))\n\n[1] 2.052469\n\n#or by taking the mean and population variance of the x values themselves\nmean(x)\n\n[1] 1.944444\n\nmean((x-mean(x))^2)\n\n[1] 2.052469"
  },
  {
    "objectID": "rv_practice.html#voltage-random-variable",
    "href": "rv_practice.html#voltage-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.23 Voltage random variable",
    "text": "6.23 Voltage random variable\nA modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term that is drawn from the set \\(\\{0,-1,-2,-3\\}\\) with respective probabilities \\(\\{4/10, 3/10, 2/10, 1/10\\}\\).\n\n\nFind the pmf of the output \\(Y\\) of the channel.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Let X be the noise\nk &lt;- seq(0,-3)\npk &lt;- c(4/10, 3/10, 2/10, 1/10)\n\ny &lt;- sort(2+k)\npy &lt;- pk[order(2+k)]\n\nbarplot(height=py, names=y, main=\"pmf of Y\")\n\n\n\n\n\n\n\n\nWhat is the probability that the channel’s output is equal to the input of the channel?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis happens when there’s no noise, with probability 4/10.\n\n\n\n\nWhat is the probability that the channel’s output is positive?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Interpreting 'positive' to be strictly positive, not zero:\nsum(py[y&gt;0])\n\n[1] 0.7\n\n\n\n\n\n\nFind the expected value and variance of \\(Y\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n(EY &lt;- sum(y*py))\n\n[1] 1\n\n(VarY &lt;- sum((y-EY)^2*py))\n\n[1] 1"
  },
  {
    "objectID": "rv_practice.html#golf-score",
    "href": "rv_practice.html#golf-score",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.24 Golf Score",
    "text": "6.24 Golf Score\nOn a given day, your golf score takes values from the numbers 1 through 10 with equal probability of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let \\(X_1, X_2\\) and \\(X_3\\) be the scores that you get, and let \\(X\\) be the minimum of these three scores.\n\n\nShow that for any discrete random variable \\(X\\) \\(p_X(k)=\\mathbb{P}(X &gt; k-1) - \\mathbb{P}(X&gt;k)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k-1) = P(X \\geq k) =P(X = k)+P(X &gt; k)\\), thus \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)\\).\n\n\n\n\nWhat is the probability that \\(\\mathbb{P}(X_1&gt;k)\\) for \\(k=1,\\ldots,10\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X_1&gt;k)=\\frac{10-k}{10}\\)\n\n\n\n\nUse (a) to determine the pmf \\(p_X(k)\\) for \\(k=1,\\ldots,10\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k) = P(X_1,X_2,X_3 &gt; k) = P(X_1 &gt; k)P(X_2&gt;k)P(X_3&gt;k)\\)\nThis means \\(P(X&gt;k) =\\frac{(10-k)^3}{10^3}\\), and \\(P(X&gt;k-1)=\\frac{(11-k)^3}{10^3}\\). From the previous result, \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)=\\frac{(11-k)^3-(10-k)^3}{10^3}\\)\n\n\n\n\nWhat is the average score improvement if you play just for one day compared with playing for three days and taking the minimum?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is asking to take the difference of the two expected values. It’s obvious that \\(E(X_1)=5.5\\); We need to find the expected value of \\(X\\).\n\nx &lt;- 1:10\npx &lt;- ((11-x)^3-(10-x)^3)/10^3\n#double check\nsum(px)\n\n[1] 1\n\n(EX &lt;- sum(x*px))\n\n[1] 3.025\n\n5.5-EX\n\n[1] 2.475\n\n\nThe average (expected) point improvement when going from a 1 day point to a minimum of 3 days is 2.475."
  },
  {
    "objectID": "rv_practice.html#functions-of-a-random-variable",
    "href": "rv_practice.html#functions-of-a-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.25 Functions of a random variable",
    "text": "6.25 Functions of a random variable\nLet \\(g(X) = \\begin{cases}1 & \\text{if }X&gt;10\\\\0 & \\text{otherwise}\\end{cases}\\) and \\(h(X) = \\begin{cases}X-10 & \\text{if }X-10&gt;0\\\\0 & \\text{otherwise}\\end{cases}\\)\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_k = p_1/k\\), find \\(\\mathbb{E}\\left[g(X)\\right]\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nk &lt;- 1:15\np1 &lt;- 1/(sum(1/k))\npk &lt;- p1/k\n\ng &lt;- function(x){\n  return(as.numeric(x&gt;10))\n}\n\nsum(g(k)*pk)\n\n[1] 0.1173098\n\n\n\n\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_{k+1} = p_k/2\\) (for \\(k&gt;1\\)), find \\(\\mathbb{E}\\left[h(X)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nh &lt;- function(x){\n  return(max(0, x-10))\n}\n\np1 &lt;- 1/(sum(1/2^(k-1)))\npk &lt;- p1*(1/2^(k-1))\n\nsum(h(k)*pk)\n\n[1] 5"
  },
  {
    "objectID": "rv_practice.html#voltage-ii",
    "href": "rv_practice.html#voltage-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.26 Voltage II",
    "text": "6.26 Voltage II\nA voltage \\(X\\) is uniformly distributed on the set \\(\\{-3,\\ldots,3,4\\}\\).\n\n\nFind the mean and variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- -3:4\npx &lt;- 1/length(x)\n\n(EX &lt;- sum(x*px))\n\n[1] 0.5\n\n(VarX &lt;- sum((x-EX)^2*px))\n\n[1] 5.25\n\n\n\n\n\n\nFind the mean and variance of \\(Y=-2X^2+3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny &lt;- -2*x^2+3\n\n(EY &lt;- sum(y*px))\n\n[1] -8\n\n(VarY &lt;- sum((y-EY)^2*px))\n\n[1] 105\n\n\n\n\n\n\nFind the mean and variance of \\(W=\\text{cos}(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- cos(pi*x/8)\n(EW &lt;- sum(w*px))\n\n[1] 0.6284174\n\n(VarW &lt;- sum((w-EW)^2*px))\n\n[1] 0.1050915\n\n\n\n\n\n\nFind the mean and variance of \\(Z=\\text{cos}^2(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz &lt;- w^2\n(EZ &lt;- sum(z*px))\n\n[1] 0.5\n\n(VarZ &lt;- sum((z-EZ)^2*px))\n\n[1] 0.125"
  },
  {
    "objectID": "rv_practice.html#discrete-random-variable-problems",
    "href": "rv_practice.html#discrete-random-variable-problems",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 Discrete Random Variable Problems",
    "text": "6.14 Discrete Random Variable Problems\n\n\nIf \\(X\\) is \\(\\text{Poisson}(\\lambda)\\), compute \\(\\mathbb{E}\\left[1/(X+1)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be handled mathematically. The formula for \\(E(1/(X+1))\\) is\n\\(E(1/(X+1))=\\sum_{x=0}^{\\infty}\\frac{1}{x+1}\\frac{\\lambda^{x}}{x!}e^{-\\lambda}=\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x}}{(x+1)!}e^{-\\lambda}\\)\nThe trick is to get get the summation to equal 1 and simplify. We multiply by \\(\\lambda/\\lambda\\)\n\\(E(1/(X+1))=\\frac{1}{\\lambda}\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x+1}}{(x+1)!}e^{-\\lambda}\\)\nNow we can make a change of variables: \\(y=x+1\\) and thus \\(x=0\\) becomes \\(y=1\\)\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}\\sum_{y=1}^{\\infty}\\frac{\\lambda^{y}}{y!}e^{-\\lambda}\\)\nThe only thing missing is that the summation starts at \\(y=1\\) instead of \\(y=0\\), But for \\(Y \\sim Poisson(\\lambda)\\), \\(P(Y=0)=e^{-\\lambda}\\) so this summation is \\(1-e^{-\\lambda}\\).\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}(1-e^{-\\lambda})\\)\n\n\n\n\nIf \\(X\\) is \\(\\text{Bernoulli}(p)\\) and \\(Y\\) is \\(\\text{Bernoulli}(q)\\), computer \\(\\mathbb{E}\\left[(X+Y)^3\\right]\\) assuming \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\((X+Y)^3 = X^3+3X^2Y+3XY^2+Y^3\\) so \\(E[(X+Y)^3]=E(X^3)+3E(X^2)E(Y)+3E(X)E(Y^2)+E(Y^2)\\)\nthis is due to independence. Since \\(X\\) an \\(Y\\) are independent, so are \\(X^2\\) and \\(Y\\), and \\(X\\) and \\(Y^2\\). \\(E(X)=E(X^2)=E(X^3)=p\\) and \\(E(Y)=E(Y^2)=E(Y^3)=q\\). Thus \\(E[(X+Y)^3]=p+6pq+q\\)\n\n\n\n\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\Delta(\\theta)=\\mathbb{E}\\left[(X-\\theta)^2\\right]\\). Find \\(\\theta\\) that minimizes the error \\(\\Delta(\\theta)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can expand the expected value and attempt to find the minimum with respect to \\(\\theta\\). \\(E[(X-\\theta)^2]=E[X^2-2\\theta X+\\theta^2]=E(X^2)-2\\theta\\mu+\\theta^2\\). Recall that \\(Var(X)=E(X^2)-\\mu^2\\) so \\(E(X^2)=\\sigma^2+\\mu^2\\) So we can write \\(\\Delta(\\theta)=\\sigma^2 + \\mu^2-2\\theta\\mu + \\theta^2\\) We want to find what value of \\(\\theta\\) minimizes this function - derivative! \\(\\Delta'(\\theta)=-2\\mu+2\\theta=0\\) thus \\(\\theta=\\mu\\) minimizes this.\n\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) are independent uniform random variables in \\(\\{0,1,\\ldots,100\\}\\). Evaluate \\(\\mathbb{P}\\left[\\text{min}(X_1,\\ldots, X_n) &gt; l\\right]\\) for any \\(l \\in \\{0,1,\\ldots,100\\}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(Y=\\min(X_1, \\ldots, X_n)\\) If \\(P(Y &gt;l)\\), that means the minimum exceeds \\(l\\), so all of the values \\(&gt;l\\). \\(P(X_1 &gt; l)=(100-l)/101\\) - you can check: \\(P(X_1&gt;0)=100/101\\). This is the same calculation for each \\(i\\). So \\(P(Y&gt;l)=\\dfrac{(100-l)^n}{101^n}\\).\n\n\n\n\nConsider a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\). \\(p_X(k)={n \\choose k} p^k(1-p)^{n-k}\\). Show that the mean is \\(\\mathbb{E}X= np\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(X)=\\sum_{k=0}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nThe first term is zero so we could write\n\\(\\sum_{k=1}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nNow the following is a fact that is needed but perhaps not well known. It’s the equivalence of \\(k{n \\choose k}=n{n-1 \\choose k-1}\\). We make this subsitution\n\\(\\sum_{k=1}^n n{n-1 \\choose k-1} p^k(1-p)^{n-k}=np\\sum_{k=1}^n {n-1\\choose k-1}p^{k-1}(1-p)^{n-k}\\)\nWe could write \\(n-k=(n-1)-(k-1)\\) and we’ll be making some substitutions: \\(m=n-1\\) and \\(j=k-1\\). This lets us write\n\\(np\\sum_{j=0}^m {m \\choose j}p^j(1-p)^{m-j}=np\\) because the summation =1, as it is just the sum of the pmf of a binomial.\n\n\n\n\n(not for 340) Consider a geometric random variable \\(X\\) with parameter \\(p\\). \\(p_X(k)=p(1-p)^k\\) for \\(k=0,1,\\ldots\\). Show that its mean is \\(\\mathbb{E}X=(1-p)/p\\).\n(not for 340) Consider a Poisson random variable \\(X\\) with parameter \\(\\lambda\\). \\(p_X(k)=\\dfrac{\\lambda^k}{k!}e^{-\\lambda}\\). Show that \\(\\text{Var}X=\\lambda\\).\n(not for 340) Consider the uniform random variable \\(X\\) over values \\(1,2,\\ldots, L\\). Show that \\(\\text{Var}X=\\dfrac{L^2-1}{12}\\). Hint: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\) and \\(\\sum_{i=1}^n i^2=\\frac{n^3}{3}+\\frac{n^2}{2}+\\frac{n}{6}\\)"
  },
  {
    "objectID": "rv_practice.html#hard-drive-failures",
    "href": "rv_practice.html#hard-drive-failures",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 Hard Drive Failures",
    "text": "6.15 Hard Drive Failures\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. The initial cost of building the player is $50, and the manufacturer offers a 1-year warranty. Within one year, the customer can ask for a free repair up to 12 times.\n\n\nLet \\(X\\) be the number of months when the player fails. What is the PMF of \\(X\\)? Hint: \\(\\mathbb{P}(X = 1)\\) may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying \\(X = 1\\) is equivalent to saying that there is only one failure in the entire 12-month period.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of failures should follow a binomial distribution with \\(n=12, p=1/12\\). Thus \\(P(X=k)={n \\choose k}(\\frac{1}{12})^k(\\frac{11}{12})^{n-k}\\)\n\n\n\n\nWhat is the average cost per player?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe cost is \\(50+20X\\) So \\(E(50+20X)=50+20E(X)=50+20\\cdot 12(\\frac{1}{12})=70\\)"
  },
  {
    "objectID": "rv_practice.html#bit-errors",
    "href": "rv_practice.html#bit-errors",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 Bit Errors",
    "text": "6.16 Bit Errors\n\nA binary communication channel has a probability of bit error of \\(p = 10^{-6}\\). Suppose that transmission occurs in blocks of 10,000 bits. Let \\(N\\) be the number of errors introduced by the channel in a transmission block.\n\n\n\nWhat is the PMF of \\(N\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(N\\) follows a binomial distribution with \\(n=10000\\) and \\(p=.000001\\)\n\n\n\n\nFind \\(\\mathbb{P}(N = 0)\\) and $(N b $ 3)$.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(0, 10000, .000001)\n\n[1] 0.9900498\n\npbinom(3, 10000, .000001)\n\n[1] 1\n\n\n\n\n\n\nFor what value of \\(p\\) will the probability of 1 or more errors in a block be 99%?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved directly. \\(P(N \\geq 1)=1-P(X=0)=1-(1-p)^{10000}\\). If we set this to .99 we can solve for \\(p\\) : \\(.99=1-(1-p)^{10000}\\) so \\(.01 = (1-p)^{10000}\\) so \\(p=1-.01^{1/10000}\\)\n\n1-.01^(1/10000)\n\n[1] 0.000460411"
  },
  {
    "objectID": "rv_practice.html#processing-orders",
    "href": "rv_practice.html#processing-orders",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 Processing Orders",
    "text": "6.17 Processing Orders\nThe number of orders waiting to be processed is given by a Poisson random variable with parameter \\(\\alpha = \\frac{\\lambda}{n\\mu}\\), where \\(\\lambda\\) is the average number of orders that arrive in a day, \\(\\mu\\) is the number of orders that an employee can process per day, and n is the number of employees. Let \\(N; = 5\\) and \\(N&lt; = 1\\). Find the number of employees required so the probability that more than four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few \\(n\\)’s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlambda=5\nmu=1\nppois(4, lambda/(1:10 * mu), lower.tail=FALSE)\n\n [1] 0.5595067149 0.1088219811 0.0275432568 0.0091242792 0.0036598468\n [6] 0.0016844329 0.0008589296 0.0004739871 0.0002784618 0.0001721156\n\n#With 3 employees P(X&gt;4) is less than 10%."
  },
  {
    "objectID": "rv_practice.html#normal-random-variable",
    "href": "rv_practice.html#normal-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.18 Normal Random Variable",
    "text": "6.18 Normal Random Variable\nIf \\(Z\\sim \\text{Normal}(\\mu=0, \\sigma^2=1^2)\\) find\n\n\n\\(\\mathbb{P}(Z &gt; 2.64)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(2.64, 0, 1, lower.tail=FALSE)\n\n[1] 0.004145301\n\n\n\n\n\n\n\\(\\mathbb{P}(0 \\leq Z &lt; 0.87)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(.87)-pnorm(0)\n\n[1] 0.3078498\n\n\n\n\n\n\n\\(\\mathbb{P}(|Z| &gt; 1.39)\\) (Hint: draw a picture)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(1.39, lower.tail=FALSE)*2\n\n[1] 0.1645289"
  },
  {
    "objectID": "rv_practice.html#identify-the-distribution",
    "href": "rv_practice.html#identify-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.19 Identify the Distribution",
    "text": "6.19 Identify the Distribution\nFor the following random experiments, decide what the distribution of X should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).\n\n\nWe throw a dart at a dart board. Let X denote the squared linear distance from the bullseye to the where the dart landed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume the dart lands somewhere on the board, and any point is equally likely (not a good assumption for a skilled dart thrower). The probability density would be proportional to the distance to the center squared - Suppose the dart board has radius \\(R\\). Let \\(X\\) be the distance to the dart from the bullseye. Then \\(P(X&lt;r)=\\pi r^2 / (\\pi R^2)=(r/R)^2\\) . The question then is what is \\(P(X^2&lt;r)\\)? Well, take a square root of both sides. \\(=P(X &lt; \\sqrt{r})=\\frac{r}{R^2}\\). This is a uniform distribution’s CDF.\n\n\n\n\nWe randomly choose a textbook from the shelf at the bookstore and let P denote the proportion of the total pages of the book devoted to exercises.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA random proportion you might want to use uniform(0,1) however this is assuming that each proportion is equally likely. This is actually a great example for a beta distribution. Beta distributions are continuous distributions that can be parameterized to model a random proportion and the distribution can can be made to be skewed in different ways.\n\n\n\n\nWe measure the time it takes for the water to completely drain out of the kitchen sink.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s assume the sink is filled to the maximum. We drain the sink and start our timer. In this case, it’s reasonable to model the length of time to drain as a normal distribution.\n\n\n\n\nWe randomly sample strangers at the grocery store and ask them how long it will take them to drive home.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe time it takes to go home could be modeled by a gamma distribution since it is a continuous distribution capped below at 0 and it is a useful way to model the length of time a random process takes to complete."
  },
  {
    "objectID": "rv_practice.html#normal-random-variable-ii",
    "href": "rv_practice.html#normal-random-variable-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.20 Normal Random Variable II",
    "text": "6.20 Normal Random Variable II\nLet \\(X\\) be a Gaussian random variable with \\(\\mu=5\\) and \\(\\sigma^2=16\\).\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(2\\leq X \\leq 7)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X&gt;4)\npnorm(4, mean=5, sd=4, lower.tail=FALSE)\n\n[1] 0.5987063\n\n#P(2 &lt;= X &lt;= 7)\npnorm(7, 5, 4)-pnorm(4,5,4)\n\n[1] 0.2901688\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X &lt; a)=0.8869\\), find \\(a\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.88695, 4)\n\n[1] 5.210466\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X&gt;b)=0.1131\\), find \\(b\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.1131, 5, 4, lower.tail=FALSE)\n\n[1] 9.840823\n\n\n\n\n\n\nIf \\(\\mathbb{P}(13 &lt; X \\leq c)=0.0011\\), find \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First find the probability less than 13\np13 &lt;- pnorm(13, 5, 4)\n#now we can find the quantile for p13+.0011\nqnorm(p13+.0011, 5, 4)\n\n[1] 13.08321\n\n#double check\npnorm(13.08321,5,4)-pnorm(13,5,4)\n\n[1] 0.001100025"
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf",
    "href": "rv_practice.html#a-continuous-cdf",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.27 A Continuous CDF",
    "text": "6.27 A Continuous CDF\nConsider a cdf\n\\(F_X(x)-\\begin{cases}0,&\\text{if }x &lt; -1\\\\ 0.5 & \\text{if }-1 \\leq x &lt; 0\\\\(1+x)/2 & \\text{if }0 \\leq x &lt; 1\\\\1&\\text{otherwise}\\end{cases}\\)\nFind \\(\\mathbb{P}(X &lt; -1)\\), \\(\\mathbb{P}(-0.5 &lt; X &lt; 0.5)\\), and \\(\\mathbb{P}(X&gt;0.5)\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X &lt; -1) = 0 because F(x) only goes up to .5 at x=-1, not when x &lt; -1\n\n#P(-.5 &lt; X &lt; .5) = F(.5) - F(-.5)\n(1+.5)/2 - .5\n\n[1] 0.25\n\n#P(X &gt; 0.5) = 1-P(X&lt;.5) = 1-F(.5) \n1- (1+.5)/2\n\n[1] 0.25"
  },
  {
    "objectID": "rv_practice.html#a-continuous-cdf-ii",
    "href": "rv_practice.html#a-continuous-cdf-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.28 A Continuous CDF II",
    "text": "6.28 A Continuous CDF II\nLet \\(X\\) have PDF \\[f(x) = \\begin{cases} 1-x/2,&0\\leq x \\leq 2\\\\0,&\\text{otherwise}\\end{cases}\\]\n\nSketch the PDF and show geometrically that this is a valid PDF.\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X\\geq 0]\\)\nFind \\(Pr[X&gt;1]\\) and \\(Pr[X\\geq 1]\\)\nUse calculus or software to calculate the CDF from the PDF. Write the expression for \\(F(x)\\) and plot the PDF and CDF.\nUse calculus or software to calculate the expected value of \\(X\\)."
  },
  {
    "objectID": "rv_practice.html#two-random-variables",
    "href": "rv_practice.html#two-random-variables",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.29 Two Random Variables",
    "text": "6.29 Two Random Variables\nSuppose that \\(X\\) and \\(Y\\) are random variables with \\(\\mathbb{E}(X)=12\\) and $(Y)=8.\n\nFind \\(\\mathbb{E}(X-Y)\\)\nFind \\(\\mathbb{E}(5X-6Y)\\)\nIs it possible to determine \\(\\mathbb{E}(X^2)\\) with the given information? Explain."
  },
  {
    "objectID": "rv_practice.html#pmf-formula",
    "href": "rv_practice.html#pmf-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.1 PMF Formula",
    "text": "7.1 PMF Formula\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=1,2,\\ldots\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis was done above; because \\(\\sum_{i=1}^\\infty 1/2^k = 1\\), the value of \\(c\\) must be \\(1\\).\n\n\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(6\\leq X \\leq 8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X&gt;4) = 1-P(X \\leq 3)=1-(\\frac12 + \\frac14 + \\frac18)\\)\n\n1-(1/2+1/4+1/8)\n\n[1] 0.125\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe expected value can be calculated by taking the sum \\(\\sum k p_k = \\sum_{k=1}^\\infty \\frac{k}{2^k}\\) which we can show using facts from calculus equals 2. Why? Well, as long as \\(|p|&lt;1, \\sum_{k=1}^{\\infty}p^k=\\frac{p}{1-p}\\) (this is a geometric series). If we take a derivative of both sides we get \\(\\sum_{k=1}^\\infty kp^{k-1}=\\frac{1}{(1-p)^2}\\). Multiply both sides by \\(p\\) to get \\(\\sum_{k=1}^\\infty kp^{k}=\\frac{p}{(1-p)^2}\\). In our case, \\(p=\\frac12\\). Plugging this in we get \\(\\frac{.5}{.5^2}=2\\)."
  },
  {
    "objectID": "rv_practice.html#pmf-formula-ii",
    "href": "rv_practice.html#pmf-formula-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.2 PMF Formula II",
    "text": "7.2 PMF Formula II\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=-1,0,1,2,3,4,5\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sum of the probabilities are \\(c(2 + 1 + \\frac{1}{2}+\\frac{1}{4}+\\frac{1}8+\\frac{1}{16}+\\frac{1}{32})=c\\frac{127}{32}\\) so \\(c=\\frac{32}{127}\\).\n\n\n\n\nFind \\(\\mathbb{P}(1\\leq X &lt; 3)\\) and \\(\\mathbb{P}(1 &lt; X \\leq 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc=32/127\nk = seq(-1,5)\npk=c/2^k\nnames(pk) &lt;- k\nsum(pk[as.character(2)])\n\n[1] 0.06299213\n\nsum(pk[as.character(2:5)])\n\n[1] 0.1181102\n\n\n\n\n\n\nFind \\(\\mathbb{P}(X^3 &lt; 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(X^3 &lt; 5\\) that means \\(X^3 \\leq 4\\) and thus \\(X \\leq 4^{1/3}\\approx 1.587\\)\n\nsum(pk[k&lt;=4^(1/3)])\n\n[1] 0.8818898\n\nsum(pk[k^3&lt;5])\n\n[1] 0.8818898\n\n\n\n\n\n\nFind the pmf and the cdf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(height=pk, names=k, main='pmf of X')\n\n\n\nbarplot(height=cumsum(pk), names=k, main='cdf of X')"
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-i",
    "href": "rv_practice.html#properties-of-expectation-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.3 Properties of Expectation I",
    "text": "7.3 Properties of Expectation I\nFor a continuous random variable \\(X\\) and constant \\(c\\), prove that \\(\\mathbb{E}(cX)=c\\mathbb{E}(X)\\)"
  },
  {
    "objectID": "rv_practice.html#properties-of-expectation-ii",
    "href": "rv_practice.html#properties-of-expectation-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.4 Properties of Expectation II",
    "text": "7.4 Properties of Expectation II\nFor a continuous random variable \\(X\\) and constants \\(a,b\\), prove that \\(\\mathbb{E}(aX+b)=a\\mathbb{E}(X)+b\\)"
  },
  {
    "objectID": "rv_practice.html#two-fair-coins",
    "href": "rv_practice.html#two-fair-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 Two Fair Coins",
    "text": "6.4 Two Fair Coins\nAlex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets.\n\n\nFind and sketch the joint PMF \\(p_{X,Y} (x, y)\\).\nFind the marginal PMF \\(p_X(x)\\) and \\(p_Y (y)\\).\nFind the conditional PMF \\(P_{X|Y} (x | y)\\). Does \\(P_{X|Y} (x | y) = P_X(x)\\)? Why or why not?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- 0:1\npx &lt;- c(.25, .75) \n\ny&lt;- 0:1\npy &lt;- c(.75, .25)\n\npmf &lt;- px %*% t(py)\nrownames(pmf) &lt;- x\ncolnames(pmf) &lt;- y \naddmargins(pmf)\n\n         0      1  Sum\n0   0.1875 0.0625 0.25\n1   0.5625 0.1875 0.75\nSum 0.7500 0.2500 1.00\n\n\n\nprop.table(pmf, 2)\n\n     0    1\n0 0.25 0.25\n1 0.75 0.75\n\n\nEach column gives P(X=x|Y=y) for the two values of y; you can see that they are the same; the reason is because the value of X and Y are independent."
  },
  {
    "objectID": "rv_practice.html#two-fair-dice",
    "href": "rv_practice.html#two-fair-dice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 Two Fair Dice",
    "text": "6.5 Two Fair Dice\nTwo fair dice are rolled. Find the joint PMF of \\(X\\) and \\(Y\\) when\n\n\n\\(X\\) is the larger value rolled, and \\(Y\\) is the sum of the two values.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmax(die1, die2)\noutcomes$y &lt;- die1+die2\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             2          3          4          5          6          7\n  1 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.05555556 0.02777778 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.05555556 0.05555556 0.02777778 0.00000000\n  4 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556\n   \n             8          9         10         11         12\n  1 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  4 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000\n  5 0.05555556 0.05555556 0.02777778 0.00000000 0.00000000\n  6 0.05555556 0.05555556 0.05555556 0.05555556 0.02777778\n\n\n\n\n\n\n\\(X\\) is the smaller, and \\(Y\\) is the larger value rolled.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmin(die1, die2)\noutcomes$y &lt;- pmax(die1,die2)\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             1          2          3          4          5          6\n  1 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n  2 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556\n  3 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556\n  4 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778"
  },
  {
    "objectID": "rv_practice.html#signal-and-noise",
    "href": "rv_practice.html#signal-and-noise",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.6 Signal and Noise",
    "text": "6.6 Signal and Noise\nLet \\(Y = X+N\\), where \\(X\\) is the input, \\(N\\) is the noise, and \\(Y\\) is the output of a system. Assume that \\(X\\) and \\(N\\) are independent random variables. It is given that \\(E[X] = 0\\), \\(Var[X] = \\sigma^2_X\\), \\(E[N] = 0\\), and \\(Var[N] = \\sigma^2_N\\).\n\n\nFind the correlation coefficient \\(\\rho\\) between the input \\(X\\) and the output \\(Y\\) .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\rho(X,Y)= \\dfrac{Cov(X,Y)}{SD(X)SD(Y)}=\\dfrac{E(XY)-E(X)E(Y)}{\\sigma_X \\cdot \\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(E(XY)=E(X^2+XN)=E(X^2)+E(X)E(N)\\). Because \\(E(X)=E(N)=0\\) this simplifies to\n\\(\\rho(X,y)=\\dfrac{E(X^2)}{\\sigma_x\\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(Var(X)=E(X^2)-E(X)^2 = E(X^2)\\) so we can replace the numerator with \\(\\sigma^2_X\\). So\n\\(\\rho(X,Y) = \\sqrt{\\dfrac{\\sigma^2_X}{\\sigma^2_X+\\sigma^2_N}}\\)\n\n#example: sigma_X = 5, sigma_N=2\nX &lt;- rnorm(10000, 0, 5)\nN &lt;- rnorm(10000, 0, 2)\ncor(X, X+N)\n\n[1] 0.9304768\n\nsqrt(5^2/(5^2+2^2))\n\n[1] 0.9284767\n\n\n\n\n\n\nSuppose we estimate the input \\(X\\) by a linear function \\(g(Y ) = aY\\) . Find the value of a that minimizes the mean squared error \\(E[(X − aY )^2]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E[(X-aY)^2]=E[X^2-2aXY+a^2Y^2]=E[X^2]-2aE[XY]+a^2E[Y^2]\\)\nBecause \\(E(Y)=0\\), \\(E[Y^2]=Var(Y)=\\sigma_X^2+\\sigma_N^2\\). We already have that \\(E(X^2)=\\sigma_X^2\\) and \\(E(XY)=\\sigma_X^2\\). Thus\n$E[(X-aY)^2]=(_X^2+_N2)a2- 2_X^2a+_X^2 $\nThis is a quadratic function in \\(a\\), and the vertex can be found at \\(-\\frac{B}{2A}\\) or in this case \\(a^*=\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\)\n\n\n\n\nExpress the resulting mean squared error in terms of \\(\\eta = \\sigma^2_X/\\sigma^2_N\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPlugging this in for \\(a\\) we get\n\\(E[(X-aY)^2]=(\\sigma_X^2+\\sigma_N^2)\\dfrac{\\sigma_X^4}{(\\sigma_X^2+\\sigma_N^2)^2}-2\\dfrac{\\sigma_X^4}{\\sigma_X^2+\\sigma_N^2}+\\sigma_X^2=\\sigma_X^2\\left(1-\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\right)\\)\n\\(=\\sigma_X^2\\left(\\dfrac{\\sigma_N^2}{\\sigma_X^2+\\sigma_N^2}\\right)=\\dfrac{\\sigma_X^2}{\\eta+1}\\)"
  },
  {
    "objectID": "rv_practice.html#cat-genetics",
    "href": "rv_practice.html#cat-genetics",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.7 Cat Genetics",
    "text": "6.7 Cat Genetics\nThe gene that controls white coat color in cats, KIT , is known to be responsible for multiple phenotypes such as deafness and blue eye color. A dominant allele W at one location in the gene has complete penetrance for white coat color; all cats with the W allele have white coats. There is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. However, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors.\n\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes.\nAbout 73% of white cats with two blue eyes are deaf\n40% of white cats with one blue eye are deaf.\nOnly 19% of white cats with other eye colors are deaf.\n\n\nCalculate the prevalence of deafness among white cats.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn.blue &lt;- c(0,1,2)\np.n.blue &lt;- c(.60, .30, .10)\n\np.deaf.given.b &lt;- c(.19, .40, .73) #for 0, 1 ,2 blue eyes\n\n#P(Deafness) = P(0b)*P(deaf|0b) + P(1b)*P(deaf|1b) + P(2b)*P(deaf|2b)\n(p.deaf &lt;- sum(p.n.blue * p.deaf.given.b))\n\n[1] 0.307\n\n#Check\nnCats &lt;- 10000\nnblue &lt;- sample(n.blue, size=nCats, replace=TRUE, prob=p.n.blue)\nisdeaf &lt;- FALSE\nfor(i in 1:nCats){\n  isdeaf[i] &lt;- runif(1) &lt; p.deaf.given.b[nblue[i]+1]\n}\nmean(isdeaf)\n\n[1] 0.3088\n\n\n\n\n\n\nGiven that a white cat is deaf, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | deaf) = P(2b)*P(deaf|2b) / p(deaf)\np.n.blue[3] * p.deaf.given.b[3] / p.deaf\n\n[1] 0.237785\n\n#check\nmean(nblue[isdeaf]==2)\n\n[1] 0.2270078\n\n\n\n\n\n\nSuppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.\nWhat is the prevalence of blindness among deaf, white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\np.blind.given.nodeaf &lt;- 0.10\np.blind.given.deaf.and.nblue &lt;- c(0.20, 0.4, 0.2) #for 0, 1, 2 blue eyes\n\n#P(blind & deaf) = P(0b)*P(deaf|0b)*P(blind|deaf&0b)+\n#  P(1b)*P(deaf|1b)*P(blind|deaf&1b)+\n#  P(2b)*P(deaf|2b)*P(blind|deaf&2b)+\np.blind.and.deaf &lt;- sum(p.n.blue * p.deaf.given.b * p.blind.given.deaf.and.nblue)\n\n#P(blind | deaf ) = P(blind & deaf) / P(deaf)\n(p.blind.given.deaf = p.blind.and.deaf / p.deaf)\n\n[1] 0.2781759\n\n#check\nisBlind &lt;- FALSE\nfor(i in 1:nCats){\n  if(!isdeaf[i]){\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.nodeaf\n  } else {\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.deaf.and.nblue[nblue[i]+1]\n  }\n}\n#check\nmean(isBlind[isdeaf])\n\n[1] 0.2898316\n\n\n\n\n\n\nWhat is the prevalence of blindness among white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(blind) = P(deaf & blind) + P(nondeaf & blind)\n\n#P(nondeaf & blind ) = P(nondeaf) * P(blind | nondeaf)\np.blind.and.nondeaf &lt;- (1-p.deaf)*p.blind.given.nodeaf\n\n(p.blind &lt;- p.blind.and.deaf + p.blind.and.nondeaf)\n\n[1] 0.1547\n\n#check\nmean(isBlind)\n\n[1] 0.1565\n\n\n\n\n\n\nGiven that a cat is white and blind, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | blind) = P(2b & blind) / p(blind)\n#numerator: P(2b & blind) = P(2b) * [P(deaf|2b) * p(blind|deaf & 2b) + P(nondeaf|2b) * p(blind|nondeaf & 2b)]\np.blind.and.2b &lt;- p.n.blue[3] * (p.deaf.given.b[3]*p.blind.given.deaf.and.nblue[3] + \n (1-p.deaf.given.b[3]) * p.blind.given.nodeaf)\n\n(p.2b.given.blind = p.blind.and.2b / p.blind)\n\n[1] 0.1118293\n\n#check\nmean(nblue[isBlind]==2)\n\n[1] 0.1022364"
  },
  {
    "objectID": "rv_practice.html#gss-survey-i",
    "href": "rv_practice.html#gss-survey-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 GSS Survey I",
    "text": "6.8 GSS Survey I\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable?\n\n\nLinda is a banker.\nLinda is a banker and considers herself a liberal Democrat.\n\nTo answer this question we will use data from the GSS survey found at https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv.\nThe code for “Banking and related activities” in the indus10 variable is 6870. The values of the column sex are encoded like this:\n1: Male, 2: Female\nThe values of polviews are on a seven-point scale:\n1 Extremely liberal\n2 Liberal\n3 Slightly liberal\n4 Moderate\n5 Slightly conservative\n6 Conservative\n7 Extremely conservative\nDefine “liberal” as anyone whose political views are 3 or below. The values of partyid are encoded:\n0 Strong democrat\n1 Not strong democrat\n2 Independent, near democrat\n3 Independent\n4 Independent, near republican\n5 Not strong republican\n6 Strong republican\n7 Other party\nYou need to compute:\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngss &lt;- read.csv(\"https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv\")\n#banker : indus10==6870\n#female : sex == 2\n#liberal: polviews &lt;= 3\n#democrat: partyid &lt;= 2\n\n#In my reading, I am interpreteing that 'Linda is female' is given from the context;\n\n#P(banker | female) \nmean(gss[gss$sex==2,]$indus10==6870)\n\n[1] 0.02116103\n\n#P(liberal banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3,]$indus10==6870)\n\n[1] 0.01723195\n\n#P(liberal Dem banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3 & gss$partyid &lt;= 1,]$indus10==6870)\n\n[1] 0.01507289"
  },
  {
    "objectID": "rv_practice.html#gss-survey-ii",
    "href": "rv_practice.html#gss-survey-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.9 GSS Survey II",
    "text": "6.9 GSS Survey II\nCompute the following probabilities:\n\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$partyid&lt;=1,]$polviews&lt;=3)\n\n[1] 0.389132\n\n\n\n\n\n\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews&lt;=3,]$partyid&lt;=1)\n\n[1] 0.5206403"
  },
  {
    "objectID": "rv_practice.html#gss-survey-iii",
    "href": "rv_practice.html#gss-survey-iii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.10 GSS Survey III",
    "text": "6.10 GSS Survey III\nThere’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65. For these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\nCompute the following probabilities. For each statement, think about whether it is expressing a conjunction, a conditional probability, or both. For the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &lt;30 & gss$polviews &lt;=3)\n\n[1] 0.06579428\n\n\n\n\n\n\nWhat is the probability that a young person is liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$age &lt;30,]$polviews &lt;=3)\n\n[1] 0.3385177\n\n\n\n\n\n\nWhat fraction of respondents are old conservatives?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &gt;65 & gss$polviews &gt;= 5)\n\n[1] 0.06226415\n\n\n\n\n\n\nWhat fraction of conservatives are old?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews &gt;=5,]$age &gt;65)\n\n[1] 0.1820933"
  },
  {
    "objectID": "rv_practice.html#two-child-paradox",
    "href": "rv_practice.html#two-child-paradox",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.11 Two Child Paradox",
    "text": "6.11 Two Child Paradox\nSuppose you meet someone and learn that they have two children. You ask if either child is a girl and they say yes. What is the probability that both children are girls? (Hint: Start with four equally likely hypotheses.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBefore we know anything about their two kids, the number of girls \\(X\\) they have could be 0, 1 or 2. Simplifying the scenario with the ‘equally likely hypothesis’ means that we assume each kid has a 50% chance of being a girl, independently. Thus the probabilities are 0.25, 0.5 and 0.25 respectively.\nIf we learn that at least one of the kids is a girl, that tells us that the first possibility, that \\(x=0\\) is not possible. Thus \\(P(X=2 | X&gt;0) = .\\dfrac{25}{.5+.25}= \\frac{1}{3}\\)**"
  },
  {
    "objectID": "rv_practice.html#monty-hall",
    "href": "rv_practice.html#monty-hall",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.12 Monty Hall",
    "text": "6.12 Monty Hall\nThere are many variations of the Monty Hall problem. For example, suppose Monty always chooses Door 2 if he can, and only chooses Door 3 if he has to (because the car is behind Door 2).\n\n\nIf you choose Door 1 and Monty opens Door 2, what is the probability the car is behind Door 3?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#3 equally likely possibilities:\n#C1  -&gt; Monty chooses Door 2\n#C2  -&gt; Monty cannot choose Door 2, chooses Door 3\n#C3  -&gt; Monty chooses Door 2\n\n#So if Monty chooses Door 2, either C1 or C3 must be the case, each equally likely. \n#Thus P(C3 | Monty chooses 2) = .50\n\n\n\n\n\nIf you choose Door 1 and Monty opens Door 3, what is the probability the car is behind Door 2?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#If he chose Door 3, that means that it must be the case that the car is behind \n#door 2; he would *ONLY* choose door 3 in that case.\n\n#Thus P(C2 | Monty chooses 3) = 1.0"
  },
  {
    "objectID": "rv_practice.html#mms",
    "href": "rv_practice.html#mms",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.13 M&Ms",
    "text": "6.13 M&Ms\nM&M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M’s, changes the mixture of colors from time to time. In 1995, they introduced blue M&M’s.\n\nIn 1994, the color mix in a bag of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan.\nIn 1996, it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n\nSuppose a friend of mine has two bags of M&M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n(Hint: The trick to this question is to define the hypotheses and the data carefully.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(G|94) = .10, P(G|96) = .20\\) \\(P(Y|94) = .20, P(Y|96) = .14\\) \\(P(94)=P(96)=.50\\) assuming equally likely. It’s important to realize only one of two situations could have occurred:\n\nSituation 1: A G was chosen from the 94 bag and a Y was chosen from the 96 bag\nSituation 2: A Y was chosen from the 94 bag and a G was chosen from the 96 bag.\n\nThe corresponding probabilities are \\((.10)(.14)=.014\\) and \\((.20)(.20)=.04)\\). The question could be stated: What is the probability that Sit.1 occurred given that either Sit1 or Sit2 occured.\nThe likelihood is \\(.014 / (.014+.04) = .2592593\\)"
  },
  {
    "objectID": "rv_practice.html#two-coins",
    "href": "rv_practice.html#two-coins",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 Two Coins",
    "text": "6.14 Two Coins\nSuppose you have two coins in a box. One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides. You choose a coin at random and see that one of the sides is heads. What is the probability that you chose the trick coin?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is actually similar to the “family with two girls” problem. The equally likely coins are “HT” and “HH”.\n\\(P(Heads | HH) = 1\\) \\(P(Heads | TH) = .5\\)\nBayes Theorem tells us that \\(P(HH | Heads) = \\dfrac{P(HH)*P(Heads|HH)}{P(HH)P(Heads|HH)+P(TH)P(Heads|TH)}=\\dfrac{.5}{.5+.25}=\\frac23\\)\nSeems counter intuitive! If I pick a random coin from the two, you already know that one of the sides is a head without looking at it. Somehow then, when you look at just one side of the coin, seeing a H makes you 67% sure that it is the trick coin. The reason is that seeing the head side is like flipping it once and getting a H. That is less likely to occur with the fair coin, hence this outcome lends evidence to the “trick coin” hypothesis."
  },
  {
    "objectID": "rv_practice.html#online-sales-i",
    "href": "rv_practice.html#online-sales-i",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 Online Sales I",
    "text": "6.15 Online Sales I\n(From Introduction to Probability and Statistics for Data Science) Customers can buy one of four produts, each having its own web page with a ``buy’’ link. When they click on the link, they are redirected to a common page containing a registration and payment orm. Once there the customer eithe rbuys the desired product (labeled 1,2,3 or 4) or they fail to complete and a sale is lost. Let event \\(A_i\\) be that a customer is on produt \\(i\\)’s web page and let event \\(B\\) be the event that the customer buys the product. For the purposes of this problem, assume that each potential customer visits at most one product page and so he or she buys at most one produt. For the probabilities shown in the table below, find the probability that a customer buys a product.\n\n\n\nProduct \\((i)\\)\n\\(Pr[B|A_i]\\)\n$Pr[A_i]\n\n\n\n\n1\n\\(0.72\\)\n\\(0.14\\)\n\n\n2\n\\(0.90\\)\n\\(0.04\\)\n\n\n3\n\\(0.51\\)\n\\(0.11\\)\n\n\n4\n\\(0.97\\)\n\\(0.02\\)"
  },
  {
    "objectID": "rv_practice.html#online-sales-ii",
    "href": "rv_practice.html#online-sales-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 Online Sales II",
    "text": "6.16 Online Sales II\nContinuing from the previous problem, if a random purchase is selected, find the probability that it was item 1 that was purchased. Do the same for items 2, 3 and 4."
  },
  {
    "objectID": "rv_practice.html#uav",
    "href": "rv_practice.html#uav",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 UAV",
    "text": "6.17 UAV\nConsider a new type of commercial unmanned aerial vehicle (UAV) that has been outfitted with a transponder so that if it crashes it can easily be found and reused. Other older UAVs do not have transponders. Eighty percent of al lUAVs are recovered and, of those recovered, 75% have a transponder. Further, of those not recovered, 90% do not have a transponder. Denote recovery as \\(R+\\) and failure to recover as \\(R-\\). Denote having a transponder as \\(T+\\) and not having a transponder as \\(T-\\). Find\n\n\\(Pr[T+]\\)\nThe probability of not recovering a UAV given that it has a transponder."
  },
  {
    "objectID": "rv_practice.html#disease-screening",
    "href": "rv_practice.html#disease-screening",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.18 Disease Screening",
    "text": "6.18 Disease Screening\nSuppose that the test for a disease has sensitivity 0.90 and specificity 0.999. The base rate for the disease is 0.002. Find\n\nThe probability that someone selected at random from the population tests positive.\nThe probability that the person has the diseases given a positive test result (the positive predictive value).\nThe probability that the person does not have the disease given a negative test result (the negative predictive value)."
  },
  {
    "objectID": "rv_practice.html#pmf-practice",
    "href": "rv_practice.html#pmf-practice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.21 PMF practice",
    "text": "6.21 PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that."
  },
  {
    "objectID": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "href": "rv_practice.html#variance-of-a-rv-from-the-pmf",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.5 Variance of a RV from the pmf",
    "text": "7.5 Variance of a RV from the pmf\nLet \\(X\\) be a random variable with pmf \\(p_k = 1/2^k\\) for \\(k=1,2,\\ldots\\).\n\nFind \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe variance is \\(E(X^2)-(EX)^2=E(X^2)-4\\). The expected value of \\(X^2\\) can be derived, though it’s not so fun…\nStart by taking the equation \\(\\sum_k kp^k = \\frac{p}{(1-p)^2}\\) and take a derivative again. We get \\(\\sum_k k^2 p^{k-1} = \\frac{(1-p)^2+2p(1-p)}{(1-p)^4}\\). Multiply through by \\(p\\) to get \\(\\sum_k k^2 p^k = \\frac{p(1-p)^2+2p^2(1-p)}{(1-p)^4}\\). If we let \\(p=\\frac12\\) we have found \\(E(X^2)=\\sum_{k=1}^\\infty k^2(\\frac12)^k=\\dfrac{\\frac18-\\frac{2}{8}}{\\frac{1}{16}}=6\\). Thus \\(Var(X)=E(X^2)-E(X)^2 = 6-4=2\\). It should be noted that this random variable is actually a geometric random variable (well, according to the “number of trials until and including the first success definition). If we define \\(Y\\sim Geom(.5)\\) using our definition of “number of failures before the first success” then We can let \\(X=Y+1\\). \\(E(X)=E(Y+1)=\\frac{1-.5}{.5}+1=2\\) and \\(Var(X)=Var(Y)=\\frac{1-p}{p2}=\\frac{.5}{.25^2}=2\\)."
  },
  {
    "objectID": "rv_practice.html#variance-formula",
    "href": "rv_practice.html#variance-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "7.6 Variance Formula",
    "text": "7.6 Variance Formula\nShow that \\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2-\\mu^2\\). Hint: expand the quantity \\((X-\\mu)^2\\) and distribute the expectation over the resulting terms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe proof goes like this: We first FOIL \\((X-\\mu)^2\\):\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}(X^2 - 2\\mu X + \\mu^2)\\)\nWe next split the expected value into 3 expected values using the fact that \\(\\mathbb{E}\\) is a linear operator.\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mathbb{E}X + \\mathbb{E}\\mu^2\\)\nWe next observe that \\(\\mu^2\\) is constant and \\(\\mathbb{X}=\\mu\\)\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mu + \\mu^2\\)\nWe can simplify the expression and we’re done!"
  },
  {
    "objectID": "distr_practice.html",
    "href": "distr_practice.html",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "",
    "text": "9 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "distr_practice.html#using-wikipedia-in-college",
    "href": "distr_practice.html#using-wikipedia-in-college",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.1 Using Wikipedia in college",
    "text": "9.1 Using Wikipedia in college\nA recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \\(X\\) equal the number of students in a random sample of size \\(n = 31\\) who have used Wikipedia as a source.\n\nHow is \\(X\\) distributed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming independence in sampling, and a representative sample, we can use a Binomial distribution with \\(n=31\\) and \\(p=0.447\\).\n\n\n\n\nSketch the probability mass function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(dbinom(0:31, 31,.447), names=0:31, ylab=\"probability\", main=\"PMF of Binomial(31,.447)\")\n\n\n\n\n\n\n\n\nSketch the cumulative distribution function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(pbinom(0:31, 31, .447), type=\"s\", ylab=\"cumulative prob.\", main=\"CDF of Binomial(31, .447)\")\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\) is equal to 17.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(17, 31, .447)\n\n[1] 0.07532248\n\n\n\n\n\n\nFind the probability that \\(X\\) is at most 13.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npbinom(13, 31, .447)\n\n[1] 0.451357\n\n\n\n\n\n\nFind the probability that \\(X\\) is bigger than 11.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(12:31, 31, .447))\n\n[1] 0.8020339\n\n#or\npbinom(11, 31, .447, lower.tail=FALSE)\n\n[1] 0.8020339\n\n#or\n1-pbinom(11, 31, .447)\n\n[1] 0.8020339\n\n\n\n\n\n\nFind the probability that \\(X\\) is at least 15.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X at least 15)\nsum(dbinom(15:31,31,.447))\n\n[1] 0.406024\n\n\n\n\n\n\nFind the probability that \\(X\\) is between 16 and 19, inclusive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(16:19, 31, .447))\n\n[1] 0.2544758\n\n\n\n\n\n\nGive the mean of \\(X\\), denoted \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(X)=n*p\n31*.447\n\n[1] 13.857\n\n#or you can also do this (but it's too much work)\nsum( (0:31) * dbinom(0:31, 31, .447))\n\n[1] 13.857\n\n\n\n\n\n\nGive the variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Var(X) = n * p * (1-p)\n31 * .447 * (1-.447)\n\n[1] 7.662921\n\n#or - if you want (but why would you want to?)\nsum((0:31 - 31*.447)^2 * dbinom(0:31, 31, .447))\n\n[1] 7.662921\n\n\n\n\n\n\nGive the standard deviation of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#SD(X) = sqrt(n*p*(1-p))\nsqrt(31*.447*(1-.447))\n\n[1] 2.768198\n\n\n\n\n\n\nFind \\(\\mathbb{E}(4X+51.324)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(4X+51.324) = 4*E(X)+51.324\n4*(31*.447) + 51.324\n\n[1] 106.752"
  },
  {
    "objectID": "distr_practice.html#a-uniform-pmf",
    "href": "distr_practice.html#a-uniform-pmf",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.2 A Uniform PMF",
    "text": "9.2 A Uniform PMF\nLet \\(X\\) have discrete uniform PMF on the values \\(x\\in\\left\\{-1,0,1\\right\\}\\).\n\nWrite the equation for its PMF.\nFind \\(Pr[X&lt;-1]\\) and \\(Pr[X\\leq -1]\\)\nFind \\(Pr[X&gt;0]\\) and \\(Pr[X \\geq 0]\\)\nCalculate the CDF from the PDF. Write out an expression for \\(F(x)\\) and plot the PMF and CDF."
  },
  {
    "objectID": "distr_practice.html#discrete-random-variable-problems",
    "href": "distr_practice.html#discrete-random-variable-problems",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.3 Discrete Random Variable Problems",
    "text": "9.3 Discrete Random Variable Problems\n\n\nIf \\(X\\) is \\(\\text{Poisson}(\\lambda)\\), compute \\(\\mathbb{E}\\left[1/(X+1)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be handled mathematically. The formula for \\(E(1/(X+1))\\) is\n\\(E(1/(X+1))=\\sum_{x=0}^{\\infty}\\frac{1}{x+1}\\frac{\\lambda^{x}}{x!}e^{-\\lambda}=\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x}}{(x+1)!}e^{-\\lambda}\\)\nThe trick is to get get the summation to equal 1 and simplify. We multiply by \\(\\lambda/\\lambda\\)\n\\(E(1/(X+1))=\\frac{1}{\\lambda}\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x+1}}{(x+1)!}e^{-\\lambda}\\)\nNow we can make a change of variables: \\(y=x+1\\) and thus \\(x=0\\) becomes \\(y=1\\)\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}\\sum_{y=1}^{\\infty}\\frac{\\lambda^{y}}{y!}e^{-\\lambda}\\)\nThe only thing missing is that the summation starts at \\(y=1\\) instead of \\(y=0\\), But for \\(Y \\sim Poisson(\\lambda)\\), \\(P(Y=0)=e^{-\\lambda}\\) so this summation is \\(1-e^{-\\lambda}\\).\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}(1-e^{-\\lambda})\\)\n\n\n\n\nIf \\(X\\) is \\(\\text{Bernoulli}(p)\\) and \\(Y\\) is \\(\\text{Bernoulli}(q)\\), computer \\(\\mathbb{E}\\left[(X+Y)^3\\right]\\) assuming \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\((X+Y)^3 = X^3+3X^2Y+3XY^2+Y^3\\) so \\(E[(X+Y)^3]=E(X^3)+3E(X^2)E(Y)+3E(X)E(Y^2)+E(Y^2)\\)\nthis is due to independence. Since \\(X\\) an \\(Y\\) are independent, so are \\(X^2\\) and \\(Y\\), and \\(X\\) and \\(Y^2\\). \\(E(X)=E(X^2)=E(X^3)=p\\) and \\(E(Y)=E(Y^2)=E(Y^3)=q\\). Thus \\(E[(X+Y)^3]=p+6pq+q\\)\n\n\n\n\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\Delta(\\theta)=\\mathbb{E}\\left[(X-\\theta)^2\\right]\\). Find \\(\\theta\\) that minimizes the error \\(\\Delta(\\theta)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can expand the expected value and attempt to find the minimum with respect to \\(\\theta\\). \\(E[(X-\\theta)^2]=E[X^2-2\\theta X+\\theta^2]=E(X^2)-2\\theta\\mu+\\theta^2\\). Recall that \\(Var(X)=E(X^2)-\\mu^2\\) so \\(E(X^2)=\\sigma^2+\\mu^2\\) So we can write \\(\\Delta(\\theta)=\\sigma^2 + \\mu^2-2\\theta\\mu + \\theta^2\\) We want to find what value of \\(\\theta\\) minimizes this function - derivative! \\(\\Delta'(\\theta)=-2\\mu+2\\theta=0\\) thus \\(\\theta=\\mu\\) minimizes this.\n\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) are independent uniform random variables in \\(\\{0,1,\\ldots,100\\}\\). Evaluate \\(\\mathbb{P}\\left[\\text{min}(X_1,\\ldots, X_n) &gt; l\\right]\\) for any \\(l \\in \\{0,1,\\ldots,100\\}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(Y=\\min(X_1, \\ldots, X_n)\\) If \\(P(Y &gt;l)\\), that means the minimum exceeds \\(l\\), so all of the values \\(&gt;l\\). \\(P(X_1 &gt; l)=(100-l)/101\\) - you can check: \\(P(X_1&gt;0)=100/101\\). This is the same calculation for each \\(i\\). So \\(P(Y&gt;l)=\\dfrac{(100-l)^n}{101^n}\\).\n\n\n\n\nConsider a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\). \\(p_X(k)={n \\choose k} p^k(1-p)^{n-k}\\). Show that the mean is \\(\\mathbb{E}X= np\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(X)=\\sum_{k=0}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nThe first term is zero so we could write\n\\(\\sum_{k=1}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nNow the following is a fact that is needed but perhaps not well known. It’s the equivalence of \\(k{n \\choose k}=n{n-1 \\choose k-1}\\). We make this subsitution\n\\(\\sum_{k=1}^n n{n-1 \\choose k-1} p^k(1-p)^{n-k}=np\\sum_{k=1}^n {n-1\\choose k-1}p^{k-1}(1-p)^{n-k}\\)\nWe could write \\(n-k=(n-1)-(k-1)\\) and we’ll be making some substitutions: \\(m=n-1\\) and \\(j=k-1\\). This lets us write\n\\(np\\sum_{j=0}^m {m \\choose j}p^j(1-p)^{m-j}=np\\) because the summation =1, as it is just the sum of the pmf of a binomial.\n\n\n\n\n(not for 340) Consider a geometric random variable \\(X\\) with parameter \\(p\\). \\(p_X(k)=p(1-p)^k\\) for \\(k=0,1,\\ldots\\). Show that its mean is \\(\\mathbb{E}X=(1-p)/p\\).\n(not for 340) Consider a Poisson random variable \\(X\\) with parameter \\(\\lambda\\). \\(p_X(k)=\\dfrac{\\lambda^k}{k!}e^{-\\lambda}\\). Show that \\(\\text{Var}X=\\lambda\\).\n(not for 340) Consider the uniform random variable \\(X\\) over values \\(1,2,\\ldots, L\\). Show that \\(\\text{Var}X=\\dfrac{L^2-1}{12}\\). Hint: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\) and \\(\\sum_{i=1}^n i^2=\\frac{n^3}{3}+\\frac{n^2}{2}+\\frac{n}{6}\\)"
  },
  {
    "objectID": "distr_practice.html#hard-drive-failures",
    "href": "distr_practice.html#hard-drive-failures",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.4 Hard Drive Failures",
    "text": "9.4 Hard Drive Failures\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. The initial cost of building the player is $50, and the manufacturer offers a 1-year warranty. Within one year, the customer can ask for a free repair up to 12 times.\n\n\nLet \\(X\\) be the number of months when the player fails. What is the PMF of \\(X\\)? Hint: \\(\\mathbb{P}(X = 1)\\) may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying \\(X = 1\\) is equivalent to saying that there is only one failure in the entire 12-month period.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of failures should follow a binomial distribution with \\(n=12, p=1/12\\). Thus \\(P(X=k)={n \\choose k}(\\frac{1}{12})^k(\\frac{11}{12})^{n-k}\\)\n\n\n\n\nWhat is the average cost per player?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe cost is \\(50+20X\\) So \\(E(50+20X)=50+20E(X)=50+20\\cdot 12(\\frac{1}{12})=70\\)"
  },
  {
    "objectID": "distr_practice.html#bit-errors",
    "href": "distr_practice.html#bit-errors",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.5 Bit Errors",
    "text": "9.5 Bit Errors\n\nA binary communication channel has a probability of bit error of \\(p = 10^{-6}\\). Suppose that transmission occurs in blocks of 10,000 bits. Let \\(N\\) be the number of errors introduced by the channel in a transmission block.\n\n\n\nWhat is the PMF of \\(N\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(N\\) follows a binomial distribution with \\(n=10000\\) and \\(p=.000001\\)\n\n\n\n\nFind \\(\\mathbb{P}(N = 0)\\) and $(N b $ 3)$.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(0, 10000, .000001)\n\n[1] 0.9900498\n\npbinom(3, 10000, .000001)\n\n[1] 1\n\n\n\n\n\n\nFor what value of \\(p\\) will the probability of 1 or more errors in a block be 99%?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved directly. \\(P(N \\geq 1)=1-P(X=0)=1-(1-p)^{10000}\\). If we set this to .99 we can solve for \\(p\\) : \\(.99=1-(1-p)^{10000}\\) so \\(.01 = (1-p)^{10000}\\) so \\(p=1-.01^{1/10000}\\)\n\n1-.01^(1/10000)\n\n[1] 0.000460411"
  },
  {
    "objectID": "distr_practice.html#processing-orders",
    "href": "distr_practice.html#processing-orders",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.6 Processing Orders",
    "text": "9.6 Processing Orders\nThe number of orders waiting to be processed is given by a Poisson random variable with parameter \\(\\alpha = \\frac{\\lambda}{n\\mu}\\), where \\(\\lambda\\) is the average number of orders that arrive in a day, \\(\\mu\\) is the number of orders that an employee can process per day, and n is the number of employees. Let \\(N; = 5\\) and \\(N&lt; = 1\\). Find the number of employees required so the probability that more than four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few \\(n\\)’s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlambda=5\nmu=1\nppois(4, lambda/(1:10 * mu), lower.tail=FALSE)\n\n [1] 0.5595067149 0.1088219811 0.0275432568 0.0091242792 0.0036598468\n [6] 0.0016844329 0.0008589296 0.0004739871 0.0002784618 0.0001721156\n\n#With 3 employees P(X&gt;4) is less than 10%."
  },
  {
    "objectID": "distr_practice.html#normal-random-variable",
    "href": "distr_practice.html#normal-random-variable",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.7 Normal Random Variable",
    "text": "9.7 Normal Random Variable\nIf \\(Z\\sim \\text{Normal}(\\mu=0, \\sigma^2=1^2)\\) find\n\n\n\\(\\mathbb{P}(Z &gt; 2.64)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(2.64, 0, 1, lower.tail=FALSE)\n\n[1] 0.004145301\n\n\n\n\n\n\n\\(\\mathbb{P}(0 \\leq Z &lt; 0.87)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(.87)-pnorm(0)\n\n[1] 0.3078498\n\n\n\n\n\n\n\\(\\mathbb{P}(|Z| &gt; 1.39)\\) (Hint: draw a picture)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(1.39, lower.tail=FALSE)*2\n\n[1] 0.1645289"
  },
  {
    "objectID": "distr_practice.html#identify-the-distribution",
    "href": "distr_practice.html#identify-the-distribution",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.8 Identify the Distribution",
    "text": "9.8 Identify the Distribution\nFor the following random experiments, decide what the distribution of X should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).\n\n\nWe throw a dart at a dart board. Let X denote the squared linear distance from the bullseye to the where the dart landed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume the dart lands somewhere on the board, and any point is equally likely (not a good assumption for a skilled dart thrower). The probability density would be proportional to the distance to the center squared - Suppose the dart board has radius \\(R\\). Let \\(X\\) be the distance to the dart from the bullseye. Then \\(P(X&lt;r)=\\pi r^2 / (\\pi R^2)=(r/R)^2\\) . The question then is what is \\(P(X^2&lt;r)\\)? Well, take a square root of both sides. \\(=P(X &lt; \\sqrt{r})=\\frac{r}{R^2}\\). This is a uniform distribution’s CDF.\n\n\n\n\nWe randomly choose a textbook from the shelf at the bookstore and let P denote the proportion of the total pages of the book devoted to exercises.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA random proportion you might want to use uniform(0,1) however this is assuming that each proportion is equally likely. This is actually a great example for a beta distribution. Beta distributions are continuous distributions that can be parameterized to model a random proportion and the distribution can can be made to be skewed in different ways.\n\n\n\n\nWe measure the time it takes for the water to completely drain out of the kitchen sink.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s assume the sink is filled to the maximum. We drain the sink and start our timer. In this case, it’s reasonable to model the length of time to drain as a normal distribution.\n\n\n\n\nWe randomly sample strangers at the grocery store and ask them how long it will take them to drive home.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe time it takes to go home could be modeled by a gamma distribution since it is a continuous distribution capped below at 0 and it is a useful way to model the length of time a random process takes to complete."
  },
  {
    "objectID": "distr_practice.html#normal-random-variable-ii",
    "href": "distr_practice.html#normal-random-variable-ii",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.9 Normal Random Variable II",
    "text": "9.9 Normal Random Variable II\nLet \\(X\\) be a Gaussian random variable with \\(\\mu=5\\) and \\(\\sigma^2=16\\).\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(2\\leq X \\leq 7)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X&gt;4)\npnorm(4, mean=5, sd=4, lower.tail=FALSE)\n\n[1] 0.5987063\n\n#P(2 &lt;= X &lt;= 7)\npnorm(7, 5, 4)-pnorm(4,5,4)\n\n[1] 0.2901688\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X &lt; a)=0.8869\\), find \\(a\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.88695, 4)\n\n[1] 5.210466\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X&gt;b)=0.1131\\), find \\(b\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.1131, 5, 4, lower.tail=FALSE)\n\n[1] 9.840823\n\n\n\n\n\n\nIf \\(\\mathbb{P}(13 &lt; X \\leq c)=0.0011\\), find \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First find the probability less than 13\np13 &lt;- pnorm(13, 5, 4)\n#now we can find the quantile for p13+.0011\nqnorm(p13+.0011, 5, 4)\n\n[1] 13.08321\n\n#double check\npnorm(13.08321,5,4)-pnorm(13,5,4)\n\n[1] 0.001100025"
  },
  {
    "objectID": "distr_practice.html#choose-the-distribution",
    "href": "distr_practice.html#choose-the-distribution",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.10 Choose the distribution",
    "text": "9.10 Choose the distribution\n\nFor the following situations, decide what the distribution of \\(X\\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)\n\n\nWe shoot basketballs at a basketball hoop, and count the number of shots until we make a basket. Let X denote the number of missed shots. On a normal day we would typically make about 37% of the shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of missed shots before the first basket, assuming independence, can be modeled by a Geometric random variable with parameter \\(p=.37\\).\n\n\n\n\nIn a local lottery in which a three digit number is selected randomly, let X be the number selected.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming that all 3 digit numbers are equally likely (A reasonable assumption) the number selected can be modeled by a discrete uniform distribution with minimum 100 and maximum 999.\n\n\n\n\nWe drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let X be the number of times the cup lands perfectly right side up.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we drop the cup 20 times, and the result each time is independent with a constant probability of landing right side up, the number of times it does can be modeled by a Binomial random variable with parameters \\(n=20\\) and \\(p\\) (unknown).\n\n\n\n\nWe toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let X denote the number of missed shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGeometric random variable (unknown parameter value for \\(p\\)).\n\n\n\n\nWorking for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let X count the number of containers that illegally contain contraband.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTechnically we should use a hypergeometric random variable for this situation (since it is a small population size of 557), but since we do not cover the hypergeometric the closest random variable we have is the binomial.\n\n\n\n\nAt the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let X denote the number of birds in the bush.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a discrete random variable, but without other information it’s hard to say. The distribution is likely unimodal and bell-curved. You could probably model this using a normal distribution rounded off to the nearest integer.\n\n\n\n\nWe count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe observation window is the circular area, and the 10 minutes during observation. Assuming the rate of rainfall is constant, the number of raindrops in the circle can be modeled using a Poisson random variable.\n\n\n\n\nWe count the number of moth eggs on our window screen.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCounting indicates a discrete random variable. A binomial or a rounded normal distribution may be appropriate, but we lack enough details to be sure.\n\n\n\n\nWe count the number of blades of grass in a one square foot patch of land.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe location of the sprouting grass could be modeled well by a Poisson random variable - the \\(\\lambda\\) parameter would likely be very large, in the range of 1000 or 10000, and as such the distribution would look very much like a normal distribution.\n\n\n\n\nWe count the number of pats on a baby’s back until (s)he burps.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we define a geometric random variable, we let \\(X\\) be the number of failures before the first success. The last pat (that causes the burp) is the success in this context. So we could use a geometric random variable, but we would have to add 1 to it in order to count all burps (the failures + 1 success)."
  },
  {
    "objectID": "distr_practice.html#two-normal-rvs",
    "href": "distr_practice.html#two-normal-rvs",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.11 Two Normal RVs",
    "text": "9.11 Two Normal RVs\nLet X and Y be zero-mean, unit-variance independent Gaussian random variables. Find the value of r for which the probability that \\((X, Y )\\) falls inside a circle of radius r is 1/2.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000)\n\nr &lt;- seq(1.1, 1.2, .005)\np &lt;- 0\nfor (i in 1:length(r)){\n  p[i] &lt;- mean(sqrt(x^2+y^2) &lt;= r[i])\n}\ndata.frame(r,p)\n\n       r      p\n1  1.100 0.4577\n2  1.105 0.4615\n3  1.110 0.4651\n4  1.115 0.4688\n5  1.120 0.4714\n6  1.125 0.4744\n7  1.130 0.4770\n8  1.135 0.4798\n9  1.140 0.4827\n10 1.145 0.4861\n11 1.150 0.4886\n12 1.155 0.4918\n13 1.160 0.4954\n14 1.165 0.4984\n15 1.170 0.5015\n16 1.175 0.5040\n17 1.180 0.5074\n18 1.185 0.5101\n19 1.190 0.5133\n20 1.195 0.5157\n21 1.200 0.5186\n\n#X^2 + Y^2 ~ Chisq(2)\n#so the square root of the 50th percentile from that distribution should be the answer\nsqrt(qchisq(.5,2))\n\n[1] 1.17741"
  },
  {
    "objectID": "distr_practice.html#uniform-random-angle",
    "href": "distr_practice.html#uniform-random-angle",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "9.12 Uniform Random Angle",
    "text": "9.12 Uniform Random Angle\nLet \\(\\Theta ∼ Uniform[0, 2\\pi]\\).\n\n\nIf \\(X = cos \\Theta\\), \\(Y = sin \\Theta\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, they are uncorrelated, because (x,y) can be any point on the circumference of a circle of radius 1 with uniform likelihood. However, they are not independent. If we know the value of \\(Y\\) for example, there are only 2 possible values of \\(X\\).\n\nthetas &lt;- runif(10000, 0, 2*pi)\ncor(cos(thetas), sin(thetas))\n\n[1] 0.004416689\n\n\n\n\n\n\nIf \\(X = cos(\\Theta/4)\\), \\(Y = sin(\\Theta/4)\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case (x,y) can only be found in the first quadrant. In this case they are going to be negatively correlated, since that portion of the unit circle in the first quadrant slopes downwards.\n\ncor(cos(thetas/4), sin(thetas/4))\n\n[1] -0.9180884"
  },
  {
    "objectID": "distr_practice.html#variance-of-a-uniform-rv",
    "href": "distr_practice.html#variance-of-a-uniform-rv",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "10.1 Variance of a Uniform RV",
    "text": "10.1 Variance of a Uniform RV\nCalculate the variance of \\(X \\sim \\text{Unif}(a,b)\\). (Hint: First calculate \\(\\mathbb{E}X^2\\))"
  },
  {
    "objectID": "distr_practice.html#expectation-and-binomial",
    "href": "distr_practice.html#expectation-and-binomial",
    "title": "8  Independence and Conditional Probability Practice",
    "section": "10.2 Expectation and Binomial",
    "text": "10.2 Expectation and Binomial\nIf \\(X \\sim \\text{Binom}(n,p)\\) show that \\(\\mathbb{E}X(X-1)=n(n-1)p^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can just expand the product \\(\\mathbb{E}(X^2-X)\\) and we can split this up into two expected values: \\(\\mathbb{E}X^2 - \\mathbb{E}X = \\mathbb{E}X^2-\\mu\\). Recall that \\(Var(X)=\\mathbb{E}X^2-\\mu^2\\) So \\(\\mathbb{E}X^2=Var(X)+\\mu^2\\). For a binomial, \\(Var(X)=np(1-p)\\) and \\(\\mu=np\\). Thus we have\n\\(\\mathbb{E}X^2 - \\mu=[np(1-p) + n^2p^2] - np = np\\left(1-p+np-1\\right)\\)\nTidying up a little bit we get \\(np(np-p)=np^2(n-1)\\), and we’re done."
  },
  {
    "objectID": "RV_summary.html#rv-summary",
    "href": "RV_summary.html#rv-summary",
    "title": "Appendix A — Random Variable Summary",
    "section": "A.1 RV summary",
    "text": "A.1 RV summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\nBinomial\nGeometric\nPoisson\nDiscrete Uniform\nNormal\n(Continuous) Uniform\nExponential\n\n\n\n\nType\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nContinuous\nContinuous\nContinuous\n\n\nParameters\n\\(n\\), \\(p\\)\n\\(p\\)\n\\(\\lambda\\)\n\\(a\\), \\(b\\)\n\\(\\mu\\), \\(\\sigma^2\\)\n\\(a\\), \\(b\\)\n\\(\\lambda\\)\n\n\nDescription\nNumber of successes in \\(n\\) independent trials with \\(p\\) probability of success for each trial  (Note: Bernoulli is just Binomial with \\(n\\!=\\!1\\))\nNumber of failures BEFORE the first success while independently repeating trial with \\(p\\) probability of success\nCount of number of occurrences of an event with constant mean rate \\(\\lambda\\) that’s independent of previous occurrences\n\\(n\\)-sided fair die\nNormal distributions usually arise from CLT (i.e. they’re processes that are the sum of many smaller independent processes)\nGeneralizing \\(n\\)-sided fair die to a continuous interval\nWaiting time between Poisson events\n\n\nOutcomes\n\\(0,1,\\ldots,n\\)\n\\(0,1,\\ldots\\)\n\\(0,1,\\ldots\\)\n\\(a,a\\!+\\!1,\\ldots,b\\)\n\\((-\\infty,\\infty)\\)\n\\([a,b]\\)\n\\([0,\\infty)\\)\n\n\nPDF/PMF at \\(k\\)\n\\({n\\choose k}p^k(n-p)^{n-k}\\)\n\\(p(1-p)^k\\)\n\\(\\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\n\\(\\frac1{b-(a-1)}\\)\n\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac12\\left(\\frac{x-\\mu}\\sigma\\right)^2}\\)\n\\(\\frac1{b-a}\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(P(X\\le k)\\)\n\n\\(1-(1-p)^{\\lfloor k\\rfloor+1}\\)\n\n\\(\\frac{\\lfloor k\\rfloor-(a-1)}{b-(a-1)}\\)\n\n\\(\\frac{x-a}{b-a}\\)\n\\(1-e^{-\\lambda x}\\)\n\n\nMean\n\\(np\\)\n\\(\\frac{1-p}p\\)\n\\(\\lambda\\)\n\\(\\frac{a+b}2\\)\n\\(\\mu\\)\n\\(\\frac{a+b}2\\)\n\\(\\frac1\\lambda\\)\n\n\nVariance\n\\(np(1-p)\\)\n\\(\\frac{1-p}{p^2}\\)\n\\(\\lambda\\)\n\\(\\frac{(b-(a-1))^2-1}{12}\\)\n\\(\\sigma^2\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\\(\\frac1{\\lambda^2}\\)\n\n\nR functions\ndbinom, pbinom, qbinom, rbinom\ndgeom, pgeom, qgeom, rgeom\ndpois, ppois, qpois, rpois\nsample\ndnorm, pnorm, qnorm, rnorm\ndunif, punif, qunif, runif\ndexp, pexp, qexp, rexp"
  }
]