[
  {
    "objectID": "rv.html",
    "href": "rv.html",
    "title": "Random Variables",
    "section": "",
    "text": "These notes will discuss the most fundamental object in statistics: random variables.\nWe use random variables, within the framework of probability theory, to model how our data came to be.\nWe will first introduce the idea of a random variable (and its associated distribution) and review probability theory.\nThen, we will walk through a number of different basic distributions and discuss the kinds of data for which these different models are appropriate."
  },
  {
    "objectID": "rv.html#learning-objectives",
    "href": "rv.html#learning-objectives",
    "title": "3  Random Variables",
    "section": "3.1 Learning objectives",
    "text": "3.1 Learning objectives\nAfter this lesson, you will be able to\n\nExplain what a random variable is\nIdentify appropriate random variables for modeling different real-world events and explain why one choice might be better or worse than another\nCombine random variables to build simple models of real-world phenomena\nCompute the probabilities of simple events under different probability distributions using R."
  },
  {
    "objectID": "rv.html#what-is-a-random-variable",
    "href": "rv.html#what-is-a-random-variable",
    "title": "3  Random Variables",
    "section": "3.2 What is a random variable?",
    "text": "3.2 What is a random variable?\nConsider the following quantities/events:\n\nWhether or not a coin flip comes up heads or tails.\nHow many people in the treatment group of a vaccine trial are hospitalized.\nThe water level measured in Lake Mendota on a given day.\nHow many customers arrive at a store between 2pm and 3pm today.\nHow many days between installing a lightbulb and when it burns out.\n\nAll of these are examples of events that we might reasonably model according to different random variables.\nLater in your studies you will learn a more formal definition of what a random variable is. For now, let’s be content with saying that a random variable is a (random) number \\(X\\) about which we can compute quantities of the form \\(\\Pr[ X \\in S ]\\), where \\(S\\) is a set."
  },
  {
    "objectID": "rv.html#aside-probability-refresher",
    "href": "rv.html#aside-probability-refresher",
    "title": "3  Random Variables",
    "section": "3.3 Aside: probability refresher",
    "text": "3.3 Aside: probability refresher\nBefore moving on, let’s briefly review some basic ideas from probability theory.\nWe have a set of possible outcomes, usually denoted \\(\\Omega\\).\n\nwhen we flip a coin, it can land either heads (\\(H\\)) or tails (\\(T\\)), so the outcomes are \\(\\Omega = \\{H, T\\}\\).\nWhen we roll a six-sided die, there are six possible outcomes, \\(\\Omega = \\{1,2,3,4,5,6\\}\\).\nIn other settings, the outcomes might be an infinite set.\n\nEx: if we measure the depth of Lake Mendota, the outcome may be any positive real number (at least theoretically, anyway!)\n\n\nIn the vast majority of situations, \\(\\Omega\\) will be either discrete (e.g., \\(\\{1,2,\\dots\\}\\)) or continuous (e.g., \\([0,1]\\)) and we call the associated random variable discrete or continuous, respectively.\nA subset \\(E \\subseteq \\Omega\\) of the outcome space is called an event.\nA probability is a function that maps events to numbers, with the properties that\n\n\\(\\Pr[ E ] \\in [0,1]\\) for all events \\(E\\)\n\\(\\Pr[ \\Omega ] = 1\\)\nFor \\(E_1,E_2 \\subseteq \\Omega\\) with \\(E_1 \\cap E_2 = \\emptyset\\), \\(\\Pr[ E_1 \\cup E_2 ] = \\Pr[ E_1 ] + \\Pr[ E_2 ]\\)\n\nTwo events \\(E_1\\) and \\(E_2\\) are independent if \\(\\Pr[ E_1 \\cap E_2 ] = \\Pr[ E_1 ] \\Pr[ E_2 ]\\).\nTwo random variables \\(X\\) and \\(Y\\) are independent if for all sets \\(S_1,S_2\\), we have \\(\\Pr[ X \\in S_1 ~\\&~ Y \\in S_2 ] = \\Pr[ X \\in S_1 ] \\Pr[ Y \\in S_2 ]\\).\nRoughly speaking, two random variables are independent if learning information about one of them doesn’t tell you anything about the other.\n\nFor example, if each of us flips a coin, it is reasonable to model them as being independent.\nLearning whether my coin landed heads or tails doesn’t tell us anything about your coin.\n\nExample: Coin flipping\nConsider a coin toss, in which the possible outcomes are \\(\\Omega = \\{ H, T \\}\\).\nThis is a discrete random experiment, but beause the outcomes are not numeric it is not a random variable. If, however we define \\(X\\) to be 1 when we flip an \\(H\\) and 0 when we flip \\(T\\), then we have defined a discrete random variable, because the outcome set \\(\\{0,1\\}\\) is discrete.\nIf we have a fair coin, then it is sensible that \\(\\Pr[ X=1 ] = \\Pr[ X=0 ] = 1/2\\).\nExercise (optional): verify that this probability satisfies the above properties!\nWe will see in a moment that this is a special case of a Bernoulli random variable, which you are probably already familiar with.\nExample: Six-sided die\nIf we roll a die, the outcome space is \\(\\Omega = \\{1,2,3,4,5,6\\}\\), and the events are all the subsets of this six-element set.\nSo, for example, we can talk about the event that we roll an odd number \\(E_{\\text{odd}} = \\{1,3,5\\}\\) or the event that we roll a number larger than \\(4\\), \\(E_{&gt;4} = \\{5,6\\}\\).\nExample: Human heights\nConsider our human height example from our previous lecture.\nWe pick a random person and measure their height in, say, centimeters. What is the outcome space?\n\nOne option: the outcome space is the set of positive reals, in which case this is a continuous random variable.\nAlternatively, we could assume that the outcome space is the set of all real numbers.\n\nThis highlights the importance of specifying our assumptions and the outcome space we are working with in a particular problem. We will see these kinds of issues again and again this semester.\n\n3.3.1 A note on models, assumptions and approximations\nNote that we are already making an approximation– our outcome sets aren’t really exhaustive, here.\nWhen you toss a coin, there are possible outcomes other than heads and tails.\n\nPerhaps the coin lands on its side (I have personally seen this happen with a nickel flipped onto an old wooden floor).\nSimilarly, perhaps the die lands on its side.\n\nWe can see a kind of idealization in our human height example.\n\nWe can only measure a height to some finite precision (say, two decimal places), so it is a bit silly to take the outcome space to be the real numbers.\nAfter all, if we can only measure a height to two decimal places, then there is no way to ever obtain the event, “height is 160.3333333… centimeters”.\n\nThese kinds of approximations and idealizations are good to be aware of, but they usually don’t bother us much\nWe will see below and in future lectures the kinds of approximation errors that are more concerning and warrant our attention."
  },
  {
    "objectID": "rv.html#random-variables",
    "href": "rv.html#random-variables",
    "title": "3  Random Variables",
    "section": "3.4 Random variables",
    "text": "3.4 Random variables\nA random variable is specified by a probability.\nThat is, a random variable \\(X\\) is specified by an outcome set \\(\\Omega\\) and a function that specifies probabilities of the form \\(\\Pr[ X \\in E ]\\) where \\(E \\subseteq \\Omega\\) is an event.\nLet’s look at some commonly-used random variables. In the process, we will discuss some of the real-world phenomena to which these random variables are best-suited.\n\n3.4.1 Bernoulli\nA Bernoulli random variable has outcome set \\(\\Omega = \\{0,1\\}\\).\nAs discussed above, to specify a probability on this set, it is enough for us to specify \\(\\Pr[ \\{0 \\} ]\\) and \\(\\Pr[ \\{1\\} ]\\).\nTypically, we do this by specifying the success probability \\(p = \\Pr[ \\{1\\} ] \\in [0,1]\\). Once we have done this, it is immediate that (check!) \\(\\Pr[ \\{0\\} ] = 1-p\\).\nNote that we can check that this gives us a probability by verifying that it sums to 1: \\[\n\\Pr[ \\Omega ] = \\Pr[ \\{0\\} \\cup \\{1\\} ] = \\Pr[ \\{0\\} ] + \\Pr[ \\{1\\} ] = 1-p + p = 1.\n\\] Bernoulli random variables are commonly used to model “yes or no” events. That is, events of the form “whether or not event \\(A\\) happens”. Common examples:\n\nCoin flips\nwhether or not a person gets sick with a disease\nwhether or not a team wins a game.\n\nIf \\(Z\\) is a Bernoulli random variable with probability of success \\(p\\), then we write \\(Z \\sim \\operatorname{Bernoulli}(p)\\).\nWe read this as something like “\\(Z\\) is distributed as Bernoulli \\(p\\)”.\n\n\n3.4.2 Binomial\nA Bernoulli random variable is like a single coin flip.\nWhat if we flip many coins, all with the same probability of coming up heads?\nThen the total number of heads is distributed as a binomial random variable.\nIn particular, we describe a binomial distribution by specifying two parameters:\n\nthe number of trials (i.e., coins flipped) \\(n\\), often called the size parameter and\nthe success probability \\(p\\) (i.e., the probability that an individual coin lands heads).\n\nOften we will write \\(\\operatorname{Binomial}(n,p)\\) to denote this distribution.\nSo if \\(X\\) is a Binomial random variable with \\(n\\) trials and success probability \\(p\\), we write \\(X \\sim \\operatorname{Binomial}(n,p)\\).\nExample: modeling COVID-19\nIn a population of 250,000 people (approximately the population of Madison), we may imagine that each person has some probability \\(p\\) of becoming seriously ill with COVID-19.\nThen, in a sense, the total number of people in Madison who become seriously ill with COVID-19 is like the total number of probability-\\(p\\) coin flips that land heads when we flip \\(250,000\\) coins.\nWe might then model the number of COVID-19 patients by a binomial random variable with \\(n=250,000\\) and \\(p=0.01\\) (just to be clear, we are completely making up this choice of \\(p\\) here, just for the sake of example!).\n\n3.4.2.1 Generating random binomial RVs\nWe can generate binomial random variables using the rbinom function. Think “r for random”.\n\n# rbinom takes three arguments.\n# The first is the number of random variables we want to generate (confusingly, this is called n in the R docs).\n# The size argument specifies the number of coins to flip, i.e., n in our notation above (I know! Confusing!)\n# The prob argument specifies the probability that one coin lands heads, i.e., p in our notation above.\nrbinom(1, size = 10, prob = 0.3) # produces a random number from {0,1,2,...,10}, with 2,3,4 being most common (because np = 3 is the expected value we'll come back to this!)\n\n[1] 3\n\n\n\n# If we repeat the experiment a few times, we get different random values.\nrbinom(1, size = 10, prob = 0.3)\n\n[1] 3\n\nrbinom(1, size = 10, prob = 0.3)\n\n[1] 3\n\nrbinom(1, size = 10, prob = 0.3)\n\n[1] 3\n\nrbinom(1, size = 10, prob = 0.3)\n\n[1] 4\n\nrbinom(1, size = 10, prob = 0.3)\n\n[1] 4\n\n\nWe can also use the binomial to generate Bernoulli random variables, by setting the size argument to 1 (i.e., flip 1 coin):\n\nrbinom(1, size = 1, prob = 0.5) # 1 is \"heads\", 0 is \"tails\"\n\n[1] 1\n\n\nImportant note: if you read the R documentation, there is a possible notational confusion waiting for you, alluded to in the comments of the code above. The signature of the rbinom function is given as rbinom(n, size, prob). Based on the \\(\\operatorname{Binomial}(n,p)\\) notation we used above, you might expect that n in the rbinom function is the number of coins and prob is the success probability. Unfortunately, that isn’t quite right. n is the number of Binomial random variables to generate. size specifies the size parameter (\\(n\\) in our math notation above).\nCompare the following:\n\nrbinom(n = 3, size = 10, prob = 0.5) # 3 draws from a Binomial(10,0.5)\n\n[1] 5 4 8\n\n\n\nrbinom(n = 10, size = 3, prob = 0.5) # 10 draws from a Binomial(3,0.5)\n\n [1] 3 1 2 1 0 1 1 3 3 0\n\n\nAll of the R functions for generating random variables take n as the number of draws from the distribution. This is in keeping with the convention in most of probability and statistics that \\(n\\) is a sample size. Unfortunately, this is just one of those places where two different notational conventions collide. It’s unfortunate that it arises in such a common and early-stage part of R!\n\n\n\n3.4.3 Aside: expectation review\nBefore we continue with more random variables, let’s take a pause to discuss one more important probability concept: expectation. You will hopefully recall from previous courses in probability and/or statistics the notion of expectation of a random variable.\nExpectation: long-run averages\nThe expectation of a random variable \\(X\\), which we write \\(\\mathbb{E} X\\), is the “long-run average” of the random variable.\nRoughly speaking, the expectation is what we would see on average if we observed many copies of \\(X\\).\nThat is, we observe \\(X_1,X_2,\\dots,X_n\\), and consider their average, \\(\\bar{X} = n^{-1} \\sum_{i=1}^n X_i\\).\nThe law of large numbers (LLN) states that in a certain sense, as \\(n\\) gets large, \\(\\bar{X}\\) gets very close to \\(\\mathbb{E} X\\). (actually, there are two LLNs, the weak law and strong law, but that’s a matter for a later course!).\nBy analogy with our calculus class, we would like to say something like \\[\n\\lim_{n \\rightarrow \\infty} \\frac{1}{n} \\sum_{i=1}^n X_i = \\mathbb{E} X.\n\\] But \\(n^{-1} \\sum_i X_i\\) is a random sum, so how can we take a limit?\nWell, again, the details are a matter for your probability theory class, but roughly speaking, for \\(n\\) large, with high probability, \\(\\bar{X}\\) is close to \\(\\mathbb{E}\\).\nExpectation: formal definition\nMore formally, if \\(X\\) is a discrete random variable, we define its expectation \\(\\mu\\) to be \\[\n\\mathbb{E} X = \\sum_k k \\Pr[ X = k]\n\\] where the sum is over all \\(k\\) such that \\(\\Pr[ X=k ] &gt; 0\\).\n\nNote that this set could be finite or infinite.\nIf the set is infinite, the sum might not converge, in which case we say that the expectation is either infinite or doesn’t exist. But that won’t be an issue this semester.\n\nQuestion: can you see how this definition is indeed like the “average behavior” of \\(X\\)?\nExercise: compute the expectation of a Bernoulli random variable with success probability \\(p\\). What about a \\(\\operatorname{Binomial}(n,p)\\) random variable? Hint: the expectation of a sum of RVs is the sum of their expectations. Write the Binomial RV as a sum of Bernoullis.\nImportant take-away: the law of large numbers says that if we take the average of a bunch of independent RVs, the average will be close to the expected value.\n\nSometimes it’s hard to compute the expected value exactly (e.g., because the math is hard– not all sums are nice!)\nThis is where Monte Carlo methods come in– instead of trying to compute the expectation exactly, we just generate lots of RVs and take their average!\nIf we generate enough RVs, the LLN says we can get as close as we want.\nWe’ll have lots to say about this in our lectures on Monte Carlo methods next week.\n\nVariance: formal definition\nAlso recall the variance is the expectation of the squared deviation from the mean, i.e: \\[\n\\operatorname{Var}(X)=\\mathbb{E}[(X-\\mathbb{E}X)^2]=\\sum_k (k-\\mu)^2 \\Pr[ X = k]\n\\] Similar to how the sample mean \\(\\bar{X}\\) converges to \\(\\mathbb{E}X\\), the sample variance \\(s^2\\) will converge to the true variance \\(\\sigma^2\\), i.e: \\[\n\\lim_{n\\to\\infty}\\frac1{n-1}\\sum_{i=1}^n(X_i-\\bar{X})^2=\\operatorname{Var}(X)\n\\]\n\n\n3.4.4 Geometric\nLet’s consider a different coin-flipping experiment. We flip a coin repeatedly and we count how many flips it takes before it lands heads.\nSo perhaps we flip the coin and it comes up heads immediately, in which case we would count zero (because there were no flips before the one where the coin landed heads). If we flipped the coin and it came up heads for the first time on the fourth toss, then we would count three, and so on.\nThis game describes the geometric distribution.\nIts behavior is controlled by a single parameter, the probability \\(p\\) of landing heads.\nThe geometric distribution is a natural model for “time to failure” experiments.\nFor example, suppose we install a light bulb, and measure how many days until the lightbulb burns out (one such experiment has been ongoing for a very long time!.\nWe can generate random geometric random variables using the rgeom function:\n\nrgeom(1, prob = 0.5) # Generate one geometric random variable with p=0.5. Most likely outcomes: 0,1,2\n\n[1] 0\n\n\nThe probability that a \\(\\operatorname{Geom}(p)\\) random variable \\(X\\) takes a particular value \\(k\\) (\\(k=0,1,2,\\dots\\)) is given by \\(\\Pr[ X = k ] = (1-p)^k p.\\)\nThis is the probability mass function of the geometric distribution. It assigns, to each singleton in the outcome space, a nonnegative number (i.e., its probability), and it does so in such a way that these nonnegative numbers sum to 1 (this ensures that these numbers do indeed specify a probability distribution!). Some texts will refer to this function as a distribution function, but I will avoid it in our notes and lectures, because there is possibility of confusion with other related terms (e.g., the cumulative distribution function, which we’ll see soon).\nLet’s plot this as a function of \\(k\\):\n\nlibrary(ggplot2)\nk &lt;- seq(0, 15)\np &lt;- 0.3\ndf &lt;- data.frame(\"k\" = k, \"Probk\" = p * (1 - p)^k)\npp &lt;- ggplot(df, aes(x = k, y = Probk)) +\n  geom_col()\npp\n\n\n\n\nLooking at the plot, we see that the geometric distribution puts most of its probability close to zero– the most likely outcomes are 0, then 1, then 2, and so on.\nWe plotted the distribution only up to \\(k=15\\), but a geometric random variable can, technically, take any non-negative integer as a value.\nFor any value of \\(k\\), \\(\\\\Pr[ X = k ] = p(1-p)^k\\) is non-zero (as long as \\(0 &lt; p &lt; 1\\)).\nSo for any non-negative integer, there is a small but non-zero probability that a geometric random variable takes that integer as a value.\nWe say that the geometric random variable has infinite support.\nThe support of a discrete random variable is the set of values that have non-zero probability mass. A random variable has infinite support if this set is infinite.\nExercise: verify that this is a bona fide probability by checking that \\(\\sum_{k=0}^\\infty p(1-p)^k = 1\\).\nNote: some resources follow a slightly different convention, whereby a geometric random variable counts the total number of attempts (i.e., coinflips) before success, so the support is \\(\\{1,2,\\dots\\}\\). Our discussion above follows the convention of most textbooks and research papers (and the convention followed by R– see ?rgeom), but this is an important thing to be aware of!\n\n\n3.4.5 Poisson\nLet’s look at one more discrete distribution.\nSuppose we are going fishing on lake Mendota, and we want to model how many fish we catch in an hour.\nA common choice for this situation is the Poisson distribution (named after a French mathematician named Poisson, but “poisson” is also French for “fish”).\nThe Poisson distribution is a common choice for modeling “arrivals” or other events that happen per unit time. Common examples include\n\ncustomers arriving to a store on a given day\ncalls to a phone line between 2pm and 3pm\nphotons or other particles hitting a detector during an experiment\ncars arriving at an intersection\n\nThe Poisson distribution has probability mass function \\[\n\\Pr[ X=k ] = \\frac{ \\lambda^k e^{-\\lambda} }{ k! }, ~ ~ ~ ~ ~ ~ k=0,1,2,\\dots\n\\] The parameter \\(\\lambda &gt; 0\\), often called the “rate parameter”, controls the average behavior of the random variable– larger choices of \\(\\lambda\\) mean that the resulting random variable is larger, on average (we will make this statement more precise in a few lectures). That is, the larger \\(\\lambda\\) is, the more arrivals happen per unit time– the larger \\(\\lambda\\) is, the higher the rate!\nWe can generate Poisson random variables using rpois:\n\nrpois(1, lambda = 10.5) # Generate Poisson RV with lambda=10.5 most likely value is 10.\n\n[1] 15\n\n\nWhat if I want several random Poissons, instead of just one?\nThe n argument to rpois (and all the other random variable generation functions) specifies a number of variables to generate.\nSo, for example, to get ten random Poissons, we can write\n\nrpois(10, lambda = 10.5) # Generate 10 Poisson RVs with the same parameter lambda=10.5\n\n [1]  8 17  9  9 13 12 10 13 11  7\n\n\nWhat does the probability mass function of the Poisson look like? Once again, the Poisson distribution has infinite support, since \\(\\Pr[X=k] &gt; 0\\) for all \\(k=0,1,2,\\dots\\) (check this!), but let’s plot its first few values.\n\nk &lt;- seq(0, 30)\nlambda &lt;- 10.5 # On average, we should get back the value 10.5,\ndf &lt;- data.frame(\"k\" = k, \"Probk\" = dpois(k, lambda))\npp &lt;- ggplot(df, aes(x = k, y = Probk)) +\n  geom_col()\npp\n\n\n\n\nThe function dpois above evaluates the Poisson probability mass function.\nThe R documentation calls this a density, which is correct, but… well, we will return to this.\nFor now, just remember “r for random”, “d for density”. rpois generates random Poisson variables, dpois evaluates its probability mass function.\n\n\n3.4.6 The Discrete uniform\nThe roll of a fair 6 sided die (or any \\(n\\)-sided die) can be modeled using a discrete uniform random variable. We will more often refer to the continuous uniform random variable, (which we’ll talk about shortly), but it’s worth mentioning that the roll of a die can be modeled easily.\nIn R, however, the discrete uniform is not a named distribution. Instead we would use the sample function. For example, suppose we wanted to simulate rolling a 6-sided die 100 times.\n\n# The sample function takes 3 important parameters.\n# x :      this can either be a vector of values to sample from\n#         or in this case simply the upper limit R will automatically\n#         create a vector from 1 to this value to sample from\n# size :   how many samples to generate\n# replace: this indicates whether we are able to re-sample the same value\n#         more than once. Careful, the default value is FALSE\nrolls &lt;- sample(x = 6, size = 100, replace = TRUE)\nrolls\n\n  [1] 6 5 1 2 4 1 2 2 4 3 3 3 1 4 4 1 3 6 6 4 6 1 3 6 5 5 3 4 1 1 5 3 5 6 6 1 5\n [38] 2 1 1 6 6 5 2 1 4 4 1 5 2 2 4 6 6 5 4 2 6 4 4 1 4 1 2 5 1 3 4 4 3 6 4 6 3\n [75] 5 1 5 5 5 5 5 5 3 6 3 1 4 2 4 6 6 1 5 1 2 4 6 3 6 5\n\n\n\n\n3.4.7 Aside: approximating one random variable with another\nInterestingly, we can obtain the Poisson distribution from the binomial distribution.\nLet’s make two assumptions about our fish population:\n\nThere are many fish in the lake. Let’s call the number of fish \\(N\\), which is a large number.\nFor each fish, there is a small probability \\(p\\) that we catch it (the same probability for each fish, for the sake of simplicity)\n\nIf we let \\(N\\) get arbitrarily large (“infinite” a limit like you remember from calculus) while \\(p\\) stays “small”, the Binomial distribution comes to be equal to the Poisson distribution with rate \\(Np\\).\nFor this reason, the Poisson is often a good approximation to the Bernoulli when \\(N\\) is large and \\(p\\) is small.\nJust to illustrate, let’s plot the density of the binomial with \\(N\\) really large and \\(p\\) really small, but chosen so that \\(Np = 10.5\\) to match \\(\\lambda = 10.5\\) above.\n\nk &lt;- seq(0, 30)\nlambda &lt;- 10.5\nN &lt;- 1e6\np &lt;- lambda / N # On average, we should get back the value lambda\npoisprob &lt;- dpois(k, lambda) # Vector of Poisson probabilities\nbinomprob &lt;- dbinom(k, size = N, prob = p) # Binomial probs.\n# We need a column in our data frame encoding which of the two distributions a number comes from.\n# This isn't the only way to do this, but it is the easiest way to get things to play nice with the ggplot2 facet_wrap, which displays separate plots for different values in a particular column.\ndist &lt;- c(rep(\"Poisson\", length(k)), rep(\"Binom\", length(k)))\n\n# Construct our data frame. Note that we have to repeat the k column, because our data frame is going to look like\n# Distribution  k  Probk\n# Poisson       0  dpois( 0, lambda )\n# Poisson       1  dpois( 1, lambda )\n# ...           ...\n# Poisson       30 dpois( 30, lambda )\n# Binomial      0  dbinom( 0, N, p )\n# ...\n# Binomial      30  dbinom( 30, N, p )\ndf &lt;- data.frame(\"dist\" = dist, \"k\" = rep(k, 2), \"Probk\" = c(poisprob, binomprob))\n\npp &lt;- ggplot(df, aes(x = k, y = Probk)) +\n  geom_col() +\n  facet_wrap(~dist)\n# facet_wrap tells ggplot to create a separate plot for each group (i.e., value) in the dist column.\npp\n\n\n\n\nWe will see several examples like this during the semester, in which two distributions become (approximately) equivalent if we fiddle with the parameters in the right way."
  },
  {
    "objectID": "rv.html#continuous-random-variables",
    "href": "rv.html#continuous-random-variables",
    "title": "3  Random Variables",
    "section": "3.5 Continuous random variables",
    "text": "3.5 Continuous random variables\nSo far we’ve seen a few different discrete random variables. Their set of possible values are discrete sets like \\(\\{0,1\\}\\) or \\(\\{0,1,2,\\dots\\}\\).\nThis is in contrast to continuous random variables, which take values in “continuous” sets like the interval \\([0,1]\\) or the real like \\(\\mathbb{R}\\).\nDiscrete random variables have probability mass functions, like \\(\\Pr[ X=k ] = p(1-p)^k\\), \\(k=0,1,2,\\dots\\).\nIn contrast, continuous random variables have probability density functions, which we will usually write as \\(f(x)\\) or \\(f(t)\\).\nThese random variables are a little trickier to think about at first, because it doesn’t make sense to ask about the probability that a continuous random variable takes a specific value. That is, \\(\\Pr[ X = k ]\\) doesn’t really make sense when \\(X\\) is continuous (actually– in a precise sense this does make sense, but the probability is always zero you’ll see why below).\nLet’s see some examples.\n\n3.5.1 Normal\nThe normal or Gaussian (after Carl Friedrich Gauss) random variable is undoubtedly the most fundamental in all of statistics.\nYou have likely heard of it before both in your previous courses and just… well, everywhere, in the form of the famous bell curve.\nThe normal distribution really is everywhere, and there are good reasons for this, which we will return to in a few lectures (see here for a preview).\nThe normal distribution is controlled by two parameters: a mean \\(\\mu \\in \\mathbb{R}\\) and a variance \\(\\sigma^2 &gt; 0\\).\nThe standard normal has \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\), and it “looks like” this:\n\nx &lt;- seq(-4, 4, 0.1)\nf &lt;- dnorm(x, 0, 1) # eval density at x values, mean mu=0, standard deviation sigma=1.\ndf &lt;- data.frame(\"x\" = x, \"density\" = f)\npp &lt;- ggplot(df, aes(x = x, y = density)) +\n  geom_line(size = 1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\nThis is the probability density function of the standard normal. Of course, the curve extends out to infinity on the right and negative infinity on the left we just haven’t plotted it.\nHopefully this is a familiar shape to you. If not, no worries– you’ll see it plenty as you continue your studies.\nThis is a probability density, not a mass function, because to evaluate something like the probability that a normal random variable \\(X\\) falls between, say, \\(-1\\) and \\(1\\), we have to integrate the area under this curve between these two endpoints.\nThis is why we refer to this as a density– recall from physics that integrating a density over a region (i.e., a volume) gives us a mass (compare to the discrete case, where we did call it a probability mass function).\nSaid another way, integrating the density over a region gives us the probability of that region. So if \\(X\\) is a normal random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\), \\[\n\\Pr[ -1 \\le X \\le 1 ] = \\int_{-1}^1 f(t; \\mu, \\sigma^2) dt,\n\\] where \\(f(t; \\mu, \\sigma^2)\\) is the density function of the normal distribution, \\[\nf(t;  \\mu, \\sigma^2) = \\frac{1}{\\sqrt{ 2\\pi \\sigma^2 } } \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right \\}.\n\\] The weird semicolon notation is to emphasize that the density is a function of \\(t\\), but its shape depends on \\(\\mu\\) and \\(\\sigma^2\\).\nThink of \\(\\mu\\) and \\(\\sigma\\) as two knobs we can twiddle to change the shape of the curve.\nChanging \\(\\mu\\) while keeping \\(\\sigma^2\\) fixed just shifts the distribution:\n\nx &lt;- seq(-6, 6, 0.1) # Some x values to evaluate at\nf1 &lt;- dnorm(x, -2, 1) # mean mu=-1, standard deviation sigma=1.\nf2 &lt;- dnorm(x, 0.5, 1) # mean mu=1.5, standard deviation sigma=1.\nf3 &lt;- dnorm(x, 1.5, 1) # mean mu=1.5, standard deviation sigma=1.\nmu &lt;- c(rep(-1, length(x)), rep(1.5, length(x)), rep(2.0, length(x)))\ndf &lt;- data.frame(\"x\" = rep(x, 3), \"density\" = c(f1, f2, f3), \"mean\" = as.factor(mu))\npp &lt;- ggplot(df, aes(x = x, y = density, color = mean)) +\n  geom_line(size = 1)\npp\n\n\n\n\nChanging \\(\\sigma\\) changes the standard deviation (and hence also the variance, \\(\\sigma^2\\)). Larger \\(\\sigma\\) means higher variance, which means a “wider” distribution:\n\nx &lt;- seq(-6, 6, 0.1) # Some x values to evaluate at\nf1 &lt;- dnorm(x, 0, 0.5) # mean 0, standard deviation sigma=1.\nf2 &lt;- dnorm(x, 0, 1) # mean 0, standard deviation sigma=2.\nf3 &lt;- dnorm(x, 0, 2) # mean 0, standard deviation sigma=3.\nsigma2 &lt;- c(rep(0.5, length(x)), rep(1, length(x)), rep(2, length(x)))\ndf &lt;- data.frame(\"x\" = rep(x, 3), \"density\" = c(f1, f2, f3), \"variance\" = as.factor(sigma2))\npp &lt;- ggplot(df, aes(x = x, y = density, color = variance)) +\n  geom_line(size = 1)\npp\n\n\n\n\nWe’ll have plenty more to say about this later in the semester when we talk about fitting models to data. For now, think back to our human heights example from our introductory lecture– we played with the \\(\\mu\\) and \\(\\sigma^2\\) parameters to find a normal distribution that “looked” a lot like our observed data.\nWe can generate normal random variables in R using the rnorm function:\n\nrnorm(1, mean = 1, sd = 2) # Note that we pass the standard deviation (sd), not the variance sigma^2.\n\n[1] 0.7080544\n\nrnorm(1) # If we don't specify a mean and sd, they default to 0 and 1, respectively\n\n[1] -1.023645\n\n\nLet’s generate a bunch of normal RVs and plot their (normalized) histogram:\n\n# Reminder: if mu, sigma unspecified, they default to mu=0, sigma=1\ndata &lt;- rnorm(1e6) # 1e6 = 1.0x10^6 = one million standard normal RVs\npp &lt;- ggplot(data.frame(\"x\" = data), aes(x = data))\npp &lt;- pp + geom_histogram(aes(y = ..density..), binwidth = 0.25, color = \"black\", fill = \"white\")\npp\n\nWarning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(density)` instead.\n\n\n\n\n\nNote that this is a normalized histogram– it is scaled so that the areas of the rectangles sums to 1, like a probability distribution.\nNow, let’s overlay the normal density function on this.\n\nx &lt;- seq(-5, 5, 0.1)\nf &lt;- dnorm(x, mean = 0, sd = 1) # Evaluating the density at points x\ndf_dnorm &lt;- data.frame(\"x\" = x, \"f\" = f)\npp &lt;- pp + geom_line(data = df_dnorm, aes(x = x, y = f), size = 1, color = \"red\")\npp\n\n\n\n\nLook at how closely the histogram matches the density! This is no accident. The density describes the average-case behavior of the random variable.\nBecause we plotted a standard normal, one unit on the x-axis of the plot above is one standard deviation.\nYou will hopefully recall from any previous statistics courses that a normal random variable falls\n\nwithin one standard deviation of the mean (i.e., between -1 and 1 in the plot above) with probability about 0.68,\nwithin two standard deviations of the mean (i.e., between -2 and 2 in the plot above) with probability about 0.9545\nwithin three standard deviations of the mean (i.e., between -3 and 3 in the plot above) with probability about 0.997.\n\nWe can approximately check this by counting how many of our simulated normals fall in these ranges. What fraction of these data points fell within one standard deviation of the mean?\n\nsum(data &gt; -1 & data &lt; 1) / length(data) # Should be about 0.68\n\n[1] 0.682746\n\n\n\nsum(data &gt; -2 & data &lt; 2) / length(data) # should be about 0.9545\n\n[1] 0.954521\n\n\n\nsum(data &gt; -3 & data &lt; 3) / length(data) # should be about 0.997\n\n[1] 0.99727\n\n\nOf course, because the data is random, the proportions are not exactly equal to their predicted values, but they are quite close.\nThis is a nice illustration of the law of large numbers!\n\n3.5.1.1 Computing probabilities\nNow, let’s check this fact by using R to compute probabilities of the form \\(\\Pr[ a \\le X \\le b ]\\) for a normal RV \\(X\\), where \\(a \\ge b\\). How can we compute this probability?\nLet’s start by writing something slightly different: \\[\n\\Pr[ X \\le b ] = \\Pr[ X \\le a ] + \\Pr[ a \\le X \\le b].\n\\]\nQuestion: why is this true? What “rule” of probability are we using?\nNow, let’s rearrange that expression: \\[\n\\Pr[ a \\le X \\le b] = \\Pr[ X \\le b ] - \\Pr[ X \\le a ].\n\\]\nHow exactly does this help us? We replaced one probability with a sum of two… that seems worse!\n…at least until I tell you (or you remember from previous courses) that for all the “nice” distributions, R makes it very easy to compute probabilities of the form \\(\\Pr[ X \\le a]\\). The function \\[\nF_X(t) = \\Pr[ X \\le t]\n\\] is called the cumulative distribution function (CDF) of \\(X\\).\nWe compute the CDF of the normal in R with pnorm. So, for example, to compute the probability that a standard normal random variable is \\(\\le 0\\), we can write\n\npnorm(0) # Once again, mu, sigma default to 0,1.\n\n[1] 0.5\n\n\nAside/exercise: You might enjoy spending some time thinking on why it is “obvious” that a normal random variable is less than or equal to its mean with probability one half. Remember (look at the density function above, or just look at our plots) that the normal density is symmetric about the mean \\(\\mu\\). If you’re feeling up to it (warning: calculus ahead!), you can try computing the appropriate integrals to show that under the normal, \\(\\Pr[ X \\le \\mu ] = \\Pr[ X &gt; \\mu ] = 1/2\\).\nSo suppose that we want to compute the probability that a standard normal random variable is less or equal to 1. This would be\n\npnorm(1) # again, recall the default behavior.\n\n[1] 0.8413447\n\n\nAnd to compute the probability that we are within one standard deviation of the mean, we need to compute \\[\n\\Pr[ -1 \\le X \\le 1] = \\Pr[ X \\le 1 ] - \\Pr[ X \\le -1 ] = \\texttt{ pnorm(1) - pnorm(-1) }\n\\]\nThat is,\n\npnorm(1) - pnorm(-1)\n\n[1] 0.6826895\n\n\nwhich matches our 0.68-.9545-0.997 rule so far. Let’s check the other two.\n\npnorm(2) - pnorm(-2)\n\n[1] 0.9544997\n\n\n\npnorm(3) - pnorm(-3)\n\n[1] 0.9973002\n\n\nBy the way, if you’ve seen this bell-shaped curve before in your classes, you probably associate it with Z-scores and the magic number 1.96.\nWe’ll see a bit of that this semester, but for the most part, we will use Monte Carlo simulation to do testing.\nThis is a different approach to statistics that lets us avoid worrying so much about Z-tables and 1.96 and all that, but in the interest of full disclosure: really most of the basic parts of R are just fancy Z-tables. So R does the annoying work for us!\n\n\n\n3.5.2 Recap: RVs in R (so far)\nSo far, we have seen three functions for working with different distributions.\nFor the normal, we have rnorm, dnorm and pnorm, corresponding to random variable generation, the density function and cumulative distribution function (CDF), respectively.\nOther RVs have similar patterns. For example, with the geometric random variable, we have rgeom, dgeom and pgeom, which work analogously to the normal case.\nThere is one more of these functions yet to come, but we aren’t quite ready to introduce it.\n\n\n3.5.3 Aside: Expectation for continuous random variables\nPreviously, we defined the expectation of a discrete random variable \\(X\\) to be \\[\n\\mathbb{E} X = \\sum_k k \\Pr[ X = k ],\n\\] with the summand \\(k\\) ranging over all allowable values of \\(X\\).\nWhen \\(X\\) is continuous, the sum doesn’t make sense, so how should we define the expectation?\nWell, just change the sum to an integral! After all, the integral sign is just a fancy S for “sum” (okay, it’s for the Latin word “summa” or something along those lines, but close enough!).\n\\[\n\\mathbb{E} X = \\int_\\Omega t f(t) dt,\n\\]\nwhere \\(f(t)\\) is the density of \\(X\\) and \\(\\Omega\\) is the support.\nExercise: Let’s flex those calculus muscles! Check that the mean of a normal with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) is indeed \\(\\mu\\). That is, check that \\[\n\\int_{-\\infty}^\\infty \\frac{ t }{ \\sqrt{ 2\\pi \\sigma^2}} \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\} dt = \\mu.\n\\]\nHint: Make the substitution \\(u=t-\\mu\\) and use the facts that\n\n\\(f(t) = e^{-(t-\\mu)^2/2\\sigma^2}/\\sqrt{2\\pi \\sigma^2}\\) integrates to \\(1\\) (because it is a probability density), and\n\\(g(u) = u \\exp\\{ -u^2/2\\sigma^2 \\}\\) is symmetric about zero (and thus integrates to zero).\n\n\n\n3.5.4 Uniform\nThe uniform distribution is a continuous random variable whose density is constant on its outcome space.\nThat is, for continuous set \\(\\Omega \\subseteq \\mathbb{R}\\), the density is identically \\(f(t) = c\\).\nRemember that our probability has to sum to 1, but since we have a continuous support, this sum becomes an integral:\n\\[\n\\int_{\\Omega} f(t) dt = \\int_{\\Omega} c dt = c \\int_{\\Omega} 1 dt.\n\\]\nSo to make the probability integrate to \\(1\\), we need \\(c = \\int_{\\Omega} 1 dt\\).\nMost commonly, we take \\(\\Omega = [0,1]\\), and call the resulting random variable “uniform 0-1”, written \\(\\operatorname{Unif(0,1)}\\).\nThe density function of \\(\\operatorname{Unif(0,1)}\\) is then given by \\[\nf(t) = \\begin{cases}\n  1 &\\mbox{ if } 0 \\le t \\le 1 \\\\\n  0 &\\mbox{ otherwise. } \\end{cases}\n\\]\nExercise: check that this indeed integrates to \\(1\\), i.e., that it is a valid probability density. The support is \\([0,1]\\), so we need to check that \\(\\int_0^1 1 dt = 1\\).\nThe most common application of uniform random variables is in simulations when we need to make a random decision.\nFor example, suppose we didn’t have the Bernoulli distribution available to us for some reason, but we still wanted to generate random coin flips.\nTo generate a \\(\\operatorname{Bernoulli}(p)\\) random variable, we could first draw a uniform random variable \\(U \\sim \\operatorname{Unif}(0,1)\\) and then output \\(1\\) (or “heads” or “true”) if \\(U \\le p\\) and out put \\(0\\) (or “tails” or “false”) otherwise.\n\n\n3.5.5 Exponential\nLet’s look at one more continuous random variable.\nThe exponential distribution is most commonly used to model “waiting times”, like how long until the bus arrives.\nIn many ways, the exponential distribution is like the continuous version of the geometric distribution.\nLike geometric random variables, the exponential distribution is non-negative and is controlled by a single parameter \\(\\lambda &gt; 0\\), called the rate (because larger \\(\\lambda\\) means less time before the event, hence more events per unit time, i.e., a higher rate of events).\nThe density is given by \\[\nf(t  \\lambda ) = \\begin{cases}\n  \\lambda \\exp\\{ - \\lambda t \\} &\\mbox{ if } t \\ge 0 \\\\\n  0 &\\mbox{ otherwise. }\n  \\end{cases}\n\\]\nExercise: check that this defines a probability distribution by checking that for any \\(\\lambda &gt; 0\\), (1) \\(f(t \\lambda) \\ge 0\\) and (2) \\(\\int_0^\\infty f(t \\lambda) dt = 1\\).\nLet’s plot this density as a function of \\(t\\) for \\(\\lambda = 1\\).\n\nx &lt;- seq(0, 10, 0.1)\nf &lt;- dexp(x, rate = 1)\ndf_exp &lt;- data.frame(\"x\" = x, \"density\" = f)\npp &lt;- ggplot(df_exp, aes(x = x, y = density)) +\n  geom_line(size = 2)\npp\n\n\n\n\nLooking at the density, we see that most of the probability is near zero.\nRoughly, it looks like the vast majority of the time, \\(X \\sim \\operatorname{Exp}(1)\\) should be less than 5. Let’s check!\n\ndata &lt;- rexp(n = 1e6, rate = 1) # rexp to generate random exponentials.\nsum(data &lt; 5) / length(data) # something like 99% of the generated points should be below 5.\n\n[1] 0.993268\n\n\n\n\n\n3.5.6 Bonus: Building Bigger Models\nThere are plenty more named distributions out there. See here.\nIndeed, we can make all sorts of distributions– ultimately we just need to specify a support and a density or mass function, depending on whether we want a discrete or continuous variable.\nMore often, though, the processes out in the world that we want to model require that we build more complicated models from simple ones.\n\n\n3.5.7 Social network models\nSeveral professors in the statistics department study networks. A common example of these are social networks (e.g., Facebook and twitter).\nThese data usually take the form of a list specifying who is friends with whom.\nIn most models for social networks, each pair of people \\(i\\) and \\(j\\) become friends, independently, with some probability \\(p_{ij}\\).\nOften, these probabilities depend on other random variables.\n\nFor example, we might have a (random) number for every person that describes how much that person likes sports, and pairs of people who both have a high “sports score” are more likely to be friends (i.e., \\(p_{ij}\\) is higher).\n\nIn this way, we combine different elementary random variables (e.g., Bernoullis and normals) to yield more complicated distributions (say, a probability distribution over “possible social networks”).\nHaving obtained a network from Twitter or Facebook data, we apply all sorts of different functions to it that describe the network’s structure (e.g., compute shortest paths or eigenvectors, if you know what those are).\nIt is really difficult to do the math out exactly to determine analytically how these different functions behave.\nInstead, we often appeal to Monte Carlo methods, which we will discuss in the next lecture.\n\n\n3.5.8 Binomial asset pricing model\nAnother way to create new random variables is to take a function of another random variable. This is what happens in the pricing of “options” in finance.\nSuppose that \\(X\\) is the price of stock XYZ one month from today.\nAn “option” pays you \\(f(X)\\) on that day for some function \\(f\\).\nFor example, suppose stock XYZ costs $120 today and the function is\n\\[\nf(X)= \\begin{cases} 120−X &\\mbox{ if } X&lt;120 \\\\\n    0 &\\mbox{ otherwise. } \\end{cases}\n\\]\nThis is often referred to as a put option. It is essentially giving you the option to purchase the stock in one month and sell it at today’s price.\n\nIf the price goes up, you would not use that option and you would make zero (but not lose any additional money, such as the cost of buying the option in the first place).\nOtherwise, you would make \\(120−X\\).\nIn effect, you are betting that the price will go down.\n\nSuppose you are a bank and someone wants to purchase this put option. You need to determine the price.\nWhat would be the fair price to charge them?\nTo make a good guess, we need a model for the asset price \\(X\\).\nOnce we have such a model, we can derive analytical expressions for \\(f(X)\\) or use Monte Carlo methods, which we will discuss soon.\nOne of the simplest models for \\(X\\) is the Binomial Asset Pricing Model, which says that at every time step (e.g., every minute), the price goes up by one penny with probability \\(p\\), or down by one penny with probability \\(1−p\\).\nIn this example, both \\(X\\) and \\(f(X)\\) are random variables.\nQuestion: The Binomial Asset Pricing Model is a very simple model, especially given how complicated the stock market is. Can you think of any possible problems with the model?\n\n\n3.5.9 Election models\nWill the Democratic (D) or Republican (R) presidential candidate win Wisconsin in 20xx?\nWhat is the distribution of this random variable \\(W\\), where \\(W=−1\\) if D and \\(W=1\\) if R?\nWhat about neighboring Michigan \\(M \\in \\{-1,1\\}\\)?\nWisconsin and Michigan are not so different, so if we find out that the Republican candidate won Michigan (i.e., we learn about \\(M\\)), then that certainly tells us something about \\(W\\).\nThat is to say, \\(W\\) and \\(M\\) are not independent.\nWhat if we wanted to model both Wisconsin and Michigan together?\nWe usually write this as a pair \\((W,M)\\).\nOne thing you could do is model the proportion of votes for D vs R in Wisconsin (ignoring third parties) as normally distributed.\n\nPerhaps you consider this to be \\(W_p \\sim \\operatorname{Normal}(1/2,.05)\\).\nThen, \\(W\\) is 1 if \\(W_p&gt;.5\\) and \\(W=−1\\) if \\(W_p&lt;.5\\).\n\nThis helps because we can do the same thing for \\(M\\), based on and \\(M_p\\).\nWe can model \\((W_p,M_p)\\) as being correlated via the multivariate normal.\nTo simulate these, you need to specify both the mean, which is now a two-dimensional “vector” and a covariance matrix \\(\\Sigma \\in \\mathbb{R}^{2 \\times 2}\\).\nThe diagonal of the matrix \\(\\Sigma\\) specifies the variances of \\(W_p\\) and \\(M_p\\) respectively, and the off-diagonal \\(\\Sigma_{1,2}\\) specifies the covariance between \\(W_p\\) and \\(M_p\\).\nIf you haven’t heard of covariance before, think of it like a measure of how closely two variables track one another, similar to correlation.\n\nWhen the covariance is large and positive, the two variables tend to track one another.\nIf the covariance is large and negative, the two variables are inversely related to one another.\n\nLet’s try simulating some election results.\n\nmu &lt;- c(.5, .5) # Vector of means both W_p and M_p are mean 1/2.\nCovMx &lt;- matrix(c(.05^2, .04^2, .04^2, .05^2), nrow = 2) # Make a two-by-two symmetric matrix.\nCovMx\n\n       [,1]   [,2]\n[1,] 0.0025 0.0016\n[2,] 0.0016 0.0025\n\n\n\nlibrary(MASS) # This library includes a multivariate normal\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nWpMp &lt;- mvrnorm(n = 2000, mu = mu, Sigma = CovMx) # mvrnorm is the multivariate version of rnorm.\nplot(WpMp, xlab = \"Wisconsin proportion\", ylab = \"Michigan proportion\")\nlines(c(.5, .5), c(-10, 10), col = \"red\")\nlines(c(-10, 10), c(.5, .5), col = \"red\")\n\n\n\n\nEach point in this plot corresponds to one simulated election.\nQuestions:\n\nWhat region of this plot corresponds to \\(W=−1\\) and \\(M=+1\\)?\nDoes it make sense that there are fewer points in the top left compared to the top right?\nAre Michigan and Wisconsin positively or negatively correlated (based on this plot, anyway)?\n\n\n\n3.5.10 Review:\nIn these notes we covered:\n\nThe basic rules of probability: outcome spaces, events\nthe concept of a random variable\nFamilies of discrete random variables: Bernoulli, binomial, geometric, Poisson and uniform\nFamilies of continuous random variables: Gaussian (normal), exponential and uniform\nThe concept of expected value\nPMF, PDF and CDF\ncomputing probabilities\nsome applications of random variables"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "2  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "References"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT 340: Data Science II",
    "section": "",
    "text": "Here are the course notes, practice problems and other course materials for STAT 340 at UW Madison.\nAuthor credits to Bi Cheng Wu, Brian Powers, Keith Levin\nExample shiny live app\nNormal PDF CDF Inverse CDF"
  },
  {
    "objectID": "intro.html#what-is-data-science",
    "href": "intro.html#what-is-data-science",
    "title": "2  Introduction",
    "section": "2.3 What is data science?",
    "text": "2.3 What is data science?\nIs “data science” just a rebranding of applied statistics?\nWell, perhaps, to an extent. But the emergence of “data science” has coincided with huge changes in how science and statistics gets done.\nComputers and the internet have made it easy to collect, share and analyze data in ways that were unimaginable only a decade or two ago.\nSeveral times this semester, we will talk about fundamental parts of the data science toolkit that are usable only because computers are fast and cheap.\nThis change in technology has changed how we do science. In this sense, “data science” is a culture or a way of thinking, more than it is a field."
  },
  {
    "objectID": "intro.html#wearing-different-hats",
    "href": "intro.html#wearing-different-hats",
    "title": "2  Introduction",
    "section": "2.4 Wearing different hats",
    "text": "2.4 Wearing different hats\nBeing a good data scientist requires that we be able to draw on many different fields and ways of thinking.\nA “well rounded” data scientist should move fluidly among multiple ways of thinking and approaching a problem.\nHere are a few of the different kinds of hats I (Keith) find myself wearing in my day-to-day work:\n\nscientist: understanding data domain, developing questions, “story telling”\nsoftware dev: data processing and wrangling / reproducibility\nmathematician: linear algebra, probability theory, optimization\nmethodologist: regression, unsupervised learning, visualizations\nscience communicator: summarizing results, explaining broader impacts\n\nThis hints at why it is so hard to become a truly good data scientist– you need to be pretty good at a lot of different things.\nIf nothing else, going into this line of work requires that you be ready and willing to be a life-long student There are always new techniques, methods, frameworks and application domains to be learned!"
  },
  {
    "objectID": "intro.html#topics-covered",
    "href": "intro.html#topics-covered",
    "title": "2  Introduction",
    "section": "2.5 Topics covered",
    "text": "2.5 Topics covered\nWe will cover five basic topics this semester:\n\nSampling\nEstimation\nTesting\nPrediction\nObservational/exploratory data analysis\n\nLet’s briefly discuss each of these.\n\n2.5.1 Sampling\nElection polls and other survey data often get reported with response rates and other details about how survey respondents were selected.\nExample: At the end of October 2020, pollsters were trying to predict the outcome of the 2020 presidential election. A common technique for this is “random digit dialing”, which is exactly what it sounds like.\nThis poll reached 806 registered voters in Wisconsin. After a significant amount of statistical work, the pollsters reported that\n\n48% of likely voters would choose Biden\n43% would vote for Trump\n2% for Jorgensen, and\n7% remained undecided.\n\nIn order to reach those 806 participants, many more numbers need to be dialed. In this poll, the response rate was 4.3%. If you read the methodology report, you’ll see that in fact over 100,000 numbers had to be dialed to get these 806 respondents. The vast majority of those 100,000 calls were never picked up. Among those who did pick up, 806 were registered voters and agreed to participate in the survey, but another were 1113 refused to participate (or hung up).\nIn the actual election,\n\nBiden received 49.45% of votes cast in Wisconsin and\nTrump received 48.82%.\n\nQuestions:\n\nHow does this compare with the predicted vote shares of 48% for Biden and 43% for Trump?\nHow might we explain the discrepancies?”\n\n\n\n2.5.2 Testing\nYou have no doubt heard that “correlation does not imply causation”. Well, this is true.\n\nIce cream consumption is correlated with drowning, but we don’t think that ice cream causes drowning.\nHospitals are full of sick people, but we don’t think that hospitals cause sickness.\n\nNonetheless, to paraphrase a relevant comic, causality often does give awfully strong hints.\n\n\n\nCredit: Randall Munroe, XKCD (https://xkcd.com/552)\n\n\nExample: On November 16, 2020 Moderna released results from their phase 3 clinical trial for their COVID-19 vaccine.\nThere were approximately 30,000 people in the trial, split (approximately) evenly between treatment (got the vaccine) and control (got a placebo).\n\nIn total, there were 95 cases of COVID-19 among the participants; 90 among the placebo group and 5 among the treated group.\nOf the 95 cases, 11 were severe cases, all in the placebo group.\n\nIn this study, vaccination is correlated with reduced risk of infection.\n\nDoes this mean that the vaccination causally reduced COVID-19 infection? Why or why not?\nHow do we know that we aren’t fooling ourselves when we say that the Moderna vaccine is effective?\n\n\n\n2.5.3 Estimation\nThe news is perpetually full of stories about different economic indicators and how they are changing over time.\n\nThe Consumer Price Index (CPI) is meant to measure the change over time in prices of consumer goods and services.\nMost surveys (e.g., public opinion or election surveys) are reported with a +/- 3% “confidence interval” or “sampling error”.\n\nWhat is that all about?\nExample: In the Wisconsin poll discussed above, the margin of error was reported to be +/- 4.3%.\nRecall that the pollsters predicted 48% of likely voters would choose Biden and 43% Trump, while the actual outcome was 49.45% for Biden and 48.82% for Trump.\n\nIs this outcome within the stated margin of error?\nMore generally, what does it mean to give a confidence interval for a quantity of interest?”\n\n\n\n2.5.4 Prediction\nInvesting successfully (in real estate, stocks, etc) requires that we be able to predict the future behavior of an asset based on what we know about it currently.\n\nBased on the size of a house, its proximity to schools or infrastructure, walkability of its neighborhood, etc., we might hope to predict its “true” price.\nMany psychology and sociology studies aim to predict future student outcomes based on performance on a standardized test\n\nIn these kinds of problems, our goal is to predict an outcome or response (e.g., house price) based on one or more predictors (e.g., square footage of the house).\nMost of machine learning is concerned with prediction problems. For example, labeling an image according to whether or not it contains a cat can be stated as a prediction problem.\nExample: this plot shows different people’s incomes (in tens of thousands of dollars per year) as a function of their years of education.\n\n\n\nEducation by income\n\n\nIt certainly looks like more years of education correlate with higher income.\nQuestion: Suppose I tell you that someone has 18 years of education. What would you predict their income to be?\n\n\n2.5.5 Observational/exploratory data analysis\nSuppose that a colleague or client gives you a data set that looks like this:\n\nWhat would you do? There is clearly some kind of a cluster structure present here.\nThe goal of exploratory data analysis is to identify interesting structures in our data that might warrant further study.\nExample: In my own research, I (Keith) collaborate a lot with neuroscientists, who are interested in identifying functional subnetworks of the brain. These are groups of neurons that work together, typically because they are associated with the same activity (e.g., attention, motion, speech).\nThis is an example of clustering, in which our goal is to group data points in a sensible way, without necessarily saying ahead of time what those groups mean\nOftentimes, we obtain observational data. That is, data that does not come from a carefully-designed experiment.\nExample: Much data in the modern age of “big data” is collected by scraping the web or collected in other “messy” ways.\n\nScraping data from a social media site such as Twitter.\nMeasuring the socioeconomic status of people in different zip codes\n\nThere are lots of interesting scientific questions we might like to ask about such a data set, but because this data isn’t the result of a carefully-controlled experiment, we are often much more limited in what we can say."
  },
  {
    "objectID": "intro.html#world-data-model-hat-tip-to-karl-rohe",
    "href": "intro.html#world-data-model-hat-tip-to-karl-rohe",
    "title": "2  Introduction",
    "section": "2.6 World, Data, Model (hat tip to Karl Rohe)",
    "text": "2.6 World, Data, Model (hat tip to Karl Rohe)\nUnless you subscribe to some rather outlandish philosophical beliefs (see, e.g., here), there is a world out there, which we would like to learn about.\nTo figure things out about the world, we take measurements. That is to say, we collect data. These data describe the world, but it remains to build a model that explains how these data came to be.\nThe process of inference is how we use a specific set of data to guide our beliefs about the world.\nAs a simple example, consider human height. What is the average adult human height?\nThis is a question about the world. In fact, we could compute the average adult human height exactly if we could go out and measure the height of every adult human.\n\nOf course this would be a complicated and expensive process.\nInstead, we could just measure the heights of a few adult humans (say, a few thousand).\n\nOf course, this small collection of humans would need to be chosen randomly and in such a way that they would be representative of the population as a whole, but let’s ignore that concern until later in the course.\nThe few thousand heights that we measure would constitute our data– measurements that were taken out there in the world.\nHere is a simulation of what that data might look like with a sample of size 2000.\n\n# heights.R contains code for generating our synthetic data set.\nsource('r_scripts/heights.R');\n\nLoading required package: ggplot2\n\npp &lt;- ggplot( df_heights, aes(x=heights ) );\npp &lt;- pp + geom_histogram(aes(y=..density..), fill=\"grey\", color=\"black\", binwidth=2, );\npp &lt;- pp + geom_vline(xintercept=mean(heights), color=\"black\", linetype='f8', linewidth=1);\npp\n\n\n\n\nThe vertical dashed line indicates the mean of these 2000 sampled heights.\nBecause this is a random sample (and because heights vary due to factors like nutrition and genetics), this sample mean need not be equal to the population mean (i.e., the true average adult human height).\nInstead, the heights in our sample (and their mean) will vary randomly about the population average height. We use a statistical model (i.e., probability theory) to describe this variation.\nA common choice for modeling random variation like this is the normal distribution (in the coming lectures we will see plenty of other distributions).\nWe assume that our data are generated by a normal distribution with some mean \\(\\mu\\) (i.e., the average height) and some standard deviation \\(\\sigma\\). We call these parameters of the model.\nEstimating the population average height then reduces to estimating the “true” value of the parameter \\(\\mu\\) based on the data.\nThis step of using our data to determine something about our model is called inference. In this case, our goal is to estimate the value of the model parameter \\(\\mu\\), which will in turn be our estimate of the average adult human height."
  },
  {
    "objectID": "intro.html#all-models-are-wrong-but-some-are-useful",
    "href": "intro.html#all-models-are-wrong-but-some-are-useful",
    "title": "2  Introduction",
    "section": "2.7 All models are wrong, but some are useful",
    "text": "2.7 All models are wrong, but some are useful\nAn important point in our human height example above was our assumption that heights are normally distributed. In practice, modeling assumptions like these are never strictly true.\nOur model is just that– a model of the world; a set of simplifying assumptions that we hope are at least a good approximation to the truth.\nThink of your physics courses, where we assume that things happen in a frictionless void and use Newtonian mechanics instead of quantum mechanics.\nWe make assumptions like these because they often make the math easier while still being a good approximation to reality.\nIn our example above, the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) says that with some (perhaps very small) probability, we might observe a negative number.\nIf we model human heights as being normally distributed, our model predicts that we should, on occasion, meet people who have negative height.\n\nWhy might that be a problem?\nWhy might it be okay to still use this model anyway?\n\nLet’s push ahead and “fit” a normal to our data. We’ll have lots to say about this later. For now, think of this as choosing, out of all the possible normal distributions, the one that “best agrees”” with our data.\n\n# The dataframe df_normal contains a column of x-values in the same range as our synthetic data.\n# df_normal$y contains the normal pdf with mean and standard deviation fit to our synthetic data,\n# evaluated at these x-values.\npp &lt;- pp + geom_line( aes(x=df_normal$x, y=df_normal$y), size=1, color='red' );\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\nHuh. Our data seems to be a bit “flatter” than the normal distribution would predict.\nPerhaps this is just due to random fluctuation, but in fact there is a very simple reason for this: We didn’t tell you about it, but this sample includes both males and females.\nLet’s plot the same histogram, but this time, let’s break out the heights according to sex.\n\npp &lt;- ggplot( df_heights, aes(x=heights, color=sex, fill=sex ) );\npp &lt;- pp + geom_histogram( aes(y=..density..), position=\"identity\", alpha=0.5, binwidth=2);\npp\n\n\n\n\nHuman heights are bimodal– female heights are approximately normal about some mean, and male heights are approximately normal about another.\nWe can fit a normal to each of these separately and we see that our model agrees with the data much better.\n\n# df_bimodal$x contains x values that agree with the range of our height data.\n# df_bimodal$density is the density of a normal distribution, fit to the male or female heights,\n#        evaluated at these x-values.\n# df_bimodal$sex encodes male/female\\n\",\npp &lt;- pp + geom_line(data=df_bimodal, aes(x=x, y=density, color=sex), size=2, alpha=0.5);\npp\n\n\n\n\nThis is a good (albeit simple) illustration of the kind of iterative workflow that we typically use in data science.\n\nWe obtain our data, fit a model to it, and then we examine the shortcomings of that model.\nAfter some thought, it becomes clear how to improve our model.\nWe implement those changes (in this case, we incorporated the variable sex into our model), and examine our findings again.\n\nTypically, we repeat this cycle several times before reaching a conclusion that we are confident in."
  },
  {
    "objectID": "intro.html#overview",
    "href": "intro.html#overview",
    "title": "2  Introduction",
    "section": "2.1 Overview",
    "text": "2.1 Overview\nIn this introductory set of notes, we will:\n\nget a flavor for what data science is\ndiscuss why you might want to take a course like this one and\nhighlight the topics that we will cover this semester."
  },
  {
    "objectID": "intro.html#motivating-questions",
    "href": "intro.html#motivating-questions",
    "title": "2  Introduction",
    "section": "2.2 Motivating Questions",
    "text": "2.2 Motivating Questions\nConsider the following questions:\n\nHow effective is the Pfizer vaccine against the delta variant of COVID-19?\nDo early intervention programs (e.g., Head Start) improve educational outcomes for low-income children?\nAre record-high temperatures in recent years explainable by chance?\n\nThese questions are a little too complicated for an introductory course like this, but they are the kinds of questions that data science is equipped to answer.\nOur job as data scientists is to draw on tools from statistics, computer science and mathematics, in collaboration with domain experts, to answer questions like these.\nThat being said, collecting, cleaning and analyzing data is only part of the job.\nThe most important skill you will learn in this course is not a statistical or computational tool (though those are important!). It is the abilitity to clearly organize and explain your findings in a way that is appropriate for your intended audience."
  },
  {
    "objectID": "cov.html#learning-objectives",
    "href": "cov.html#learning-objectives",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.1 Learning objectives",
    "text": "4.1 Learning objectives\nAfter this lesson, you will be able to\n\nExplain what it means for variables to be dependent or independent and assess how reasonable independence assumptions are in simple statistical models.\nExplain expectations and variances of sums of variables are influenced by the dependence or independence of those random variables.\nExplain correlation, compute the correlation of two random variables, and explain the difference between correlation and dependence.\nDefine the conditional probability of an event \\(A\\) given an event \\(B\\) and calculate this probability given the appropriate joint distribution.\nUse Bayes’ rule to compute \\(\\Pr[B \\mid A]\\) in terms of \\(\\Pr[A \\mid B]\\), \\(\\Pr[A]\\) and \\(\\Pr[B]\\)."
  },
  {
    "objectID": "cov.html#recap-rvs-events-and-independence.",
    "href": "cov.html#recap-rvs-events-and-independence.",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.2 Recap: RVs events and independence.",
    "text": "4.2 Recap: RVs events and independence.\nWhen working with random variables, we start with a set of possible outcomes, usually denoted \\(\\Omega\\).\nA subset \\(E \\subseteq \\Omega\\) of the outcome space is called an event.\nA probability is a function that maps events to numbers, with the properties that\n\n\\(\\Pr[ E ] \\in [0,1]\\) for all events \\(E\\)\n\\(\\Pr[ \\Omega ] = 1\\)\nFor \\(E_1,E_2 \\in \\Omega\\) with \\(E_1 \\cap E_2 = \\emptyset\\), \\(\\Pr[ E_1 \\cup E_2 ] = \\Pr[ E_1 ] + \\Pr[ E_2 ]\\)\n\nNote: If \\(\\Pr[E_1 \\cap E_2]=\\emptyset\\), we say that \\(E_1\\) and \\(E_2\\) are mutually exclusive (or disjoint)\nWe say that two events \\(E_1\\) and \\(E_2\\) are independent if \\(\\Pr[ E_1 \\cap E_2 ] = \\Pr[ E_1 ] \\Pr[ E_2 ]\\). Note that it is very common to write \\(\\Pr[ E_1, E_2]\\) to mean \\(\\Pr[ E_1 \\cap E_2 ]\\). We usually read that as “the probability of events \\(E_1\\) and \\(E_2\\)” or “the probability that \\(E_1\\) and \\(E_2\\) occur”.\nTwo random variables \\(X\\) and \\(Y\\) are independent if for all sets \\(S_1,S_2\\), we have \\(\\Pr[ X \\in S_1,~ Y \\in S_2 ] = \\Pr[ X \\in S_1 ] \\Pr[ Y \\in S_2 ]\\).\nRoughly speaking, two random variables are independent if learning information about one of them doesn’t tell you anything about the other.\n\nFor example, if each of us flips a coin, it is reasonable to model them as being independent.\nLearning whether my coin landed heads or tails doesn’t tell us anything about your coin.\n\n\n4.2.1 Example: dice and coins\nSuppose that you roll a die and I flip a coin. Let \\(D\\) denote the (random) outcome of the die roll, and let \\(C\\) denote the (random) outcome of the coin flip. So \\(D \\in \\{1,2,3,4,5,6\\}\\) and \\(C \\in \\{H,T\\}\\). Suppose that for all \\(d \\in \\{1,2,3,4,5,6\\}\\) and all \\(c \\in \\{H,T\\}\\), \\(\\Pr[ D=d, C=c ] = 1/12\\).\nQuestion: Verify that the random variables \\(D\\) and \\(C\\) are independent, or at least check that it’s true for two particular events \\(E_1 \\subseteq \\{1,2,3,4,5,6\\}\\) and \\(E_2 \\subseteq \\{H,T\\}\\).\n\n\n4.2.2 Example: more dice\nSuppose that we roll a six-sided die. Consider the following two events: \\[\n\\begin{aligned}\nE_1 &= \\{ \\text{The die lands on an even number} \\} \\\\\nE_2 &= \\{ \\text{The die lands showing 3} \\}.\n\\end{aligned}\n\\]\nAre these two events independent? Our intuitive definition of independence is that learning about one event shouldn’t change the probability of the other event. These two events surely fail that test: if I tell you that the die landed on an even number, then it’s certainly impossible that it landed showing a 3, since 3 isn’t even.\nLet’s verify this intuition by checking that these two events fail the formal definition of independence. That is, let’s verify that \\[\n\\Pr[ E_1 \\cap E_2 ] \\neq \\Pr[ E_1 ] \\Pr[ E_2 ].\n\\]\nThere are six sides on our die, numbered 1, 2, 3, 4, 5, 6, and three of those sides are even numbers, so \\(\\Pr[ E_1 ] = 1/2\\).\nThe probability that the die lands showing 3 is exactly \\(\\Pr[ E_2 ] = 1/6\\).\nPutting these together, \\(\\Pr[E_1] \\Pr[E_2] = 1/12\\).\nOn the other hand, let’s consider \\(E_1 \\cap E_2\\). This is the event that the die lands showing an even number and it lands showing three. These two events cannot both happen!\nThat means that \\(E_1 \\cap E_2 = \\emptyset\\). That is, the intersection of these two events is the empty set. The laws of probability require that \\(\\Pr[ \\emptyset ] = 0\\) (Aside: why? Hint: \\(\\Pr[ \\Omega ] = 1\\) and \\(\\emptyset \\cap \\Omega = \\emptyset\\); now use the fact that the probability of the union of disjoint events is the sum of their probabilities).\nSo we have \\[\n\\Pr[ E_1 \\cap E_2 ] = 0 \\neq \\frac{1}{12} =\\Pr[ E_1 ] \\Pr[ E_2 ].\n\\]\nOur two events are indeed not independent.\nImportant note: it’s not always so obvious that two events are independent! Any probability textbook will have a collection of good examples of less intuitive independent and dependent events."
  },
  {
    "objectID": "cov.html#independent-random-variables",
    "href": "cov.html#independent-random-variables",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.3 Independent Random Variables",
    "text": "4.3 Independent Random Variables\nWhat do we mean by independent random variables? Well, a strict, formal definition is going to have to wait for a future math class.\nFor this semester, we’ll say that two random variables \\(X\\) and \\(Y\\) are independent if any two events concerning those random variables are independent.\nThat is, for any event \\(E_X\\) concerning \\(X\\) (i.e., \\(E_X = \\{ X \\in S \\}\\) for \\(S \\subseteq \\Omega)\\) and any event \\(E_Y\\) concerning \\(Y\\), the events \\(E_X\\) and \\(E_Y\\) are independent.\nSaid another way, if two random variables \\(X\\) and \\(Y\\) are independent, then for any two sets \\(S_1, S_2 \\subset \\Omega\\), \\[\n\\Pr[ X \\in S_1, Y \\in S_2 ]\n=\n\\Pr[ X \\in S_1] \\Pr[ Y \\in S_2 ].\n\\]\nIn particular, if \\(X\\) and \\(Y\\) are both discrete, then for any \\(k\\) and \\(\\ell\\), \\[\n\\Pr[ X=k, Y=\\ell ]\n=\n\\Pr[ X=k ] \\Pr[ Y=\\ell ].\n\\]\nSimilarly, if \\(X\\) and \\(Y\\) are continuous, then the joint density has the same property: \\[\nf_{X,Y}(s,t) = f_X(s) f_Y(t).\n\\]\nDon’t worry about these mathematical details too much at this stage– We just want to make sure that you’ve seen a little bit of this so that it isn’t completely new to you when you go off and do your readings and, more importantly, when you see these ideas in your later classes."
  },
  {
    "objectID": "cov.html#independence-expectation-and-variance",
    "href": "cov.html#independence-expectation-and-variance",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.4 (in)dependence, expectation and variance",
    "text": "4.4 (in)dependence, expectation and variance\nRecall the definition of the expectation of a random variable \\(X\\) with outcome set \\(\\Omega\\), \\[\n\\mathbb{E} X = \\int_\\Omega t f_X(t) dt,\n\\]\nif \\(X\\) is continuous with density \\(f_X\\), and \\[\n\\mathbb{E} X = \\sum_{k \\in \\Omega} k \\Pr[X=k]\n\\]\nif \\(X\\) is discrete with probability mass function \\(\\Pr[ X=k]\\).\nWith the expectation defined, we can also define the variance, \\[\n\\operatorname{Var} X = \\mathbb{E} (X - \\mathbb{E} X)^2\n= \\mathbb{E} X^2 - \\mathbb{E}^2 X.\n\\]\nThat second equality isn’t necessarily obvious– we’ll see why it’s true in a moment.\nNote: we often write \\(\\mathbb{E}^2 X\\) as short for \\((\\mathbb{E} X)^2\\). This is standard notation, but it’s sometimes a source of confusion to beginner students, so be careful!\nA basic property of expectation is that it is linear. For any constants (i.e., non-random) \\(a,b \\in \\mathbb{R}\\), \\[\n\\mathbb{E} (a X + b) = a \\mathbb{E} X + b.\n\\] If \\(X,Y\\) are random variables, then \\[\n\\mathbb{E}( X + Y) = \\mathbb{E} X + \\mathbb{E} Y.\n\\]\nThis linearity of expectation property may look/sound a bit weird, but this isn’t the first time you’ve seen it– derivatives and integrals are linear, too! For example, \\[\n(a ~f(t) + b ~g(t))' = a ~ f'(t) + b ~g'(t)\n\\]\nand \\[\n\\int(a f(t) + b g(t)) dt = a \\int f(t) dt + b \\int g(t) dt\n\\]\nBecause expected value is simply an integral (or summation), the linearity of expectation follows directly from the definition.\nExercise: prove that \\(\\mathbb{E} (a X + b) = a \\mathbb{E} X + b\\) for discrete r.v. \\(X\\).\nExercise: prove that \\(\\mathbb{E}( X + Y) = \\mathbb{E} X + \\mathbb{E} Y\\) for discrete \\(X\\) and \\(Y\\).\nExercise: Use the linearity of expectation to prove that \\(\\mathbb{E} (X - \\mathbb{E} X)^2 = \\mathbb{E} X^2 - \\mathbb{E}^2 X\\). Hint: \\(\\mathbb{E}( X \\mathbb{E} X) = \\mathbb{E}^2 X\\) because \\(\\mathbb{E} X\\) is NOT random– it pops right out of the expectation just like \\(a\\) does in the equation above.\nThe definition of variance and the linearity of expectation are enough to give us a property of variance:\nFor any constants (i.e., non-random) \\(a,b \\in \\mathbb{R}\\), \\[\n\\operatorname{Var} (a X + b) = a^2 \\operatorname{Var} (X).\n\\]\nExercise: Use the definition \\(\\operatorname{Var}(X)=\\mathbb{E} X^2 - \\mathbb{E}^2 X\\) to prove the above.\nThis linearity property implies that the expectation of a sum is the sum of the expectations: \\[\n\\mathbb{E}[ X_1 + X_2 + \\dots + X_n]\n= \\mathbb{E} X_1 + \\mathbb{E} X_2 + \\dots + \\mathbb{E} X_n.\n\\]\nWhat about variance? Is the variance of the sum the sum of the variances?\nWell, sadly, not always. To see why this isn’t always true, consider RVs \\(X\\) and \\(Y\\). \\[\n\\begin{aligned}\n\\operatorname{Var}(X + Y)\n&= \\mathbb{E}[ X + Y - \\mathbb{E}(X + Y) ]^2 \\\\\n&= \\mathbb{E}[ (X - \\mathbb{E} X) + (Y - \\mathbb{E} Y) ]^2,\n\\end{aligned}\n\\]\nwhere the second equality follows from applying linear of expectation to write \\(\\mathbb{E}(X+Y) = \\mathbb{E}X + \\mathbb{E}Y\\).\nNow, let’s expand the square in the expectation. \\[\n\\begin{aligned}\n\\operatorname{Var}(X + Y)\n&=\n\\mathbb{E}[ (X - \\mathbb{E} X) + (Y - \\mathbb{E} Y) ]^2 \\\\\n&= \\mathbb{E}[(X - \\mathbb{E} X)^2 + 2(X - \\mathbb{E} X)(Y - \\mathbb{E} Y)\n              + (Y - \\mathbb{E} Y)^2 ] \\\\\n&= \\mathbb{E} (X - \\mathbb{E} X)^2 + 2 \\mathbb{E} (X - \\mathbb{E} X)(Y - \\mathbb{E} Y) + \\mathbb{E} (Y - \\mathbb{E} Y)^2,\n\\end{aligned}\n\\] where the last equality is just using the linearity of expectation.\nNow, the first and last terms there are the variances of \\(X\\) and \\(Y\\): \\[\n\\operatorname{Var} X = \\mathbb{E}(X - \\mathbb{E} X)^2,~~~\n\\operatorname{Var} Y = \\mathbb{E}(Y - \\mathbb{E} Y)^2.\n\\] So \\[\n\\operatorname{Var}(X + Y)\n= \\operatorname{Var} X + 2 \\mathbb{E} (X - \\mathbb{E} X)(Y - \\mathbb{E} Y)\n  + \\operatorname{Var} Y.\n\\]\nSo what’s up with that middle term? This term might be familiar to you– it is (two times) the covariance of \\(X\\) and \\(Y\\), often written \\[\n\\operatorname{Cov}(X,Y)\n= \\mathbb{E}( X - \\mathbb{E}X)( Y - \\mathbb{E} Y).\n\\]\nNow, if \\(\\operatorname{Cov}(X,Y) = 0\\), then \\[\n\\operatorname{Var}(X + Y) = \\operatorname{Var} X + \\operatorname{Var} Y.\n\\]\nBut when does \\(\\operatorname{Cov}(X,Y) = 0\\)?\nWell, one sufficient condition is that \\(X\\) and \\(Y\\) be independent. That is, if \\(X\\) and \\(Y\\) are independent random variables, then \\(\\operatorname{Cov}(X,Y) = 0\\).\nNote: We will skip the proof that independence of \\(X\\) and \\(Y\\) implies \\(Cov(X,Y)=0\\), but you can find this proof in many places online."
  },
  {
    "objectID": "cov.html#uncorrelation-and-independence",
    "href": "cov.html#uncorrelation-and-independence",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.5 (Un)correlation and independence",
    "text": "4.5 (Un)correlation and independence\nCovariance might look familiar to you from a quantity that you saw in STAT240 (and a quantity that is very important in statistics!). The (Pearson) correlation between random variables \\(X\\) and \\(Y\\) is defined to be \\[\n\\rho_{X,Y} = \\frac{ \\operatorname{Cov}(X,Y) }{ \\sqrt{ (\\operatorname{Var} X)(\\operatorname{Var} Y)} }.\n\\]\nNote that if \\(X\\) and \\(Y\\) are independent, then \\(\\rho_{X,Y}=0\\) and we say that they are uncorrelated.\nBut the converse isn’t true– it is possible to cook up examples of random variables that are uncorrelated (i.e., \\(\\rho_{X,Y} = 0\\)), but which are not independent.\n\n4.5.1 Example: Uncorrelated but not independent\nSuppose \\(X \\sim Unif(-1, 1)\\) and \\(Y=X^2\\). You can see from the definition that \\(Y\\) is most definitely dependent on \\(X\\). If, for example, you know that \\(x=.5\\), then you know that \\(y=.5^2=.25\\). A proof that the covariance is zero is going to be difficult, but we can simulate some data to help us be confident that the claim is true.\n\nx &lt;- runif(10000, -1, 1)\ny &lt;- x^2\n\ncov(x,y)\n\n[1] 0.0003628265\n\n\nThe covariance of this sample of 10,000 observations of \\(X\\) and \\(Y\\) is very close to 0 (probably a little below or a little above due to randomness). The sample correlation is\n\ncor(x,y)\n\n[1] 0.002149034\n\n\nAgain - this is a number that is very close to zero.\n\n\n4.5.2 Example: sums of independent normals\nLet’s consider two independent normals: \\[\nX_1 \\sim \\operatorname{Normal}(1,1)\n~\\text{ and }~\nX_2 \\sim \\operatorname{Normal}(2,2).\n\\]\nSince \\(X_1\\) and \\(X_2\\) are independent,\n\nthe variance of their sum should be the sum of their variances, and\ntheir correlation should be zero\n\nLet’s check both of those facts in simulation. We’ll generate lots of copies of \\(X_1\\) and \\(X_2\\), and then we’ll compute their\n\n(sample) variances separately,\n(sample) variance of their sum, and\n(sample) correlation\n\nOf course, all three of these quantities will be estimated from samples. The law of large numbers tells us that these quantities computed on our data will be close to the truth, but not necessarily precisely equal to the truth.\nOkay, let’s generate data.\n\nM &lt;- 1e5; # Generate 100K Monte Carlo samples\nX1 &lt;- rnorm( n=M, mean=1, sd=sqrt(1) );\nX2 &lt;- rnorm( n=M, mean=2, sd=sqrt(2) );\n\n# Compute the (sample) variances of the copies of X1 and X2.\nv1 &lt;- var(X1);\nv2 &lt;- var(X2);\n\n# v1 should be close to 1=Var X_1, v2 close to 2=Var X_2.\nc( v1, v2 )\n\n[1] 0.9964867 2.0028233\n\n\nAnd let’s check that these two independent variables have covariance (approximately) zero.\n\n# cor( x, y) computes the (sample) covariance between\n# the entries of vectors x and y.\n# See ?cor for details.\ncor( X1, X2 );\n\n[1] 0.006530369\n\n\nAgain, those sample-based quantities will never be precisely equal to 1, 2, and 0, but they will be very close!\nFinally, let’s check that the variance of the sum \\(X_1 + X_2\\) is the sum of variances, as it should be if the RVs are independent. So we should see \\[\n\\operatorname{Var}(X_1 + X_2)\n= \\operatorname{Var} X_1 + \\operatorname{Var} X_2\n= 1 + 2 = 3.\n\\]\nOkay, let’s check.\n\nvar(X1 + X2)\n\n[1] 3.017761\n\n\nAs we predicted!\n\n\n4.5.3 Example: multivariate normal\nRemember that the multivariate normal is a way of generating multiple normal random variables that are correlated with one another.\nHere’s our code from our example modeling the voter shares in Wisconsin and Michigan.\n\nmu &lt;- c(.5,.5); # Vector of means; both W_p and M_p are mean 1/2.\nCovMx &lt;- matrix( c(.05^2,.04^2,.04^2,.05^2), nrow = 2); # Make a two-by-two symmetric matrix.\nCovMx;\n\n       [,1]   [,2]\n[1,] 0.0025 0.0016\n[2,] 0.0016 0.0025\n\n\nThe code above generates a multivariate normal with two entries. Both will have means \\(0.5\\), encoded in the vector mu.\nThe variances and covariance between the two normals is encoded by CovMx. It encodes a matrix (fancy word for an array of numbers), which looks like \\[\n\\Sigma = \\begin{bmatrix}\n0.05^2 & 0.04^2 \\\\\n0.04^2 & 0.05^2\n\\end{bmatrix}.\n\\] That \\(0.4^2\\) in the off-diagonal entries is the covariance of the two normals.\nSo this will generate two normal random variables, both having mean \\(0.5\\), and variance \\(0.05^2\\), but these two normals will be correlated, with covariance \\(0.04^2\\).\nLet’s have a look:\n\nlibrary(MASS); # This library includes a multivariate normal\nWpMp = mvrnorm(n=2000, mu=mu, Sigma=CovMx); #mvrnorm is the multivariate version of rnorm.\nplot(WpMp, xlab = \"Wisconsin proportion\", ylab = \"Michigan proportion\");\n\n\n\n\nIt’s clear that the Wisconsin and Michigan voter shares are correlated– we can see it in the plot!\nBut just to be sure:\n\n# WpMp is an array with two columns and 500 rows.\n# If we call cov on it directly, we get something shaped\n# like our covariance matrix.\ncov(WpMp )\n\n            [,1]        [,2]\n[1,] 0.002434066 0.001654258\n[2,] 0.001654258 0.002638478\n\n\nThe diagonal entries are the (sample) variances computed along the columns. The off-diagonal entries (note that they are both the same) tell us the (sample) covariance. Unsurprisingly, the off-diagonal is close to the true covariance \\(0.0016\\).\nAlso worth noting is the fact that the on-diagonal entries are (approximately) 0.025. The on-diagonal entries are computing covariances of our two columns of data with themselves. That is, these are computing something like \\(\\operatorname{Cov}(X,X)\\).\nWhat is a random variable’s covariance with itself? Let’s plug in the definition: \\[\n\\operatorname{Cov}(X,X)\n= \\mathbb{E} (X - \\mathbb{E} X)(X - \\mathbb{E} X)\n= \\mathbb{E} (X - \\mathbb{E} X)^2\n\\]\nHey, that’s the variance! So \\(\\operatorname{Cov}(X,X) = \\operatorname{Var} X\\)."
  },
  {
    "objectID": "cov.html#how-reasonable-is-independence",
    "href": "cov.html#how-reasonable-is-independence",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.6 How reasonable is independence?",
    "text": "4.6 How reasonable is independence?\nIn most applications, it is pretty standard that we assume that our data are drawn independently and identically distributed according to some distribution. We say “i.i.d.”. For example, if \\(X_1, X_2, \\ldots, X_n\\) are all continuous uniform random variables between 0 and 1, we would say \\[\nX_i \\overset{\\text{iid}}{\\sim} \\text{Uniform}(0,1), \\text{ for } i=1,\\ldots, n\n\\]\nThis notation is common to denote iid random variables.\nAs another example, when we perform regression (as you did in STAT240, and which we’ll revisit in more detail later this semester), we imagine that the observations (i.e., predictor-response pairs) \\((X_1,Y_1),(X_2,Y_2),\\dots,(X_n,Y_n)\\) are independent.\nMost standard testing procedures (e.g., the t-test) assume that data are drawn i.i.d.\nHow reasonable are these assumptions?\nWell, of course, in the end, it depends on where the data comes from! We have to draw on what we know about the data, either from our own knowledge or from that of our clients, to assess what assumptions are and aren’t reasonable.\nLike most modeling assumptions, we usually acknowledge that independence may not be exactly true, but it’s often a good approximation to the truth!\nOf course, we have to be careful.\nExample: suppose we are modeling the value of a stock over time. We model the stock’s price on days 1, 2, 3, etc as \\(X_1, X_2, X_3, \\dots\\). What is wrong with modeling these prices as being independent of one another? Why might it still be a reasonable modeling assumption?\nWhat if instead we look at the change in stock price from day to day? For example, let \\(Y_i = X_{i+1}-X_{i}\\). In other words, \\(X_{i+1}=X_i+Y_i\\). Would it be more reasonable to assume that the \\(Y_i\\)’s are independent?\nWhat if instead of considering a stock’s returns on one day after another, we look at a change in stock price on one day, then at the change 10 days from that, and 10 days from that, and so on? Surely there is still dependence, but a longer time lag between observations might make us more willing to accept that our observations are close to independent (or at least have much smaller covariance!).\nNote: Tobler’s first law of geography states ‘Everything is related to everything else, but near things are more related than distant things.’ Does that ring true in this context?\nExample: suppose we randomly sample 1000 UW-Madison students to participate in a survey, and record their responses as \\(X_1,X_2,\\dots,X_{1000}\\). What might be the problem with modeling these responses as being independent? Why might be still be a reasonable modeling assumption?"
  },
  {
    "objectID": "cov.html#conditional-probability",
    "href": "cov.html#conditional-probability",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.7 Conditional probability",
    "text": "4.7 Conditional probability\nWe can’t talk about events and independence without discussing conditional probability.\nTo motivate this, consider the following: suppose I roll a six-sided die. What is the probability that the die lands showing 2?\nNow, suppose that I don’t tell you the number on the die, but I do tell you that the die landed on an even number (i.e., one of 2, 4 or 6). Now what is the probability that the die is showing 2?\nWe can work out the probabilities by simply counting possible outcomes. Are the probabilities the same?\nExample: disease screening\nHere’s a more real-world (and more consequential example): suppose we are screening for a rare disease. A patient takes the screening test, and tests positive. What is the probability that the patient has the disease, given that they have tested positive for it?\nWe will need to establish the rules of conditional probability before we can tackle a problem such as this.\n\n4.7.1 Introducing conditional probability\nThese kinds of questions, in which we want to ask about the probability of an event given that something else has happened, require that we be able to define a “new kind” of probability, called conditional probability.\nLet \\(A\\) and \\(B\\) be two events.\n\nExample: \\(A\\) could be the event that a die lands showing 2 and \\(B\\) is the event that the die landed on an even number.\nExample: \\(A\\) could be the event that our patient has a disease and \\(B\\) is the event that the patient tests positive on a screening test.\n\nProvided that \\(\\Pr[ B ] &gt; 0\\), we define the conditional probability of \\(A\\) given \\(B\\), written \\(\\Pr[ A \\mid B]\\), according to \\[\n\\Pr[ A \\mid B ] = \\frac{ \\Pr[ A \\cap B ] }{ \\Pr[ B ] }.\n\\] Note that if \\(\\Pr[B] = 0\\), then the ratio on the right-hand side is not defined, hence why we demanded that \\(\\Pr[B] &gt; 0\\). Later in your career, you’ll see that we can actually define conditional probability in a sensible way even if \\(\\Pr[B] = 0\\), but that’s a matter for a later class.\n\n\n\nVenn diagram illustrating conditional probability\n\n\nLet’s try computing one of these conditional probabilities: what is the probability that the die is showing 2 conditional on the fact that it landed on an even number?\nWell,\n\n\\(\\Pr[ \\text{ even } ] = 1/2\\), because there are three even numbers on the die, and all six numbers are equally likely: \\(3/6 = 1/2\\).\n\\(\\Pr[ \\text{ die lands 2 } \\cap \\text{even} ] = \\Pr[ \\text{ die lands 2 }]\\), since \\(2\\) is an even number.\n\nSo the conditional probability is \\[\n\\begin{aligned}\n\\Pr[ \\text{ die lands 2 } \\mid \\text{ even }]\n&= \\frac{ \\Pr[ \\text{ die lands 2 } \\cap \\text{even} ] }{ \\Pr[ \\text{ even } ] } \\\\\n&= \\frac{ \\Pr[ \\text{ die lands 2 }]}\n    { \\Pr[ \\text{ even } ] } \\\\\n&= \\frac{ 1/6 }{ 1/2 } = 1/3.\n\\end{aligned}\n\\] This makes sense– given that the die lands on an even number, we are choosing from among three outcomes: \\(\\{2,4,6\\}\\). The probability that we choose \\(2\\) from among these three possible equally-likely outcomes is \\(1/3\\).\n\n\n4.7.2 Disease screening\nWhat about our disease testing example? What is the probability that our patient has the disease given that they tested positive?\nWell, applying the definition of conditional probability, \\[\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n= \\frac{ \\Pr[ \\text{ disease} \\cap \\text{ positive test }] }{ \\Pr[ \\text{positive test} ] }\n\\]\nOkay, but what is \\(\\Pr[ \\text{ positive test} ]\\)? I guess it’s just the probability that a random person (with the disease or not) tests positive? For that matter, what is \\(\\Pr[ \\text{ disease} \\cap \\text{ positive test }]\\)? These can be hard events to assign probabilities to! Luckily, there is a famous equation that often gives us a way forward."
  },
  {
    "objectID": "cov.html#bayes-rule",
    "href": "cov.html#bayes-rule",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.8 Bayes’ rule",
    "text": "4.8 Bayes’ rule\nThe Reverend Thomas Bayes was the first to suggest an answer to this issue. Bayes’ rule, as it is now called, tells us how to relate \\(\\Pr[ A \\mid B]\\) to \\(\\Pr[ B \\mid A]\\): \\[\n\\Pr[ A \\mid B ] = \\frac{ \\Pr[ B \\mid A ] \\Pr[ A ]}{ \\Pr[ B ]}.\n\\]\nThis is useful, because it is often easier to write one or the other of these two probabilities.\nApplying this to our disease screening example, \\[\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n=\n\\frac{ \\Pr[\\text{ positive test } \\mid \\text{ disease}]\n        \\Pr[ \\text{ disease}]}\n        { \\Pr[ \\text{ positive test } ]  }\n\\]\nThe advantage of using Bayes’ rule in this context is that the probabilities appearing on the right-hand side are all straight-forward to think about (and estimate!).\n\n\\(\\Pr[ \\text{ disease}]\\) is the just the probability that a randomly-selected person has the disease. This is known as the prevelance of the diseases in the population. We could estimate this probability by randomly selecting a random group of people and determining if they have the disease (hopefully not using the screening test we are already using…).\n\\(\\Pr[\\text{ positive test } \\mid \\text{ disease}]\\) is the probability that when we give our screening test to a patient who has the disease in question, the test returns positive. This is often called the sensitivity of a test, a term you may recall hearing frequently in the early days of the COVID-19 pandemic.\n\\(\\Pr[ \\text{ positive test } ]\\) is just the probability that a test given to a (presumably randomly selected) person returns a positive result. We just said about that this is the hard thing to estimate. In your homework, you’ll explore one way to get at this quantity, but for now we’ll have to just assume that we can estimate it somehow or other."
  },
  {
    "objectID": "cov.html#example-testing-for-a-rare-disease",
    "href": "cov.html#example-testing-for-a-rare-disease",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.9 Example: testing for a rare disease",
    "text": "4.9 Example: testing for a rare disease\nSuppose that we are testing for a rare disease, say, \\[\n\\Pr[ \\text{ disease}] = \\frac{1}{10^6},\n\\]\nand suppose that a positive test is also rare, in keeping with the fact that our disease is rare and our test presumably has a low false positive rate: \\[\n\\Pr[ \\text{ positive test} ] = 1.999*10^{-6}\n\\] Note that this probability actually depends on the sensitivity \\(\\Pr[\\text{ positive test } \\mid \\text{ disease}]\\) and the specificity \\(1-\\Pr[\\text{ positive test } \\mid \\text{ healthy}]\\) of our test. You’ll explore this part more on your homework, but we’re just going to take this number as given for now.\nFinally, let’s suppose that our test is 99.99% accurate: \\[\n\\Pr[\\text{ positive test } \\mid \\text{ disease}]\n= 0.9999 = 1-10^{-4}\n\\]\nTo recap, \\[\n\\begin{aligned}\n\\Pr[ \\text{ disease}] &= \\frac{1}{10^6} \\\\\n\\Pr[ \\text{ positive test} ] &= 1.999*10^{-6} \\\\\n\\Pr[\\text{ positive test } \\mid \\text{ disease}]\n&= 0.9999.\n\\end{aligned}\n\\]\nNow, suppose that a patient is given the screening test and receives a positive result. Bayes’ rule tells us \\[\n\\begin{aligned}\n\\Pr[ \\text{ disease} \\mid \\text{ positive test }]\n&=\n\\frac{ \\Pr[\\text{ positive test } \\mid \\text{ disease}]\n        \\Pr[ \\text{ disease}]}\n        { \\Pr[ \\text{ positive test } ]  }\n= \\frac{ 0.9999 * 10^{-6} }{ 1.999*10^{-6} } \\\\\n&= 0.5002001.\n\\end{aligned}\n\\]\nSo even in light of our positive screening test result, the probability that our patient has the disease in question is still only about 50%!\nThis is part of why, especially early on in the pandemic when COVID-19 was especially rare, testing for the disease in the absence of symptoms was not considered especially useful.\nMore generally, this is why most screenings for rare diseases are not done routinely– doctors typically screen for rare diseases only if they have a reason to think a patient is more likely to have that disease for other reasons (e.g., family history of a genetic condition or recent exposure to an infectious disease)."
  },
  {
    "objectID": "cov.html#calculating-the-denominator-in-bayes-rule",
    "href": "cov.html#calculating-the-denominator-in-bayes-rule",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.10 Calculating the denominator in Bayes’ Rule",
    "text": "4.10 Calculating the denominator in Bayes’ Rule\nThe denominator can be decomposed into two parts using a property known as the Law of Total Probability.\n\\[\n\\Pr[ \\text{ positive test} ] = \\Pr[ \\text{ positive test} \\cap \\text{disease}]+\\Pr[ \\text{ positive test} \\cap \\text{no disease}]\n\\]\nIn other words, all positive results are either true positives or false positives. Because these are mutually exclusive events, the total probability of a positive result is the probability of a true positive plus the probability of a false positive. We can expand each of these terms using the conditional probability rule.\n\\[\n\\Pr[ \\text{ positive test} \\cap \\text{disease}] = \\Pr[\\text{ positive test } \\mid \\text{ disease}]  \\Pr[ \\text{ disease}]\n\\] \\[\n\\Pr[ \\text{ positive test} \\cap \\text{no disease}] = \\Pr[\\text{ positive test } \\mid \\text{ no disease}]  \\Pr[ \\text{ no disease}]\n\\]\nFor example, suppose that a genetic condition occurs in roughly 1 out of 800 individuals. A simple saliva test is available. If a person has the gene, the test is positive with 97% probability. If a person does not have the gene, a false positive occurs with 4% probability.\nTo simplify notation, let \\(G\\) represent “the individual has the gene” and \\(G'\\) be the complementary event that “the individual does not have the gene.” Furthermore, let \\(Pos\\) and \\(Neg\\) represent the test results.\nIf a random person from the population takes the test and gets a positive result, what is the probability they have the genetic condition?\nBayes’ Rule to the rescue:\n\\[\n\\begin{aligned}\nP[G | Pos] &= \\dfrac{P[Pos | G] P[G]}{P[Pos | G] P[G] + P[Pos | G'] P[G']}\\\\\n&= \\dfrac{(.97)(1/800)}{(.97)(1/800)+(.04)(799/800)}\\\\\n&\\approx 0.0295\n\\end{aligned}\n\\] In other words, a positive test result would raise the likelihood of the gene being present from \\(1/800=0.00125\\) up to \\(.0295\\)."
  },
  {
    "objectID": "cov.html#dependent-free-throw-shots",
    "href": "cov.html#dependent-free-throw-shots",
    "title": "4  Independence, Conditional Probability and Bayes’ Rule",
    "section": "4.11 Dependent free throw shots",
    "text": "4.11 Dependent free throw shots\nSuppose a basketball player’s likelihood of making a basket when making a free throw depends on the previous attempt. On the first throw, they have a probability of \\(0.67\\) of making the basket. On the second throw, following a basket the probability goes up to \\(.75\\). If the first throw is a miss, the probability of a basket on the second throw goes down to \\(0.62\\).\nExercise: If the second throw is a basket, what is the likelihood the first throw is a basket?\nExercise: Given that the player scores at least 1 point, what is the probability that they score 2 points total?\n\n4.11.1 Review:\nIn these notes we covered:\n\nThe concept of independent events\nIndependent random variables\nDefinition of variance\nExpectation of a linear combination of r.v.s\nVariance of a linear combination of r.v.s\nCovariance and correlation\nRelationship between correlation and independence\nWhen the independence assumption is reasonable\nConditional probability & the general multiplication rule\nBayes’ rule"
  },
  {
    "objectID": "rv_practice.html",
    "href": "rv_practice.html",
    "title": "5  Probability and Random Variables Practice",
    "section": "",
    "text": "6 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "rv_practice.html#using-wikipedia-in-college",
    "href": "rv_practice.html#using-wikipedia-in-college",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.1 1. Using Wikipedia in college",
    "text": "6.1 1. Using Wikipedia in college\nA recent national study showed that approximately 44.7% of college students have used Wikipedia as a source in at least one of their term papers. Let \\(X\\) equal the number of students in a random sample of size \\(n = 31\\) who have used Wikipedia as a source.\n\nHow is \\(X\\) distributed?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming independence in sampling, and a representative sample, we can use a Binomial distribution with \\(n=31\\) and \\(p=0.447\\).\n\n\n\n\nSketch the probability mass function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(dbinom(0:31, 31,.447), names=0:31, ylab=\"probability\", main=\"PMF of Binomial(31,.447)\")\n\n\n\n\n\n\n\n\nSketch the cumulative distribution function (roughly).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(pbinom(0:31, 31, .447), type=\"s\", ylab=\"cumulative prob.\", main=\"CDF of Binomial(31, .447)\")\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\) is equal to 17.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(17, 31, .447)\n\n[1] 0.07532248\n\n\n\n\n\n\nFind the probability that \\(X\\) is at most 13.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npbinom(13, 31, .447)\n\n[1] 0.451357\n\n\n\n\n\n\nFind the probability that \\(X\\) is bigger than 11.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(12:31, 31, .447))\n\n[1] 0.8020339\n\n#or\npbinom(11, 31, .447, lower.tail=FALSE)\n\n[1] 0.8020339\n\n#or\n1-pbinom(11, 31, .447)\n\n[1] 0.8020339\n\n\n\n\n\n\nFind the probability that \\(X\\) is at least 15.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X at least 15)\nsum(dbinom(15:31,31,.447))\n\n[1] 0.406024\n\n\n\n\n\n\nFind the probability that \\(X\\) is between 16 and 19, inclusive.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(dbinom(16:19, 31, .447))\n\n[1] 0.2544758\n\n\n\n\n\n\nGive the mean of \\(X\\), denoted \\(\\mathbb{E}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(X)=n*p\n31*.447\n\n[1] 13.857\n\n#or you can also do this (but it's too much work)\nsum( (0:31) * dbinom(0:31, 31, .447))\n\n[1] 13.857\n\n\n\n\n\n\nGive the variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Var(X) = n * p * (1-p)\n31 * .447 * (1-.447)\n\n[1] 7.662921\n\n#or - if you want (but why would you want to?)\nsum((0:31 - 31*.447)^2 * dbinom(0:31, 31, .447))\n\n[1] 7.662921\n\n\n\n\n\n\nGive the standard deviation of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#SD(X) = sqrt(n*p*(1-p))\nsqrt(31*.447*(1-.447))\n\n[1] 2.768198\n\n\n\n\n\n\nFind \\(\\mathbb{E}(4X+51.324)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#E(4X+51.324) = 4*E(X)+51.324\n4*(31*.447) + 51.324\n\n[1] 106.752"
  },
  {
    "objectID": "rv_practice.html#choose-the-distribution",
    "href": "rv_practice.html#choose-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.2 2. Choose the distribution",
    "text": "6.2 2. Choose the distribution\n\nFor the following situations, decide what the distribution of \\(X\\) should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not hold in practice.)\n\n\nWe shoot basketballs at a basketball hoop, and count the number of shots until we make a basket. Let X denote the number of missed shots. On a normal day we would typically make about 37% of the shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of missed shots before the first basket, assuming independence, can be modeled by a Geometric random variable with parameter \\(p=.37\\).\n\n\n\n\nIn a local lottery in which a three digit number is selected randomly, let X be the number selected.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssuming that all 3 digit numbers are equally likely (A reasonable assumption) the number selected can be modeled by a discrete uniform distribution with minimum 100 and maximum 999.\n\n\n\n\nWe drop a Styrofoam cup to the floor twenty times, each time recording whether the cup comes to rest perfectly right side up, or not. Let X be the number of times the cup lands perfectly right side up.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf we drop the cup 20 times, and the result each time is independent with a constant probability of landing right side up, the number of times it does can be modeled by a Binomial random variable with parameters \\(n=20\\) and \\(p\\) (unknown).\n\n\n\n\nWe toss a piece of trash at the garbage can from across the room. If we miss the trash can, we retrieve the trash and try again, continuing to toss until we make the shot. Let X denote the number of missed shots.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGeometric random variable (unknown parameter value for \\(p\\)).\n\n\n\n\nWorking for the border patrol, we inspect shipping cargo as when it enters the harbor looking for contraband. A certain ship comes to port with 557 cargo containers. Standard practice is to select 10 containers randomly and inspect each one very carefully, classifying it as either having contraband or not. Let X count the number of containers that illegally contain contraband.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nTechnically we should use a hypergeometric random variable for this situation (since it is a small population size of 557), but since we do not cover the hypergeometric the closest random variable we have is the binomial.\n\n\n\n\nAt the same time every year, some migratory birds land in a bush outside for a short rest. On a certain day, we look outside and let X denote the number of birds in the bush.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is a discrete random variable, but without other information it’s hard to say. The distribution is likely unimodal and bell-curved. You could probably model this using a normal distribution rounded off to the nearest integer.\n\n\n\n\nWe count the number of rain drops that fall in a circular area on a sidewalk during a ten minute period of a thunder storm.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe observation window is the circular area, and the 10 minutes during observation. Assuming the rate of rainfall is constant, the number of raindrops in the circle can be modeled using a Poisson random variable.\n\n\n\n\nWe count the number of moth eggs on our window screen.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCounting indicates a discrete random variable. A binomial or a rounded normal distribution may be appropriate, but we lack enough details to be sure.\n\n\n\n\nWe count the number of blades of grass in a one square foot patch of land.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe location of the sprouting grass could be modeled well by a Poisson random variable - the \\(\\lambda\\) parameter would likely be very large, in the range of 1000 or 10000, and as such the distribution would look very much like a normal distribution.\n\n\n\n\nWe count the number of pats on a baby’s back until (s)he burps.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs we define a geometric random variable, we let \\(X\\) be the number of failures before the first success. The last pat (that causes the burp) is the success in this context. So we could use a geometric random variable, but we would have to add 1 to it in order to count all burps (the failures + 1 success)."
  },
  {
    "objectID": "rv_practice.html#variance-formula",
    "href": "rv_practice.html#variance-formula",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.3 3. Variance Formula",
    "text": "6.3 3. Variance Formula\nmove to chapter on covariance\nShow that \\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2-\\mu^2\\). Hint: expand the quantity \\((X-\\mu)^2\\) and distribute the expectation over the resulting terms.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe proof goes like this: We first FOIL \\((X-\\mu)^2\\):\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}(X^2 - 2\\mu X + \\mu^2)\\)\nWe next split the expected value into 3 expected values using the fact that \\(\\mathbb{E}\\) is a linear operator.\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mathbb{E}X + \\mathbb{E}\\mu^2\\)\nWe next observe that \\(\\mu^2\\) is constant and \\(\\mathbb{X}=\\mu\\)\n\\(\\mathbb{E}(X-\\mu)^2 = \\mathbb{E}X^2 -2\\mu \\mu + \\mu^2\\)\nWe can simplify the expression and we’re done!"
  },
  {
    "objectID": "rv_practice.html#expectation-and-binomial",
    "href": "rv_practice.html#expectation-and-binomial",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.4 4. Expectation and Binomial",
    "text": "6.4 4. Expectation and Binomial\nextra If \\(X \\sim \\text{Binom}(n,p)\\) show that \\(\\mathbb{E}X(X-1)=n(n-1)p^2\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can just expand the product \\(\\mathbb{E}(X^2-X)\\) and we can split this up into two expected values: \\(\\mathbb{E}X^2 - \\mathbb{E}X = \\mathbb{E}X^2-\\mu\\). Recall that \\(Var(X)=\\mathbb{E}X^2-\\mu^2\\) So \\(\\mathbb{E}X^2=Var(X)+\\mu^2\\). For a binomial, \\(Var(X)=np(1-p)\\) and \\(\\mu=np\\). Thus we have\n\\(\\mathbb{E}X^2 - \\mu=[np(1-p) + n^2p^2] - np = np\\left(1-p+np-1\\right)\\)\nTidying up a little bit we get \\(np(np-p)=np^2(n-1)\\), and we’re done."
  },
  {
    "objectID": "rv_practice.html#pmf-practice",
    "href": "rv_practice.html#pmf-practice",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.5 5. PMF practice",
    "text": "6.5 5. PMF practice\nConsider an information source that produces numbers \\(k\\) in the set \\(S_X=\\{1,2,3,4\\}\\). Find and plot the pmf in the following cases:\n\n\\(p_k = p_1/k\\) for \\(k=1,2,3,4\\). Hint: find \\(p_1\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nUse the fact that \\(p_1+p_2+p_3+p_4=1\\). In other words, \\(p_1/1+p_1/2+p_1/3+p_1/4=p_1(12/12+6/12+4/12+3/12) = p_1(25/12) =1\\), so \\(p_1=12/25\\). Then \\(p_2=6/25\\), \\(p_3=4/25\\) and \\(p_4=3/25\\).\n\nbarplot(height=c(12/25, 6/25, 4/25, 3/25), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\). Following this pattern, \\(p_3=p_1/4\\) and \\(p_4=p_1/8\\). If we add these together we have \\(p_1(8/8 + 4/8 + 2/8 + 1/8) = 15/8\\). Thus we have \\(p_1=8/15, p_2=4/15, p_3=2/15\\) and \\(p_4=1/15\\)\n\nbarplot(height=c(8/15, 4/15, 2/15, 1/15), names=1:4)\n\n\n\n\n\n\n\n\n\\(p_{k+1}=p_k/2^k\\) for \\(k=1,2,3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nStarting with \\(k=1\\) we have \\(p_2 = p_1/2\\), \\(p_3=p_2/2^2= p_2/4 = (p_1/2)/4 = p_1/8\\). \\(p_4 = p_3/2^3 = p_3/8 = (p_1/8)/8) = p_1/64\\). The sum is \\(p_1(64/64 + 32/64 + 8/64 + 1/64) = p_1(105/64)\\) so \\(p_1 = 64/105, p_2=32/105, p_3=8/105, p_4=1/105\\)\n\nbarplot(height=c(64/105, 32/105, 8/105, 1/105), names=1:4)\n\n\n\n\n\n\n\n\nCan the random variables in parts a. through c. be extended to take on values in the set \\(\\{1,2,\\ldots,\\}\\)? Why or why not? (Hint: You may use the fact that the series \\(1+\\frac12+\\frac13+\\cdots\\) diverges.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nConsider the pmf for part a. The sum of the probabilities would be \\(\\sum_{k=1}^\\infty p_1/k\\). However \\(\\sum_{k=1}^\\infty \\frac{1}{k}\\) does not converge, so no matter what \\(p_1\\) is, the sum of probabilities will exceed 1.\nFor part b, the sum of the probabilities is \\(\\sum_{k=1}^\\infty 2p_1/{2^{k}}\\). Because \\(\\sum_{k=1}^\\infty \\frac{1}{2^k}=1\\), then it would be possible to define a random variable with support \\(1,2,\\ldots\\) with this pmf.\nFor part c, because \\(p_k/2^k \\leq p_k/2\\), we at least know that \\(\\sum_k p_k\\) is finite, so such a random variable with infinite support is certainly feasible. The exact value of \\(p_1\\) is not as simple to calculate, but we were not asked to do that."
  },
  {
    "objectID": "rv_practice.html#dice-difference",
    "href": "rv_practice.html#dice-difference",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.6 6. Dice Difference",
    "text": "6.6 6. Dice Difference\nTwo dice are tossed. Let \\(X\\) be the absolute difference in the number of dots facing up.\n\n\nFind and plot the pmf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#It may be simplest to calculate all possible values of X. \nx &lt;- vector(\"numeric\")\nfor(i in 1:6){\n  for(j in 1:6){\nx[length(x)+1] = abs(i-j)\n  }\n}\n#now that we have all equally likely values, we can just calculate the pmf in a prop.table\npX &lt;- prop.table(table(x))\n#And create a barplot.\nbarplot(pX)\n\n\n\n\n\n\n\n\nFind the probability that \\(X\\leq 2\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The probability that X &lt;= 2 is easy to find using the pmf\n#The columns are named with strings, so we can convert 0 1 and 2 to strings to pull out the proper probabilities.\nsum(pX[as.character(0:2)])\n\n[1] 0.6666667\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\) and \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#The expected value and variance can be calculated from the pmf.\n\n(EX &lt;- sum(0:5 * pX))\n\n[1] 1.944444\n\n(VarX &lt;- sum((0:5 - EX)^2 * pX))\n\n[1] 2.052469\n\n#or by taking the mean and population variance of the x values themselves\nmean(x)\n\n[1] 1.944444\n\nmean((x-mean(x))^2)\n\n[1] 2.052469\n\n\n\n\n\n##7. PMF Formula extra, move to next chapter\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=1,2,\\ldots\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis was done above; because \\(\\sum_{i=1}^\\infty 1/2^k = 1\\), the value of \\(c\\) must be \\(1\\).\n\n\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(6\\leq X \\leq 8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X&gt;4) = 1-P(X \\leq 3)=1-(\\frac12 + \\frac14 + \\frac18)\\)\n\n1-(1/2+1/4+1/8)\n\n[1] 0.125\n\n\n\n\n\n\nFind \\(\\mathbb{E}X\\) and \\(\\text{Var}X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe expected value can be calculated by taking the sum \\(\\sum k p_k = \\sum_{k=1}^\\infty \\frac{k}{2^k}\\) which we can show using facts from calculus equals 2. Why? Well, as long as \\(|p|&lt;1, \\sum_{k=1}^{\\infty}p^k=\\frac{p}{1-p}\\) (this is a geometric series). If we take a derivative of both sides we get \\(\\sum_{k=1}^\\infty kp^{k-1}=\\frac{1}{(1-p)^2}\\). Multiply both sides by \\(p\\) to get \\(\\sum_{k=1}^\\infty kp^{k}=\\frac{p}{(1-p)^2}\\). In our case, \\(p=\\frac12\\). Plugging this in we get \\(\\frac{.5}{.5^2}=2\\).\nThe variance is \\(E(X^2)-(EX)^2=E(X^2)-4\\). The expected value of \\(X^2\\) can be derived, though it’s not so fun…\nStart by taking the equation \\(\\sum_k kp^k = \\frac{p}{(1-p)^2}\\) and take a derivative again. We get \\(\\sum_k k^2 p^{k-1} = \\frac{(1-p)^2+2p(1-p)}{(1-p)^4}\\). Multiply through by \\(p\\) to get \\(\\sum_k k^2 p^k = \\frac{p(1-p)^2+2p^2(1-p)}{(1-p)^4}\\). If we let \\(p=\\frac12\\) we have found \\(E(X^2)=\\sum_{k=1}^\\infty k^2(\\frac12)^k=\\dfrac{\\frac18-\\frac{2}{8}}{\\frac{1}{16}}=6\\). Thus \\(Var(X)=E(X^2)-E(X)^2 = 6-4=2\\). It should be noted that this random variable is actually a geometric random variable (well, according to the “number of trials until and including the first success definition). If we define \\(Y\\sim Geom(.5)\\) using our definition of “number of failures before the first success” then We can let \\(X=Y+1\\). \\(E(X)=E(Y+1)=\\frac{1-.5}{.5}+1=2\\) and \\(Var(X)=Var(Y)=\\frac{1-p}{p2}=\\frac{.5}{.25^2}=2\\)."
  },
  {
    "objectID": "rv_practice.html#pmf-formula-ii",
    "href": "rv_practice.html#pmf-formula-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.7 8. PMF Formula II",
    "text": "6.7 8. PMF Formula II\nextra\nLet \\(X\\) be a random variable with pmf \\(p_k = c/2^k\\) for \\(k=-1,0,1,2,3,4,5\\).\n\n\nDetermine the value of \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe sum of the probabilities are \\(c(2 + 1 + \\frac{1}{2}+\\frac{1}{4}+\\frac{1}8+\\frac{1}{16}+\\frac{1}{32})=c\\frac{127}{32}\\) so \\(c=\\frac{32}{127}\\).\n\n\n\n\nFind \\(\\mathbb{P}(1\\leq X &lt; 3)\\) and \\(\\mathbb{P}(1 &lt; X \\leq 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nc=32/127\nk = seq(-1,5)\npk=c/2^k\nnames(pk) &lt;- k\nsum(pk[as.character(2)])\n\n[1] 0.06299213\n\nsum(pk[as.character(2:5)])\n\n[1] 0.1181102\n\n\n\n\n\n\nFind \\(\\mathbb{P}(X^3 &lt; 5)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf \\(X^3 &lt; 5\\) that means \\(X^3 \\leq 4\\) and thus \\(X \\leq 4^{1/3}\\approx 1.587\\)\n\nsum(pk[k&lt;=4^(1/3)])\n\n[1] 0.8818898\n\nsum(pk[k^3&lt;5])\n\n[1] 0.8818898\n\n\n\n\n\n\nFind the pmf and the cdf of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nbarplot(height=pk, names=k, main='pmf of X')\n\n\n\nbarplot(height=cumsum(pk), names=k, main='cdf of X')"
  },
  {
    "objectID": "rv_practice.html#voltage-random-variable",
    "href": "rv_practice.html#voltage-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.8 9. Voltage random variable",
    "text": "6.8 9. Voltage random variable\nA modem transmits a +2 voltage signal into a channel. The channel adds to this signal a noise term that is drawn from the set \\(\\{0,-1,-2,-3\\}\\) with respective probabilities \\(\\{4/10, 3/10, 2/10, 1/10\\}\\).\n\n\nFind the pmf of the output \\(Y\\) of the channel.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Let X be the noise\nk &lt;- seq(0,-3)\npk &lt;- c(4/10, 3/10, 2/10, 1/10)\n\ny &lt;- sort(2+k)\npy &lt;- pk[order(2+k)]\n\nbarplot(height=py, names=y, main=\"pmf of Y\")\n\n\n\n\n\n\n\n\nWhat is the probability that the channel’s output is equal to the input of the channel?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis happens when there’s no noise, with probability 4/10.\n\n\n\n\nWhat is the probability that the channel’s output is positive?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Interpreting 'positive' to be strictly positive, not zero:\nsum(py[y&gt;0])\n\n[1] 0.7\n\n\n\n\n\n\nFind the expected value and variance of \\(Y\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n(EY &lt;- sum(y*py))\n\n[1] 1\n\n(VarY &lt;- sum((y-EY)^2*py))\n\n[1] 1"
  },
  {
    "objectID": "rv_practice.html#golf-score",
    "href": "rv_practice.html#golf-score",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.9 10. Golf Score",
    "text": "6.9 10. Golf Score\nOn a given day, your golf score takes values from the numbers 1 through 10 with equal probability of getting each one. Assume that you play golf for three days, and assume that your three performances are independent. Let \\(X_1, X_2\\) and \\(X_3\\) be the scores that you get, and let \\(X\\) be the minimum of these three scores.\n\n\nShow that for any discrete random variable \\(X\\) \\(p_X(k)=\\mathbb{P}(X &gt; k-1) - \\mathbb{P}(X&gt;k)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k-1) = P(X \\geq k) =P(X = k)+P(X &gt; k)\\), thus \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)\\).\n\n\n\n\nWhat is the probability that \\(\\mathbb{P}(X_1&gt;k)\\) for \\(k=1,\\ldots,10\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X_1&gt;k)=\\frac{10-k}{10}\\)\n\n\n\n\nUse (a) to determine the pmf \\(p_X(k)\\) for \\(k=1,\\ldots,10\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(X &gt; k) = P(X_1,X_2,X_3 &gt; k) = P(X_1 &gt; k)P(X_2&gt;k)P(X_3&gt;k)\\)\nThis means \\(P(X&gt;k) =\\frac{(10-k)^3}{10^3}\\), and \\(P(X&gt;k-1)=\\frac{(11-k)^3}{10^3}\\). From the previous result, \\(P(X=k)=P(X&gt;k-1)-P(X&gt;k)=\\frac{(11-k)^3-(10-k)^3}{10^3}\\)\n\n\n\n\nWhat is the average score improvement if you play just for one day compared with playing for three days and taking the minimum?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is asking to take the difference of the two expected values. It’s obvious that \\(E(X_1)=5.5\\); We need to find the expected value of \\(X\\).\n\nx &lt;- 1:10\npx &lt;- ((11-x)^3-(10-x)^3)/10^3\n#double check\nsum(px)\n\n[1] 1\n\n(EX &lt;- sum(x*px))\n\n[1] 3.025\n\n5.5-EX\n\n[1] 2.475\n\n\nThe average (expected) point improvement when going from a 1 day point to a minimum of 3 days is 2.475."
  },
  {
    "objectID": "rv_practice.html#functions-of-a-random-variable",
    "href": "rv_practice.html#functions-of-a-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.10 11. Functions of a random variable",
    "text": "6.10 11. Functions of a random variable\nLet \\(g(X) = \\begin{cases}1 & \\text{if }X&gt;10\\\\0 & \\text{otherwise}\\end{cases}\\) and \\(h(X) = \\begin{cases}X-10 & \\text{if }X-10&gt;0\\\\0 & \\text{otherwise}\\end{cases}\\)\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_k = p_1/k\\), find \\(\\mathbb{E}\\left[g(X)\\right]\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nk &lt;- 1:15\np1 &lt;- 1/(sum(1/k))\npk &lt;- p1/k\n\ng &lt;- function(x){\n  return(as.numeric(x&gt;10))\n}\n\nsum(g(k)*pk)\n\n[1] 0.1173098\n\n\n\n\n\n\nFor \\(X \\in S_X=\\{1,2,\\ldots,15\\}\\) with \\(p_{k+1} = p_k/2\\) (for \\(k&gt;1\\)), find \\(\\mathbb{E}\\left[h(X)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nh &lt;- function(x){\n  return(max(0, x-10))\n}\n\np1 &lt;- 1/(sum(1/2^(k-1)))\npk &lt;- p1*(1/2^(k-1))\n\nsum(h(k)*pk)\n\n[1] 5"
  },
  {
    "objectID": "rv_practice.html#voltage-ii",
    "href": "rv_practice.html#voltage-ii",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.11 12. Voltage II",
    "text": "6.11 12. Voltage II\nA voltage \\(X\\) is uniformly distributed on the set \\(\\{-3,\\ldots,3,4\\}\\).\n\n\nFind the mean and variance of \\(X\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- -3:4\npx &lt;- 1/length(x)\n\n(EX &lt;- sum(x*px))\n\n[1] 0.5\n\n(VarX &lt;- sum((x-EX)^2*px))\n\n[1] 5.25\n\n\n\n\n\n\nFind the mean and variance of \\(Y=-2X^2+3\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ny &lt;- -2*x^2+3\n\n(EY &lt;- sum(y*px))\n\n[1] -8\n\n(VarY &lt;- sum((y-EY)^2*px))\n\n[1] 105\n\n\n\n\n\n\nFind the mean and variance of \\(W=\\text{cos}(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nw &lt;- cos(pi*x/8)\n(EW &lt;- sum(w*px))\n\n[1] 0.6284174\n\n(VarW &lt;- sum((w-EW)^2*px))\n\n[1] 0.1050915\n\n\n\n\n\n\nFind the mean and variance of \\(Z=\\text{cos}^2(\\pi X/8)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nz &lt;- w^2\n(EZ &lt;- sum(z*px))\n\n[1] 0.5\n\n(VarZ &lt;- sum((z-EZ)^2*px))\n\n[1] 0.125"
  },
  {
    "objectID": "rv_practice.html#discrete-random-variable-problems",
    "href": "rv_practice.html#discrete-random-variable-problems",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.12 13. Discrete Random Variable Problems",
    "text": "6.12 13. Discrete Random Variable Problems\n\n\nIf \\(X\\) is \\(\\text{Poisson}(\\lambda)\\), compute \\(\\mathbb{E}\\left[1/(X+1)\\right]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be handled mathematically. The formula for \\(E(1/(X+1))\\) is\n\\(E(1/(X+1))=\\sum_{x=0}^{\\infty}\\frac{1}{x+1}\\frac{\\lambda^{x}}{x!}e^{-\\lambda}=\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x}}{(x+1)!}e^{-\\lambda}\\)\nThe trick is to get get the summation to equal 1 and simplify. We multiply by \\(\\lambda/\\lambda\\)\n\\(E(1/(X+1))=\\frac{1}{\\lambda}\\sum_{x=0}^{\\infty}\\frac{\\lambda^{x+1}}{(x+1)!}e^{-\\lambda}\\)\nNow we can make a change of variables: \\(y=x+1\\) and thus \\(x=0\\) becomes \\(y=1\\)\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}\\sum_{y=1}^{\\infty}\\frac{\\lambda^{y}}{y!}e^{-\\lambda}\\)\nThe only thing missing is that the summation starts at \\(y=1\\) instead of \\(y=0\\), But for \\(Y \\sim Poisson(\\lambda)\\), \\(P(Y=0)=e^{-\\lambda}\\) so this summation is \\(1-e^{-\\lambda}\\).\n\\(E(1/(X+1)) = \\frac{1}{\\lambda}(1-e^{-\\lambda})\\)\n\n\n\n\nIf \\(X\\) is \\(\\text{Bernoulli}(p)\\) and \\(Y\\) is \\(\\text{Bernoulli}(q)\\), computer \\(\\mathbb{E}\\left[(X+Y)^3\\right]\\) assuming \\(X\\) and \\(Y\\) are independent.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\((X+Y)^3 = X^3+3X^2Y+3XY^2+Y^3\\) so \\(E[(X+Y)^3]=E(X^3)+3E(X^2)E(Y)+3E(X)E(Y^2)+E(Y^2)\\)\nthis is due to independence. Since \\(X\\) an \\(Y\\) are independent, so are \\(X^2\\) and \\(Y\\), and \\(X\\) and \\(Y^2\\). \\(E(X)=E(X^2)=E(X^3)=p\\) and \\(E(Y)=E(Y^2)=E(Y^3)=q\\). Thus \\(E[(X+Y)^3]=p+6pq+q\\)\n\n\n\n\nLet \\(X\\) be a random variable with mean \\(\\mu\\) and variance \\(\\sigma^2\\). Let \\(\\Delta(\\theta)=\\mathbb{E}\\left[(X-\\theta)^2\\right]\\). Find \\(\\theta\\) that minimizes the error \\(\\Delta(\\theta)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWe can expand the expected value and attempt to find the minimum with respect to \\(\\theta\\). \\(E[(X-\\theta)^2]=E[X^2-2\\theta X+\\theta^2]=E(X^2)-2\\theta\\mu+\\theta^2\\). Recall that \\(Var(X)=E(X^2)-\\mu^2\\) so \\(E(X^2)=\\sigma^2+\\mu^2\\) So we can write \\(\\Delta(\\theta)=\\sigma^2 + \\mu^2-2\\theta\\mu + \\theta^2\\) We want to find what value of \\(\\theta\\) minimizes this function - derivative! \\(\\Delta'(\\theta)=-2\\mu+2\\theta=0\\) thus \\(\\theta=\\mu\\) minimizes this.\n\n\n\n\nSuppose that \\(X_1, \\ldots, X_n\\) are independent uniform random variables in \\(\\{0,1,\\ldots,100\\}\\). Evaluate \\(\\mathbb{P}\\left[\\text{min}(X_1,\\ldots, X_n) &gt; l\\right]\\) for any \\(l \\in \\{0,1,\\ldots,100\\}\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet \\(Y=\\min(X_1, \\ldots, X_n)\\) If \\(P(Y &gt;l)\\), that means the minimum exceeds \\(l\\), so all of the values \\(&gt;l\\). \\(P(X_1 &gt; l)=(100-l)/101\\) - you can check: \\(P(X_1&gt;0)=100/101\\). This is the same calculation for each \\(i\\). So \\(P(Y&gt;l)=\\dfrac{(100-l)^n}{101^n}\\).\n\n\n\n\nConsider a binomial random variable \\(X\\) with parameters \\(n\\) and \\(p\\). \\(p_X(k)={n \\choose k} p^k(1-p)^{n-k}\\). Show that the mean is \\(\\mathbb{E}X= np\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E(X)=\\sum_{k=0}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nThe first term is zero so we could write\n\\(\\sum_{k=1}^n k{n \\choose k} p^k(1-p)^{n-k}\\)\nNow the following is a fact that is needed but perhaps not well known. It’s the equivalence of \\(k{n \\choose k}=n{n-1 \\choose k-1}\\). We make this subsitution\n\\(\\sum_{k=1}^n n{n-1 \\choose k-1} p^k(1-p)^{n-k}=np\\sum_{k=1}^n {n-1\\choose k-1}p^{k-1}(1-p)^{n-k}\\)\nWe could write \\(n-k=(n-1)-(k-1)\\) and we’ll be making some substitutions: \\(m=n-1\\) and \\(j=k-1\\). This lets us write\n\\(np\\sum_{j=0}^m {m \\choose j}p^j(1-p)^{m-j}=np\\) because the summation =1, as it is just the sum of the pmf of a binomial.\n\n\n\n\n(not for 340) Consider a geometric random variable \\(X\\) with parameter \\(p\\). \\(p_X(k)=p(1-p)^k\\) for \\(k=0,1,\\ldots\\). Show that its mean is \\(\\mathbb{E}X=(1-p)/p\\).\n(not for 340) Consider a Poisson random variable \\(X\\) with parameter \\(\\lambda\\). \\(p_X(k)=\\dfrac{\\lambda^k}{k!}e^{-\\lambda}\\). Show that \\(\\text{Var}X=\\lambda\\).\n(not for 340) Consider the uniform random variable \\(X\\) over values \\(1,2,\\ldots, L\\). Show that \\(\\text{Var}X=\\dfrac{L^2-1}{12}\\). Hint: \\(\\sum_{i=1}^n i = \\frac{n(n+1)}{2}\\) and \\(\\sum_{i=1}^n i^2=\\frac{n^3}{3}+\\frac{n^2}{2}+\\frac{n}{6}\\)"
  },
  {
    "objectID": "rv_practice.html#hard-drive-failures",
    "href": "rv_practice.html#hard-drive-failures",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.13 14. Hard Drive Failures",
    "text": "6.13 14. Hard Drive Failures\nAn audio player uses a low-quality hard drive. The probability that the hard drive fails after being used for one month is 1/12. If it fails, the manufacturer offers a free-of-charge repair for the customer. For the cost of each repair, however, the manufacturer has to pay $20. The initial cost of building the player is $50, and the manufacturer offers a 1-year warranty. Within one year, the customer can ask for a free repair up to 12 times.\n\n\nLet \\(X\\) be the number of months when the player fails. What is the PMF of \\(X\\)? Hint: \\(\\mathbb{P}(X = 1)\\) may not be very high because if the hard drive fails it will be fixed by the manufacturer. Once fixed, the drive can fail again in the remaining months. So saying \\(X = 1\\) is equivalent to saying that there is only one failure in the entire 12-month period.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe number of failures should follow a binomial distribution with \\(n=12, p=1/12\\). Thus \\(P(X=k)={n \\choose k}(\\frac{1}{12})^k(\\frac{11}{12})^{n-k}\\)\n\n\n\n\nWhat is the average cost per player?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe cost is \\(50+20X\\) So \\(E(50+20X)=50+20E(X)=50+20\\cdot 12(\\frac{1}{12})=70\\)"
  },
  {
    "objectID": "rv_practice.html#bit-errors",
    "href": "rv_practice.html#bit-errors",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.14 15. Bit Errors",
    "text": "6.14 15. Bit Errors\n\nA binary communication channel has a probability of bit error of \\(p = 10^{-6}\\). Suppose that transmission occurs in blocks of 10,000 bits. Let \\(N\\) be the number of errors introduced by the channel in a transmission block.\n\n\n\nWhat is the PMF of \\(N\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(N\\) follows a binomial distribution with \\(n=10000\\) and \\(p=.000001\\)\n\n\n\n\nFind \\(\\mathbb{P}(N = 0)\\) and $(N b $ 3)$.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndbinom(0, 10000, .000001)\n\n[1] 0.9900498\n\npbinom(3, 10000, .000001)\n\n[1] 1\n\n\n\n\n\n\nFor what value of \\(p\\) will the probability of 1 or more errors in a block be 99%?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis can be solved directly. \\(P(N \\geq 1)=1-P(X=0)=1-(1-p)^{10000}\\). If we set this to .99 we can solve for \\(p\\) : \\(.99=1-(1-p)^{10000}\\) so \\(.01 = (1-p)^{10000}\\) so \\(p=1-.01^{1/10000}\\)\n\n1-.01^(1/10000)\n\n[1] 0.000460411"
  },
  {
    "objectID": "rv_practice.html#processing-orders",
    "href": "rv_practice.html#processing-orders",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.15 16. Processing Orders",
    "text": "6.15 16. Processing Orders\nThe number of orders waiting to be processed is given by a Poisson random variable with parameter \\(\\alpha = \\frac{\\lambda}{n\\mu}\\), where \\(\\lambda\\) is the average number of orders that arrive in a day, \\(\\mu\\) is the number of orders that an employee can process per day, and n is the number of employees. Let \\(N; = 5\\) and \\(N&lt; = 1\\). Find the number of employees required so the probability that more than four orders are waiting is less than 10%.\nHint: You need to use trial and error for a few \\(n\\)’s.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlambda=5\nmu=1\nppois(4, lambda/(1:10 * mu), lower.tail=FALSE)\n\n [1] 0.5595067149 0.1088219811 0.0275432568 0.0091242792 0.0036598468\n [6] 0.0016844329 0.0008589296 0.0004739871 0.0002784618 0.0001721156\n\n#With 3 employees P(X&gt;4) is less than 10%."
  },
  {
    "objectID": "rv_practice.html#normal-random-variable",
    "href": "rv_practice.html#normal-random-variable",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.16 17. Normal Random Variable",
    "text": "6.16 17. Normal Random Variable\nIf \\(Z\\sim \\text{Normal}(\\mu=0, \\sigma^2=1^2)\\) find\n\n\n\\(\\mathbb{P}(Z &gt; 2.64)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(2.64, 0, 1, lower.tail=FALSE)\n\n[1] 0.004145301\n\n\n\n\n\n\n\\(\\mathbb{P}(0 \\leq Z &lt; 0.87)\\)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(.87)-pnorm(0)\n\n[1] 0.3078498\n\n\n\n\n\n\n\\(\\mathbb{P}(|Z| &gt; 1.39)\\) (Hint: draw a picture)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npnorm(1.39, lower.tail=FALSE)*2\n\n[1] 0.1645289"
  },
  {
    "objectID": "rv_practice.html#identify-the-distribution",
    "href": "rv_practice.html#identify-the-distribution",
    "title": "5  Probability and Random Variables Practice",
    "section": "6.17 18. Identify the Distribution",
    "text": "6.17 18. Identify the Distribution\nFor the following random experiments, decide what the distribution of X should be. In nearly every case, there are additional assumptions that should be made for the distribution to apply; identify those assumptions (which may or may not strictly hold in practice).\n\n\nWe throw a dart at a dart board. Let X denote the squared linear distance from the bullseye to the where the dart landed.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAssume the dart lands somewhere on the board, and any point is equally likely (not a good assumption for a skilled dart thrower). The probability density would be proportional to the distance to the center squared - Suppose the dart board has radius \\(R\\). Let \\(X\\) be the distance to the dart from the bullseye. Then \\(P(X&lt;r)=\\pi r^2 / (\\pi R^2)=(r/R)^2\\) . The question then is what is \\(P(X^2&lt;r)\\)? Well, take a square root of both sides. \\(=P(X &lt; \\sqrt{r})=\\frac{r}{R^2}\\). This is a uniform distribution’s CDF.\n\n\n\n\nWe randomly choose a textbook from the shelf at the bookstore and let P denote the proportion of the total pages of the book devoted to exercises.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA random proportion you might want to use uniform(0,1) however this is assuming that each proportion is equally likely. This is actually a great example for a beta distribution. Beta distributions are continuous distributions that can be parameterized to model a random proportion and the distribution can can be made to be skewed in different ways.\n\n\n\n\nWe measure the time it takes for the water to completely drain out of the kitchen sink.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nLet’s assume the sink is filled to the maximum. We drain the sink and start our timer. In this case, it’s reasonable to model the length of time to drain as a normal distribution.\n\n\n\n\nWe randomly sample strangers at the grocery store and ask them how long it will take them to drive home.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe time it takes to go home could be modeled by a gamma distribution since it is a continuous distribution capped below at 0 and it is a useful way to model the length of time a random process takes to complete.\n\n\n\n\nLet \\(X\\) be a Gaussian random variable with \\(\\mu=5\\) and \\(\\sigma^2=16\\).\n\n\n\nFind \\(\\mathbb{P}(X&gt;4)\\) and \\(\\mathbb{P}(2\\leq X \\leq 7)\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X&gt;4)\npnorm(4, mean=5, sd=4, lower.tail=FALSE)\n\n[1] 0.5987063\n\n#P(2 &lt;= X &lt;= 7)\npnorm(7, 5, 4)-pnorm(4,5,4)\n\n[1] 0.2901688\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X &lt; a)=0.8869\\), find \\(a\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.88695, 4)\n\n[1] 5.210466\n\n\n\n\n\n\nIf \\(\\mathbb{P}(X&gt;b)=0.1131\\), find \\(b\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nqnorm(.1131, 5, 4, lower.tail=FALSE)\n\n[1] 9.840823\n\n\n\n\n\n\nIf \\(\\mathbb{P}(13 &lt; X \\leq c)=0.0011\\), find \\(c\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#First find the probability less than 13\np13 &lt;- pnorm(13, 5, 4)\n#now we can find the quantile for p13+.0011\nqnorm(p13+.0011, 5, 4)\n\n[1] 13.08321\n\n#double check\npnorm(13.08321,5,4)-pnorm(13,5,4)\n\n[1] 0.001100025\n\n\n\n\n\n\nConsider a cdf\n\n\\(F_X(x)-\\begin{cases}0,&\\text{if }x &lt; -1\\\\ 0.5 & \\text{if }-1 \\leq x &lt; 0\\\\(1+x)/2 & \\text{if }0 \\leq x &lt; 1\\\\1&\\text{otherwise}\\end{cases}\\)\nFind \\(\\mathbb{P}(X &lt; -1)\\), \\(\\mathbb{P}(-0.5 &lt; X &lt; 0.5)\\), and \\(\\mathbb{P}(X&gt;0.5)\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(X &lt; -1) = 0 because F(x) only goes up to .5 at x=-1, not when x &lt; -1\n\n#P(-.5 &lt; X &lt; .5) = F(.5) - F(-.5)\n(1+.5)/2 - .5\n\n[1] 0.25\n\n#P(X &gt; 0.5) = 1-P(X&lt;.5) = 1-F(.5) \n1- (1+.5)/2\n\n[1] 0.25"
  },
  {
    "objectID": "cov_practice.html",
    "href": "cov_practice.html",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "",
    "text": "7 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "cov_practice.html#two-fair-coins",
    "href": "cov_practice.html#two-fair-coins",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.1 1. Two Fair Coins",
    "text": "7.1 1. Two Fair Coins\nAlex and Bob each flips a fair coin twice. Use “1” to denote heads and “0” to denote tails. Let X be the maximum of the two numbers Alex gets, and let Y be the minimum of the two numbers Bob gets.\n\n\nFind and sketch the joint PMF \\(p_{X,Y} (x, y)\\).\nFind the marginal PMF \\(p_X(x)\\) and \\(p_Y (y)\\).\nFind the conditional PMF \\(P_{X|Y} (x | y)\\). Does \\(P_{X|Y} (x | y) = P_X(x)\\)? Why or why not?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- 0:1\npx &lt;- c(.25, .75) \n\ny&lt;- 0:1\npy &lt;- c(.75, .25)\n\npmf &lt;- px %*% t(py)\nrownames(pmf) &lt;- x\ncolnames(pmf) &lt;- y \naddmargins(pmf)\n\n         0      1  Sum\n0   0.1875 0.0625 0.25\n1   0.5625 0.1875 0.75\nSum 0.7500 0.2500 1.00\n\n\n\nprop.table(pmf, 2)\n\n     0    1\n0 0.25 0.25\n1 0.75 0.75\n\n\nEach column gives P(X=x|Y=y) for the two values of y; you can see that they are the same; the reason is because the value of X and Y are independent."
  },
  {
    "objectID": "cov_practice.html#two-fair-dice",
    "href": "cov_practice.html#two-fair-dice",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.2 2. Two Fair Dice",
    "text": "7.2 2. Two Fair Dice\nTwo fair dice are rolled. Find the joint PMF of \\(X\\) and \\(Y\\) when\n\n\n\\(X\\) is the larger value rolled, and \\(Y\\) is the sum of the two values.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmax(die1, die2)\noutcomes$y &lt;- die1+die2\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             2          3          4          5          6          7\n  1 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.05555556 0.02777778 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.05555556 0.05555556 0.02777778 0.00000000\n  4 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.05555556\n   \n             8          9         10         11         12\n  1 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  2 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  3 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000\n  4 0.02777778 0.00000000 0.00000000 0.00000000 0.00000000\n  5 0.05555556 0.05555556 0.02777778 0.00000000 0.00000000\n  6 0.05555556 0.05555556 0.05555556 0.05555556 0.02777778\n\n\n\n\n\n\n\\(X\\) is the smaller, and \\(Y\\) is the larger value rolled.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndie1 &lt;- rep(1:6, 6)\ndie2 &lt;- rep(1:6, rep(6,6))\noutcomes &lt;- data.frame(die1, die2)\noutcomes$x &lt;- pmin(die1, die2)\noutcomes$y &lt;- pmax(die1,die2)\npmf &lt;- prop.table(table(outcomes$x, outcomes$y))\npmf\n\n   \n             1          2          3          4          5          6\n  1 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556 0.05555556\n  2 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556 0.05555556\n  3 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556 0.05555556\n  4 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556 0.05555556\n  5 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778 0.05555556\n  6 0.00000000 0.00000000 0.00000000 0.00000000 0.00000000 0.02777778"
  },
  {
    "objectID": "cov_practice.html#two-normal-rvs",
    "href": "cov_practice.html#two-normal-rvs",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.3 3. Two Normal RVs",
    "text": "7.3 3. Two Normal RVs\nLet X and Y be zero-mean, unit-variance independent Gaussian random variables. Find the value of r for which the probability that \\((X, Y )\\) falls inside a circle of radius r is 1/2.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx &lt;- rnorm(10000)\ny &lt;- rnorm(10000)\n\nr &lt;- seq(1.1, 1.2, .005)\np &lt;- 0\nfor (i in 1:length(r)){\n  p[i] &lt;- mean(sqrt(x^2+y^2) &lt;= r[i])\n}\ndata.frame(r,p)\n\n       r      p\n1  1.100 0.4654\n2  1.105 0.4685\n3  1.110 0.4707\n4  1.115 0.4745\n5  1.120 0.4764\n6  1.125 0.4798\n7  1.130 0.4831\n8  1.135 0.4855\n9  1.140 0.4888\n10 1.145 0.4918\n11 1.150 0.4954\n12 1.155 0.4987\n13 1.160 0.5017\n14 1.165 0.5042\n15 1.170 0.5074\n16 1.175 0.5105\n17 1.180 0.5130\n18 1.185 0.5168\n19 1.190 0.5201\n20 1.195 0.5223\n21 1.200 0.5247\n\n#X^2 + Y^2 ~ Chisq(2)\n#so the square root of the 50th percentile from that distribution should be the answer\nsqrt(qchisq(.5,2))\n\n[1] 1.17741"
  },
  {
    "objectID": "cov_practice.html#uniform-random-angle",
    "href": "cov_practice.html#uniform-random-angle",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.4 4. Uniform Random Angle",
    "text": "7.4 4. Uniform Random Angle\nLet \\(\\Theta ∼ Uniform[0, 2\\pi]\\).\n\n\nIf \\(X = cos \\Theta\\), \\(Y = sin \\Theta\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nYes, they are uncorrelated, because (x,y) can be any point on the circumference of a circle of radius 1 with uniform likelihood. However, they are not independent. If we know the value of \\(Y\\) for example, there are only 2 possible values of \\(X\\).\n\nthetas &lt;- runif(10000, 0, 2*pi)\ncor(cos(thetas), sin(thetas))\n\n[1] 0.01024295\n\n\n\n\n\n\nIf \\(X = cos(\\Theta/4)\\), \\(Y = sin(\\Theta/4)\\). Are \\(X\\) and \\(Y\\) uncorrelated?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn this case (x,y) can only be found in the first quadrant. In this case they are going to be negatively correlated, since that portion of the unit circle in the first quadrant slopes downwards.\n\ncor(cos(thetas/4), sin(thetas/4))\n\n[1] -0.9184621\n\n\n\n\n\n##5. Signal and Noise Let \\(Y = X+N\\), where \\(X\\) is the input, \\(N\\) is the noise, and \\(Y\\) is the output of a system. Assume that \\(X\\) and \\(N\\) are independent random variables. It is given that \\(E[X] = 0\\), \\(Var[X] = \\sigma^2_X\\), \\(E[N] = 0\\), and \\(Var[N] = \\sigma^2_N\\).\n\n\nFind the correlation coefficient \\(\\rho\\) between the input \\(X\\) and the output \\(Y\\) .\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(\\rho(X,Y)= \\dfrac{Cov(X,Y)}{SD(X)SD(Y)}=\\dfrac{E(XY)-E(X)E(Y)}{\\sigma_X \\cdot \\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(E(XY)=E(X^2+XN)=E(X^2)+E(X)E(N)\\). Because \\(E(X)=E(N)=0\\) this simplifies to\n\\(\\rho(X,y)=\\dfrac{E(X^2)}{\\sigma_x\\sqrt{\\sigma^2_X+\\sigma^2_N}}\\)\n\\(Var(X)=E(X^2)-E(X)^2 = E(X^2)\\) so we can replace the numerator with \\(\\sigma^2_X\\). So\n\\(\\rho(X,Y) = \\sqrt{\\dfrac{\\sigma^2_X}{\\sigma^2_X+\\sigma^2_N}}\\)\n\n#example: sigma_X = 5, sigma_N=2\nX &lt;- rnorm(10000, 0, 5)\nN &lt;- rnorm(10000, 0, 2)\ncor(X, X+N)\n\n[1] 0.9298284\n\nsqrt(5^2/(5^2+2^2))\n\n[1] 0.9284767\n\n\n\n\n\n\nSuppose we estimate the input \\(X\\) by a linear function \\(g(Y ) = aY\\) . Find the value of a that minimizes the mean squared error \\(E[(X − aY )^2]\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(E[(X-aY)^2]=E[X^2-2aXY+a^2Y^2]=E[X^2]-2aE[XY]+a^2E[Y^2]\\)\nBecause \\(E(Y)=0\\), \\(E[Y^2]=Var(Y)=\\sigma_X^2+\\sigma_N^2\\). We already have that \\(E(X^2)=\\sigma_X^2\\) and \\(E(XY)=\\sigma_X^2\\). Thus\n$E[(X-aY)^2]=(_X^2+_N2)a2- 2_X^2a+_X^2 $\nThis is a quadratic function in \\(a\\), and the vertex can be found at \\(-\\frac{B}{2A}\\) or in this case \\(a^*=\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\)\n\n\n\n\nExpress the resulting mean squared error in terms of \\(\\eta = \\sigma^2_X/\\sigma^2_N\\).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nPlugging this in for \\(a\\) we get\n\\(E[(X-aY)^2]=(\\sigma_X^2+\\sigma_N^2)\\dfrac{\\sigma_X^4}{(\\sigma_X^2+\\sigma_N^2)^2}-2\\dfrac{\\sigma_X^4}{\\sigma_X^2+\\sigma_N^2}+\\sigma_X^2=\\sigma_X^2\\left(1-\\dfrac{\\sigma_X^2}{\\sigma_X^2+\\sigma_N^2}\\right)\\)\n\\(=\\sigma_X^2\\left(\\dfrac{\\sigma_N^2}{\\sigma_X^2+\\sigma_N^2}\\right)=\\dfrac{\\sigma_X^2}{\\eta+1}\\)"
  },
  {
    "objectID": "cov_practice.html#cat-genetics",
    "href": "cov_practice.html#cat-genetics",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.5 6. Cat Genetics",
    "text": "7.5 6. Cat Genetics\nThe gene that controls white coat color in cats, KIT , is known to be responsible for multiple phenotypes such as deafness and blue eye color. A dominant allele W at one location in the gene has complete penetrance for white coat color; all cats with the W allele have white coats. There is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. However, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors.\n\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes.\nAbout 73% of white cats with two blue eyes are deaf\n40% of white cats with one blue eye are deaf.\nOnly 19% of white cats with other eye colors are deaf.\n\n\nCalculate the prevalence of deafness among white cats.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn.blue &lt;- c(0,1,2)\np.n.blue &lt;- c(.60, .30, .10)\n\np.deaf.given.b &lt;- c(.19, .40, .73) #for 0, 1 ,2 blue eyes\n\n#P(Deafness) = P(0b)*P(deaf|0b) + P(1b)*P(deaf|1b) + P(2b)*P(deaf|2b)\n(p.deaf &lt;- sum(p.n.blue * p.deaf.given.b))\n\n[1] 0.307\n\n#Check\nnCats &lt;- 10000\nnblue &lt;- sample(n.blue, size=nCats, replace=TRUE, prob=p.n.blue)\nisdeaf &lt;- FALSE\nfor(i in 1:nCats){\n  isdeaf[i] &lt;- runif(1) &lt; p.deaf.given.b[nblue[i]+1]\n}\nmean(isdeaf)\n\n[1] 0.3057\n\n\n\n\n\n\nGiven that a white cat is deaf, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | deaf) = P(2b)*P(deaf|2b) / p(deaf)\np.n.blue[3] * p.deaf.given.b[3] / p.deaf\n\n[1] 0.237785\n\n#check\nmean(nblue[isdeaf]==2)\n\n[1] 0.2342166\n\n\n\n\n\n\nSuppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness differs according to eye color. While deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. White cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color.\nWhat is the prevalence of blindness among deaf, white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\np.blind.given.nodeaf &lt;- 0.10\np.blind.given.deaf.and.nblue &lt;- c(0.20, 0.4, 0.2) #for 0, 1, 2 blue eyes\n\n#P(blind & deaf) = P(0b)*P(deaf|0b)*P(blind|deaf&0b)+\n#  P(1b)*P(deaf|1b)*P(blind|deaf&1b)+\n#  P(2b)*P(deaf|2b)*P(blind|deaf&2b)+\np.blind.and.deaf &lt;- sum(p.n.blue * p.deaf.given.b * p.blind.given.deaf.and.nblue)\n\n#P(blind | deaf ) = P(blind & deaf) / P(deaf)\n(p.blind.given.deaf = p.blind.and.deaf / p.deaf)\n\n[1] 0.2781759\n\n#check\nisBlind &lt;- FALSE\nfor(i in 1:nCats){\n  if(!isdeaf[i]){\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.nodeaf\n  } else {\nisBlind[i] &lt;- runif(1) &lt; p.blind.given.deaf.and.nblue[nblue[i]+1]\n  }\n}\n#check\nmean(isBlind[isdeaf])\n\n[1] 0.2744521\n\n\n\n\n\n\nWhat is the prevalence of blindness among white cats?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(blind) = P(deaf & blind) + P(nondeaf & blind)\n\n#P(nondeaf & blind ) = P(nondeaf) * P(blind | nondeaf)\np.blind.and.nondeaf &lt;- (1-p.deaf)*p.blind.given.nodeaf\n\n(p.blind &lt;- p.blind.and.deaf + p.blind.and.nondeaf)\n\n[1] 0.1547\n\n#check\nmean(isBlind)\n\n[1] 0.1531\n\n\n\n\n\n\nGiven that a cat is white and blind, what is the probability that it has two blue eyes?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#P(2b | blind) = P(2b & blind) / p(blind)\n#numerator: P(2b & blind) = P(2b) * [P(deaf|2b) * p(blind|deaf & 2b) + P(nondeaf|2b) * p(blind|nondeaf & 2b)]\np.blind.and.2b &lt;- p.n.blue[3] * (p.deaf.given.b[3]*p.blind.given.deaf.and.nblue[3] + \n (1-p.deaf.given.b[3]) * p.blind.given.nodeaf)\n\n(p.2b.given.blind = p.blind.and.2b / p.blind)\n\n[1] 0.1118293\n\n#check\nmean(nblue[isBlind]==2)\n\n[1] 0.1071195"
  },
  {
    "objectID": "cov_practice.html#gss-survey-i",
    "href": "cov_practice.html#gss-survey-i",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.6 7. GSS Survey I",
    "text": "7.6 7. GSS Survey I\nLinda is 31 years old, single, outspoken, and very bright. She majored in philosophy. As a student, she was deeply concerned with issues of discrimination and social justice, and also participated in anti-nuclear demonstrations. Which is more probable?\n\n\nLinda is a banker.\nLinda is a banker and considers herself a liberal Democrat.\n\nTo answer this question we will use data from the GSS survey found at https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv.\nThe code for “Banking and related activities” in the indus10 variable is 6870. The values of the column sex are encoded like this:\n1: Male, 2: Female\nThe values of polviews are on a seven-point scale:\n1 Extremely liberal\n2 Liberal\n3 Slightly liberal\n4 Moderate\n5 Slightly conservative\n6 Conservative\n7 Extremely conservative\nDefine “liberal” as anyone whose political views are 3 or below. The values of partyid are encoded:\n0 Strong democrat\n1 Not strong democrat\n2 Independent, near democrat\n3 Independent\n4 Independent, near republican\n5 Not strong republican\n6 Strong republican\n7 Other party\nYou need to compute:\n\nThe probability that Linda is a female banker,\nThe probability that Linda is a liberal female banker, and\nThe probability that Linda is a liberal female banker and a Democrat.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngss &lt;- read.csv(\"https://github.com/AllenDowney/ThinkBayes2/raw/master/data/gss_bayes.csv\")\n#banker : indus10==6870\n#female : sex == 2\n#liberal: polviews &lt;= 3\n#democrat: partyid &lt;= 2\n\n#In my reading, I am interpreteing that 'Linda is female' is given from the context;\n\n#P(banker | female) \nmean(gss[gss$sex==2,]$indus10==6870)\n\n[1] 0.02116103\n\n#P(liberal banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3,]$indus10==6870)\n\n[1] 0.01723195\n\n#P(liberal Dem banker | female)\nmean(gss[gss$sex==2 & gss$polviews &lt;=3 & gss$partyid &lt;= 1,]$indus10==6870)\n\n[1] 0.01507289\n\n\n\n\n\n##8. GSS Survey II Compute the following probabilities:\n\n\nWhat is the probability that a respondent is liberal, given that they are a Democrat?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$partyid&lt;=1,]$polviews&lt;=3)\n\n[1] 0.389132\n\n\n\n\n\n\nWhat is the probability that a respondent is a Democrat, given that they are liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews&lt;=3,]$partyid&lt;=1)\n\n[1] 0.5206403"
  },
  {
    "objectID": "cov_practice.html#gss-survey-iii",
    "href": "cov_practice.html#gss-survey-iii",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.7 9. GSS Survey III",
    "text": "7.7 9. GSS Survey III\nThere’s a famous quote about young people, old people, liberals, and conservatives that goes something like:\n\nIf you are not a liberal at 25, you have no heart. If you are not a conservative at 35, you have no brain.\n\nWhether you agree with this proposition or not, it suggests some probabilities we can compute as an exercise. Rather than use the specific ages 25 and 35, let’s define young and old as under 30 or over 65. For these thresholds, I chose round numbers near the 20th and 80th percentiles. Depending on your age, you may or may not agree with these definitions of “young” and “old”.\nI’ll define conservative as someone whose political views are “Conservative”, “Slightly Conservative”, or “Extremely Conservative”.\nCompute the following probabilities. For each statement, think about whether it is expressing a conjunction, a conditional probability, or both. For the conditional probabilities, be careful about the order of the arguments. If your answer to the last question is greater than 30%, you have it backwards!\n\nWhat is the probability that a randomly chosen respondent is a young liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &lt;30 & gss$polviews &lt;=3)\n\n[1] 0.06579428\n\n\n\n\n\n\nWhat is the probability that a young person is liberal?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$age &lt;30,]$polviews &lt;=3)\n\n[1] 0.3385177\n\n\n\n\n\n\nWhat fraction of respondents are old conservatives?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss$age &gt;65 & gss$polviews &gt;= 5)\n\n[1] 0.06226415\n\n\n\n\n\n\nWhat fraction of conservatives are old?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(gss[gss$polviews &gt;=5,]$age &gt;65)\n\n[1] 0.1820933"
  },
  {
    "objectID": "cov_practice.html#two-child-paradox",
    "href": "cov_practice.html#two-child-paradox",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.8 10. Two Child Paradox",
    "text": "7.8 10. Two Child Paradox\nSuppose you meet someone and learn that they have two children. You ask if either child is a girl and they say yes. What is the probability that both children are girls? (Hint: Start with four equally likely hypotheses.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBefore we know anything about their two kids, the number of girls \\(X\\) they have could be 0, 1 or 2. Simplifying the scenario with the ‘equally likely hypothesis’ means that we assume each kid has a 50% chance of being a girl, independently. Thus the probabilities are 0.25, 0.5 and 0.25 respectively.\nIf we learn that at least one of the kids is a girl, that tells us that the first possibility, that \\(x=0\\) is not possible. Thus \\(P(X=2 | X&gt;0) = .\\dfrac{25}{.5+.25}= \\frac{1}{3}\\)**"
  },
  {
    "objectID": "cov_practice.html#monty-hall",
    "href": "cov_practice.html#monty-hall",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.9 11. Monty Hall",
    "text": "7.9 11. Monty Hall\nThere are many variations of the Monty Hall problem. For example, suppose Monty always chooses Door 2 if he can, and only chooses Door 3 if he has to (because the car is behind Door 2).\n\n\nIf you choose Door 1 and Monty opens Door 2, what is the probability the car is behind Door 3?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#3 equally likely possibilities:\n#C1  -&gt; Monty chooses Door 2\n#C2  -&gt; Monty cannot choose Door 2, chooses Door 3\n#C3  -&gt; Monty chooses Door 2\n\n#So if Monty chooses Door 2, either C1 or C3 must be the case, each equally likely. \n#Thus P(C3 | Monty chooses 2) = .50\n\n\n\n\n\nIf you choose Door 1 and Monty opens Door 3, what is the probability the car is behind Door 2?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#If he chose Door 3, that means that it must be the case that the car is behind \n#door 2; he would *ONLY* choose door 3 in that case.\n\n#Thus P(C2 | Monty chooses 3) = 1.0"
  },
  {
    "objectID": "cov_practice.html#mms",
    "href": "cov_practice.html#mms",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.10 12. M&Ms",
    "text": "7.10 12. M&Ms\nM&M’s are small candy-coated chocolates that come in a variety of colors. Mars, Inc., which makes M&M’s, changes the mixture of colors from time to time. In 1995, they introduced blue M&M’s.\n\nIn 1994, the color mix in a bag of plain M&M’s was 30% Brown, 20% Yellow, 20% Red, 10% Green, 10% Orange, 10% Tan.\nIn 1996, it was 24% Blue , 20% Green, 16% Orange, 14% Yellow, 13% Red, 13% Brown.\n\nSuppose a friend of mine has two bags of M&M’s, and he tells me that one is from 1994 and one from 1996. He won’t tell me which is which, but he gives me one M&M from each bag. One is yellow and one is green. What is the probability that the yellow one came from the 1994 bag?\n(Hint: The trick to this question is to define the hypotheses and the data carefully.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\(P(G|94) = .10, P(G|96) = .20\\) \\(P(Y|94) = .20, P(Y|96) = .14\\) \\(P(94)=P(96)=.50\\) assuming equally likely. It’s important to realize only one of two situations could have occurred:\n\nSituation 1: A G was chosen from the 94 bag and a Y was chosen from the 96 bag\nSituation 2: A Y was chosen from the 94 bag and a G was chosen from the 96 bag.\n\nThe corresponding probabilities are \\((.10)(.14)=.014\\) and \\((.20)(.20)=.04)\\). The question could be stated: What is the probability that Sit.1 occurred given that either Sit1 or Sit2 occured.\nThe likelihood is \\(.014 / (.014+.04) = .2592593\\)"
  },
  {
    "objectID": "cov_practice.html#two-coins",
    "href": "cov_practice.html#two-coins",
    "title": "6  Independence and Conditional Probability Practice",
    "section": "7.11 13. Two Coins",
    "text": "7.11 13. Two Coins\nSuppose you have two coins in a box. One is a normal coin with heads on one side and tails on the other, and one is a trick coin with heads on both sides. You choose a coin at random and see that one of the sides is heads. What is the probability that you chose the trick coin?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is actually similar to the “family with two girls” problem. The equally likely coins are “HT” and “HH”.\n\\(P(Heads | HH) = 1\\) \\(P(Heads | TH) = .5\\)\nBayes Theorem tells us that \\(P(HH | Heads) = \\dfrac{P(HH)*P(Heads|HH)}{P(HH)P(Heads|HH)+P(TH)P(Heads|TH)}=\\dfrac{.5}{.5+.25}=\\frac23\\)\nSeems counter intuitive! If I pick a random coin from the two, you already know that one of the sides is a head without looking at it. Somehow then, when you look at just one side of the coin, seeing a H makes you 67% sure that it is the trick coin. The reason is that seeing the head side is like flipping it once and getting a H. That is less likely to occur with the fair coin, hence this outcome lends evidence to the “trick coin” hypothesis."
  },
  {
    "objectID": "RV_summary.html#rv-summary",
    "href": "RV_summary.html#rv-summary",
    "title": "Appendix A — Random Variable Summary",
    "section": "A.1 RV summary",
    "text": "A.1 RV summary\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X\\)\nBinomial\nGeometric\nPoisson\nDiscrete Uniform\nNormal\n(Continuous) Uniform\nExponential\n\n\n\n\nType\nDiscrete\nDiscrete\nDiscrete\nDiscrete\nContinuous\nContinuous\nContinuous\n\n\nParameters\n\\(n\\), \\(p\\)\n\\(p\\)\n\\(\\lambda\\)\n\\(a\\), \\(b\\)\n\\(\\mu\\), \\(\\sigma^2\\)\n\\(a\\), \\(b\\)\n\\(\\lambda\\)\n\n\nDescription\nNumber of successes in \\(n\\) independent trials with \\(p\\) probability of success for each trial  (Note: Bernoulli is just Binomial with \\(n\\!=\\!1\\))\nNumber of failures BEFORE the first success while independently repeating trial with \\(p\\) probability of success\nCount of number of occurrences of an event with constant mean rate \\(\\lambda\\) that’s independent of previous occurrences\n\\(n\\)-sided fair die\nNormal distributions usually arise from CLT (i.e. they’re processes that are the sum of many smaller independent processes)\nGeneralizing \\(n\\)-sided fair die to a continuous interval\nWaiting time between Poisson events\n\n\nOutcomes\n\\(0,1,\\ldots,n\\)\n\\(0,1,\\ldots\\)\n\\(0,1,\\ldots\\)\n\\(a,a\\!+\\!1,\\ldots,b\\)\n\\((-\\infty,\\infty)\\)\n\\([a,b]\\)\n\\([0,\\infty)\\)\n\n\nPDF/PMF at \\(k\\)\n\\({n\\choose k}p^k(n-p)^{n-k}\\)\n\\(p(1-p)^k\\)\n\\(\\frac{\\lambda^ke^{-\\lambda}}{k!}\\)\n\\(\\frac1{b-(a-1)}\\)\n\\(\\frac1{\\sigma\\sqrt{2\\pi}}e^{-\\frac12\\left(\\frac{x-\\mu}\\sigma\\right)^2}\\)\n\\(\\frac1{b-a}\\)\n\\(\\lambda e^{-\\lambda x}\\)\n\n\n\\(P(X\\le k)\\)\n\n\\(1-(1-p)^{\\lfloor k\\rfloor+1}\\)\n\n\\(\\frac{\\lfloor k\\rfloor-(a-1)}{b-(a-1)}\\)\n\n\\(\\frac{x-a}{b-a}\\)\n\\(1-e^{-\\lambda x}\\)\n\n\nMean\n\\(np\\)\n\\(\\frac{1-p}p\\)\n\\(\\lambda\\)\n\\(\\frac{a+b}2\\)\n\\(\\mu\\)\n\\(\\frac{a+b}2\\)\n\\(\\frac1\\lambda\\)\n\n\nVariance\n\\(np(1-p)\\)\n\\(\\frac{1-p}{p^2}\\)\n\\(\\lambda\\)\n\\(\\frac{(b-(a-1))^2-1}{12}\\)\n\\(\\sigma^2\\)\n\\(\\frac{(b-a)^2}{12}\\)\n\\(\\frac1{\\lambda^2}\\)\n\n\nR functions\ndbinom, pbinom, qbinom, rbinom\ndgeom, pgeom, qgeom, rgeom\ndpois, ppois, qpois, rpois\nsample\ndnorm, pnorm, qnorm, rnorm\ndunif, punif, qunif, runif\ndexp, pexp, qexp, rexp"
  },
  {
    "objectID": "R01_RandomVariables.html#bernoulli",
    "href": "R01_RandomVariables.html#bernoulli",
    "title": "4  Random Variables R Examples",
    "section": "4.1 Bernoulli",
    "text": "4.1 Bernoulli\nLike the flip of a fair or unfair coin. X=1 if the coin comes up heads, 0 on tails.\n\n#Parameters\np &lt;- .3 # the probability of a success\n\n#Generating 10 random values\nrbinom(10, size=1, prob=p)\n\n [1] 0 0 1 1 0 0 0 0 0 1\n\n\nExpected value is \\[\\sum_k k\\cdot Pr[X=k]\\]\n\n#All possible values\nk &lt;- 0:1\n#Associated probabilities\nPr &lt;- dbinom(k, size=1, prob=p)\n#expectation\nsum(k*Pr)\n\n[1] 0.3\n\n\nThe expected value is \\(p\\) and the variance is \\(p(1-p)\\)\n\nbernoulli.sim &lt;- rbinom(100, size=1, prob=p)\n\n# Expectation\np\n\n[1] 0.3\n\n# from sample\nmean(bernoulli.sim)\n\n[1] 0.32\n\n# Variance\np*(1-p)\n\n[1] 0.21\n\n#from sample\nvar(bernoulli.sim)\n\n[1] 0.219798"
  },
  {
    "objectID": "R01_RandomVariables.html#binomial",
    "href": "R01_RandomVariables.html#binomial",
    "title": "4  Random Variables R Examples",
    "section": "4.2 Binomial",
    "text": "4.2 Binomial\nLike flipping a fair (or unfair) coin \\(n\\) times, counting the number of heads.\n\n#Parameters\nn &lt;- 8  #the number of flips / attempts\np &lt;- .4 #the probability of success\n\n#Generate 10 random values\nrbinom(10, size=n, prob=p)\n\n [1] 4 1 5 3 3 4 2 4 2 1\n\n#Calculate P(X=3)\ndbinom(x=3, size=n, prob=p)\n\n[1] 0.2786918\n\n#Calculate all probabilities\ndbinom(x=0:8, size=n, prob=p)\n\n[1] 0.01679616 0.08957952 0.20901888 0.27869184 0.23224320 0.12386304 0.04128768\n[8] 0.00786432 0.00065536\n\n#Calculate P(X &lt;= 3)\npbinom(q=3, size=n, prob=p)\n\n[1] 0.5940864\n\n\nExpected value is \\[\\sum_k k\\cdot Pr[X=k]\\]\n\n#All possible values\nk &lt;- 0:n\n#Associated probabilities\nPr &lt;- dbinom(k, size=n, prob=p)\n#expectation\nsum(k*Pr)\n\n[1] 3.2\n\nn*p\n\n[1] 3.2\n\n\nThe probability mass function\n\nbarplot(Pr, names=k, main=\"Probability Mass Function of Binomial(8,.4)\")\n\n\n\n\nThe cumulative distribution function\n\nx &lt;- 0:8\ncdfx &lt;- pbinom(x, size=n,prob=.4)\nplot(x, cdfx, type=\"s\", main=\"Cumulative Distribution Function of Binomial(8,.4)\", ylim=c(0,1))\n\n\n\n\nThe expected value is \\(np\\) and the variance is \\(np(1-p)\\)\n\nbinomial.sim &lt;- rbinom(10000, size=n, prob=p)\n\n# Expectation\nn*p\n\n[1] 3.2\n\n# from sample\nmean(binomial.sim)\n\n[1] 3.2196\n\n# Variance\nn*p*(1-p)\n\n[1] 1.92\n\n#from sample\nvar(binomial.sim)\n\n[1] 1.899366"
  },
  {
    "objectID": "R01_RandomVariables.html#geometric",
    "href": "R01_RandomVariables.html#geometric",
    "title": "4  Random Variables R Examples",
    "section": "4.3 Geometric",
    "text": "4.3 Geometric\nCounting how many tails before the first head; the number of failures before the first success (independent trials, probability of success remains constant)\n\n#Parameters\np &lt;- .4 #The probability of success on each trial\n\n#Generate 10 random values\nrgeom(10, prob=p)\n\n [1] 2 0 0 4 0 4 1 4 2 0\n\n\nThe probability mass function…. only going out to 20, but the support is infinite!\n\nk &lt;- 0:20\nPr &lt;- dgeom(k, prob=p)\nbarplot(Pr, names=k, main=\"Probability Mass Function of Geom(.4)\")\n\n\n\n\nThe cumulative distribution function\n\ncdfx &lt;- cumsum(Pr)\n\n# if you want it to look really proper\nn &lt;- length(k)\nplot(x = NA, y = NA, pch = NA, \n     xlim = c(0, max(k)), \n     ylim = c(0, 1),\n     ylab = \"Cumulative Probability\",\n     main = \"Cumulative Distribution Function of Geom(.4)\")\npoints(x = k[-n], y = cdfx[-n], pch=19)\npoints(x = k[-1], y = cdfx[-n], pch=1)\nfor(i in 1:(n-1)) points(x=k[i+0:1], y=cdfx[c(i,i)], type=\"l\")\n\n\n\n\nBut it’s probably faster to just use a step type plot. The vertical lines are not technically part of the plot though.\n\nplot(k, cdfx, type=\"s\", main=\"Cumulative Distribution Function of Geom(.4)\", ylim=c(0,1))\npoints(k, cdfx, pch = 16, col = \"blue\")\n\n\n\n\nYou can get the cumulative probabilities from the pgeom function\n\nplot(k, pgeom(k, prob=.4), type=\"s\", main=\"Cumulative Distribution Function of Geom(.4)\", ylim=c(0,1))\npoints(k, cdfx, pch = 16, col = \"blue\")\n\n\n\n\nThe expected value is \\(\\dfrac{1-p}{p}\\) and the variance is \\(\\dfrac{1-p}{p^2}\\)\n\ngeom.sim &lt;- rgeom(10000, prob=p)\n\n# Expectation\n(1-p)/p\n\n[1] 1.5\n\n# from sample\nmean(geom.sim)\n\n[1] 1.5128\n\n# Variance\n(1-p)/(p^2)\n\n[1] 3.75\n\n#from sample\nvar(geom.sim)\n\n[1] 3.83542"
  },
  {
    "objectID": "R01_RandomVariables.html#poisson",
    "href": "R01_RandomVariables.html#poisson",
    "title": "4  Random Variables R Examples",
    "section": "4.4 Poisson",
    "text": "4.4 Poisson\nLike the number of times something occurs during a fixed time window\n\n#Parameters\nl &lt;- 10.5 #The rate parameter, average occurrences per unit time\n     #It is lambda, but I'll call it \"l\"\n\n#Generate 10 random values\nrpois(10, lambda=l)\n\n [1]  9 15  8 18  9 12 14 12  8 15\n\n\nPMF and CDF\n\npar(mfrow=c(1,2))\nk &lt;- 0:20\nPr &lt;- dpois(k,l)\n\nbarplot(Pr, names=k, main=\"PMF of Pois(3)\")\nplot(k, ppois(k, l), type=\"s\", main=\"CDF of Pois(3)\", ylim=c(0,1))\npoints(k, ppois(k, l), pch = 16, col = \"blue\")\n\n\n\n\nThe expected value and variance are both is \\(\\lambda\\)\n\npois.sim &lt;- rpois(10000, lambda=l)\n\n# Expectation & Variance\nl\n\n[1] 10.5\n\n# mean from sample\nmean(pois.sim)\n\n[1] 10.5085\n\n#variance from sample\nvar(pois.sim)\n\n[1] 10.52458"
  },
  {
    "objectID": "R01_RandomVariables.html#uniform-discrete",
    "href": "R01_RandomVariables.html#uniform-discrete",
    "title": "4  Random Variables R Examples",
    "section": "4.5 Uniform (Discrete)",
    "text": "4.5 Uniform (Discrete)\nLike rolling a fair die\n\n#Parameters\na &lt;- 1 #lower bound, inclusive\nb &lt;- 6 #upper bound, inclusive\n\n#Generate 10 random values\nsample(a:b, 10, replace=TRUE) #replace=TRUE is important\n\n [1] 6 5 3 5 2 4 6 6 2 1\n\n\nThe PMF and CDF\n\npar(mfrow=c(1,2))\nk &lt;- a:b\nPr &lt;- rep(1/(b-a+1), length(k))\n\nbarplot(Pr, names=k, main=\"PMF of Unif(1,6)\")\nplot(k, cumsum(Pr), type=\"s\", main=\"CDF of Unif(1,6)\", ylim=c(0,1))\npoints(k, cumsum(Pr), pch = 16, col = \"blue\")"
  },
  {
    "objectID": "R01_RandomVariables.html#continuous-uniform",
    "href": "R01_RandomVariables.html#continuous-uniform",
    "title": "4  Random Variables R Examples",
    "section": "4.6 Continuous Uniform",
    "text": "4.6 Continuous Uniform\nThe backbone of random variable generation - for example a random decimal between 0 and 1\n\n# Parameters\na &lt;- 0 #lower bound\nb &lt;- 1 #upper bound\n\n#generate 10 random values\nrunif(10, min=a, max=b)\n\n [1] 0.3491863 0.7421214 0.1316501 0.5038041 0.2094663 0.9037216 0.2234822\n [8] 0.3452239 0.4463721 0.8647587\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(a,b, length.out=100)\n\nplot(x, dunif(x, a, b), type=\"l\", main=\"PDF of Unif(0,1)\", ylab=\"density\", ylim=c(0,1))\nplot(x, punif(x, a, b), type=\"l\", main=\"CDF of Unif(0,1)\", ylab=\"F(x)\")\n\n\n\n\nThe expected value is \\(\\frac{a+b}{2}\\) and the variance is \\(\\frac{(b-a)^2}{12}\\)\n\nunif.sim &lt;- runif(10000, min=a, max=b)\n\n# Expectation\n(a+b)/2\n\n[1] 0.5\n\n# mean from sample\nmean(unif.sim)\n\n[1] 0.4970099\n\n#variance\n(b-a)^2/12\n\n[1] 0.08333333\n\n#variance from sample\nvar(unif.sim)\n\n[1] 0.08397942"
  },
  {
    "objectID": "R01_RandomVariables.html#normal-gaussian",
    "href": "R01_RandomVariables.html#normal-gaussian",
    "title": "4  Random Variables R Examples",
    "section": "4.7 Normal / Gaussian",
    "text": "4.7 Normal / Gaussian\nMany things in the world are normally distributed - useful for modeling when the distribution is symmetric and probability is densest in the middle with decreasing tails.\n\n#Parameters\nmu &lt;- 5 #the mean/location of the distribution\nsigma2 &lt;- 4 #the variance is in squared units!!\nsigma &lt;- sqrt(sigma2) #sigma is the standard deviation, not the variance\n\n#generate 10 random values\nrnorm(10, mean=mu, sd=sigma)\n\n [1] 7.025694 3.265699 6.840293 6.140499 5.576147 7.260201 6.228823 5.524191\n [9] 4.620259 3.816703\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(mu-3*sigma, mu+3*sigma, length.out=100)\n\nplot(x, dnorm(x, mu, sigma), type=\"l\", main=\"PDF of Normal(5, 2^2)\", ylab=\"density\")\nplot(x, pnorm(x, mu, sigma), type=\"l\", main=\"CDF of Normal(5, 2^2)\", ylab=\"F(x)\")\n\n\n\n\nApproximate the expected value numerically\n\\[\\int_{-\\infty}^\\infty t f(t) dt\\]\n\nmu &lt;- 15\nsigma &lt;- 4\nt &lt;- seq(mu-4*sigma, mu+4*sigma, length.out=1000)\nd &lt;- dnorm(t, mean=mu, sd=sigma)\n\nw &lt;- t[2]-t[1]\n\nhead(t)\n\n[1] -1.0000000 -0.9679680 -0.9359359 -0.9039039 -0.8718719 -0.8398398\n\nhead(d)\n\n[1] 3.345756e-05 3.454551e-05 3.566656e-05 3.682162e-05 3.801165e-05\n[6] 3.923763e-05\n\n#Expected value\nsum( t * (d*w))\n\n[1] 14.99907\n\n\nThe expected value is \\(\\mu\\) and the variance is \\(\\sigma^2\\)\n\nnormal.sim &lt;- rnorm(10000, mean=mu, sd=sigma)\n\n# Expectation\nmu\n\n[1] 15\n\n# mean from sample\nmean(normal.sim)\n\n[1] 15.01193\n\n#variance\nsigma2\n\n[1] 4\n\n#variance from sample\nvar(normal.sim)\n\n[1] 16.19469\n\n\n\n4.7.1 Simulate a Normal probability\n\nnormal.sim &lt;- rnorm(1000000)\n\nx &lt;- .25 &lt;= normal.sim & normal.sim &lt;=.75\n\nsum(x)/1000000\n\n[1] 0.175348\n\npnorm(.75)-pnorm(.25)\n\n[1] 0.1746663\n\n\nLet’s consider a geometric that counts the number of days before a electronic component malfunctions Suppose that every day there’s a 25% chance of malfunction.\n\np=.25\nrandom.geom &lt;- rgeom(10000, prob=p)\nhist(random.geom, breaks=25)\n\n\n\n\nWhat if more than one malfunction could happen per day? Like we replace the part immediately when it malfunctions. We could divide the day into \\(N\\) parts and use a geometric for each part of the day. If \\(X\\)=k, then the proportion of the day it took to break is \\(k/N\\) Let’s start with \\(N\\)=2 and go up from there\n\npar (mfrow=c(2,2))\nN &lt;- 2\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.5305\n\nN &lt;- 6\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.775683\n\nN &lt;- 24\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.9357\n\nN &lt;- 100\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\n\n\n\nmean(geom.sim)\n\n[1] 4.048527"
  },
  {
    "objectID": "R01_RandomVariables.html#simulate-a-normal-probability",
    "href": "R01_RandomVariables.html#simulate-a-normal-probability",
    "title": "4  Random Variables R Examples",
    "section": "4.8 Simulate a Normal probability",
    "text": "4.8 Simulate a Normal probability\n\nnormal.sim &lt;- rnorm(1000000)\n\nx &lt;- .25 &lt;= normal.sim & normal.sim &lt;=.75\n\nsum(x)/1000000\n\n[1] 0.17541\n\npnorm(.75)-pnorm(.25)\n\n[1] 0.1746663\n\n\nLet’s consider a geometric that counts the number of days before a electronic component malfunctions Suppose that every day there’s a 25% chance of malfunction.\n\np=.25\nrandom.geom &lt;- rgeom(10000, prob=p)\nhist(random.geom, breaks=25)\n\n\n\n\nWhat if more than one malfunction could happen per day? Like we replace the part immediately when it malfunctions. We could divide the day into \\(N\\) parts and use a geometric for each part of the day. If \\(X\\)=k, then the proportion of the day it took to break is \\(k/N\\) Let’s start with \\(N\\)=2 and go up from there\n\npar (mfrow=c(2,2))\nN &lt;- 2\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.5163\n\nN &lt;- 6\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.840067\n\nN &lt;- 24\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\nmean(geom.sim)\n\n[1] 3.928362\n\nN &lt;- 100\ngeom.sim &lt;- rgeom(10000, p/N) / N\nhist(geom.sim, breaks=100)\n\n\n\nmean(geom.sim)\n\n[1] 4.014897"
  },
  {
    "objectID": "R01_RandomVariables.html#exponential",
    "href": "R01_RandomVariables.html#exponential",
    "title": "4  Random Variables R Examples",
    "section": "4.8 Exponential",
    "text": "4.8 Exponential\nFor modeling waiting times - the length of time until an event occurs. It’s a continuous version of a geometric, if you start looking at smaller and smaller time units (e.g. days -&gt; hours -&gt; minutes -&gt; seconds)\n\n#Parameters\nl &lt;- 3 #The rate parameter, the average number of occurrences per unit time\n\n#Generate 10 random values\nrexp(10, rate=l)\n\n [1] 0.14532250 0.34245138 0.83001118 0.05330771 0.16092295 0.09114442\n [7] 0.12318770 0.17958063 0.19486000 0.23542797\n\n\nProbability density function and cumulative distribution function\n\npar(mfrow=c(1,2))\nx &lt;- seq(0, 6/l, length.out=100)\n\nplot(x, dexp(x, l), type=\"l\", main=\"PDF of Exp(3)\", ylab=\"density\")\nplot(x, pexp(x, l), type=\"l\", main=\"CDF of Exp(3)\", ylab=\"F(x)\")\n\n\n\n\nThe expected value is \\(\\dfrac{1}{\\lambda}\\) and the variance is \\(\\frac{1}{\\lambda^2}\\)\n\nexp.sim &lt;- rexp(10000, l)\n\n# Expectation\n1/l\n\n[1] 0.3333333\n\n# mean from sample\nmean(exp.sim)\n\n[1] 0.3294719\n\n#variance\n1/l^2\n\n[1] 0.1111111\n\n#variance from sample\nvar(exp.sim)\n\n[1] 0.1065475\n\n\n\n4.8.1 Relationship between an exponential and a Poisson\nlambda = 10\n\nsimulated.exp &lt;- rexp(10000, rate=10)\nhist(simulated.exp)\n\n\n\nhead(simulated.exp)\n\n[1] 0.12408529 0.09626517 0.34601610 0.15449199 0.27343656 0.01666181\n\nconverted.poisson &lt;- as.vector(table(floor(cumsum(simulated.exp))))\npar(mfrow=c(1,2))\n    hist(converted.poisson)\n    hist(rpois(500, lambda=10))"
  },
  {
    "objectID": "R01_RandomVariables.html#relationship-between-an-exponential-and-a-poisson",
    "href": "R01_RandomVariables.html#relationship-between-an-exponential-and-a-poisson",
    "title": "4  Random Variables R Examples",
    "section": "4.10 Relationship between an exponential and a Poisson",
    "text": "4.10 Relationship between an exponential and a Poisson\nlambda = 10\n\nsimulated.exp &lt;- rexp(10000, rate=10)\nhist(simulated.exp)\n\n\n\nhead(simulated.exp)\n\n[1] 0.11366027 0.05467349 0.05377255 0.06814024 0.09737352 0.06935747\n\nconverted.poisson &lt;- as.vector(table(floor(cumsum(simulated.exp))))\npar(mfrow=c(1,2))\n    hist(converted.poisson)\n    hist(rpois(500, lambda=10))"
  },
  {
    "objectID": "R01_RandomVariables.html#simulate-stock-prices",
    "href": "R01_RandomVariables.html#simulate-stock-prices",
    "title": "4  Random Variables R Examples",
    "section": "4.9 Simulate stock prices",
    "text": "4.9 Simulate stock prices\nHere’s an example of how you could combine normally distributed random variables for a model of daily stock prices. This is an example of a random walk.\n\ndeltas &lt;- rnorm(30)\nX &lt;- 50 + c(0,cumsum(deltas))\n\nplot(x=1:31, y=X, type=\"l\")"
  },
  {
    "objectID": "R02_IndepCondBayes.html#example",
    "href": "R02_IndepCondBayes.html#example",
    "title": "7  Independence, Conditional Probability and Bayes Theorem R Examples",
    "section": "7.1 Example",
    "text": "7.1 Example\nTwo random variables that are Not independent, but have a 0 covariance\n\nX = seq(-1,1,.001)\nY = X^2\nplot(X,Y)\nabline(h=mean(Y))\nabline(v=mean(X))\n\n\n\ncov(X,Y)\n\n[1] 1.39546e-17\n\ncor(X,Y)\n\n[1] 8.090705e-17\n\n\n\n#A estimate of the pdf of Y\nplot(density(Y), xlim=c(0,1))\n\n\n\nhist(Y)"
  },
  {
    "objectID": "R02_IndepCondBayes.html#two-normally-distributed-random-variables",
    "href": "R02_IndepCondBayes.html#two-normally-distributed-random-variables",
    "title": "7  Independence, Conditional Probability and Bayes Theorem R Examples",
    "section": "7.2 Two normally distributed random variables",
    "text": "7.2 Two normally distributed random variables\n\n# X1 ~ Normal(1,1^2)\nx1 &lt;- rnorm(10000, mean=1, sd=1)\n\n#X2 ~ Normal(2, 2^2)\nx2 &lt;- rnorm(10000, mean=2, sd=2)\n\npar(mfrow=c(1,2))\nhist(x1)\nhist(x2)\n\n\n\n\nLet’s check the variances individually\n\nvar(x1)\n\n[1] 1.021324\n\nvar(x2)\n\n[1] 3.978341\n\n\nIn theory, Var(X1+X2) = Var(X1) + Var(X2) We check with sample variance\n\nvar(x1+x2)\n\n[1] 5.04692\n\nvar(x1)+var(x2)\n\n[1] 4.999665\n\nvar(x1)+var(x2)+2*cov(x1,x2)\n\n[1] 5.04692\n\n\nLook at the plot of these two variables\n\nmean(x1+x2)\n\n[1] 3.015705\n\nmean(x1)+mean(x2)\n\n[1] 3.015705\n\nplot(x1,x2)\n\n\n\nhist(x1+x2, breaks=50)"
  },
  {
    "objectID": "R02_IndepCondBayes.html#uniform-uniform",
    "href": "R02_IndepCondBayes.html#uniform-uniform",
    "title": "7  Independence, Conditional Probability and Bayes Theorem R Examples",
    "section": "7.3 Uniform + uniform",
    "text": "7.3 Uniform + uniform\n\nX &lt;- runif(100000, 0, 1)\nhist(X)\n\n\n\nY &lt;- runif(100000, 0, 1)\nhist(Y)\n\n\n\nhist(X+Y)\n\n\n\nZ &lt;- runif(100000, 0, 1)\nhist(X+Y+Z, breaks=100)"
  },
  {
    "objectID": "R02_IndepCondBayes.html#conditional-probability",
    "href": "R02_IndepCondBayes.html#conditional-probability",
    "title": "7  Independence, Conditional Probability and Bayes Theorem R Examples",
    "section": "7.4 Conditional Probability",
    "text": "7.4 Conditional Probability\nExample: X: roll a 6 sided die Y: flip a coin x times, count the number of heads\nWe could ask the question: What is the covariance ?!?!?\nSuppose we were to simulate doing this thing many many times. Each outcome from the simulation is representing one equally likely outcome, right?\nThis is the idea of Monte Carlo sampling - which we’ll look at next week\n\nX &lt;- sample(6, size=10000, replace=TRUE)\nY &lt;- rbinom(10000, size=X, prob=.5)\ncor(X,Y)\n\n[1] 0.6769082\n\nplot(jitter(Y)~jitter(X), col=rgb(0,0,0,.01), pch=16)"
  },
  {
    "objectID": "R02_IndepCondBayes.html#disease-screening",
    "href": "R02_IndepCondBayes.html#disease-screening",
    "title": "7  Independence, Conditional Probability and Bayes Theorem R Examples",
    "section": "7.5 Disease Screening",
    "text": "7.5 Disease Screening\nSay that the flu is currently infecting 3% of the population (prevelance). A person can take a flu test that has a sensitivity of 99% (i.e. if they have the virus, there is a 99% chance the test will give a positive result) and a specificity of 95% (i.e. if they don’t have the virus, there’s a 95% chance the test gives them a negative result).\nThe sensitivity is also known as the True positive rate (TPR). The complement of specificity is the False Positive Rate (FPR), and in this case it is 1-.95=.05 or 5%.\nSo a person takes the test and gets a positive test result, what is the probability that they actually have the flu?\n\np.flu &lt;- .03; p.noflu &lt;- 1-p.flu\np.pos.given.flu &lt;- .99; p.neg.given.flu &lt;- 1-p.pos.given.flu\np.neg.given.noflu &lt;- .95; p.pos.given.noflu &lt;- 1-p.neg.given.noflu\n\n# P(flu | pos) = P(flu)*P(pos|flu)/ [P(flu)*P(pos|flu) + P(noflu)*P(pos|noflu) ]\np.flu*p.pos.given.flu / (p.flu*p.pos.given.flu + p.noflu*p.pos.given.noflu)\n\n[1] 0.3797954\n\n\nSurprising? Well, if we didn’t do the test we’d guess a 5% chance of flu. Now that the test results are in that estimation increases by more than 7x to about 38%. Why isn’t it higher? There is a high chance of false positives muddying the waters."
  },
  {
    "objectID": "mc.html#learning-objectives",
    "href": "mc.html#learning-objectives",
    "title": "8  Monte Carlo",
    "section": "8.1 Learning objectives",
    "text": "8.1 Learning objectives\nAfter this lesson, you will be able to\n\nExplain the basic idea behind Monte Carlo methods\nImplement simple Monte Carlo methods in R to estimate probabilities of random events\nImplement simple Monte Carlo methods in R to estimate expectations \\(\\mathbb{E} g(X)\\) for random variables \\(X\\) distributed according to “nice” distributions (i.e., distributions supported by R)\nExplain and implement the “inverse trick” to generate random variables from an arbitrary cumulative distribution function \\(F\\)."
  },
  {
    "objectID": "mc.html#origin-of-monte-carlo-and-a-nice-example",
    "href": "mc.html#origin-of-monte-carlo-and-a-nice-example",
    "title": "8  Monte Carlo",
    "section": "8.2 Origin of Monte Carlo (and a nice example!)",
    "text": "8.2 Origin of Monte Carlo (and a nice example!)\nStanisław Ulam was a famous nuclear physicist (fun fact: after fleeing Europe during WWII, Ulam was a professor at UW-Madison before he was recruited to work on the Manhattan project at Los Alamos).\nSupposedly, while recovering from a surgery, Ulam was playing a lot of solitaire and came to wonder what was the probability \\(p\\) that a randomly-dealt game of solitaire could be played out.\nHe quickly found that calculating this probability precisely (e.g., by doing a bunch of the algebra and combinatorics that you may remember less than fondly from a probability course) was not very easy. The idea occurred to him that even if the probability \\(p\\) could not be calculated directly, it could be estimated by dealing lots of games of solitaire and counting how many of them he won.\nThus, Monte Carlo methods were born.\nThe name “Monte Carlo” is a reference to the Monte Carlo casino, later a frequent setting of James Bond movies. The idea is that like at a casino, Monte Carlo methods involve lots of randomness."
  },
  {
    "objectID": "mc.html#basics-of-monte-carlo",
    "href": "mc.html#basics-of-monte-carlo",
    "title": "8  Monte Carlo",
    "section": "8.3 Basics of Monte Carlo",
    "text": "8.3 Basics of Monte Carlo\nThe basic idea behind Monte Carlo methods, then, is the following: suppose that we have an event \\(E = \\{ X \\in S \\}\\) where \\(S \\subset \\Omega\\) is some set of outcomes and \\(X\\) is a random variable with outcome set \\(\\Omega\\). Suppose that we want to know the probability of event \\(E\\), \\(\\Pr[E]\\).\nOf course, one option would be to draw on the ideas we saw last week. We could do an integral or sum (depending on whether \\(X\\) is continuous or discrete) and just calculate what this probability is.\nUnfortunately, often this integral or sum is very hard to do, either because the integral/sum is very hard to solve (either exactly or numerically). When this happens, we need to take a different approach.\nThat is where Monte Carlo comes in. We give up on doing an integral or sum, and instead just use the fact that we can generate lots of random variables.\nTo (approximately) compute \\(\\Pr[ E ]\\), we\n\ngenerate lots of replicates of \\(X\\), say \\(X_1,X_2,\\dots,X_M\\) for some number \\(M\\) of Monte Carlo replicates.\nCount how many of these replicates correspond to event \\(E\\) occurring. That is, how many \\(i\\) are there such that \\(X_i \\in S\\).\nEstimate \\(\\Pr[ E]\\) as \\(M^{-1} \\sum_{i=1}^M 1_{\\{X_i \\in S\\}}\\).\n\nNote: this \\(1_{ \\{ \\cdots \\} }\\) notation is called an indicator function. For an event \\(E\\), \\[\n1_{E} = \\begin{cases}\n  1 &\\mbox{ if } E~\\text{ occurs } \\\\\n  0 &\\mbox{ otherwise. }\n  \\end{cases}\n\\]\nSo, in other words, to estimate \\(\\Pr[ E ]\\), Monte Carlo says that we just run our experiment of interest a bunch of times and count what proportion of the time the event \\(E\\) happens.\n\n8.3.1 Example: events under the normal\nLet’s start by considering a calculation similar to one that we already did in a previous lecture. Let \\(X\\) be a normal random variable with mean \\(1\\) and variance \\(3\\). What is the probability that \\(0 \\le X \\le 3\\)?\nWell, we already know how to compute this. It’s given by the integral \\[\n\\Pr[ 0 \\le X \\le 3]\n=\n\\int_0^3 \\frac{1}{\\sqrt{2\\pi * 3}} \\exp\\left\\{ \\frac{ -(t-1)^2 }{ 2* 3}  \\right\\} d t\n\\]\nwhere we have taken the normal density and plugged in \\(\\mu = 1, \\sigma^2=3\\).\nNote that this integral actually doesn’t have a closed form– your calculus classes might have tricked you into thinking that most integrals are “nice”. Still, we can use pnorm to compute it numerically (i.e., approximately), and we find that this probability is\n\npnorm(3, mean=1, sd=sqrt(3)) - pnorm(0, mean=1, sd=sqrt(3))\n\n[1] 0.594042\n\n\nNote that we have specified the standard deviation to be \\(\\sqrt{3}\\)– the variance is \\(\\sigma^2 = 3\\), so standard deviation is \\(\\sigma = \\sqrt{3}\\).\nLet’s suppose, however, that we didn’t know this probability and that it wasn’t so easy to compute in R. Note that Monte Carlo is most useful when we don’t have things like pnorm at our disposal, so this is a setting where in real life we wouldn’t ever use Monte Carlo. We’re starting in this nice simple familiar setting as a warm-up.\nOur event of interest is \\(E = \\{ 0 \\le X \\le 3 \\}\\). Monte Carlo says that to estimate \\(\\Pr[ E ]\\), we repeat our experiment lots of times and count what fraction of the time the event \\(E\\) happens.\nSo we should generate lots of copies of \\(X \\sim \\mathcal{N}(\\mu=1,\\sigma^2=3)\\) and count how often \\(0 \\le X \\le 3\\). Let’s do just that.\n\n# Write a function that checks whether or not our event E happens\n# The argument x is the output of our experiment.\n# (i.e., a draw from the normal)\nevent_E_happened &lt;- function( x ) {\n  if( 0 &lt;= x & x &lt;= 3 ) {\n    return( TRUE ) # The event happened\n  } else {\n    return( FALSE ) # The event DIDN'T happen\n  }\n}\n\n# Now MC says that we should generate lots of copies of X...\nNMC &lt;- 1000; # 1000 seems like \"a lot\".\nresults &lt;- rep( 0, NMC ); # We're going to record outcomes here.\nfor( i in 1:NMC) {\n  # Generate a draw from the normal, and then...\n  X &lt;- rnorm( 1, mean=1, sd=sqrt(3) );\n  # ...record whether or not our event of interest happened.\n  results[i] &lt;- event_E_happened(X);\n}\n# Now, compute what fraction of our trials were \"successes\" (i.e., E happened)\nsum( results )/NMC\n\n[1] 0.602\n\n\nJust as a reminder, the true number, which we just happen to know in this case, is\n\npnorm( 3, mean=1, sd=sqrt(3) ) - pnorm( 0, mean=1, sd=sqrt(3) )\n\n[1] 0.594042\n\n\nNow, our estimate is random, so every time we recompile this document it will be a bit different, but the estimate should be between 0.56 and 0.63.\nWe can make the estimate still more accurate by increasing the number of MC replicates \\(M\\)– larger \\(M\\) will make the estimation error smaller, on average. We’ll be able to make that description more precise later.\n\n\n8.3.2 Example: the birthday problem\nHere’s a classic probability question: In a group of \\(n\\) people, what is the probability that two or more of them have the same birthday? For simplicity, we’ll assume that there are no leap days (i.e., the year has 365 days) and that all 365 days are equally likely birthdays. You’ll implement a solution to a more general version of this problem in your homework, but let’s get an intuition here.\nWe need to simulate assigning \\(n\\) people to birthdays. We will represent the days of the year by the numbers 1 through 365, so assigning a birthday just amounts to choosing one of these 365 numbers at random. Then we need to check whether or not there are any repeated birthdays.\n\ngen_birthdays &lt;- function( n ) {\n  # Generate n random birthdays.\n  \n  # Generating a random birthday just corresponds to choosing a random number\n  # between 1 and 365 inclusive.\n  \n  # Pick uniformly at random from {1,2,3,...,365}.\n  # We sample WITH replacement,\n  # because it is possible for a birthday to be repeated.\n  # See ?sample for details\n  return( sample( 365, size=n, replace=TRUE ) );\n}\n\n# One run of our experiment will consist of generating n birthdays,\n# and then checking whether or not the birthdays contain a repeat.\n# Return TRUE if more than one person shares a birthday,\n# FALSE otherwise.\nrun_bday_expt &lt;- function( n ) {\n  # Generate n birthdays\n  bdays &lt;- gen_birthdays( n );\n  \n  # Now, check if there is a repeat.\n  # There are lots of ways to do this in R.\n  # Let's use the table function, which takes a vector and returns a\n  # vector of counts.   # See ?table for details\n  \n  # The important thing for us is that:\n  # If every entry of bdays appears exactly once,\n  # then table(bdays) will just be a vector of ones.\n  # If one or more entries in the vector bdays is repeated,\n  # then table(bdays) will have one or more entries that are larger than 1.\n  \n  # So we can examine max( table(bdays) ).\n  # max( table(bdays)) &gt; 1 if and only if there is a repeated entry in bdays.\n  return( max(table(bdays)) &gt; 1 );\n}\n\nNow let’s pick a value for the number of people \\(n\\) and repeat our experiment a bunch of times, keeping track of how often it returns TRUE (i.e., there is a repeated birthday).\n\nn &lt;- 22;\nNMC &lt;- 1000;\nNTRUE &lt;- 0;\nfor( i in 1:NMC) {\n  if( run_bday_expt(n) ) {\n    NTRUE &lt;- NTRUE + 1;\n  }\n}\npbday_estimated &lt;- NTRUE/NMC\npbday_estimated\n\n[1] 0.496\n\n\nOnce again, this is a situation where we can actually compute an answer exactly– we are using this to build intuition, not because it is a problem that strictly speaking requires Monte Carlo methods.\nThe probability that one or more birthdays is repeated is one minus the probability that all \\(n\\) birthdays are distinct. \\[\np_{\\text{repeated}} = 1 - p_{\\text{distinct}}.\n\\]\nThe probability that all \\(n\\) birthdays are distinct is\n\\[\np_{\\text{distinct}}\n= \\frac{365}{365}*\\frac{364}{365} * \\frac{ 363 }{ 365 } * \\cdots * \\frac{ 365-n+1}{365}\n= \\frac{ 365! }{ (365-n)! 365^n }.\n\\]\n\n\n8.3.3 Aside: wait, what?\nIf you’ve taken a probability class before, this computation will be simple and familiar. If you haven’t taken a probability class before, this warrants some unpacking.\nI’ll do that unpacking here in these notes, but we won’t go into detail about this in lecture, owing to time constraints.\nWe want to compute the probability that all \\(n\\) birthdays are distinct. Let’s label these people \\(1,2,3,\\dots,n\\), and consider generating their birthdays in order.\nThe way we usually compute probabilities in problems like this is to count how many different outcomes there are, and then look at how many of those outcomes match our event of interest. That is, we compute a probability as \\[\nN_{\\text{event is true}} / N_{\\text{outcomes}} ,\n\\]\nwhere \\(N_{\\text{outcomes}}\\) is the total number of possible outcomes and \\(N_{\\text{event is true}}\\) is the number of possible outcomes where our event of interest is true.\nIn this case, the outcomes are assignments of \\(n\\) people to 365 birthdays. Each of those \\(n\\) people can be assigned to one of 365 birthdays, so \\[\nN_{\\text{outcomes}} = 365^n.\n\\]\nNow, to compute the numerator in our probability, let’s count how many ways we can assign \\(n\\) people to \\(n\\) unique birthdays out of 365 available birthdays.\nThe first birthday is unique no matter what: for any of the 365 birthdays we pick, there are no other birthdays for it to “duplicate”.\nNow, let’s consider the 2nd birthday. The first person’s birthday is off limits, so that leaves 364 remaining birthdays to choose from, so there are \\(365*364\\) ways to choose distinct birthdays for the first two people.\nNow, consider the third person. If the first and second person have distinct birthdays, there are \\(365-2 = 363\\) birthdays left to choose from. There are \\(365*364\\) ways to pick distinct birthdays for the first two people, and so there are \\(365*364*363\\) ways to choose distinct birthdays for the first three people.\nContinuing this argument up to the \\(n\\)-th person, if the first \\(n-1\\) people all have distinct birthdays, then there are \\(365-(n-1) = 365 - n + 1\\) birthdays remaining for the \\(n\\)-th person to choose from that would maintain the distinct birthdays property. So there are \\(365*364*363*\\cdots*(365-n+1)\\) ways to choose distinct birthdays for all \\(n\\) people.\nOf course, this argument only makes sense if \\(1 \\le n \\le 365\\), an assumption we were making implicitly, but worth making explicit here.\nSo, plugging this back into our numerator, we have determined that the probability that our \\(n\\) people are assigned distinct birthdays is\n\\[\n\\frac{365*364*363*\\cdots*(365-n+1) }{ 365^n}\n=\n\\frac{ 365! }{ (365-n)! } \\frac{1}{365^n },\n\\]\nwhere we have used the fact that \\(m*(m-1)*\\cdots*(m-k+1) = m!/(m-k)!\\) for non-negative integers \\(k\\) and \\(m\\) with \\(k \\le m\\).\n\n\n8.3.4 End of aside. what’s the punchline?\nThus, the probability of a repeated birthday is\n\\[\np_{\\text{repeated}} = 1 - p_{\\text{distinct}}\n= \\frac{ (365-n)! 365^n - 365! }{ (365-n)! 365^n }.\n\\] Let’s implement that as a function and get the true values to verify our MC estimates.\n\np_repeated &lt;- function( n ) {\n  # Compute p_distinct, first, then subtract it from 1.\n  numer &lt;- prod( seq(365,365-n+1) );\n  denom &lt;- 365**n;\n  p_distinct &lt;- numer/denom;\n  return( 1 - p_distinct);\n}\n\np_repeated( 23 )\n\n[1] 0.5072972\n\n\n\np_repeated( 22 )\n\n[1] 0.4756953\n\n\nJust as a reminder, our estimate for p_repeated( 22 ) was\n\npbday_estimated\n\n[1] 0.496\n\n\nNot bad!\n\n\n8.3.5 Why does Monte Carlo work?\nOkay, before we start looking at more complicated or more interesting examples of Monte Carlo, we have to ask… why does this work, anyway? We’re estimating a number \\(\\Pr[E]\\) by just repeating an experiment a bunch of times and recording the fraction of the time that our event \\(E\\) happened…\nWell, in one sense, the fact that this gets us a good estimate of \\(\\Pr[E]\\) should be “obvious”. Our “definition” of a probability is as a long-run average– the probability of an event is the proportion of the time we would expect that event to happen if we repeated the experiment many times. So it comes as no surprise that when we run our experiment a bunch of times and average out how often our event happens, well… we get our probability!\nOkay, but that explanation feels a bit circular… To answer the question more carefully, we need to use just a bit of math. I promise it’s just a bit, and we’ll go through it step by step. We won’t even have to compute any integrals– just look at them!\nTo start with, let’s imagine that our variable of interest \\(X\\) is continuous with density function \\(f_X(t)\\). The same argument will apply for discrete RVs; just change integrals to sums. Then with \\(E = \\{ X \\in S \\}\\) for any set \\(S \\subseteq \\Omega\\), \\[\n\\Pr[ E ] = \\int_S f_X(t) dt = \\int_\\Omega 1_{t \\in S} ~f_X(t) dt\n\\]\nThe first equality there is just our definition of probability– to get the probability of an event, we integrate a density over the event set.\nThe second equality is just rewriting the integral. Instead of only integrating over \\(S\\), we integrate over the whole set \\(\\Omega\\) and add in an indicator \\(1_{t \\in S}\\). Remember, this function is \\(1\\) when \\(t \\in S\\) and \\(0\\) otherwise.\nBut now, let’s recall the definition of expectation. For a function \\(g(X)\\), \\[\n\\mathbb{E} g(X) = \\int_\\Omega g(t) f_X(t) dt\n\\]\nWe say that we have “integrated \\(g\\) against the density \\(f_X\\)”. Plugging in \\(g(t) = 1_{t \\in S}\\), \\[\n\\mathbb{E} 1_{X \\in S} = \\int_\\Omega 1_{t \\in S} ~f_X(t) dt.\n\\]\nBut we showed a few equations ago that this integral on the right-hand side is \\(\\Pr[ E ]\\).\nSo we have shown that \\(\\Pr[E] = \\mathbb{E} 1_{X \\in S}\\).\nOkay, we’re almost there. We have shown that the probability we want to estimate, \\(\\Pr[ E ]\\), is really just equal to an expectation, \\(\\mathbb{E} 1_{X \\in S}\\).\nNow, let’s think back to the law of large numbers. If we generate lots of independent copies of a random variable \\(X\\), say \\(X_1,X_2,\\dots,X_M\\), and look at the average \\(M^{-1} \\sum_{i=1}^M g(X_i)\\), then for large \\(M\\), this average is close to the expectation \\(\\mathbb{E} g(X)\\): \\[\n\\frac{1}{M} \\sum_{i=1}^M g(X_i) \\approx \\mathbb{E} g(X)\n\\] Let’s again take \\(g(t) = 1_{t \\in S}\\). Then \\[\n\\frac{1}{M} \\sum_{i=1}^M 1_{X_i \\in S} \\approx  \\mathbb{E} 1_{X \\in S} = \\Pr[ E ].\n\\]\nMonte Carlo works for estimating probabilities because probabilities are just expectations (that’s from the calculus we did above), and sample averages are close to their expectations (by the law of large numbers)!\n\n\n8.3.6 Example: Buffon’s needle\nLet’s look at a slightly more interesting example, based on a question first asked by Georges-Louis Leclerc, Comte de Buffon in the 1700s:\n\nSuppose we have a floor made of parallel strips of wood, each of width 1, and we drop a needle of length 1 onto the floor. What is the probability that the needle will lie across a line between two strips?\n\nNow, even beginning to answer this question requires that we specify what we have in mind when we say that we drop a needle onto the floor. That is, we need to specify our probabilistic/statistical model of the experiment of dropping a needle on the floor.\nPresumably we mean that the needle lands at a random point on the floor and that its orientation (i.e., angle that the needle makes with the cracks between the floor boards) is uniformly random.\nThese are the kinds of assumptions that we always want to be aware of when we build a model of the world, and these kinds of specifications can make a big difference in terms of the answers that we get. A nice example of this is Bertrand’s paradox, presented beautifully in this YouTube video by the always wonderful Numberphile and 3blue1brown.\nSo let’s specify what we mean, here.\nLet’s consider a 2-by-2 square with corners at \\((\\pm1,\\pm1)\\) and divide it into 2 halves: the top half is one strip of wood, and the bottom half is another strip of wood.\n\nrequire(ggplot2)\npp &lt;- ggplot() + geom_rect(aes(xmin=-1,xmax=1,ymin=0,ymax=1),\n                           fill=\"grey30\", color=NA);\npp &lt;- pp + geom_rect(aes(xmin=-1,xmax=1,ymin=-1,ymax=0),\n                     fill=\"grey80\",color=NA);\npp &lt;- pp + scale_x_continuous(expand=c(0,0)) + scale_y_continuous(expand=c(0,0))\npp\n\n\n\n\nTo model our randomly dropped pin, we first sample a point in this square uniformly at random. We will imagine that the pin is just a line, and this sampled point will be the location that the middle of the pin lands on. Then we randomly pick an angle uniformly over \\([0,\\pi)\\) and extend the ends of the needle each 0.5 units of length, for a needle of length \\(1\\). Finally, we check if the needle crosses any of the lines \\(y=0,\\pm1\\).\nLet’s write that function and plot a few random needles to make sure it’s doing what we want.\n\n# define needle dropping function\nneedle = function(){\n  \n  # Randomly pick the location of the center of the needle.\n  # We need an x and y coordinate, hence n=2 in runif\n  # This means that xy is a length-two vector,\n  # kind of like we wrote c( xcoord, ycoord ).\n  # This will come up below when we compute the endpoints of our\n  # needle, p1 and p2, below.\n  xy = runif(n=2, min=-1, max=1)\n  \n  # randomly pick an angle uniformly between 0 and pi.\n  angle = runif(n=1, min=0, max=pi)\n  \n  # calculate delta x and delta y, the distance of the ends of\n  # the needle from center in the horizontal and vertical directions.\n  # We are defining the angle using the standard definition\n  # (going counterclockwise from positive x axis).\n  # We will use these numbers to compute the coordinates of the needle's\n  # endpoints below, \n  dx = 0.5*cos(angle);\n  dy = 0.5*sin(angle);\n  \n  # Calculate coordinates of the needle's end points\n  # Our dy is always positive, so p1 is always higher than p2.\n  # This will be useful later for checking if our needle crosses a line\n  p1 = setNames(xy + c(dx,dy), c(\"p1x\",\"p1y\"))\n  p2 = setNames(xy - c(dx,dy), c(\"p2x\",\"p2y\"))\n  # Note: setNames is explained where we use it below\n  # in the function `crosses()` in the next code block.\n  \n  # return endpoints\n  return(c(p1,p2))\n}\n\n# plot a few needle drops\nlibrary(plyr)\n\nWarning: package 'plyr' was built under R version 4.2.3\n\nndl = function(i) needle()\n# needle() takes no arguments, but we want to call it 10 times,\n# so ldply needs a dummy variable,\n# which we've called (totally arbitrarily) i.\nneedles = ldply(1:10,ndl);\n\n# First plot out board again.\npp &lt;-ggplot() + geom_rect(aes(xmin=-1,xmax=1,ymin=0,ymax=1),\n                          fill=\"grey30\",color=NA);\npp &lt;- pp + geom_rect(aes(xmin=-1,xmax=1,ymin=-1,ymax=0),\n                     fill=\"grey80\",color=NA);\n# Now, add in the needles, in red.\npp &lt;- pp + geom_segment(data=needles,\n                        mapping=aes(x=p1x,y=p1y,xend=p2x,yend=p2y),\n                        color=\"red\",size=1.5);\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\nNow, Monte Carlo says that we need to repeat this experiment many times (i.e., many more than 10 needles) and count how many of our dropped needles cross a line. So first and foremost we need a function to check if a needle crosses a line.\n\ncrosses &lt;- function( pts ) {\n  # Remember that needle() returns a pair of (x,y) points, p1,p2.\n  # We only need the y-coordinates to check if they cross a line.\n  p1y = pts['p1y'] # Note: this indexing trick is why we used setNames above\n  p2y = pts['p2y']\n  \n  # check if p1 and p2 are on opposite sides of a line\n  # We do this by simply checking if the y-coordinates are on different sides\n  # of one of the three lines y=-1, y=0 or y=1.\n  if( (p1y &gt; -1 & p2y &lt; -1) || (p1y &gt; 0 & p2y &lt; 0) || (p1y &gt; 1 & p2y &lt; 1) ){\n    return( TRUE );\n  } else {\n    return( FALSE );\n  }\n}\n\nGreat. So let’s generate lots of random needles, and use our new function crosses to check which ones cross a crack in the floorboards.\n\n# set size\nNMC = 1e5; # 10K MC replicates. Crank this number up for better accuracy.\n\n# create vector to count crossings\ncrossings = 0\n\n# run function\nfor(i in 1:NMC){\n  # drop a needle\n  points = needle()\n  # Check if it crosses and update counts accordingly.\n  if( crosses( points ) ) {\n    crossings &lt;- crossings + 1;\n  }\n}\n\nNow, our estimate of the probability of a crossing is just the fraction of our dropped needles (i.e., Monte Carlo replicates) that crossed.\n\ncrossings/NMC\n\n[1] 0.63686\n\n\nWe can compare with the correct answer and see how close we are. The actual probability, \\(2/\\pi\\) can be obtained using calculus (see the Wikipedia page for a derivation).\n\ntrue.cross = 2/pi\nMC.cross &lt;- crossings/NMC\ncat( sprintf(\" true value: %.5f \\n  our value: %.5f \\n Percent relative error: %.3f%%\",  true.cross, MC.cross, 100*abs(true.cross-MC.cross)/true.cross ) )\n\n true value: 0.63662 \n  our value: 0.63686 \n Percent relative error: 0.038%\n\n\nExercise: Write a function that takes 3 arguments: L for the length of the needle, T for the width of the floor boards, and M for the number of Monte Carlo replicates; performs the above Monte Carlo simulation; and returns the (estimated) probability of a crossing. Refer to the Wikipedia page on Buffon’s needle for the probability of crossing for a needle length \\(L\\) and board width \\(T\\)."
  },
  {
    "objectID": "mc.html#more-uses-for-monte-carlo-integration-by-darts",
    "href": "mc.html#more-uses-for-monte-carlo-integration-by-darts",
    "title": "8  Monte Carlo",
    "section": "8.4 More uses for Monte Carlo: integration by darts",
    "text": "8.4 More uses for Monte Carlo: integration by darts\nThe power of Monte Carlo methods goes way beyond estimating the probabilities of events. Monte Carlo refers to a much broader class of methods– using simulation to answer questions that would otherwise be hard or impossible to answer just using math (i.e., problems that do not have closed-form solutions).\nWe just saw above that we can use Monte Carlo to estimate probabilities of the form \\(\\Pr[ E ]\\) by approximating expectations of the form \\(\\mathbb{E} 1_E\\).\nWell, what is to stop us from replacing that indicator with a more interesting function \\(g(X)\\)? For example, suppose that \\(X\\) is normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\) and we want to compute \\(\\mathbb{E} \\log |X|\\). We could set up and solve the integral \\[\n\\mathbb{E} \\log |X|\n= \\int_{-\\infty}^\\infty \\left( \\log |t| \\right) f( t; \\mu, \\sigma) dt\n= \\int_{-\\infty}^\\infty \\frac{ \\log |t| }{ \\sqrt{2\\pi \\sigma^2} }\n                  \\exp\\left\\{ \\frac{ -(t-\\mu)^2 }{ 2\\sigma^2 } \\right\\} dt.\n\\]\nAlternatively, we could just draw lots of Monte Carlo replicates \\(X_1,X_2,\\dots,X_M\\) from a normal with mean \\(\\mu\\) and variance \\(\\sigma^2\\), and look at the sample mean \\(M^{-1} \\sum_{i=1}^M \\log |X_i|\\), once again appealing to the law of large numbers to ensure that this sample mean is close to its expectation.\nExercise: Do that! Write code to obtain a Monte Carlo estimate of \\(\\mathbb{E} \\log |X|\\) for \\(X \\sim \\mathcal{N}(\\mu, \\sigma^2 )\\) (choose values for \\(\\mu\\) and \\(\\sigma^2\\) as you like).\nIndeed, this idea can be pushed still further. Suppose that we want to compute an integral \\[\n\\int_D g(x) dx,\n\\]\nwhere \\(D\\) is some domain of integration and \\(g\\) is a function. Let \\(f(x)\\) be the density of some random variable with \\(f(x) &gt; 0\\) for all \\(x \\in D\\) and \\(f(x) = 0\\) for \\(x \\not \\in D\\). In other words, \\(f\\) is the density of a random variable supported on \\(D\\). Then we can rewrite the integral as \\[\n\\int_D g(x) dx = \\int_D \\frac{ g(x) }{ f(x) } f(x) dx.\n\\]\nDefining \\(h(x) = g(x)/f(x)\\), what we have really shown is that \\[\n\\int_D g(x) dx = \\mathbb{E} h(X),\n\\]\nwhere \\(X\\) is a random variable with density \\(f\\).\nSo if we want to (approximately) compute \\(\\int_D g(x) dx\\), we can just generate lots of copies of \\(X\\) with density \\(f\\), and compute \\(M^{-1} \\sum_{i=1}^M h(X_i)\\). This is why you’ll sometimes see Monte Carlo methods referred to as “integration by darts”!\n\n\n8.4.1 Example: integrate \\((x+1)^{-3}\\) from 0 to \\(\\infty\\)\n\nlibrary(ggplot2)\nf = function(x) (x+1)^-3\nggplot() + geom_function(fun=f)\n\n\n\n\n\n\n\n\nWe know from calculus \\(\\int_0^\\infty (x+1)^{-3}\\text{dx}=\\left.-\\frac1{2(x+1)^2}\\right\\vert_0^\\infty=\\frac12\\).\nSuppose we want to find this approximately using an MC approach. Let \\(g(x)=(x+1)^{-3}\\). We need a random variable defined from 0 to \\(\\infty\\). Let’s choose \\(X=\\text{Exponential}(\\lambda=1)\\) with density \\(f(x)=e^{-x}\\).\nThen we can find \\(h(x)=g(x)/f(x)=(x+1)^{-3}\\,\\cdot\\,e^x\\). Now, we just need to find \\(E(h(X))\\), which we can do by drawing a sample \\(X_i\\) using rexp(...) and finding \\(\\frac1n\\sum_{i=1}^n h(X_i)\\).\n\nn = 1e6\n\n# draw a sampe of Xi\nx = rexp(n, rate = 1)\n\n# find the sample mean of h(Xi)\nmean((x+1)^-3 / exp(-x))\n\n[1] 0.4985974\n\n\nNote that we can pick any \\(X\\) that is only defined over \\([0,\\infty)\\), as long as we know its PDF \\(f(x)\\).\nSuppose instead we had picked \\(X\\) as \\(|Z|\\), i.e. as the absolute value of a standard normal. It’s easy to see by symmetry \\(f(x) = 2\\,\\cdot\\texttt{dnorm(x,mean=0,sd=1)}\\). We can then also compute it as this:\n\nn = 1e6\n\n# draw a sample of Xi\nx = abs(rnorm(n, mean=0, sd=1))\n\n# find the sample mean of h(Xi)\nmean((x+1)^-3 / (2*dnorm(x, mean=0, sd=1)))\n\n[1] 0.4964237\n\n\nAnd we again get approximately the right answer 0.5 as expected."
  },
  {
    "objectID": "mc.html#mc-estimator-example-compute-pi",
    "href": "mc.html#mc-estimator-example-compute-pi",
    "title": "8  Monte Carlo",
    "section": "8.5 MC estimator example: compute \\(\\pi\\)",
    "text": "8.5 MC estimator example: compute \\(\\pi\\)\nHere’s a fun example. Of course, we all know the value of \\(\\pi\\) (at least to a few decimal points). But let’s suppose we didn’t know \\(\\pi\\), for the sake of argument. We do know that \\(\\pi\\) is the ratio of the area of a circle to radius².\nHow can we use this fact, in conjunction with Monte Carlo methods, to estimate \\(\\pi\\)?\nWell, let’s have a look at the diagram below, which shows a square of side length 2, with an inscribed circle of radius 1 (hence area \\(\\pi\\)). Some random points have been chosen in the square, and colored according to whether or not they lie inside the circle.\n\n\n\nEstimating Pi\n\n\nSuppose that we pick a point uniformly at random in this two-by-two square. What is the probability that the point lies inside of the circle? Well, the circle has area \\(\\pi\\) and the square has area \\(4\\), so the probability is \\(p_{\\text{in}} = \\pi/4\\).\nThis means we can estimate \\(\\pi\\) as \\(4 * p_{\\text{in}}\\). So one way to estimate \\(\\pi\\) would be to estimate \\(p_{\\text{in}}\\) using Monte Carlo methods and then multiply by \\(4\\).\nWe can formalize this using the following setup:\nDrop \\(N\\) points uniformly and independently in the square. We know each point has probability \\(\\frac\\pi4\\) of being in the circle. Let \\(X_i\\) be \\(1\\) if the \\(i\\)-th point is in the circle and \\(0\\) if it is not. Note that \\(X_i\\sim\\text{Bernoulli}(p=\\frac\\pi4)\\)\nWe know then that \\(p_{\\text{in}}=E[X_i]\\approx\\bar X=\\frac1N\\sum_{i=1}^NX_i\\). Thus, our estimator can be represented as\n\\[\\hat\\pi:=4*p_\\text{in}\\approx4*\\frac1N\\sum_{i=1}^NX_i\\]\nLet’s try this.\n\nNMC &lt;- 1000; # Number of Monte Carlo replicates\nin_circ &lt;- 0; # Count how many points landed in the circle\n\n# For-loop over the MC replicates\nfor(i in 1:NMC){\n    # for each point, generate 2 coordinates (x,y) randomly between -1 and 1\n    point &lt;- runif(n=2, min=-1, max=1);\n    \n    # to be inside circle, our point must satisfy x^2 + y^2 &lt;= 1\n    if(point[1]^2 + point[2]^2 &lt;= 1){\n        # if inside, add to count\n        in_circ &lt;- in_circ+1\n    }\n}\n\n# To get proportion of square covered, take in_circ/N\nprop &lt;- in_circ/NMC\n# to get our estimate of pi, multiply by 4.\npi.mc &lt;- 4*prop\n\nNow, let’s assess how good our estimate is by computing its relative error, i.e., the size of the error compared to the size of the thing we are trying to estimate. Note that this is a number we can compute only because we are in a situation where we know the correct answer. Of course, in more realistic settings, we don’t know the true value of the quantity we are trying to approximate.\n\n# what are our estimate and percentage error?\n# See ?sprintf for more information on creating formatted strings in R.\ncat(sprintf(\"estimate: %.4f\\n %% error: %.2f%%\",pi.mc,100*abs(pi.mc-pi)/pi))\n\nestimate: 3.1240\n % error: 0.56%\n\n\nBonus: try changing NMC in the code above. How does the relative error change? What happens if you increase NMC up to 10000? What if you decrease NMC down to 100?\n\n8.5.1 Expectation and Variance of \\(\\hat\\pi\\)\nWe can easily derive the expectation and variance of this estimator: \\[\\begin{align}\nE(\\hat\\pi)&=E\\left(\\frac4N\\sum_{i=1}^NX_i\\right)\\\\\n&=\\frac4NE\\left(\\sum_{i=1}^NX_i\\right)\\\\\n&=\\frac4N\\sum_{i=1}^NE(X_i)\\\\\n&=\\frac4N\\sum_{i=1}^N\\left[(1)(\\tfrac\\pi4)+(0)(1-\\tfrac\\pi4)\\right]\\\\\n&=\\frac4N\\sum_{i=1}^N\\frac\\pi4\\\\\n&=\\frac4N\\left(N*\\frac\\pi4\\right)\\\\\n&=\\pi\n\\end{align}\\]\n\\[\\begin{align}\nVar(\\hat\\pi)&=Var\\left(\\frac4N\\sum_{i=1}^NX_i\\right)\\\\\n&=\\left(\\frac4N\\right)^2Var\\left(\\sum_{i=1}^NX_i\\right)\\\\\n&=\\frac{16}{N^2}\\sum_{i=1}^NVar(X_i)\\\\\n&=\\frac{16}{N^2}\\sum_{i=1}^N\\left[(1-\\tfrac\\pi4)^2(\\tfrac\\pi4)+(0-\\tfrac\\pi4)^2(1-\\tfrac\\pi4)\\right]\\\\\n&=\\frac{16}{N^2}\\sum_{i=1}^N\\frac\\pi4\\left(1-\\frac\\pi4\\right)\\\\\n&=\\frac{16}{N^2}\\left(N*\\frac\\pi4\\left(1-\\frac\\pi4\\right)\\right)\\\\\n&=\\frac{\\pi(4-\\pi)}N\n\\end{align}\\]\nbonus exercise: derive same equations for \\(E\\) and \\(Var\\) by treating \\(\\sum X_i\\) as a binomial variable and applying the binomial RV equations for \\(E\\) and \\(Var\\).\nWe can easily verify the truth of these formulae by simulating multiple “runs” of this experiment and looking at the distribution of estimates we obtain.\n\n# choose M (number of times to repeat MC experiment)\nM = 1000\n\n# create vector to save results in\nmc.est = rep(NA,M)\n\n# for each experiment, do all the steps done before, get an estimate, and save it\nfor(j in 1:M){\n    \n    # these lines are copied exactly from above\n    N = 1000\n    in.circ = 0\n    for(i in 1:N){\n        point = runif(n=2, min=-1, max=1)\n        if(point[1]^2 + point[2]^2 &lt; 1){\n            in.circ = in.circ+1\n        }\n    }\n    prop = in.circ/N\n    pi.mc = prop * 4\n    \n    # save result in vector\n    mc.est[j] = pi.mc\n}\n\n\n# what do the estimates look like? print the first 100 values\noptions(max.print=100)\nmc.est\n\n  [1] 3.156 3.112 3.216 3.228 3.228 3.212 3.224 3.084 3.088 3.140 3.136 3.264\n [13] 3.112 3.156 3.020 3.184 3.100 3.204 3.080 3.104 3.120 3.072 3.076 3.084\n [25] 3.160 3.168 3.088 3.116 3.276 3.148 3.092 3.140 3.228 3.168 3.112 3.136\n [37] 3.072 3.168 3.160 3.160 3.096 3.148 3.136 3.212 3.040 3.068 3.176 3.212\n [49] 3.096 3.144 3.124 3.196 3.164 3.188 3.120 3.088 3.116 3.168 3.152 3.028\n [61] 3.088 3.064 3.132 3.196 3.172 3.188 3.160 3.144 3.116 3.168 3.196 3.224\n [73] 3.136 3.220 3.124 3.096 3.160 3.136 3.192 3.224 3.220 3.176 3.104 3.152\n [85] 3.172 3.060 3.212 3.184 3.240 3.196 3.140 3.100 3.076 3.196 3.080 3.120\n [97] 3.164 3.192 3.148 3.060\n [ reached getOption(\"max.print\") -- omitted 900 entries ]\n\n\n\nmean(mc.est)\n\n[1] 3.14024\n\n\n\nvar(mc.est)\n\n[1] 0.002513464\n\n\n\nvar.theory = pi*(4-pi)/N\nvar.theory\n\n[1] 0.002696766\n\n\n\n# deviation of our mean and variance from theory:\ncat(sprintf(\"%% deviation from E  : %.3f%% \\n%% deviation from Var: %.3f%%\",\n            abs(mean(mc.est)-pi)/pi*100,abs(var(mc.est)-var.theory)/var.theory*100))\n\n% deviation from E  : 0.043% \n% deviation from Var: 6.797%"
  },
  {
    "objectID": "mc.html#important-notes",
    "href": "mc.html#important-notes",
    "title": "8  Monte Carlo",
    "section": "8.6 Important notes:",
    "text": "8.6 Important notes:\n\nthe estimator is unbiased, since \\(E(\\hat\\pi)-\\pi=0\\), which means there’s no systematic error\nthe estimator variance \\(\\propto1/N\\), so we can increase our precision by simulating more points\n\nnote the standard deviation \\(\\propto1/\\sqrt N\\), so if you want to lower error by factor of \\(\\frac12\\) you need to increase simulation size by factor of \\(4\\)\n\n\n\n\n\nBonus: (click to show) alternate setup: find \\(E[1_\\bigcirc(X,Y)]\\) for indep. uniform \\(X,Y\\)\n\n\nWe can also represent this in the following alternate setup.\nConsider a sample \\(X_i,Y_i\\) where all \\(X_i\\) and \\(Y_i\\) are i.i.d. drawn from \\(\\text{Uniform}(-1,1)\\). Then, \\((X_i,Y_i)\\) can represent picking the \\(i\\)_th point uniformly in the square. Then, define the indicator function \\(1_\\bigcirc(x,y)\\) which gives \\(1\\) if \\((x,y)\\) is in the circle and \\(0\\) otherwise, i.e.\n\\[1_\\bigcirc(x,y)=\\begin{cases}1 & x^2+y^2\\le1^2 \\\\ 0 & \\text{otherwise}\\end{cases}\\]\nNow, from calculus we know the area of a region \\(D\\) can be represented as the integral \\(\\iint_D\\text{dxdy}\\), thus we have\n\\[\\pi=\\text{circle area} = \\iint_\\bigcirc\\text{dxdy}=\\iint_\\square1_\\bigcirc(x,y)\\text{ dxdy}=\\iint_\\square\\frac{1_\\bigcirc(x,y)}{f(x,y)}\\cdot f(x,y)\\text{ dxdy}=E[h(X,Y)]\\]\nwhere \\(h(x,y)=\\frac{1_\\bigcirc(x,y)}{f(x,y)}\\), and \\(f(x,y)\\) is the “joint density” of \\(X,Y\\) over the square, which is basically the 2D version of a PDF for a coordinate \\((X,Y)\\). It’s easy to show for this example \\(f(x,y) = \\frac14\\). Then,\n\\[E[h(X,Y)]=E\\left[\\frac{1_\\bigcirc(X,Y)}{1/4}\\right]=4\\,\\cdot E[1_\\bigcirc(x,y)]\\approx4\\,\\cdot\\,\\frac1N\\sum_{i=1}^N1_\\bigcirc(X_i,Y_i)\\]"
  },
  {
    "objectID": "mc.html#generating-random-variables-and-the-importance-of-randomness",
    "href": "mc.html#generating-random-variables-and-the-importance-of-randomness",
    "title": "8  Monte Carlo",
    "section": "8.7 Generating random variables and the importance of randomness",
    "text": "8.7 Generating random variables and the importance of randomness\nCrucial to Monte Carlo methods is that we be able to generate random variables. For example, if we want to estimate \\(\\mathbb{E} g(X)\\) for some random variable \\(X\\), then we need to be able to generate copies of the random variable \\(X\\) (and we need to be able to compute \\(g( \\cdot )\\), of course…).\nR has built-in functions that we’ve seen already for generating from the normal, Poisson, geometric, etc., but how does R generate those random numbers? And suppose that we had a particular random variable that we want to generate and R doesn’t have a built-in function for it? What do we do then?\nWell, let’s take it as a given that we know how to generate uniform random variables. Suppose that someone gives us an arbitrary density or an arbitrary CDF. How can we draw samples from it?\nWell, here’s an interesting property. For any random variable \\(X\\) with CDF \\(F\\), \\(F(X)\\) is distributed as a uniform random variable on \\([0,1]\\).\nHave a look. The code below generates a bunch of random variables from each of four different distributions, and plots the resulting histograms in the top row of an array of plots. Then, in the second row, we have histograms of \\(F(X)\\). That is, we generate the random variables and pass them into their cumulative distribution function. In all four cases, the second row look an awful lot like uniform 0-1 random variables!\n\nN &lt;- 1e5;\noptions(repr.plot.width=16)\npar(mfrow=c(2,4))\n# Generate random variables.\nXnorm &lt;- rnorm(N);\nXchisq &lt;- rchisq(N,10);\nXexp &lt;- rexp(N,1);\nXbeta &lt;- rbeta(N,.5,.5);\n# Plot plain old histograms.\nhist(Xnorm); hist(Xchisq); hist(Xexp); hist(Xbeta);\n# Plot histograms of putting the variables into their own CDFs.\nhist(pnorm(Xnorm)); hist(pchisq(Xchisq,10));\nhist(pexp(Xexp,1)); hist(pbeta(Xbeta,.5,.5));\n\n\n\n\nWe can use this fact to generate observations from any arbitrary distribution!\nLet \\(X\\) be a random variable with CDF \\(F_X\\), and suppose that \\(F_X\\) is invertible so that we can sensibly write \\(F_X^{-1}(t)\\). In later probability courses you’ll see that this trick works even if the CDF doesn’t have an inverse like you’re used to, because we can define a special kind of inverse for CDFs, but that’s for another time.\nRemember that by definition, \\[\nF_X(t) = \\Pr[ X \\le t ].\n\\]\nIf \\(U\\) is uniform \\(0\\)-\\(1\\), \\(U \\sim \\operatorname{Unif}(0,1)\\), consider \\(Y = F_X^{-1}(U)\\). We claim that this number has the same distribution as \\(X\\) (i.e., it has CDF \\(F_X\\)). If you’re interested, the next subsection will give a proof of this fact. If you don’t care about seeing why this is true, you can skip it.\n\n8.7.1 \\(F_X(X)\\) is uniform 0-1.\nLet’s check this fact. Remember, \\(Y = F_X^{-1}(U)\\), where \\(U \\sim \\operatorname{Unif}(0,1)\\) and \\(F_X\\) is the CDF of a random variable \\(X\\), and has an inverse \\(F_X^{-1}\\).\nWe need to verify that the CDF of \\(Y\\) is equal to \\(F_X\\). That is, \\(Y\\) and \\(X\\) have the same distribution.\nSo let’s look at the CDF of \\(Y\\). By definition, \\[\nF_Y(t) = \\Pr[ Y \\le t] = \\Pr[ F_X^{-1}(U) \\le t ],\n\\]\nwhere the second equality is plugging in the definition of \\(Y = F_X^{-1}(U)\\). Now, \\(F_X^{-1}(U) \\le t\\) if and only if \\(U \\le F_X(t)\\) (applying \\(F_X\\) to both sides of the inequality), so \\[\n\\Pr[ F_X^{-1}(U) \\le t ]\n=\n\\Pr[ U \\le F_X(t) ].\n\\] That is, \\[\nF_Y(t)\n=\n\\Pr[ U \\le F_X(t) ].\n\\]\nThat right-hand side is just the CDF of a uniform, evaluated at \\(F_X(t)\\). But the CDF of a uniform is just \\(\\Pr[U \\le p] = p\\) for all \\(0 \\le p \\le 1\\). So we have shown that \\(F_Y(t) = F_X(t)\\), which is to say, \\(Y = F_X^{-1}(U)\\) has the same distribution as our original random variable \\(X\\).\nIn other words, randomly sampling from the uniform distribution and then applying the inverse of the given CDF function gives the desired target distribution!\n\n\n8.7.2 Okay, proof over.\nLet’s have another look at that by sampling from a normal. In the process, we’re going to see the last of R’s built-in functions for working with random variables: qnorm.\nqnorm takes a number p between zero and one and returns the corresponding quantile of the normal distribution. That is, the number \\(t\\) such that the probability that a normal \\(X\\) is less then or equal to \\(t\\) is equal to \\(p\\). That’s a clumsy thing to say in English– put more simply, the quantile is just the inverse CDF!\n\n# reset Rmarkdown print width for readability.\noptions(repr.plot.width=7)\n\n# generate a bunch of Unif(0,1) RVs.\nunifs &lt;- runif(1e5)\n\n# Apply the built-in inverse CDF function using qnorm\n# that is, this is computing F_X^{-1}(U).\nx.normal.mc = qnorm(unifs,mean=1,sd=1)\n\n# plot results\nhist(x.normal.mc)\n\n\n\n\nAnd let’s compare those with RVs generated directly from a normal in R.\n\nhist(rnorm(1e5,mean=1,sd=1))\n\n\n\n\nAnother way of looking at this is via a Q-Q plot (no relation to QQ Express on University Ave, sadly), which might be familiar to you from previous classes.\nIf you’ve never seen this before, that’s okay– a Q-Q plot (short for quantile-quantile plot) plots points \\((x,y)\\) where the \\(x\\) values are the ordered observations from one sample and the \\(y\\) values are the ordered observations of the other sample. Q-Q plots are a good way to compare two distributions of points. We’ll have more to say about them over the course of the semester and you’ll see them plenty more during your later courses.\nFor now, it’s enough to know that if two distributions are similar, then their Q-Q plot will look like a straight line.\n\n# plot our sample quantiles against theoretical quantiles for comparison\nplot(sort(x.normal.mc),qnorm(ppoints(1e5,1),mean=1,sd=1))\n\n\n\n\nThe straight line in the Q-Q plot indicates that our two samples are very similar– as they should be!"
  },
  {
    "objectID": "mc.html#example",
    "href": "mc.html#example",
    "title": "8  Monte Carlo",
    "section": "8.8 Example:",
    "text": "8.8 Example:\nDefine a random variable \\(X\\) with density \\[\nf_X(x) = \\begin{cases}\n      2x &\\mbox{ if } 0 \\le x \\le 1 \\\\\n      0 &\\mbox{ otherwise. }\n      \\end{cases}\n\\]\n\n# here we define a *vectorized* function to evaluate the density of X\npdf_x = function(x) {\n  # ifelse is like a function version of an if else control statement\n  # We use it here to ensure that pdf_x can operate directly on vectors\n  return(ifelse(0&lt;=x & x&lt;=1 , 2*x , 0 ))\n}\n\n# showing the PDF in a plot\nplot(pdf_x,from = -.5,to = 1.5,n=1001)\n\n\n\n\nThis means that the cumulative distribution function is \\[F_X(x)=\\int_0^xf_X(x)dx=\\int_0^x2xdx=x^2\\] for \\(0 \\le x \\le 1\\) (if \\(x&lt;0\\), \\(F_X(x)=0\\), and if \\(x&gt;1\\), \\(F_X(x)=1\\)). Here’s a plot of the CDF\n\ncdf_x = function(x) {\n  return(ifelse(0&lt;=x & x&lt;=1 , x^2 , ifelse(x&lt;0,0,1) ))\n}\n\n# showing the CDF in a plot\nplot(cdf_x,from = -.2,to = 1.2,n=1001)\n\n\n\n\nHow can we write a function rx(n) (like rbinom) to sample from this random variable, where n is the size of the sample to be drawn?\nFirst, we find the inverse CDF, which here is the opposite of \\(x^2\\), i.e. \\(\\sqrt x\\). Next, we simply need to apply this to a sample from from Uniform(0,1) and this should give us the desired result.\n\nrx = function(n) sqrt(runif(n))\n\nWe can check our work by drawing a sample and plotting it.\n\nhist(rx(1e5))\n\n\n\n\n\nplot(ecdf(rx(1e5)),main=\"Plot of sample CDF\")\n\n\n\n\n\n8.8.1 Okay, but do I actually need this inverse trick?\nOkay, full disclosure: lucky for us, it’s pretty rare that we actually need to use this inverse trick, especially in a course at this level. Almost any random variable you will ever need in your life has already been implemented in R (and most other programming languages, for that matter). Still, if you understand this trick, then you understand enough about CDFs and how they work to be dangerous. Also, it’s useful on exam questions…"
  },
  {
    "objectID": "mc.html#random-v.-pseudorandom",
    "href": "mc.html#random-v.-pseudorandom",
    "title": "8  Monte Carlo",
    "section": "8.9 Random v. pseudorandom",
    "text": "8.9 Random v. pseudorandom\nOne last parting remark, included here because it’s interesting and important to know about, not because it will be on any exams.\nFor Monte Carlo, it’s important to have a good source of random numbers whose distribution is precisely known. This is a surprisingly difficult problem. There are ways of generating (as far as modern science can tell), almost perfectly uniformly random numbers, such as measuring atmospheric noise, radioactive decay, or even lava lamps (used by Cloudflare). These sources are generally considered capable of producing the most truly random numbers.\nYour computer (unless it’s attached to a Geiger counter or a wall of lava lamps) is only capable of producing pseudorandom numbers. These are made by running a pseudorandom number generator algorithm which is deterministic, meaning it always produces the same output given the same input. For example, R produces pseudorandom numbers using the Mersenne-Twister algorithm. Even though these are deterministic, they are statistically random– for all practical computational purposes, there are no discernible patterns in the output.\nYou can see this in action in R by setting the seed of the random number generator:\n\nset.seed(340)\n\nrnorm(n=10)\n\n [1] -0.1573733 -1.1988575 -0.8892049  1.0090607  0.6130407  1.0071506\n [7]  0.4144321 -1.8579099 -1.3487292  0.5188585\n\n\nNow, if we kept generating normals, we’d continue to see a bunch of normal RVs. But if set the random number generator’s seed back to 340…\n\nset.seed(340)\n\nrnorm(n=10)\n\n [1] -0.1573733 -1.1988575 -0.8892049  1.0090607  0.6130407  1.0071506\n [7]  0.4144321 -1.8579099 -1.3487292  0.5188585\n\n\nYou’re not imagining it– those are the same ten “random” normals as we saw above.\nOnce the RNG seed is set, the “random” numbers that R generates for us aren’t random at all. But someone looking at these random numbers would have a very hard time distinguishing these numbers from truly random numbers. That is what “statistical randomness” means!\nNone of this is of tremendous importance for you in the short term. It’s just important to keep in mind that the “random” numbers that we generate on our computers aren’t truly random. There are situations where these patterns actually matter, most notably in security applications. Still, for our purposes, pseudorandom numbers are good enough, and way cheaper than truly random numbers from Geiger counters or atmospheric noise or lava lamps or…\nYou can spend a whole career studying random numbers and how to generate them. Unfortunately, this deeply interesting subject is mostly outside the scope of our course. If you’re interested in learning more, though, here are a couple of videos you might enjoy:\n\n\n\n\n\nThe comprehensive textbook on the topic of generating random variables is Non-Uniform Random Variate Generation by Luc Devroye. Unfortunately, it’s a bit advanced, unless you’ve taken a bunch of calculus already, but something to strive for!\n\n8.9.1 Review:\nIn these notes we covered:\n\nThe basic steps of a Monte Carlo simulation\nEstimating the expected value of a random variable using MC\nEstimating a probability using MC\nEstimating a definite integral using MC\nWhy \\(F_x(X)\\sim Uniform(0,1)\\) for any r.v. \\(X\\)\nSimulating an aribitrary random variable using MC\nPsuedorandom number generation & the use of set.seed() in R"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#a-few-structures-for-monte-carlo",
    "href": "R03_MonteCarloExamples.html#a-few-structures-for-monte-carlo",
    "title": "9  Monte Carlo Examples",
    "section": "9.1 A few structures for Monte Carlo",
    "text": "9.1 A few structures for Monte Carlo\n\n9.1.1 For Loop\nConsider two random variables: \\(X\\sim N(5, 3^2)\\) is normally distributed with a mean of \\(\\mu=5\\) and a variance of \\(\\sigma^2=3^2\\). \\(Y\\sim exp(.2)\\), independent of X, is exponentially distributed with a rate parameter of \\(\\lambda=.2\\). The Questions is what is the \\(Pr[X &gt; Y]\\)?\nUse Monte Carlo to estimate this probability.\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nresults &lt;- rep(FALSE, NMC) # a vector for storing results\nfor(i in 1:NMC){\n  X &lt;- rnorm(1,5,3)\n  Y &lt;- rexp(1, .2)\n  results[i] &lt;- (X &gt; Y) #TRUE or FALSE\n}\nmean(results)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57455\n\n\n\n\n9.1.2 Generate Many RVs at once\nYou can also avoid the loop entirely by just generating many random variables at once\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nX &lt;- rnorm(NMC,5,3)\nY &lt;- rexp(NMC, .2)\nmean(X &gt; Y)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57829\n\n\n\n\n9.1.3 The replicate() function\nYou can also use the replicate function. It can avoid the need for a loop\n\nset.seed(2)\nNMC &lt;- 100000 #Many Monte Carlo replicates\nresults &lt;- replicate(NMC, rnorm(1,5,3)&gt;rexp(1,.2))\n#replicate( how many times, expression)\n#this creates a vector of replicates! So easy!\nmean(results)  ### the proportion of TRUES out of all replicates\n\n[1] 0.57455"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#expected-value-estimation-using-monte-carlo",
    "href": "R03_MonteCarloExamples.html#expected-value-estimation-using-monte-carlo",
    "title": "9  Monte Carlo Examples",
    "section": "9.2 Expected Value Estimation using Monte Carlo",
    "text": "9.2 Expected Value Estimation using Monte Carlo\n\n9.2.1 Weak Law of Large Numbers Example\nDemonstration of how larger samples leads the sample mean to approach expected value\nLet’s consider the following random variable:\n\nx&lt;- 1:10\npx &lt;- runif(10)\npx &lt;- px / sum(px)\nbarplot(height=px, names=x)\n\n\n\n#Let's peek behind the curtain\n(EX &lt;- sum( x * px))\n\n[1] 4.923099\n\n(VarX &lt;- sum(x^2*px) - EX^2)\n\n[1] 7.508671\n\nsum((x-EX)^2*px)\n\n[1] 7.508671\n\n\nLet’s demonstrate how our estimate of EX gets better with M growing\n\nmyMeans &lt;- vector(\"numeric\")\nMs &lt;- seq(100, 10000, 100)\nfor(M in Ms){\n  mySample &lt;- sample(x, prob=px, size=M, replace=TRUE)\n  myMeans[M/100] &lt;- mean(mySample)\n}\nplot(x= Ms, y=myMeans)\nabline(h=EX, col=\"red\")\n\n\n\n\n\n\n9.2.2 Demonstration of the law of large numbers in Monte Carlo expected value estimation\nFor example, say you want to estimate the mean of an exponential distribution. In principal, we can simulate many values \\(X_1, X_2, \\ldots, X_M\\) from this distribution, average them and that’s going to be our estimate of the \\(EX\\)\n\nNMC &lt;- 10000\nrandomExp &lt;- rexp(NMC, .4)\nnumerator &lt;- cumsum(randomExp)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=1/.4, col=\"red\")\n\n\n\n\n\n\n9.2.3 Estimate the expected value of some weird random variable\nGamma distribution takes 2 parameters - shape and scale\nSay X ~ Gamma(shape = 5, scale = 0.5)\nLet’s estimate it’s expected value using Monte carlo - with just 100 runs. We’ll do it 5 times to compare the estimates.\n\nset.seed(1)\n\nMCN &lt;- 100   #stands for Monte Carlo N\n\nfor(i in 1:5){\n  #generate a bunch of values from the random variable\n  myData &lt;- rgamma(MCN, shape=5, scale=.5)\n#  hist(myData)\n  \n  #Law of Large Numbers says that the sample average should be close to EX\n  print(mean(myData))\n  \n}\n\n[1] 2.46491\n[1] 2.381638\n[1] 2.44396\n[1] 2.536307\n[1] 2.514453\n\n\nWhat’s your guess as to the true expected value? Hint: the parameters are 5 and .5. Do you have a guess?\nLet’s generate 10 million random values and see what we get for the mean.\n\nMCN &lt;- 10000000   #stands for Monte Carlo N\n\n#generate a bunch of values from the random variable\nmyData &lt;- rgamma(MCN, shape=5, scale=.5)\n\n#Law of Large Numbers says that the sample average should be close to EX\nmean(myData)\n\n[1] 2.500108\n\n\nLet’s consider a random variable Y which is the square root of a exponential random variable X with rate parameter 3\n\\(E(X) = 1/3 = .333333\\)\nGuess what might be \\(E(Y)=E(\\sqrt{X})\\)? maybe \\(\\sqrt{1/3} = 0.5773503\\)?\nLet’s check using MC method.\n\nX &lt;- rexp(1000000, rate=3)\nY &lt;- sqrt(X)\nmean(Y)\n\n[1] 0.5116081\n\n\nNo ! it turns out that EY = .511 or so.\nIn general \\(E(g(X))\\) is not \\(g(E(X))\\)\n\n\n9.2.4 A time when Monte Carlo fails - when the expected value does not exist.\nThe Cauchy Distribution is a weird one. It has no defined expected value. It is actually just a T distribution with 1 degree of freedom! Same thing.\nHere’s a picture of its density function from -10 to 10.\n\nplot(x=seq(-10,10,.01), y=dcauchy(seq(-10,10,.01)), type=\"l\", main=\"density of Cauchy Distribution\", xlab=\"x\", ylab=\"density\")\n\n\n\n\nHere’s an example of a sample from the Cauchy:\n\nhist(rcauchy(1000))\n\n\n\n\nChances are you get at least one extreme extreme value. That’s the effect of having FAT tails.\nLet’s just look at what the long term average would be\n\nNMC &lt;- 10000\nrandomCauchy &lt;- rcauchy(NMC)\nnumerator &lt;- cumsum(randomCauchy)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\nThe cumulative average approaches 0 until one extreme value is sampled and then it throws off the average. Then the average slowly approaches 0 again until another extreme value throws everything off. This is what happens when expected value is not defined - law of large numbers cannot take effect.\nBut when the t distribution has 2 degrees of freedome we see a very different pattern emerge:\n\nNMC &lt;- 10000\nrandomT2 &lt;- rt(NMC,2)\nnumerator &lt;- cumsum(randomT2)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\nProblem is the variance is still undefined. The variance is infinity for \\(df \\leq 2\\). When the mean is defined and the variance is finite we start to see the Law of Large Numbers get involved\n\nNMC &lt;- 10000\nrandomT3 &lt;- rt(NMC,3)\nnumerator &lt;- cumsum(randomT3)\naverage &lt;- numerator / (1:NMC)\nplot(x=1:NMC, y=average, type=\"l\")\nabline(h=0, col=\"red\")\n\n\n\n\n\n\n9.2.5 Estimate the expected value of log(X) where X~Normal\n\nmu &lt;- 4\nsigma &lt;- 5\n\nNMC &lt;- 10000\n\nX &lt;- rnorm(NMC, mean=mu, sd=sigma)\n\n#Look a the histogram\nhist(X)\n\n\n\nY &lt;- log(abs(X))\n#histogram of log|X|\nhist(Y)\n\n\n\nmean(Y)\n\n[1] 1.272277\n\n\n\n\n9.2.6 St Petersburg Game\nThe game goes like this:\nI (the Casino) put one dollar in the pot I flip a coin. If it’s a tails, I double the pot If it is heads, you win the pot.\nIT costs money to play the game!!! First question: what is a fair price to play the game?\nA random variable without a defined expected value; Monte Carlo will fail us!\n\n#we can simulate M plays of the game with simply M geometric random values from geom(.5)\nM &lt;- 1000000\n\nt &lt;- rgeom(M, .5)  #t is number tails per game\nwinnings &lt;- 2^t\nmean(winnings)\n\n[1] 11.19202"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#probability-calculation-by-monte-carlo",
    "href": "R03_MonteCarloExamples.html#probability-calculation-by-monte-carlo",
    "title": "9  Monte Carlo Examples",
    "section": "9.3 Probability Calculation by Monte Carlo",
    "text": "9.3 Probability Calculation by Monte Carlo\n\n9.3.1 A Normal Example\nSuppose we want to calculate a normal RV probability. Say X follows a normal distribution with mean 8 and standard deviation 2.\nWhat is the probability that X is between 8.4 and 9.9? And let’s also imagine we don’t know how to use pnorm.\n\nM &lt;- 1000000 #this is the number of Monte Carlo replicates to make\nX &lt;- rnorm(M, mean=8, sd=2)\nmean(X &gt; 8.4 & X &lt; 9.9)\n\n[1] 0.248712\n\n\n\npnorm(9.9, 8,2)- pnorm(8.4, 8,2)\n\n[1] 0.2496842\n\n\n\n\n9.3.2 The Birthday Problem.\nIf you have n people together, what is the probability that at least 2 of them share a birthday?\n\nsimulateBirthdays &lt;- function(n){\n  #birthdays will be numbers from 1 to 365\n  #we'll assume that each day of the year is equally likely (this is probably not true in real life)\n  return (sample(x = 1:365, size=n, replace=TRUE))\n}\n\ncheckSharedBirthdays &lt;- function(birthdays){\n  #We'll use the unique function\n  #given a vector X of values, unique(X) gives the unique values\n  return (length(unique(birthdays)) &lt; length(birthdays))\n}\n\nWe’ll do a Monte Carlo simulation to estimate the probability of a shared birthday when we have 19 people\n\nNMC &lt;- 10000\nresults &lt;- vector(\"logical\")\nfor(i in 1:NMC){\n  simBirthdays &lt;- simulateBirthdays(19)\n  results[i] &lt;- checkSharedBirthdays(simBirthdays)\n}\n#estimate the probability\nmean(results)\n\n[1] 0.376\n\n\nRepeat with other values of n\n\nNMC &lt;- 100000\nresults &lt;- vector(\"logical\")\nfor(i in 1:NMC){\n  simBirthdays &lt;- simulateBirthdays(23)\n  results[i] &lt;- checkSharedBirthdays(simBirthdays)\n}\n#estimate the probability\nmean(results)\n\n[1] 0.51082\n\n\n\n\n9.3.3 Buffon’s Needles\nWe want to modify the example from class to take 3 parameters: L: length of the needles W: the width of the floorbords M: the number of Monte Carlo replicates\nThis will simulate the Buffon Needle experiment with M needles of length L on a floor with line width W. The needle.crosses function returns a single needle’s result - did it cross a floor line (TRUE or FALSE).\n\n#I will dispense with the x coordinates entirely. \nneedle.crosses &lt;- function(L, W){\n  y &lt;- runif(1, -W, W)\n  angle &lt;- runif(1, 0, pi)\n  dy &lt;- L/2 * sin(angle)\n  y1 &lt;- y-dy\n  y2 &lt;- y+dy\n  return( floor(y1/W)!= floor(y2/W)) #divide by W tells us which board number the endpoint is on, \n}\n\nbuffon.prob &lt;- function(L, W, M){\n  results &lt;- rep(0, M)\n  for(i in 1:M){\n    results[i] &lt;- needle.crosses(L,W)\n  }\n  return(mean(results))\n}\n\n#test\nbuffon.prob(1, 1, 10000)\n\n[1] 0.6381\n\n#What if the floor boards are 2 units wide??\nbuffon.prob(1, 2, 10000)\n\n[1] 0.3128\n\n\n\n\n9.3.4 Example: the Monty Hall Problem\nOkay, it’s time for a probability and statistics rite of passage: the Monty Hall problem.\nThe problem is named after Monty Hall, the original host of the game show Let’s Make a Deal.\nThe setup is as follows: you, the contestant, are faced with three doors. Behind one of the doors is the grand prize (a million dollars, a new car, a MacGuffin; use your imagination). Behind the other two doors are goats (in this hypothetical universe, a goat is not a prize you want; I disagree with this sentiment, but that’s beside the point). You get to choose a door, and you win the prize behind that door.\nHere’s a helpful illustration from Wikipedia:\n\n\n\nImage credit Wikipedia; https://en.wikipedia.org/wiki/Monty_Hall_problem\n\n\nSuppose that you choose a door at random. Having chosen that door, the host Monty Hall opens one of the other doors to reveal a goat (i.e., not the Grand Prize), and Monty Hall offers you a choice: you can stick with the door you chose and win whatever prize is behind that door, or you can switch your choice to the other door, which Monty Hall has not yet opened.\nShould you switch your choice of door?\nOn first glance, most people agree that it should make no difference– your original choice of door was random, so whether you switch your guess or not, you’re still equally likely to have chosen the door with the Grand Prize.\nBut this intuition is incorrect! Let’s check with a simulation, first.\n\ngenerate_game_state &lt;- function() {\n  # Generate a random instance of the Monty Hall game.\n  # That is, a random assignment of prizes to doors\n  # and an initial guess for our contestant.\n  # Generate an assignment of prizes/goats to doors.\n  # We'll encode the Grand Prize as a 1 and the goats as 0s.\n  # Reminder: sample(v) just returns a random permutation\n  # of the entries of v.\n  prizes &lt;- sample( c(1,0,0) );\n  # Now, let's randomly pick a door to guess.\n  # We pick door number 1, 2 or 3.\n  # Use sample() to choose from {1,2,3} uniformly at random\n  doors &lt;- c(1,2,3)\n  guess &lt;- sample( doors, size=1);\n  # Record the other two doors.\n  # x[-c] returns the entries of x NOT in vector c.\n  otherdoors &lt;- doors[-guess];\n  \n  # Return a list object, which will be easier to work with\n  # when we want to access the different pieces of game\n  # information\n  game &lt;- list(prizes=prizes, guess=guess,\n               otherdoors=otherdoors );\n  return( game )\n}\n\nrun_game_noswitch &lt;- function() {\n  # Run one iteration of Let's Make a Deal,\n  # in which we do NOT switch our guess.\n  \n  # Generate a game.\n  game &lt;- generate_game_state()\n  # Now, Monty Hall has to reveal a door to us.\n  # If we were switching our guess, we would need to do some\n  # extra work to check some conditions (see below for that),\n  # but since we're not switching, let's just cut to the\n  # chase and see if we won the prize or not.\n  # Remember, game$prizes is a vector of 0s and 1s, encoding\n  #     the prizes behind the three doors.\n  # game$guess is 1, 2 or 3, encoding which door we guessed\n  return( game$prizes[game$guess] )\n}\n\nrun_game_yesswitch &lt;- function() {\n  # Run one iteration of Let's Make a Deal,\n  # in which we DO switch our guess.\n  \n  # Generate a game.\n  game &lt;- generate_game_state()\n  guess &lt;- game$guess; # We're going to switch this guess.\n  # Now, Monty Hall has to reveal a door to us.\n  # To do that, we need to look at the other doors,\n  # and reveal that one of them has a goat behind it.\n  # game$otherdoors is a vector of length 2, encoding the\n  # two doors that we didn't look at.\n  # So let's look at them one at a time.\n  # Note: there are other, more clever ways to write\n  # this code, but this is the simplest implementation\n  # to think about, in my opinion.\n  if( game$prizes[game$otherdoors[1]]==0 ){\n    # The first non-guessed door doesn't have a goat\n    # behind it, so Monty Hall shows us the goat behind\n    # that door, and we need to switch our guess to the\n    # *other* door that we didn't choose.\n    guess &lt;- game$otherdoors[2];\n  } else {\n    # If the Grand Prize is behind otherdoors[1],\n    # so that game$otherdoors[1]]==1,\n    # then Monty Hall is going to show us a goat behind\n    # otherdoors[2], and we have the option to switch our\n    # guess to otherdoors[1],\n    # and we will exercise that option\n    guess &lt;- game$otherdoors[1];\n  }\n  # Now check if we won the prize!\n  return( game$prizes[guess] )\n}\n\nOkay, we’ve got simulations implemented for both of our two different game strategies. Let’s simulate both of these a bunch of times and compare the long-run average success.\n\nM &lt;- 1e4;\nnoswitch_wins &lt;- 0;\nfor(i in 1:M) {\n  noswitch_wins &lt;- noswitch_wins + run_game_noswitch()\n}\n\nnoswitch_wins/M\n\n[1] 0.3357\n\n\nNow, let’s see how we do if we switch our guesses.\n\nM &lt;- 1e4;\nyesswitch_wins &lt;- 0;\nfor(i in 1:M) {\n  yesswitch_wins &lt;- yesswitch_wins + run_game_yesswitch()\n}\n\nyesswitch_wins/M\n\n[1] 0.665\n\n\nWow! That’s a lot better than the strategy where we don’t switch our guess!\nThis discrepancy can be explained using Bayes’ rule, but it gets a bit involved (see the wikipedia page if you’re really curious).\nInstead, let’s just think about the following: suppose that the Grand Prize is behind door number 1. There are three possibilities, all equally likely (because we chose uniformly at random among the three doors):\n\nWe pick door 1. We have chosen the door with the Grand Prize behind it. In this situation, the other two doors both have goats behind them, and Monty Hall reveals one of those two goats to us. In this situation, we (mistakenly, so sad!) switch our Grand Prize door for a goat door and we lose.\nWe pick door 2. We have chosen a door with a goat behind it. Of the other two doors, only one has a goat, door 3. Monty Hall shows us the goat behind that door, and we switch our guess to the other door, door 1, which has the Grand Prize behind it. Hooray!\nWe pick door 3. We have chosen a door with a goat behind it. Of the other two doors, only one has a goat, door 2. Monty Hall shows us the goat behind that door, and we switch our guess to the other door, door 1, which has the Grand Prize behind it. Hooray!\n\nSo, of the three equally likely situations, we win the Grand Prize in two of them, and our probability of winning is thus \\(2/3\\). Compare that with our \\(1/3\\) probability of winning in the situation where we don’t switch doors. Not bad!\nThe important point here is that our decision to switch doors is made conditional upon the information from Monty Hall that eliminates one of the three doors for us.\n\n\n9.3.5 A Combination of Random Variables\nLet X be the product of 3 normal random variables Y1, Y2 and Y3, with means 3, 6, and -2 and standard deviations 5, 6 and 9\nWhat is Pr[X &lt; 13]?\n\nNMC&lt;- 100000\nresults &lt;- rep(0, NMC)\nfor(i in 1:NMC){\n  Y1 &lt;- rnorm(1, 3, 5)\n  Y2 &lt;- rnorm(1, 6, 6)\n  Y3 &lt;- rnorm(1, -2, 9)\n  X &lt;- Y1*Y2*Y3\n  results[i] &lt;- (X &lt; 13)\n}\nmean(results)\n\n[1] 0.60955\n\n\n\n\n9.3.6 Example A complicated random variable\nRoll 3 6-sided dice and multiply their values. Let’s find the expected product\n\nNMC &lt;- 100000\n\nresults &lt;- FALSE\nfor(i in 1:NMC){\n  results[i] &lt;- prod(sample(6, size=3, replace=TRUE)) #multiples the values in the vector\n  \n}\nmean(results)\n\n[1] 42.76313\n\nhist(results)\n\n\n\n\n\n\n9.3.7 Example: non-transitive dice\nsee here\n\ndieY &lt;- c(3,3,3,3,3,3)\ndieB &lt;- c(0,0,4,4,4,4)\ndieG &lt;- c(1,1,1,5,5,5)\ndieR &lt;- c(2,2,2,2,6,6)\n\nnotTdice &lt;- data.frame(dieY,dieB,dieG,dieR)\n\nrollDice &lt;- function(die1,die2,N){\n  #true if player 1 wins\n  return(sample(notTdice[,die1],N,replace=TRUE) &gt; sample(notTdice[,die2],N,replace=TRUE))\n}\n\nMCN &lt;- 10000\n\ndieColors &lt;- c(\"yellow\",\"blue\",\"green\",\"red\")\n\nfor(i in 1:3){\n  for(j in ((i+1):4)){\n    results =rollDice(i,j,MCN)\n    print(paste(dieColors[i],\n                \"vs\",\n                dieColors[j],\n                \":\",\n                dieColors[i],\n                \"win proportion is \",\n                mean(results)))\n  }\n}\n\n[1] \"yellow vs blue : yellow win proportion is  0.3343\"\n[1] \"yellow vs green : yellow win proportion is  0.492\"\n[1] \"yellow vs red : yellow win proportion is  0.6617\"\n[1] \"blue vs green : blue win proportion is  0.3283\"\n[1] \"blue vs red : blue win proportion is  0.4541\"\n[1] \"green vs red : green win proportion is  0.3378\"\n\n\n\n\n9.3.8 Miwin’s Dice\nMiwin’s Dice were invented in 1975 by the physicist Michael Winkelmann.\nConsider a set of three dice such that\ndie A has sides 1, 2, 5, 6, 7, 9 die B has sides 1, 3, 4, 5, 8, 9 die C has sides 2, 3, 4, 6, 7, 8\nWhat are the winning probabilities for these dice in the game?\n\ndieA &lt;- c(1, 2, 5, 6, 7, 9)\ndieB &lt;- c(1, 3, 4, 5, 8, 9)\ndieC &lt;- c(2, 3, 4, 6, 7, 8)\n\nnotTdice &lt;- data.frame(dieA,dieB,dieC)\n\nrollDice &lt;- function(die1,die2,N){\n  #true if player 1 wins\n  results &lt;- rep(0,N)\n  P1Rolls &lt;- sample(notTdice[,die1],N,replace=TRUE)\n  P2Rolls &lt;- sample(notTdice[,die2],N,replace=TRUE)\n  results[ P1Rolls &gt; P2Rolls] &lt;- 1\n  results[P1Rolls &lt; P2Rolls] &lt;- -1\n  return(results)\n}\nMCN &lt;- 1000000\ndieNames &lt;- c(\"A\",\"B\",\"C\")\nresultsTable &lt;- data.frame(P1&lt;-vector(\"character\"),\n                           P2&lt;-vector(\"character\"),\n                           Win&lt;-vector(\"numeric\"),\n                           Tie&lt;-vector(\"numeric\"),\n                           Lose&lt;-vector(\"numeric\"))\nfor(i in 1:2){\n  for(j in ((i+1\n             ):3)){\n    results =rollDice(i,j,MCN)\n    resultsTable &lt;- rbind(resultsTable,\n                          c(dieNames[i],\n                            dieNames[j],\n                            mean(results == 1),\n                            mean(results == 0),\n                            mean(results == -1)))\n  }\n}\nresultsTable\n\n  X.A. X.B. X.0.472807. X.0.08342. X.0.443773.\n1    A    B    0.472807    0.08342    0.443773\n2    A    C      0.4437   0.083683    0.472617\n3    B    C    0.472341   0.083206    0.444453\n\nnames(resultsTable) &lt;- c(\"P1 Die\",\"P2 Die\",\"P1 win\",\"Tie\",\"P2 Win\")"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#integration-by-monte-carlo-simulation",
    "href": "R03_MonteCarloExamples.html#integration-by-monte-carlo-simulation",
    "title": "9  Monte Carlo Examples",
    "section": "9.4 Integration by monte carlo simulation",
    "text": "9.4 Integration by monte carlo simulation\n\n9.4.1 Example 1\n\\[\\int_{7}^{14} (3x-2\\ln x-x^2) dx\\]\n\ng &lt;- function(x){\n  return(3*x - 2*log(x)-x^2)\n}\n\nM &lt;- 10000\nresults &lt;- vector(\"numeric\") #empty numerical vector\n\n#I am going to sample from X~unif(7,14) and I want to evalute h(x) = g(x)/f(x), where f(x) is the density of X\n\nfor(i in 1:M){\n  x &lt;- runif(1, 7, 14)\n  results[i] &lt;- g(x)/dunif(x, 7, 14) #this is h(x)\n}\nmean(results)\n\n[1] -615.2348\n\n\n\n-2*log(13492928512)-3395/6\n\n[1] -612.4842\n\n\n\n\n9.4.2 Example 2\n\\[\\int_1^7\\ln(3x^2-2)\\sin x dx\\]\n\nM &lt;- 10000\n\n#step 1, sample from unif(1,7) \nx &lt;- runif(M, 1,7)\ng &lt;- function(x){\n  log(3*x^2-2)*sin(x)\n}\nf &lt;- function(x){\n  dunif(x, 1, 7)\n}\nh &lt;- g(x)/f(x)\n\nmean(h)\n\n[1] -4.191919"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#other-examples",
    "href": "R03_MonteCarloExamples.html#other-examples",
    "title": "9  Monte Carlo Examples",
    "section": "9.5 Other Examples",
    "text": "9.5 Other Examples\n\n9.5.1 Estimating Euler’s constant\nA fact - if you sample \\(X_1, X_2, \\ldots\\) from Unif(0,1), and let \\(Y\\) be the index for which the sum exceeds 1, the expected value of \\(Y\\) is \\(e\\).\n\nNMC &lt;- 100000\nNs &lt;- 0\nfor(i in 1:NMC){\n  Ns[i] = min(which(cumsum(runif(100))&gt;1))\n}\nmean(Ns)\n\n[1] 2.72073\n\n\n\n#Check the value\nexp(1)\n\n[1] 2.718282\n\n\n\n\n9.5.2 Estimating the value of pi\n\nNMC &lt;- 10000; # Number of Monte Carlo replicates\nplot(NA,NA, xlim=c(-1,1), ylim=c(-1,1))\n\nin_circ &lt;- 0; # Count how many points landed in the circle\n# For-loop over the MC replicates\nfor(i in 1:NMC){\n  \n  # for each point, generate (x,y) randomly between -1 and 1\n  point &lt;- runif(n=2, min=-1, max=1);\n  # to be inside circle, our point must satisfy xˆ2 + yˆ2 &lt;= 1\n  if(point[1]^2 + point[2]^2 &lt;= 1){\n  # if inside, add to count\n    in_circ &lt;- in_circ+1\n    points(point[1],point[2])\n  }\n}\n\n\n\n#To get proportion of square covered, take in_circ/N\nprop &lt;- in_circ/NMC\n# to get our estimate of pi, multiply by 4.\npi.mc &lt;- 4*prop\npi.mc\n\n[1] 3.1344"
  },
  {
    "objectID": "R03_MonteCarloExamples.html#random-variable-generation",
    "href": "R03_MonteCarloExamples.html#random-variable-generation",
    "title": "9  Monte Carlo Examples",
    "section": "9.6 Random Variable Generation",
    "text": "9.6 Random Variable Generation\n\n9.6.1 Distribution of F(X) is uniform(0,1)\n\n#Sample from the normal distribution, let's get 1000 values from N(mean=5, var=8^2)\nX &lt;- rnorm(10000, 5, sd=8)\n\nhist(X)\n\n\n\n#What are the first 5 sampled values?\nX[1:5]\n\n[1] 12.6667085  9.0566258  0.2460861 19.1794222 -8.5156276\n\n#SO far we haven't looked at F(X). Remember, the CDF, F(x) is defined as Pr(X &lt;= x)\n#To find these we can look at pnorm\npnorm(X[1:5], 5, sd=8)\n\n[1] 0.83105397 0.69395003 0.27617605 0.96183821 0.04556628\n\n#what if we looked at pnorm for ALL values\n#How would the pnorms be distributed?\n\nhist(pnorm(X, 5, sd=8))\n\n\n\n\n\n\n9.6.2 Simulating Random Values\nLet’s look at a random sample of values from a normal distribution - \\(N(6,2^2)\\)\n\nn.values &lt;- rnorm(10000, mean=6, sd=2)\n\nWe can look a histogram of these values\n\nhist(n.values)\n\n\n\n\nYup. It’s a normal distribution. For any value, let p=P(X&lt;x) What about the left-tailed probabilities associated? Use pnorm.\n\nn.values[1:5]\n\n[1] 7.194574 3.433902 6.290671 5.266036 5.734321\n\npnorm(n.values[1:5], mean=6, sd=2)\n\n[1] 0.72484206 0.09973744 0.55777702 0.35681628 0.44716018\n\n\nWhat does the distribution of left-tail probabilities look like?\n\nhist(pnorm(n.values, mean=6, sd=2), breaks=10)\n\n\n\n\nWhat about some other random variable distribution? Let’s look at an exponential random variable.\n\nexp.values &lt;- rexp(10000, rate=3)\nhist(exp.values)\n\n\n\n\nAgain- that’s the shape of the distribution. What about the distribution of left-tail probabilities?\n\nhist(pexp(exp.values, rate=3))\n\n\n\n\nWhy is this the case? Think about the theory. X ~ Normal Distribution with mean=6, sd=2 What is the probability that P(X&lt;x) &lt; .1?\nWell, .1 of course! There’s a 10% probability that .2 &lt; P(X&lt;x) &lt; .3, and for each interval in this histogram.\n\nprobs &lt;- seq(.01,.99,.005)\npercentiles &lt;- qnorm(probs)\n\nplot(x=seq(-2,2,.1), pnorm(seq(-2,2,.1)), type=\"l\")\nsegments(percentiles,0,percentiles, probs, lwd=3, col=rgb(0,0,0,.5))\nsegments(-2,probs,percentiles, probs)\n\n\n\n\nWe can use this idea to generate random values from any distribution we want as long as we can specify the inverse CDF.\nWe’re going to use the inverse CDF trick to simulate a bunch of values from a normal distribution with mean 6 and standard deviation 2.\n\nu &lt;- runif(10000)  #These are my values sampled uniformly\n                   # at random from 0 to 1. These represent\n                   # Left tail probabilities\nx &lt;- qnorm(u, mean=6, sd=2)\n\nmean(x)\n\n[1] 6.021647\n\nsd(x)\n\n[1] 2.004111\n\nhist(x, probability=TRUE)\nlines(seq(0,12,.1), dnorm(seq(0,12,.1),6,2), col=\"blue\")\n\n\n\n\n\n\n9.6.3 CDF plot for some other distribution\nThe PDF of an exponential\n\nx&lt;- seq(0, 6, .1)\nplot(x, dexp(x,.4), type=\"l\", main=\"Exp(.4) density curve\")\n\n\n\n\n\nprobs &lt;- seq(.01,.99,.01)\npercentiles &lt;- qexp(probs, .4)\n\nplot(x=percentiles, pexp(percentiles,.4), type=\"l\")\nsegments(percentiles,0,percentiles, probs, lwd=3, col=rgb(0,0,0,.5))\nsegments(-2,probs,percentiles, probs)\n\n\n\n\n\n\n9.6.4 Generating random values\nSay I want to get 1000 values from an exponential distribution\n\nx&lt;- seq(0, 20,.1)\ny&lt;- dexp(x, rate=1)\nplot(x,y, type=\"l\", main=\"the density function of X ~ exp(1)\")\n\n\n\n\n\n#Here is what R would do\nU &lt;- runif(1000)\nX &lt;- qexp(U, rate=1) #This is the inverse CDF of X\n#in theory, X data should be distributed by exp(1)\nhist(X, probability=TRUE)\nlines(x,y, col=\"red\")\n\n\n\n\n\\(f(X)=\\frac38x^2\\) over \\([0,2]\\)\n\nU &lt;- runif(10000)\nX &lt;- (8*U)^(1/3)\nhist(X, probability=TRUE)\nlines(x=seq(0, 2, .1), y=3/8*seq(0, 2, .1)^2, col=\"red\")\n\n\n\n\n\n\n9.6.5 Inverse CDF Trick\nHere is a density function that doesn’t have a name The support is from 0 to 2\n\\(f(x) = \\frac38 x^2\\)\n\\(F(x) = \\frac18 x^3\\)\n\\(F^{-1}(x) = \\sqrt[3]{8x}\\)\n\nU &lt;- runif(10000, min=0, max=1)\nX &lt;- (8*U)^(1/3)\n\nhist(X, probability=TRUE)\nlines(x =seq(0,2,.1), y=3/8*(seq(0,2,.1)^2), col=\"red\")"
  },
  {
    "objectID": "R_MonteCarlo_Battleship.html",
    "href": "R_MonteCarlo_Battleship.html",
    "title": "12  Monte Carlo Battleship",
    "section": "",
    "text": "This code will demonstrate how you can use Monte Carlo Simulation to create an AI opponent in Battleship.\nFirst a few function that will be used throughout.\n\n# constants representing hits and misses.\nHIT &lt;- 9; MISS &lt;- -9\n\nplaceShip &lt;- function(board, shipIndex){\n  isLegal &lt;- FALSE\n  giveUp &lt;- 0\n  while(!isLegal & giveUp &lt; 100){\n    giveUp &lt;- giveUp+1\n    orientation = sample(2,1) #upright or across\n    if(orientation==2) {board &lt;- t(board)}\n    shipLength = shipSizes[shipIndex]\n    shipX = sample(1:(dim(board)[1]-shipLength+1),1)\n    shipY = sample(1:(dim(board)[2]),1)\n    isLegal = prod(board[shipX:(shipX+shipLength-1), shipY] %in% c(0,HIT))\n    if(isLegal){\n      board[shipX:(shipX+shipLength-1), shipY] &lt;- shipIndex\n    }\n    if(orientation==2) {board &lt;- t(board)}\n  }\n  if(isLegal){\n    return(board)\n  } else{\n    return(FALSE)\n  }\n}\n\nplaceAllShips &lt;- function(board){\n  for(i in which(shipAlive)){\n    board &lt;- placeShip(board,i)\n    if(!is.matrix(board)) return(FALSE)\n  }\n  if(sum(board==HIT)==0){\n    return(board)\n  } else {\n    return(FALSE)\n  }\n}\n\nbuildHeatMap &lt;- function(boardKnowledge, NMC=10000){\n  boardMap &lt;- matrix(data=0, nrow=boardDim[1], ncol=boardDim[2])\n  for(i in 1:NMC){\n    placeAll &lt;- placeAllShips(boardKnowledge) \n    if(is.matrix(placeAll)){\n      boardMap &lt;- boardMap + (placeAll %in% (1:length(shipAlive))[shipAlive])\n    }\n  }\n  return(boardMap)\n}\n\nguessACoord &lt;- function(boardKnowledge, NMC=10000){\n  hmap &lt;- buildHeatMap(boardKnowledge, NMC)\n  #remove known hits\n  hmap[which(boardKnowledge==HIT)] &lt;- 0\n  \n  guessIndex &lt;- which(hmap == max(hmap))[1]\n  guessX &lt;- guessIndex %% boardDim[1]\n  if(guessX == 0) {guessX &lt;- 10}\n  guessY &lt;- floor((guessIndex-1) / boardDim[2])+1\n  return(list(c(guessX,guessY), hmap))\n}\n\nresultOfGuess &lt;- function(guess){\n  x &lt;- guess[1]; y&lt;- guess[2]; returnstr &lt;- \"\"\n  if(trueBoard[x,y] == 0){\n    boardKnowledge[x,y] &lt;&lt;- MISS\n    returnstr &lt;- \"Miss\"\n  } else{\n    #Hit\n    returnstr &lt;-\"Hit!\"\n    boatIndex &lt;- abs(trueBoard[x,y])\n    trueBoard[x,y] &lt;&lt;- trueBoard[x,y] * -1\n    boardKnowledge[x,y] &lt;&lt;- HIT\n    if(sum(trueBoard == boatIndex)==0){\n      #sunk it\n      boardKnowledge[which(trueBoard==(-boatIndex))] &lt;&lt;- -boatIndex\n      returnstr &lt;- paste(returnstr,\" Sunk Boat \",boatIndex,\".\",sep=\"\")\n      shipAlive[boatIndex] &lt;&lt;- FALSE\n    } \n    if(sum(trueBoard&gt;0)==0){\n      #All Boats Sunk\n      returnstr &lt;- paste(returnstr, \"You Lose\")\n    }\n  }\n  return(returnstr)\n}\n\nTest it out. First the setup.\n\nlibrary(fields)\n#Define board\nboardDim=c(10,10)\n#Define ship sizes\nshipSizes=c(5,4,3,3,2)\n#Initialize ship states\nshipAlive = rep(TRUE, length(shipSizes))\n\n#initializeKnowledge\n# 0 unknown\n# 1,2,3, etc hit boat\nboardKnowledge=matrix(data=0, nrow=boardDim[1], ncol=boardDim[2])\ntrueBoard &lt;- placeAllShips(boardKnowledge)\n\npar(mfrow=c(1,2), mar=c(.5, 3, 4, 0))\nimage(trueBoard, main=\"Ship Locations\", col=tim.colors(),yaxt=\"n\",xaxt=\"n\"); \n  axis(3, at=seq(0,1, length.out=10), labels=LETTERS[1:10], lwd=0, pos=1, cex.axis=.75)\n  axis(2, at=seq(0,1, length.out=10), labels=10:1, lwd=0, pos=0, cex.axis=.75); \nimage(boardKnowledge, main=\"Comp Knowledge\", col=tim.colors(),yaxt=\"n\",xaxt=\"n\"); \n  axis(3, at=seq(0,1, length.out=10), labels=LETTERS[1:10], lwd=0, pos=1, cex.axis=.75)\n  axis(2, at=seq(0,1, length.out=10), labels=10:1, lwd=0, pos=0, cex.axis=.75)\n\n\n\n\nPlay the Game!!!\n\nturn &lt;- 0\nwhile(sum(shipAlive)&gt;0 | turn &lt; 9){\n  turn &lt;- turn +1\n  guessList &lt;- guessACoord(boardKnowledge,1000)\n  guess &lt;- guessList[[1]]\n  guessResult &lt;- resultOfGuess(guess)\n  print(paste(\"Turn \",turn,\": Computer Guesses \",LETTERS[guess[1]],11-guess[2],\": \",guessResult, sep=\"\"))\n  \n  flush.console()\n  par(mfrow=c(1,3), mar=c(.5, 3, 3, 0))\n  image(guessList[[2]], main=\"Heatmap\", col=tim.colors(),yaxt=\"n\",xaxt=\"n\"); \n  axis(3, at=seq(0,1, length.out=10), labels=LETTERS[1:10], lwd=0, pos=1)\n  axis(2, at=seq(0,1, length.out=10), labels=10:1, lwd=0, pos=0)\n  image(trueBoard, main=\"My Board\", col=tim.colors(),yaxt=\"n\",xaxt=\"n\");   \n  axis(3, at=seq(0,1, length.out=10), labels=LETTERS[1:10], lwd=0, pos=1)\n  axis(2, at=seq(0,1, length.out=10), labels=10:1, lwd=0, pos=0)\n  image(boardKnowledge, main=\"Comp Knowledge\", col=tim.colors(),yaxt=\"n\",xaxt=\"n\")\n  axis(3, at=seq(0,1, length.out=10), labels=LETTERS[1:10], lwd=0, pos=1)\n  axis(2, at=seq(0,1, length.out=10), labels=10:1, lwd=0, pos=0)\n}\n\n[1] \"Turn 1: Computer Guesses E5: Miss\"\n\n\n\n\n\n[1] \"Turn 2: Computer Guesses D6: Miss\"\n\n\n\n\n\n[1] \"Turn 3: Computer Guesses F4: Miss\"\n\n\n\n\n\n[1] \"Turn 4: Computer Guesses F8: Hit!\"\n\n\n\n\n\n[1] \"Turn 5: Computer Guesses E8: Hit!\"\n\n\n\n\n\n[1] \"Turn 6: Computer Guesses D8: Hit!\"\n\n\n\n\n\n[1] \"Turn 7: Computer Guesses C8: Miss\"\n\n\n\n\n\n[1] \"Turn 8: Computer Guesses G8: Hit! Sunk Boat 2.\"\n\n\n\n\n\n[1] \"Turn 9: Computer Guesses G3: Hit!\"\n\n\n\n\n\n[1] \"Turn 10: Computer Guesses F3: Miss\"\n\n\n\n\n\n[1] \"Turn 11: Computer Guesses G4: Hit!\"\n\n\n\n\n\n[1] \"Turn 12: Computer Guesses G5: Hit!\"\n\n\n\n\n\n[1] \"Turn 13: Computer Guesses G2: Hit!\"\n\n\n\n\n\n[1] \"Turn 14: Computer Guesses G1: Hit! Sunk Boat 1.\"\n\n\n\n\n\n[1] \"Turn 15: Computer Guesses H7: Miss\"\n\n\n\n\n\n[1] \"Turn 16: Computer Guesses C4: Miss\"\n\n\n\n\n\n[1] \"Turn 17: Computer Guesses I6: Miss\"\n\n\n\n\n\n[1] \"Turn 18: Computer Guesses B3: Miss\"\n\n\n\n\n\n[1] \"Turn 19: Computer Guesses D2: Miss\"\n\n\n\n\n\n[1] \"Turn 20: Computer Guesses B7: Hit!\"\n\n\n\n\n\n[1] \"Turn 21: Computer Guesses B6: Hit!\"\n\n\n\n\n\n[1] \"Turn 22: Computer Guesses B5: Miss\"\n\n\n\n\n\n[1] \"Turn 23: Computer Guesses B8: Hit! Sunk Boat 4.\"\n\n\n\n\n\n[1] \"Turn 24: Computer Guesses H10: Miss\"\n\n\n\n\n\n[1] \"Turn 25: Computer Guesses I9: Hit!\"\n\n\n\n\n\n[1] \"Turn 26: Computer Guesses I8: Hit!\"\n\n\n\n\n\n[1] \"Turn 27: Computer Guesses I10: Miss\"\n\n\n\n\n\n[1] \"Turn 28: Computer Guesses I7: Hit! Sunk Boat 3.\"\n\n\n\n\n\n[1] \"Turn 29: Computer Guesses I2: Miss\"\n\n\n\n\n\n[1] \"Turn 30: Computer Guesses F6: Miss\"\n\n\n\n\n\n[1] \"Turn 31: Computer Guesses A4: Miss\"\n\n\n\n\n\n[1] \"Turn 32: Computer Guesses B1: Miss\"\n\n\n\n\n\n[1] \"Turn 33: Computer Guesses I4: Miss\"\n\n\n\n\n\n[1] \"Turn 34: Computer Guesses E2: Miss\"\n\n\n\n\n\n[1] \"Turn 35: Computer Guesses D9: Miss\"\n\n\n\n\n\n[1] \"Turn 36: Computer Guesses D3: Miss\"\n\n\n\n\n\n[1] \"Turn 37: Computer Guesses C2: Miss\"\n\n\n\n\n\n[1] \"Turn 38: Computer Guesses J3: Miss\"\n\n\n\n\n\n[1] \"Turn 39: Computer Guesses E10: Miss\"\n\n\n\n\n\n[1] \"Turn 40: Computer Guesses B10: Miss\"\n\n\n\n\n\n[1] \"Turn 41: Computer Guesses E7: Miss\"\n\n\n\n\n\n[1] \"Turn 42: Computer Guesses H3: Miss\"\n\n\n\n\n\n[1] \"Turn 43: Computer Guesses A9: Hit!\"\n\n\n\n\n\n[1] \"Turn 44: Computer Guesses B9: Hit! Sunk Boat 5. You Lose\""
  },
  {
    "objectID": "mc_practice.html",
    "href": "mc_practice.html",
    "title": "11  Monte Carlo Practice",
    "section": "",
    "text": "12 Practice Problems",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#integration-by-darts",
    "href": "mc_practice.html#integration-by-darts",
    "title": "11  Monte Carlo Practice",
    "section": "12.1 Integration by darts",
    "text": "12.1 Integration by darts\nEstimate \\(\\int_0^3 (x^3-3x^2+x+3)\\,dx\\). To do so, use the fact that \\[\\int_a^b g(x)dx = \\int_a^b \\frac{g(x)}{f(x)}f(x)dx\\]. Let \\(f(x)\\) be the density function for a uniform random variable over \\([0,3]\\).\n\nf&lt;-function(x){return(x^3-3*x^2+x+3)}\nplot(x=seq(0,3,.1), f(seq(0,3,.1)), type=\"l\", ylim=c(0,7), xlab=\"x\", ylab=\"y\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#Solution\n\n#We can define a uniform density function between 0 and 3\n#call this g(x)=dunif(x,0,3)\n#The integral trick says that int_0^7 f(x)dx = E(f(x)/g(x)).\nNMC &lt;- 1000\nx &lt;- runif(NMC, 0,3)\nmean(f(x)/dunif(x,0,3))\n\n[1] 6.816204",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#eulers-constant",
    "href": "mc_practice.html#eulers-constant",
    "title": "11  Monte Carlo Practice",
    "section": "12.2 Euler’s Constant",
    "text": "12.2 Euler’s Constant\nSuppose \\(X_1, X_2,\\ldots, X_n \\sim Unif(0,1)\\). Let \\(N\\) be the lowest index such that \\(X_1+X_2+\\cdots+X_N &gt; 1\\). \\(\\mathbb{E}(N)=e\\). Use this fact to estimate Euler’s constant \\(e\\) using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 1000\nNs &lt;- 0\nfor(i in 1:NMC){\n  sum &lt;- 0;   N &lt;- 0\n  while(sum &lt; 1){\n    sum &lt;- sum + runif(1)\n    N &lt;- N+1\n  }\n  Ns[i] = N\n}\nmean(Ns)\n\n[1] 2.753\n\nexp(1)\n\n[1] 2.718282",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#dice-probability",
    "href": "mc_practice.html#dice-probability",
    "title": "11  Monte Carlo Practice",
    "section": "12.3 Dice Probability",
    "text": "12.3 Dice Probability\nI roll 3 six sided dice. What is the probability that the sum of the dice is at least 12? Estimate the answer with Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\nresults &lt;- 0\nfor(i in 1:NMC){\n  results[i] &lt;- sum(sample(6, 3, replace=TRUE))&gt;=12\n}\nmean(results)\n\n[1] 0.37",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#sales-and-price",
    "href": "mc_practice.html#sales-and-price",
    "title": "11  Monte Carlo Practice",
    "section": "12.4 Sales and Price",
    "text": "12.4 Sales and Price\nBased on your market research, you believe that there are equal chances that the market will be Slow, OK, or Hot.\n\nIn the “Slow market” scenario, you expect to sell 50,000 units at an average selling price of $11.00 per unit.\nIn the “OK market” scenario, you expect to sell 75,000 units, but you’ll likely realize a lower average selling price of $10.00 per unit.\nIn the “Hot market” scenario, you expect to sell 100,000 units, but this will bring in competitors who will drive down the average selling price to $8.00 per unit.\n\nAnother uncertain variable is Unit Cost.B Your firm’s production manager advises you that unit costs may be anywhere from $5.50 to $7.50. Use a Monte Carlo simulation to estimate the expected net profit (revenue - cost).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\nprofit &lt;- 0\nprices &lt;- c(11, 10, 8)\nquantity &lt;- c(50000, 75000, 100000)\nfor(i in 1:NMC){\n  market &lt;- sample(1:3, 1)\n  unitCost &lt;- runif(1, 5.5, 7.5)\n  profit[i] &lt;- quantity[market]*(prices[market]-unitCost)\n}\nmean(profit)\n\n[1] 216237.2",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#black-scholes-option-pricing",
    "href": "mc_practice.html#black-scholes-option-pricing",
    "title": "11  Monte Carlo Practice",
    "section": "12.5 Black-Scholes Option Pricing",
    "text": "12.5 Black-Scholes Option Pricing\nWe start with the Black-Scholes-Merton formula (\\(1973\\)) for the pricing of European call options on an underlying (e.g. stocks and indexes) without dividends:\n\\(\\begin{eqnarray*} C(S_t, K, t, T, r, \\sigma) &=& S_t\\cdot N(d_1) - e^{-r(T-t)}\\cdot K \\cdot N(d_2)\\newline\\newline N(d) &=& \\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^d e^{-\\frac{1}{2}x^2}dx \\newline\\newline d_1 &=& \\frac{\\log\\frac{S_t}{K} + (T-t)\\left(r + \\frac{\\sigma^2}{2}\\right)}{\\sigma\\sqrt{T-t}}\\newline\\newline d_2 &=& \\frac{\\log\\frac{S_t}{K} + (T-t)\\left(r - \\frac{\\sigma^2}{2}\\right)}{\\sigma\\sqrt{T-t}}. \\end{eqnarray*}\\)\nIn the equations above \\(S_t\\) is the price of the underlying at time \\(t\\), \\(\\sigma\\) is the constant volatility (standard deviation of returns) of the underlying, \\(K\\) is the strike price of the option, \\(T\\) is the maturity date of the option, \\(r\\) is the risk-free short rate.\nThe Black-Scholes-Merton (\\(1973\\)) stochastic differential equation is given by \\(dS_t = rS_t dt + \\sigma S_t dZ_t,\\) where $Z(t)$ is the random component of the model (a Brownian motion). In this model, the risky underlying follows, under risk neutrality, a geometric Brownian motion with a stochastic differential equation (SDE).\nWe will look at the discretized version of the BSM model (Euler discretization), given by \\(S_t = S_{t-\\Delta t} \\exp\\left(\\left(r - \\frac{\\sigma^2}{2}\\right)\\Delta t + \\sigma\\sqrt{\\Delta t}z_t \\right).\\)\nThe variable \\(z\\) is a standard normally distributed random variable, \\(0 &lt; \\Delta t &lt; T\\), a (small enough) time interval. It also holds \\(0 &lt; t \\leq T\\) with \\(T\\) the final time horizon.\nIn this simulation we use the values \\(S_0 = 100\\), \\(K = 105\\), \\(T = 1.0\\), \\(r = 0.05\\), \\(\\sigma = 0.2\\). Let’s see what is the expected option price using these parameters and assuming \\(t=0\\), then we will run a Monte Carlo simulation to find the option price under the same conditions.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nS_0 &lt;- 100; K &lt;- 105; endT &lt;- 1.0; r &lt;- 0.05; sigma &lt;- 0.2;\ndt &lt;- 0.01\n\nST &lt;- 0 #vector to hold values\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  St &lt;- S_0\n  t &lt;- 0\n  while(t &lt; endT){\n    St &lt;- St*exp((r-sigma^2/2)*dt + sigma*sqrt(dt)*rnorm(1))\n    t &lt;- t+dt\n  }\n  ST[i] &lt;- St\n}\nhist(ST)\n\n\n\n\n\n\n\nmean(ST)\n\n[1] 105.5639",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-sphere",
    "href": "mc_practice.html#average-distance-in-a-sphere",
    "title": "11  Monte Carlo Practice",
    "section": "12.6 Average Distance in a sphere",
    "text": "12.6 Average Distance in a sphere\nEstimate the average distance between two points in a sphere of radius 1 using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\nfor(i in 1:NMC){\n  #get a random point\n  repeat{\ncoord1 &lt;- runif(2, -1,1)\nif(sqrt(sum(coord1^2))&lt;=1){\n  break\n}\n  }\n  #get another random point\n  repeat{\ncoord2 &lt;- runif(2, -1,1)\nif(sqrt(sum(coord2^2))&lt;=1){\n  break\n}\n  }\n  dist[i] = sqrt(sum((coord2-coord1)^2))\n}\nmean(dist)\n\n[1] 0.8978539",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-square",
    "href": "mc_practice.html#average-distance-in-a-square",
    "title": "11  Monte Carlo Practice",
    "section": "12.7 Average distance in a square",
    "text": "12.7 Average distance in a square\nEstimate the average distance between two points in a square with side lengths 1 using Monte Carlo simulation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\nfor(i in 1:NMC){\n  coord1 &lt;- runif(2, 0, 1)\n  coord2 &lt;- runif(2, 0, 1)\n  dist[i] = sqrt(sum((coord2-coord1)^2))\n}\nmean(dist)\n\n[1] 0.491389",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#average-distance-in-a-simplex",
    "href": "mc_practice.html#average-distance-in-a-simplex",
    "title": "11  Monte Carlo Practice",
    "section": "12.8 Average distance in a simplex",
    "text": "12.8 Average distance in a simplex\nEstimate the average distance between two points on the 3-simplex (points \\((x,y,z)\\) such that \\(0\\leq x,y,z \\leq 1\\) and \\(x+y+z=1\\).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\ndist &lt;- 0\n\nfor(i in 1:NMC){\n  \n#A good way to think of the sampling is to pick two random dividing points between 0 and 1. Add 0 and 1 to the list and then sort them, then find the differences\n  pt1 &lt;- diff(sort(c(0,1,runif(2))))\n  pt2 &lt;- diff(sort(c(0,1,runif(2))))\n\n  dist[i] &lt;- sqrt(sum((pt1-pt2)^2))\n}\nmean(dist)\n\n[1] 0.4875664",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#other-examples",
    "href": "mc_practice.html#other-examples",
    "title": "11  Monte Carlo Practice",
    "section": "12.18 Other examples",
    "text": "12.18 Other examples\nCheck out some examples here",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#completion-time",
    "href": "mc_practice.html#completion-time",
    "title": "11  Monte Carlo Practice",
    "section": "12.9 Completion Time",
    "text": "12.9 Completion Time\nLet’s assume we have a process constructed from 3 stages (X1, X2, X3). Each one has an average duration (5, 10 and 15 minutes) which vary following the normal distribution and we know their standard deviation (all 1 minute). We want to know what is the probability that the process will exceed 34 minutes?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC&lt;-10000\nduration &lt;- rnorm(NMC, 5, 1)+rnorm(NMC, 10, 1) + rnorm(NMC, 15, 1)\nmean(duration &gt; 34)\n\n[1] 0.01",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#fitting-blocks",
    "href": "mc_practice.html#fitting-blocks",
    "title": "11  Monte Carlo Practice",
    "section": "12.10 Fitting Blocks",
    "text": "12.10 Fitting Blocks\nIn this example let’s assume we want to assemble three blocks inside a container of a given width. The box has a nominal width of 16.5mm, the three blocks have nominal widths of 4, 6 and 6 mm. By design there is a nominal gap of 0.5mm. However there are variations in the production of the blocks.\n\nThe box has variation in width between -0.1mm to +0.1mm uniformly at random\nThe other boxes have margins of error of 0.2, 0.3 and 0.25 respectively (+ or - uniformly at random)\n\nEstimate the probability that the 3 blocks will fit in the box.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 100\n\nbox.w &lt;- runif(NMC, 16.5-.1, 16.5+.1)\nblock1.w &lt;- runif(NMC, 4-.2, 4+.2)\nblock2.w &lt;- runif(NMC, 6-.3, 6+.3)\nblock3.w &lt;- runif(NMC, 6-.25, 6+.25)\n\nfits &lt;- box.w &gt; block1.w+block2.w+block3.w\nmean(fits)\n\n[1] 0.98",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#roomba-zoomba",
    "href": "mc_practice.html#roomba-zoomba",
    "title": "11  Monte Carlo Practice",
    "section": "12.11 Roomba Zoomba",
    "text": "12.11 Roomba Zoomba\nA circular vacuum robot that has a radius of 20cm will roll around a room that is 10m x 10m square. It begins in the exact center of the room. The way it moves is:\n\nIt picks a direction uniformly at random from 0 to 360 degrees (\\(0\\) to \\(2\\pi\\))\nIt travels for anywhere between 0m and 1m, uniformly at random.\nIf it hits a wall it bounces off at the angle of incidence.\nIt repeats\n\n\nIf there is a speck of dirt located at coordinates (1,1), what is the average amount of time it will take for the robot to collect this dirt?\nIf the robot travels at 1m/sec, what is the expected length of time for the robot to clean 50% of the floor?\n\nSolution in-progress!\n\nx &lt;- 5; y&lt;- 5\n\nmaxN &lt;- 1000\n\nfor(step in 1:maxN){\n  theta &lt;- runif(1, 0, 2*pi)\n  dist &lt;- runif(1,0,1)\n  xy_new &lt;- c(x[step],y[step]) + dist*c(cos(theta), sin(theta))\n  if(xy_new[1]&lt;.2){xy_new[1]=2*.2-xy_new[1]}\n  if(xy_new[2]&lt;.2){xy_new[2]=2*.2-xy_new[2]}\n  if(xy_new[1]&gt;9.8){xy_new[1]=2*9.8-xy_new[1]}\n  if(xy_new[2]&gt;9.8){xy_new[2]=2*9.8-xy_new[2]}\n  x[step+1] &lt;- xy_new[1]\n  y[step+1] &lt;- xy_new[2]\n}\nplot(x,y, type=\"l\", xlim=c(0,10), ylim=c(0,10))\nabline(h=c(0,10), col=\"red\")\nabline(v=c(0,10), col=\"red\")",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#chutes-and-ladders",
    "href": "mc_practice.html#chutes-and-ladders",
    "title": "11  Monte Carlo Practice",
    "section": "12.12 Chutes and Ladders",
    "text": "12.12 Chutes and Ladders\nConsider the game Chutes and Ladders. On each turn you roll a 6 sided die and move that many spaces. If you land on a ladder you move up to a new spot, and if you land on a slide you move down. Perform a monte Carlo simulation to answer the following questions:\n\n\nWhat’s the average number of rolls to win?\nHow many chutes and ladders will a user typically hit?\nAre the chutes and ladders balanced?\nAre some squares hit more often than others?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#We need to create a vector of the board\nboard &lt;- 1:100\n#ladders\nboard[1]&lt;- 38\nboard[4] &lt;- 14\nboard[9] &lt;- 31\nboard[21] &lt;- 42\nboard[36] &lt;- 44\nboard[28] &lt;- 84\nboard[51] &lt;- 67\nboard[80] &lt;- 100\nboard[71] &lt;- 91\n#chutes\nboard[16] &lt;- 6\nboard[49] &lt;- 11\nboard[48] &lt;- 26\nboard[62] &lt;- 19\nboard[87] &lt;- 24\nboard[56] &lt;- 53\nboard[64] &lt;- 60\nboard[93] &lt;- 73\nboard[95] &lt;- 75\nboard[98] &lt;- 78\n\nNMC &lt;- 100\nset.seed(1)\n\nrolls &lt;- rep(0, NMC)\nnChutes &lt;- rep(0, NMC)\nnLadders &lt;- rep(0,NMC)\nlandOn &lt;- matrix(rep(0, NMC*100),nrow=NMC)\n\nfor(i in 1:NMC){\n  position &lt;- 1\n  repeat{\nlandOn[i,position] &lt;- landOn[i,position] + 1\nif(position==100){break;}\nroll &lt;- sample(6,1)\nrolls[i] &lt;- rolls[i] + 1\nposition.new &lt;- min(100,position+roll)\nif(board[position.new] &lt; position.new){\n  nChutes[i] &lt;- nChutes[i] + 1\n}\nif(board[position.new] &gt; position.new){\n  nLadders[i] &lt;- nLadders[i] +1\n}\nposition &lt;- board[position.new]\n  }\n}\n\nmean(rolls)\n\n[1] 34.66\n\nmean(nChutes)\n\n[1] 3.93\n\nmean(nLadders)\n\n[1] 3.08\n\nplot(colMeans(landOn))\n\n\n\n\n\n\n\nlandOnDF &lt;- data.frame(space=1:100, proportion &lt;- colMeans(landOn))\ntail(landOnDF[order(landOnDF[,2]),],10)\n\n    space proportion....colMeans.landOn.\n42     42                           0.68\n31     31                           0.83\n24     24                           0.84\n6       6                           0.86\n11     11                           0.86\n26     26                           0.86\n44     44                           0.93\n1       1                           1.00\n100   100                           1.00\n84     84                           1.02\n\n\nIt takes an average of 36.78 turns to finish the game The average number of chutes slid down is 4 per game. The average number of ladders climbed is 3.22 per game. Chutes are more likely than ladders apparently; even though there are 10 chutes compared to 9 ladders, the expected number of chutes is much higher than ladders.\nThe most common spots (besides 1 and 100) are 44 and 26.\n44 is the result of landing on one of the ladders. 26 is the result of landing on one of the chutes.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#battleship",
    "href": "mc_practice.html#battleship",
    "title": "11  Monte Carlo Practice",
    "section": "12.13 Battleship",
    "text": "12.13 Battleship\nBuild a Battleship AI (https://github.com/mitchelljy/battleships_ai)\n(Rules of the game: https://www.hasbro.com/common/instruct/battleship.pdf)\nBattleship is a game played on 10x10 grid. There are 5 boats of lengths 2, 3, 3, 4, and 5. The Monte Carlo AI is built like this:\n\nTake current board state\nSimulate \\(N\\) samples, each is a random placement of a remaining ships (knowing where hits and misses have been observed.) . Be sure to not put boats in a space that has been guessed, and if a hit has been found, be sure that boats cover those spots.\nStack all of the simulations and sum the total number of ships in each square (emphasise ships that overlap existing hits)\nTake the mean for each sqaure, giving us a frequency matrix or heatmap\nPick the largest value corresponding to a legal move in the matrix\n\nThis is how the AI decides which square to guess at every state of the game. As an added challenge, you can create additional logic to have the computer make the guess - taking the spot that has the highest likelihood - and determine if it is a hit, or if it sinks a boat. You will need to initialize the board with a random arrangement of the boats.\n\nYou will need a function generate_board &lt;- function(width=10, height=10, boats=c(TRUE,TRUE,TRUE,TRUE,TRUE)). This function should randomly place boats on the board, each either vertically or horizontally, and return The layout. Number the boats 1,2,3,4,5. Be sure that numbers do not overlap each other when placed randomly. This function should return a matrix with values 0 if there is nothing placed in a spot, or the boat number. For example, when you place boat 5, it is 5 units long, so there should be 5 spots with the number 5 in them (And they should be in a straight line).\nYou will also need a function validate_board &lt;- function(board, state) which will check to see if the simulated board is legal given the state. The state should have 0s in locations that have not been tested, -1 in locations where misses have been numbers 1 through 5 where hits have been observed. If the number of \\(i\\)’s on the board equals the number of pegs in boat \\(i\\), then the AI will now that boat \\(i\\) has been sunk - you can convert all of these \\(i\\)’s into -1 so they will be ignored, and in the future we can dispance with randomly placing boat \\(i\\). A board is valid if there are no boats in a -1 spot, and any spot in an \\(i\\) spot contains boat \\(i\\).\nTo determine the next move you will need to generate boards until you have had at least 1 valid board (10 is better) to make a choice. If many spots have the same frequency, then you can just pick one at random, or pick the one with the lowest row and column.\n\nSee an implementation here",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#stock-price",
    "href": "mc_practice.html#stock-price",
    "title": "11  Monte Carlo Practice",
    "section": "12.15 Stock Price",
    "text": "12.15 Stock Price\nSuppose a stock price varies from day to day through a scaled process. If the stock price is \\(X_i\\) on day \\(i\\), \\(X_{i+1} = (1.01+Z/25) X_i\\) where \\(Z\\) is a standard normal random variable. Today the stock price is $50 per share. Use a Monte Carlo simulation to estimate the distribution of stock prices 30 days from now.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNMC &lt;- 1000\n  plot(NA, xlim=c(1,31), ylim=c(0,100))\n  finalPrice &lt;- 0\nfor(i in 1:NMC){\n  price &lt;- 50\n  for(day in 1:30){\nprice[day+1] &lt;- price[day] * (1.01 + rnorm(1, 0, 1)/25)\n  }\n  finalPrice[i] &lt;- price[31]\n  lines(x=1:31, price, col=rgb(0,0,0,.15))\n}\n\n\n\n\n\n\n\nmean(finalPrice)\n\n[1] 67.90084\n\nhist(finalPrice, prob=TRUE)\nlines(density(finalPrice))\n\n\n\n\n\n\n\n\n\n\n\nYou can buy a 30 day put option for $10 with a strike price of $65 from Sleazy Jim, the stock broker. This terms of this contract are these:\n\nYou pay $10 to Sleazy Jim today.\nIf after 30 days the stock price is below $65, you will buy a share of stock at the market price and Sleazy Jim will buy it from you for $65. You pocket the difference.\nIf after 30 days the stock price is above $65, you will do nothing.\n\n(In essence, Sleazy Jim is betting that the stock price will be higher than $65 after 30 days.)\nEven though interest rates are high now, let’s ignore the affect of interest rate. Using Monte Carlo simulation, estimate the expected value of this put option (the net profit) to determine whether it is a good idea to buy the put option.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ngain &lt;- 65 - pmin(finalPrice, 65)\ncost &lt;- 10\n\nmean(gain - 10)\n\n[1] -5.383399\n\n\nIf you enter into this trade with Sleazy Jim, you have an expected loss of $5.53. Don’t make the trade!",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#patent-pending",
    "href": "mc_practice.html#patent-pending",
    "title": "11  Monte Carlo Practice",
    "section": "12.16 Patent Pending",
    "text": "12.16 Patent Pending\nYou have decided to apply for a patent to protect your IP, but you also did that in order to increase sales as you are aware that businesses deem a patented product more worthy.\nLet’s make the following assumptions:\n\nThere is a 50% chance that your product gets patented\nIf it does get patented, your sales go up by 25% — 75%, with 50% being the most likely case. (uniform distribution)\nWithout a patent you expect to sell between $1 — $9 million next year, with $3 million being the most likely case. Let the probabilities be given in the following table\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSales ($M)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nprobability\n1/33\n4/33\n7/33\n6/33\n5/33\n4/33\n3/33\n2/33\n1/33\n\n\n\n\nWe do not have to consider any costs or expenses\n\nProblem: Suppose a wholesaler offers to buy your entire production and inventory for the year for $6 million (you won’t be able to sell anything else), would you accept the offer?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsales.p &lt;- c(1,4,7,6,5,4,3,2,1)/33\nsales.v &lt;- 1:9\n\nrevenue &lt;- 0\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  getPatent &lt;- sample(0:1, size=1, prob=c(.5,.5))\n  sales.scalar &lt;- 1\n  if(getPatent){\nsales.scalar &lt;- 1 + runif(1, .25, .75)\n  }\n  revenue[i] &lt;- sample(sales.v, size=1, prob=sales.p) * sales.scalar\n}\nhist(revenue)\n\n\n\n\n\n\n\nmean(revenue)\n\n[1] 5.401985\n\n\nThe expected revenue is 5.75 million. The offer of 6 million is an attractive offer. I would go for that.",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#social-network",
    "href": "mc_practice.html#social-network",
    "title": "11  Monte Carlo Practice",
    "section": "12.17 Social Network",
    "text": "12.17 Social Network\nConsider a social network with people \\(1,2,\\ldots, n\\). Suppose we have a symmetric social structure, in that if person \\(i\\) is friends with person \\(j\\), then person \\(j\\) is friends with person \\(i\\). Thus we can define a symmetric friendship matrix with entries \\(p_{ij}=p_{ji}\\).\nSuppose that each pair of people can be friends independently with probability \\(\\theta\\). The question we want to answer here is this: How big must \\(\\theta\\) be in order to be more than 95\\% certain that the entire social network will be connected - this means that every two people will be connected either directly or through one or more intermediate friends.\nYour goal is to write a function estimateProportionConnected(n,theta, NMC), which takes three parameters:\n\nn, the number of people in the network,\ntheta, the probability of a connection,\nNMC, the number of Monte Carlo simulations to run.\n\nThe function should\n\nCreate an empty vector to store the results (whether or not each simulation resulted in a connected network)\nIn a loop,\n\n\ngenerate a random symmetric social network of size n (a function generateNetwork(n, theta) will be useful)\nDetermine whether the network is connected or not\nSave the result in your results vector\n\n\nReturn the proportion of simulations that resulted in a connected network.\n\nHint: You can use the function concomFromMatAdj in the concom package. It takes a symmetric adjacency matrix and returns the connected components. In particular, you want to look at the number of connected components, in the ncomponents variable.\nGiven a matrix M, you just call concomFromMatAdj(M)$ncomponents\nIf the number of components is more than 1 then the social network is not connected.\nBonus: For a social network of size \\(50\\), come up with an estimate of the lowest value of \\(\\theta\\) that produces connected social networks 95% of the time.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n#install.packages(\"concom\")\nlibrary(concom)\n\nWarning: package 'concom' was built under R version 4.3.3\n\ngenerateMatrix &lt;- function(n, theta){\n  M &lt;- matrix(data=rep(0, n*n), nrow=n)\n  for(i in 1:(n-1)){\nfor(j in 2:n){\n  if(runif(1) &lt; theta){\nM[i,j]=1\nM[j,i]=1\n  }\n}\n  }\n  return(M)\n}\n\nestimateProportionConnected &lt;- function(n,theta, NMC){\n  results &lt;- 0\n  for(i in 1:NMC){\nM &lt;- generateMatrix(n, theta)\nresults[i] &lt;- concomFromMatAdj(M)$ncomponents==1\n  }\n  return(mean(results))\n}\n\nfor(theta in seq(.08,.07,by=-.001)){\n  p &lt;- estimateProportionConnected(50, theta, 500)\n  print(paste(\"When theta =\",theta,\"connected with prob\",p))\n}\n\n[1] \"When theta = 0.08 connected with prob 0.956\"\n[1] \"When theta = 0.079 connected with prob 0.966\"\n[1] \"When theta = 0.078 connected with prob 0.958\"\n[1] \"When theta = 0.077 connected with prob 0.968\"\n[1] \"When theta = 0.076 connected with prob 0.964\"\n[1] \"When theta = 0.075 connected with prob 0.956\"\n[1] \"When theta = 0.074 connected with prob 0.948\"\n[1] \"When theta = 0.073 connected with prob 0.934\"\n[1] \"When theta = 0.072 connected with prob 0.946\"\n[1] \"When theta = 0.071 connected with prob 0.924\"\n[1] \"When theta = 0.07 connected with prob 0.93\"",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "mc_practice.html#infection-spread-simulation-source",
    "href": "mc_practice.html#infection-spread-simulation-source",
    "title": "11  Monte Carlo Practice",
    "section": "12.14 Infection spread simulation source",
    "text": "12.14 Infection spread simulation source\nModel a city as a 1x1 square; The population is 2.5k, randomly located throughout the city. Pick one single individual to be infected.\nOn each time step, each individual walks in a random direction and moves .005 units (if it hits a wall, just place it at the edge of the city). After each time step, if an unaffected person is near an infected person (within .002) then the infection spreads to them, and they become infected.\n\nEstimate how many time steps it takes for 99% of the population to become infected.\nWhat if each person will become resistant after \\(X_i\\) time steps, where \\(X_i \\sim Geom(p)\\). Suppose \\(p=.5\\). This gives us the standard SIR model - individuals are susceptible, infected or resistant (either cured or dead). resistant individuals cannot become infected, and they cannot infect others. How does the esimate change?\nWhat if we change the population to 1000 individuals?\nWhat if we change the infection distance to be 0.004?\n\nSolution in progress",
    "crumbs": [
      "Sampling",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Monte Carlo Practice</span>"
    ]
  },
  {
    "objectID": "index.html#collection-of-shiny-apps",
    "href": "index.html#collection-of-shiny-apps",
    "title": "STAT 340: Data Science II",
    "section": "0.1 Collection of Shiny Apps",
    "text": "0.1 Collection of Shiny Apps\n\nNormal PDF CDF Inverse CDF \nBinomial One/Two Tail Test Power\nLogistic Model ROC Plot\nModel Selection Visualization"
  },
  {
    "objectID": "R04_Monte_Carlo_Testing.html#example-groundhogs-day",
    "href": "R04_Monte_Carlo_Testing.html#example-groundhogs-day",
    "title": "14  Monte Carlo Testing",
    "section": "14.1 Example: Groundhog’s Day",
    "text": "14.1 Example: Groundhog’s Day\nHow well has Punxsutawney Phil done in predicting the weather?\nThousands gather at Gobbler’s Knob in Punxsutawney, Pennsylvania, on the second day of February to await the spring forecast from a groundhog known as Punxsutawney Phil. According to legend, if Phil sees his shadow the United States is in store for six more weeks of winter weather. But, if Phil doesn’t see his shadow, the country should expect warmer temperatures and the arrival of an early spring.\nSource: https://www.kaggle.com/datasets/groundhogclub/groundhog-day?resource=download\n\ngroundhog&lt;-read.csv(\"data/archive.csv\")\n\n#Get rid of rows with no record or partial shadow. Let's be serious!\n\ngroundhog &lt;- subset(groundhog, groundhog$Punxsutawney.Phil %in% c(\"Full Shadow\",\"No Shadow\") & !is.na(groundhog$February.Average.Temperature))\n\nLet’s do a permutation test\n\\(H_0:\\) Phil’s not a true forecasting groundhog \\(H_1:\\) Phil has some forecasting power.\n\n14.1.1 Early Spring = February warmer than average\nWe will compare mean temperature in February as a measure of early spring. We will take those years of “early spring” prediction and those of “regular spring” and compare the average average Feb temperature in those two groups.\n\n#Isolate feb avg temperature for full shadow years\n#isolate feb avg temperature for no shadow years\nfeb.avg.shadow &lt;- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\nfeb.avg.noshadow &lt;- groundhog$February.Average.Temperature[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n#check\nfeb.avg.shadow\n\n  [1] 35.37 30.76 29.86 28.42 31.59 26.94 33.69 35.46 33.01 35.02 29.30 33.66\n [13] 31.08 29.50 29.52 35.94 33.03 30.09 33.82 32.25 33.69 36.90 31.95 29.57\n [25] 35.19 38.30 37.67 37.90 34.34 26.92 40.10 37.94 36.70 29.59 36.01 25.23\n [37] 31.73 36.00 29.98 33.85 33.53 34.50 35.28 34.88 31.41 31.62 31.91 34.54\n [49] 35.46 35.87 41.41 31.05 32.27 38.07 31.98 32.49 30.52 37.42 34.92 34.12\n [61] 31.77 32.52 31.24 32.88 32.92 32.81 33.48 33.55 33.58 34.74 39.56 36.03\n [73] 27.99 28.13 32.85 36.59 32.59 37.38 30.87 37.38 29.17 39.81 39.70 31.14\n [85] 31.87 35.69 38.71 39.78 33.98 36.39 32.79 33.57 37.94 34.83 34.70 36.77\n [97] 31.80 37.51 32.13 32.99\n\nfeb.avg.noshadow\n\n [1] 34.32 35.55 35.35 32.76 36.86 35.85 33.69 36.70 37.36 36.05 39.49 32.41\n[13] 33.04 34.77 39.47\n\n\n\n#We will use mean feb no shadow - mean feb shadow; if this &gt;0 that is evidence that the predictions work.\n\nmean(feb.avg.noshadow)-mean(feb.avg.shadow)\n\n[1] 1.8655\n\npermute_and_compute &lt;- function(sampleA, sampleB){\n  #remember - sampleA is no shadow\n  #sample B is shadow\n  pooledData &lt;- c(sampleA, sampleB)\n  shuffledData &lt;- sample(pooledData)\n  sim.sampleA &lt;- shuffledData[1:length(sampleA)]\n  sim.sampleB &lt;- shuffledData[(length(sampleA)+1):length(shuffledData)]\n  #we may modify this if we want to use a different test statistic\n  return(mean(sim.sampleA)-mean(sim.sampleB)) \n}\nt_obs &lt;- mean(feb.avg.noshadow)-mean(feb.avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(feb.avg.noshadow, feb.avg.shadow)\n}\nhist(test.stats)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0166\n\n\n\n\n14.1.2 Early Spring = warmer than average March temperatures\nMaybe he’s better at predicting March temperatures?\n\nmar.avg.shadow &lt;- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\nmar.avg.noshadow &lt;- groundhog$March.Average.Temperature[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(mar.avg.noshadow)-mean(mar.avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(mar.avg.noshadow, mar.avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0592\n\n\n\n\n14.1.3 Early Spring = Mean of Feb & March temps above average\nWhat if we average Feb and March temperatures together.\n\ngroundhog$FebMarAvg &lt;- (groundhog$February.Average.Temperature+groundhog$March.Average.Temperature)/2\navg.shadow &lt;- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\navg.noshadow &lt;- groundhog$FebMarAvg[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(avg.noshadow)-mean(avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(avg.noshadow, avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0063\n\n\nI don’t know about you, but this is staring to seem a little bit weird.\n\n\n14.1.4 Measuring Accuracy\n(New idea of what “early spring” means) If both February temperatures and march temperature are greater than average, then we say we have an early spring.\n\nfeb.avg &lt;- mean(groundhog$February.Average.Temperature)\nmar.avg &lt;- mean(groundhog$March.Average.Temperature)\n\ngroundhog$EarlySpring &lt;- groundhog$February.Average.Temperature&gt;feb.avg & groundhog$March.Average.Temperature&gt;mar.avg\n\naddmargins(table(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))\n\n             \n              FALSE TRUE Sum\n  Full Shadow    75   25 100\n  No Shadow       7    8  15\n  Sum            82   33 115\n\n\nwhen earlySpring = TRUE, and prediction was “no shadow” that would be correct when earlySpring = FALSE and prediction is “full shadow” that would be correct\n\naccuracy &lt;- function(guesses, weather){\n  predictEarlySpring &lt;- guesses==\"No Shadow\"\n  nCorrect &lt;- sum(weather==predictEarlySpring)\n  return(nCorrect/length(weather))\n}\n\naccuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring)\n\n[1] 0.7217391\n\n\nLet’s call our simulated Groundhog “Bernoulli Phil”\n\nprop.table(table(groundhog$Punxsutawney.Phil))\n\n\nFull Shadow   No Shadow \n  0.8695652   0.1304348 \n\n#A randomly guessing groundhog would see his shadow this proportion of the time\n\np.shadow &lt;- prop.table(table(groundhog$Punxsutawney.Phil))[1]\n\n\n(accuracy.obs &lt;- accuracy(groundhog$Punxsutawney.Phil, groundhog$EarlySpring))\n\n[1] 0.7217391\n\n#simulate guesses by Bernoulli Phil\nrandomGuesses &lt;- function(n=115, p=0.8695652){\n  return(sample(c(\"Full Shadow\",\"No Shadow\"), size=n, replace=TRUE, prob=c(p,1-p)))\n}\n\nNMC &lt;- 10000\nresults &lt;- rep(0,NMC)\nn &lt;- nrow(groundhog)\n\nfor(i in 1:NMC){\n  results[i] &lt;- accuracy(randomGuesses(n, p.shadow), groundhog$EarlySpring)\n}\n\nhist(results, breaks=40)\nabline(v=accuracy.obs)\n\n\n\nmean(results &gt;= accuracy.obs)\n\n[1] 0.0264\n\n\nThe actual groundhog’s accuracy is statistically much better than the simulated “random guessing” groundhog - at least better at predicting if both Feb & March will be above average temperature.\nThe scatterplot colored by shadow status makes this pattern suspiciously clear\n\nplot(groundhog$February.Average.Temperature, groundhog$March.Average.Temperature, col=(as.numeric(groundhog$Punxsutawney.Phil==\"Full Shadow\") +1), xlab=\"Avg. Feb Temp\", ylab=\"Avg. Mar Temp\", main=\"National Feb/Mar Temp vs Shadow\", pch=16)\nabline(v=mean(groundhog$February.Average.Temperature))\nabline(h=mean(groundhog$March.Average.Temperature))\nlegend(x=25, y=50, legend=c(\"early spring\",\"6 wks winter\"), pch=16, col=1:2)\n\n\n\n\nO ye unbelievers! Witness the prognosticating powers of Phil the groundhog!\n\n\n14.1.5 Looking at Deviation from a moving average\nHowever we have to be cautious - think about climate change and Phil’s predictions.\n\nlibrary(smooth)\n\nLoading required package: greybox\n\n\nPackage \"greybox\", v2.0.3 loaded.\n\n\n\nAttaching package: 'greybox'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    accuracy\n\n\nThis is package \"smooth\", v4.1.1\n\n\n\nAttaching package: 'smooth'\n\n\nThe following object is masked _by_ '.GlobalEnv':\n\n    accuracy\n\nMA &lt;-sma(groundhog$FebMarAvg, order=20)\n#predict(MA)\nplot(y=groundhog$FebMarAvg, x=groundhog$Year, col=as.numeric((as.factor(groundhog$Punxsutawney.Phil)))+1, pch=16)\nlines(y=predict(MA)$mean, x=groundhog$Year)\n\n\n\n\nNotice that he has been predicting early springs more often since the 60s, and also we’ve been having warmer and warmer temperatures. Perhaps he’s predicting early spring more often because of climate change, and thus he is correct more often.\nThis whole analysis really hinges on what we mean by an “early spring”. One could argue that with rising global temperatures, spring is actually coming earlier more often than it used to, and THAT’s why he’s seeing no shadow more often.\nIf we repeat this analysis and look not at whether the temperature is above average, but rather whether the temperature is above the moving average then it changes the conclusion a bit\n\ngroundhog$tempDev &lt;- groundhog$FebMarAvg - as.numeric(predict(MA)$mean)\nplot(y=groundhog$tempDev,x=groundhog$Year, col=as.numeric((as.factor(groundhog$Punxsutawney.Phil)))+1, pch=16, ylab=\"dev from moving avg\")\n\n\n\n\n\navg.shadow &lt;- groundhog$tempDev[groundhog$Punxsutawney.Phil==\"Full Shadow\"]\navg.noshadow &lt;- groundhog$tempDev[groundhog$Punxsutawney.Phil==\"No Shadow\"]\n\nt_obs &lt;- mean(avg.noshadow)-mean(avg.shadow)\n\ntest.stats &lt;- 0 #lazy empty vector - R will add more to it without complaining\nNMC &lt;- 10000\nfor(i in 1:NMC){\n  test.stats[i] &lt;- permute_and_compute(avg.noshadow, avg.shadow)\n}\nhist(test.stats, breaks=50)\nabline(v=t_obs, col=\"red\")\n\n\n\nmean(test.stats &gt;= t_obs)\n\n[1] 0.0653\n\n\nLast let’s consider an “early spring” when both Feb and March average temps are above the moving average.\n\nMAfeb &lt;-sma(groundhog$February.Average.Temperature, order=20)\nMAmar &lt;-sma(groundhog$March.Average.Temperature, order=20)\n\nearlySpring &lt;- groundhog$February.Average.Temperature &gt; as.numeric(predict(MAfeb)$mean)  &\n  groundhog$March.Average.Temperature &gt; as.numeric(predict(MAmar)$mean)\n\naccuracy(groundhog$Punxsutawney.Phil, earlySpring)\n\n[1] 0.7217391\n\n\nThe accuracy is now not much better than a flip of a coin. But that’s not statistical thinking - we need to see if he is better than a randomly guessing groundhog.\n\n(accuracy.obs &lt;- accuracy(groundhog$Punxsutawney.Phil, earlySpring))\n\n[1] 0.7217391\n\n#simulate guesses by Bernoulli Phil\nrandomGuesses &lt;- function(n=115, p=0.8695652){\n  return(sample(c(\"Full Shadow\",\"No Shadow\"), size=n, replace=TRUE, prob=c(p,1-p)))\n}\n\nNMC &lt;- 10000\nn &lt;- nrow(groundhog)\nresults &lt;- replicate(NMC, accuracy(randomGuesses(n, p.shadow), earlySpring))\n\nhist(results, breaks=40)\nabline(v=accuracy.obs)\n\n\n\nmean(results &gt;= accuracy.obs)\n\n[1] 0.0254\n\n\nI’m still convinced - the groundhog does significantly better than a random guesser."
  },
  {
    "objectID": "R04_Monte_Carlo_Testing.html#example-autocorrelation",
    "href": "R04_Monte_Carlo_Testing.html#example-autocorrelation",
    "title": "14  Monte Carlo Testing",
    "section": "14.2 Example: Autocorrelation",
    "text": "14.2 Example: Autocorrelation\nA question of particular importance in time series analysis is whether there is any autocorrelation. If your data is recorded regularly at timesteps \\(1,2,\\ldots\\) we could look at the relationship between \\(X_i\\) and \\(X_{i+1}\\) to see if they exhibit correlation.\nconsider the following time series data:\n\ntime_series &lt;- read.csv(\"data/time_series_data.csv\")\nplot(time_series$x, type=\"l\")\n\n\n\n\nOne possible model for how this data came about is that it is independent draws from a random variable. A quick check of the histogram can help us determine a good random variable to model the data:\n\nhist(time_series$x, breaks=20)\n\n\n\n\nThe distribution is somewhat symmetric, bell-shape. It could be a normal distribution. A better check for the distribution fit is a qqnorm plot.\n\nlibrary(car)\nqqPlot(time_series$x)\n\n\n\n\n[1] 98 83\n\n\nLet’s just pull the point estimates for the \\(\\mu\\) and \\(\\sigma\\) parameters for a normal distribution.\n\n(mu &lt;- mean(time_series$x))\n\n[1] 100.1767\n\n(sigma &lt;- sd(time_series$x))\n\n[1] 4.166514\n\n\nIf we are reasonably comfortable with the assumption that the data came from a normal distribution (seems supported by the QQ plot) we could model \\(X\\) as independent draws from \\(N(100.17, 4.17^2)\\).\nA natural test statistic to look at which measures autocorrelation is the correlation of \\(X_i\\) with \\(X_{i+1}\\).\n\ntest.stat &lt;- function(X){\n  return(cor(X[-1], X[-length(X)] ) )\n}\n(obs.t &lt;- test.stat(time_series$x))\n\n[1] 0.2991791\n\n\n\n14.2.1 A Parametric Test of Autocorrelation\nTo perform a Monte Carlo test we can simulate datasets from our null hypothesis model and record the simulated test statistic.\n\nt.sim &lt;- replicate(10000, test.stat(rnorm(100, mu, sigma)))\nhist(t.sim)\n\n\n\n\nWe will consider what proportion of the simulated test statistics are more extreme than we observed. Because this is a two-tailed distribution - and evidence against the null would be found in either the upper or the lower tail, we should make our p-value calculation by doubling the smaller of the two tails\n\n2 * min(mean(t.sim &gt;= obs.t),\n        mean(t.sim &lt;= obs.t))\n\n[1] 0.0012\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nWe have a very low p-value, indicating that if the data were in fact independent draws, we’d have a very very small chance of seeing data like this.\n\n\n14.2.2 A Permutation Test of Autocorrelation\nWe could also take a different approach for our null model. We could use a permutation test. Under the null hypothesis, every permutation of the data is equally likely. So we could produce a simulated test statistic distribution by shuffling the data around each time. This is consistent with our null hypothesis because there’s no trend in this model.\n\nt.sim &lt;- replicate(10000, test.stat(sample(time_series$x)))\n2 * min(mean(t.sim &gt;= obs.t),\n        mean(t.sim &lt;= obs.t))\n\n[1] 8e-04\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nThe p-value is pretty much the same, and so is our conclusion.\n\n\n14.2.3 A Permutation test of constant variance\nAnother violation of the null model would be if the variance of the simulated values is not consistent. We could simply split the data in half and calculate the variance of the first half of the data, comparing it with the variance of the second half of the data. We can look at the absolute value of the difference.\n\ntest.stat &lt;- function(x){\n  n1 &lt;- round(length(x)/2)\n  abs(var(x[1:n1]) - var(x[-(1:n1)]))\n}\n(obs.t &lt;- test.stat(time_series$x))\n\n[1] 13.25346\n\n\nThis test statistic is designed to be a one-sided test statistic, only large values of \\(T\\) represent evidence against the null hypothesis that \\(\\sigma^2\\) is constant.\n\nt.sim &lt;- replicate(10000, test.stat(sample(time_series$x)))\nmean(t.sim &gt;= obs.t) #right-tailed p-value\n\n[1] 0.0044\n\nhist(t.sim)\nabline(v=obs.t, col=\"red\")\n\n\n\n\nThe \\(p\\)-value is small once again. We have strong evidence that the the variance of the model generating these values is not constant."
  },
  {
    "objectID": "testing1_practice.html",
    "href": "testing1_practice.html",
    "title": "15  Monte Carlo Testing Practice",
    "section": "",
    "text": "16 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "testing1_practice.html#controlling-flies",
    "href": "testing1_practice.html#controlling-flies",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.1 Controlling Flies",
    "text": "16.1 Controlling Flies\nThe farmer selects 5 of her calves and divides them into two groups. One group contains 2, the other group has 3 animals. The first group she protects using impregnated ear tags. To the other group she applies an oil-based formulation of the same insecticide, an artificial pyrethroid. The farmer pens each calf separately, and estimates the number of stable-flies in each pen by hanging sticky flypaper above the animal. After 3 days she removes her flypapers and counts the number of stable-flies on each.\n\n\n\nFlypaper number\n1\n2\n3\n4\n5\n\n\nFlies caught\n30\n50\n7\n12\n30\n\n\nTreatment\ntag\ntag\npour\npour\npour\n\n\n\nUse the ratio of flies caught as a test statistic. How strong is the evidence that the pour-on treatment is more effective at repelling flies. (lower number of flies caught means flies are repelled)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(1)\n\ntag &lt;- c(30, 50)\npour &lt;- c(7, 12, 30)\n\npermuteAB &lt;- function(A,B){\n  combined &lt;- sample(c(A,B))\n  A.perm &lt;- combined[1:length(A)]\n  B.perm &lt;- combined[-(1:length(A))]\n  return(list(A.perm, B.perm))\n}\n\ntest.stat &lt;- function(A,B){\n  return (sum(A) / sum(B))\n}\n\nNMC &lt;- 1000\nresults &lt;- 0\nfor(i in 1:NMC){\n  permuted &lt;- permuteAB(tag,pour)\n  results[i] &lt;- test.stat(permuted[[1]], permuted[[2]])\n}\nhist(results)\nabline(v=test.stat(tag,pour))\n\n\n\nmean(results &gt;= test.stat(tag,pour))\n\n[1] 0.201\n\n\nThe probability that we observe a test statistic as extreme as we observed is 0.201, which means that the observed data would not be unusual if the treatment did not have differing effects. In other words, if the number of flies on each of the cows was drawn from the same population (regardless of treatment) we’d observe data like we had about 20% of the time, which is not very low, so we would not reject the null hypothesis."
  },
  {
    "objectID": "testing1_practice.html#babies",
    "href": "testing1_practice.html#babies",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.2 Babies",
    "text": "16.2 Babies\nWe will use the following dataset to demonstrate the use of permutations:\n\nlibrary(tidyr)\nbabies &lt;- read.table(url(\"https://raw.githubusercontent.com/genomicsclass/dagdata/master/inst/extdata/babies.txt\"), header=TRUE) \nbwt.nonsmoke &lt;- subset(babies, smoke==0)$bwt\nbwt.smoke &lt;- subset(babies, smoke==1)$bwt\n\nWe will generate the following random variable based on a sample size of 10 and observe the following difference:\n\nN=10  \nset.seed(1)  \nnonsmokers &lt;- sample(bwt.nonsmoke , N)  \nsmokers &lt;- sample(bwt.smoke , N)  \nobs &lt;- mean(smokers) - mean(nonsmokers) \n\nThe question is whether this observed difference is statistically significant. We do not want to rely on the assumptions needed for the normal or t-distribution approximations to hold, so instead we will use permutations.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nset.seed(5)\ntest.stat &lt;- function(A,B){\n  return (mean(A)-mean(B))\n}\n\nNMC &lt;- 1000\nresults &lt;- 0\nfor(i in 1:NMC){\n  permuted &lt;- permuteAB(smokers, nonsmokers)\n  results[i] &lt;- test.stat(permuted[[1]], permuted[[2]])\n}\nhist(results)\nabline(v=test.stat(smokers, nonsmokers))\n\n\n\nmean(results &lt;= test.stat(smokers, nonsmokers))\n\n[1] 0.042\n\n\nThe null hypothesis is that the distribution of birthweights for smokers is the same as the distribution for nonsmokers. The alternative hypothesis is that the mean birthweight for smokers is lower than for nonsmokers.\nThe observed test statistic has a p-value of 0.042; If the distributions are the same, we would only observe a difference of sample means this (or more) extreme 4.2% of the time, and this is lower than our significance level of 0.05, so we have strong evidence against the null hypothesis. There’s strong evidence that the mean birthweight is lower for smokers."
  },
  {
    "objectID": "testing1_practice.html#best-age",
    "href": "testing1_practice.html#best-age",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.3 Best age?",
    "text": "16.3 Best age?\nIf you could stop time and live forever in good health at a particular age, at what age would you like to live?”\nSuppose we are interested in testing the claim that the average ideal age for women is greater than men. A random sample of 3 women and 3 men were asked this question resulting in the following responses:\nWomen: 49, 42, 38\nMen: 39, 38, 50\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nWomen &lt;- c(49, 42, 38)\nMen &lt;- c(39, 38, 50)\nset.seed(1)\n\ntest.stat &lt;- function(A,B){\n  return (mean(A)-mean(B))\n}\n\nNMC &lt;- 1000\nresults &lt;- 0\nfor(i in 1:NMC){\n  permuted &lt;- permuteAB(Women, Men)\n  results[i] &lt;- test.stat(permuted[[1]], permuted[[2]])\n}\nhist(results)\nabline(v=test.stat(Women, Men))\n\n\n\nmean(results &gt;= test.stat(Women, Men))\n\n[1] 0.518\n\n\nThe permutation test does not provide us evidence to support the claim; We lack evidence that the mean ideal age for women is greater than the mean ideal age for men.\n\n\n\n\nSuppose that you work at a bicycle factory where you’d like to replace steel tubes in the company’s bicycle frames with titanium (because it’s lighter). You have carefully selected random samples of the steel and titanium tubes from your suppliers. You measure the stiffness of each tube in a testing rig. The data are (measured in pounds per square inch, psi):\n\nSteel: 30.7, 29.5, 29.8, 30.3, 29.2\nTitanium: 29.9, 31.1, 30.2, 30.8, 30.7, 31.9\nPerform a permutation test to see if there is evidence that the mean stiffness is different between steel and titanium. Use the difference of sample means as a test statistic and perform a two-tailed test.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsteel &lt;- c(30.7, 29.5, 29.8, 30.3, 29.2)\ntitanium &lt;- c(29.9, 31.1, 30.2, 30.8, 30.7, 31.9)\n\nset.seed(1)\n\ntest.stat &lt;- function(A,B){\n  return (mean(A)-mean(B))\n}\n\nNMC &lt;- 1000\nresults &lt;- 0\nfor(i in 1:NMC){\n  permuted &lt;- permuteAB(steel, titanium)\n  results[i] &lt;- test.stat(permuted[[1]], permuted[[2]])\n}\nhist(results)\nabline(v=test.stat(steel, titanium))\n\n\n\n2*min(mean(results &gt;= test.stat(steel, titanium)),\n  mean(results &lt;= test.stat(steel, titanium)))\n\n[1] 0.046\n\n\nOur 2-tailed p-value is 0.046, which is lower than our significance threshold of 0.05; we have good evidence that the mean stiffness of titanium frames is different than the mean stiffness of steel frames."
  },
  {
    "objectID": "testing1_practice.html#maximum-current",
    "href": "testing1_practice.html#maximum-current",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.4 Maximum Current",
    "text": "16.4 Maximum Current\n(From Introduction to Probability and Statistics for Data Science)\nAn electrical engineer must design a circuit to deliver the maximum amount of current to a display tube to achieve sufficient image brightness. Within her allowable design constraints, she has developed two candidate circuits and tests prototypes of each. The resutling data (in microamperes) are\nCircuit 1: 251, 255, 258, 257, 250, 251, 254, 250\nCircuit 2: 250, 253, 249, 256, 259, 252, 260, 251\n\nDevelop and test an appropriate hypothesis for this problem using Monte Carlo\nRepeat the test but this time use an appropriate parametric test. What assumptions are made?"
  },
  {
    "objectID": "testing1_practice.html#rank-based-permutation-test",
    "href": "testing1_practice.html#rank-based-permutation-test",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.5 Rank-based permutation test",
    "text": "16.5 Rank-based permutation test\nResearchers compared the effectiveness of conventional textbook worked examples to modified worked examples, which present the algebraic manipulations and explanation as part of the graphical display. They are interested if the time to complete problems improves if students can study from the modified examples compared to the conventional examples.\nSubjects: 28 ninth-year students in Sydney, Australia, with no previous exposure to coordinate geometry but have adequate math to deal with the problems given The 28 subjects were randomized to self-study one of two instructional materials. The two materials covered exactly the same problems, presented differently. Students were given as much time as they wished to study the material, but not allowed to ask questions. Following the instructional phase, all students were tested with a common examination over three problems of different difficulty. Response: the time (in seconds) required to arrive at a solution to the moderately difficult problem.\nModified Group: 68, 70, 73, 75, 77, 80, 80, 132, 148, 155, 183, 197, 206, 210\nConventional Group: 130, 139, 146, 150, 161, 177, 228, 242, 265, 300*, 300* ,300* ,300* ,300*\nNote the response is censored at 300 seconds because the time allotment for the problem is 5 minutes Five students did not complete the problem in the 5-minute (300 seconds) time allotment.\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBecause of the data truncated at 300, it’s reasonable to convert the values to ranks. This will also handle strangeness in distribution shape. We could use any statistic based on rank as a test statistic. We’ll use the mean rank from the modified group. Difference of means would be equivalent - it wouldn’t be a more powerful test statistic (you can verify this).\nWe would find evidence for the claim if the mean rank from the modified group is particularly low, so this is a left-sided test.\n\nset.seed(1)\nmod.g &lt;- c(68, 70, 73, 75, 77, 80, 80, 132, 148, 155, 183, 197, 206, 210)\nconv.g &lt;- c(130, 139, 146, 150, 161, 177, 228, 242, 265, 300, 300 ,300 ,300 ,300)\n\nranks &lt;- rank(c(mod.g, conv.g))\nmod.rank &lt;- ranks[1:length(mod.g)]\n\nobs.T &lt;- mean(mod.rank)\n\nNMC &lt;- 10000\nsim.T &lt;- numeric(NMC)\nfor(i in 1:NMC){\n  ranks.sim &lt;- sample(ranks)\n  mod.rank.sim &lt;- ranks.sim[1:length(mod.g)]\n  sim.T[i] &lt;- mean(mod.rank.sim)\n}\n\nhist(sim.T)\nabline(v=obs.T, col=\"red\")\n\n\n\nmore.Extreme &lt;- sum(sim.T &lt;= obs.T)\n(p.value &lt;- more.Extreme / NMC)\n\n[1] 5e-04\n\n\nThe \\(p\\)-value is extremely low, we have overwhelming evidence that the modified problems tend to result in lower completion time"
  },
  {
    "objectID": "testing1_practice.html#paired-sample-permutation-test",
    "href": "testing1_practice.html#paired-sample-permutation-test",
    "title": "15  Monte Carlo Testing Practice",
    "section": "16.6 Paired sample permutation test",
    "text": "16.6 Paired sample permutation test\nTwo body designs are being considered for a new car model. The time (in seconds) to parallel park each body design was recorded for 14 drivers. The following results were obtained.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDriver\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n\n\n\n\nModel 1\n37.0\n25.8\n16.2\n24.2\n22.0\n33.4\n23.8\n43.2\n33.6\n24.4\n23.4\n21.2\n36.2\n32.8\n\n\nModel 2\n17.8\n20.2\n16.8\n41.4\n21.4\n38.4\n16.8\n34.2\n27.8\n23.2\n29.6\n20.6\n32.2\n41.8\n\n\n\nDoes this data give us strong evidence that the mean time to parallel park is different for the two body designs? Use a Monte Carlo test\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe null hypothesis is that each pair of values are equally likely to be for Model 1/ Model 2 as it would be for Model 2/Model 1. The permutation thus would not be across drivers, but across models.\n\nset.seed(1)\nmodel1 &lt;- c(37.0,25.8,16.2,24.2,22.0,33.4,23.8,43.2,33.6,24.4,23.4,21.2,36.2,32.8)\nmodel2 &lt;- c(17.8,20.2,16.8,41.4,21.4,38.4,16.8,34.2,27.8,23.2,29.6,20.6,32.2,41.8)\n\nobs.T &lt;- mean(model1 - model2)\n\nNMC &lt;- 10000\nsim.T &lt;- numeric(NMC)\nfor(i in 1:NMC){\n  signs &lt;- sample(c(-1,1), length(model1), replace=TRUE)\n  sim.T[i] &lt;- mean(model1*signs - model2*signs)\n}\nhist(sim.T); abline(v=obs.T, col=\"red\")\n\n\n\nmore.Extreme &lt;- min(sum(sim.T &gt;= obs.T), sum(sim.T &lt;= obs.T))\n(p.value &lt;- 2* more.Extreme / NMC)\n\n[1] 0.6546\n\n\nThe \\(p\\)-value is above .65; If the null hypothesis were true, and the two times were just due to driver and had nothing to do with the model, we’d see a test statistic this extreme (or more) 65% of the time."
  },
  {
    "objectID": "testing2_practice.html",
    "href": "testing2_practice.html",
    "title": "18  Testing and Power Practice",
    "section": "",
    "text": "19 Practice Problems\n\n\n20 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Testing and Power Practice</span>"
    ]
  },
  {
    "objectID": "estimation1_practice.html",
    "href": "estimation1_practice.html",
    "title": "20  Point Estimation Practice",
    "section": "",
    "text": "21 Practice Problems\n\n\n22 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Point Estimation Practice</span>"
    ]
  },
  {
    "objectID": "estimation2_practice.html",
    "href": "estimation2_practice.html",
    "title": "23  Interval Estimation Practice",
    "section": "",
    "text": "24 Practice Problems\nFor the next few exercises, we will use actual polls from the 2016 election. You can load the data from the dslabs package.\n\nlibrary(dslabs)\ndata(\"polls_us_election_2016\")\n\nSpecifically, we will use all the national polls that ended within one week before the election.\n\nlibrary(tidyverse)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.1\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.2\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\nWarning: package 'stringr' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\npolls &lt;- polls_us_election_2016 %&gt;%\n\nfilter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\")\n\n\nFor the first poll, you can obtain the samples size and estimated Clinton percentage with:\n\n\n(N&lt;- polls$samplesize[1])\n\n[1] 2220\n\n(x_hat &lt;- polls$rawpoll_clinton[1]/100)\n\n[1] 0.47\n\n\nAssume there are only two candidates and construct a 95% confidence interval for the election night proportion \\(p\\).\n\nNow use dplyr to add a confidence interval as two columns, call them lower and upper, to the object polls. Then use select to show the pollster, enddate, x_hat,lower, upper variables. Hint: define temporary columns x_hat and se_hat.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx_hat &lt;- polls$rawpoll_clinton/100\nse_hat &lt;- sqrt(x_hat*(1-x_hat)/polls$samplesize)\npolls$lower &lt;- x_hat-1.96*se_hat\npolls$upper &lt;- x_hat+1.96*se_hat\n\n\n\n\n\nThe final tally for the popular vote was Clinton 48.2% and Trump 46.1%. Add a column, call it hit, to the previous table stating if the confidence interval included the true proportion \\(p = 0.482\\) or not.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npolls$hit &lt;- 0.482 &gt;=polls$lower & 0.482 &lt;= polls$upper\n\n\n\n\n\nFor the table you just created, what proportion of confidence intervals included \\(p\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(polls$hit)\n\n[1] 0.3142857\n\n\n\n\n\n\nIf these confidence intervals are constructed correctly, and the theory holds up, what proportion should include \\(p\\)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n95% of the confidnece intervals should include p.\n\n\n\n\nA much smaller proportion of the polls than expected produce confidence intervals containing \\(p\\). If you look closely at the table, you will see that most polls that fail to include \\(p\\) are underestimating. The reason for this is undecided voters, individuals polled that do not yet know who they will vote for or do not want to say. Because, historically, undecideds divide evenly between the two main candidates on election day, it is more informative to estimate the spread or the difference between the proportion of two candidates d, which in this election was \\(0.482 − 0.461 = 0.021\\). Assume that there are only two parties and that \\(d = 2p − 1\\), redefine polls as below and re-do exercise 1, but for the difference.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\npolls &lt;- polls_us_election_2016 %&gt;%\nfilter(enddate &gt;= \"2016-10-31\" & state == \"U.S.\") %&gt;%\n\nmutate(d_hat = rawpoll_clinton / 100 - rawpoll_trump / 100)\n\n(N&lt;- polls$samplesize[1])\n\n[1] 2220\n\n(x_hat &lt;- polls$d_hat[1])\n\n[1] 0.04\n\n\n\n\n\n\nNow repeat exercise 3, but for the difference.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nx_hat &lt;- polls$rawpoll_clinton/100\nse_hat &lt;- sqrt(abs(x_hat*(1-x_hat))/polls$samplesize)\npolls$lower &lt;- polls$d_hat-1.96*se_hat\npolls$upper &lt;- polls$d_hat+1.96*se_hat\n\npolls$hit &lt;- .021 &gt;=polls$lower & .021 &lt;= polls$upper\n\n\n\n\n\nNow repeat exercise 4, but for the difference.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmean(polls$hit)\n\n[1] 0.6\n\n\n\n\n\n\nAlthough the proportion of confidence intervals goes up substantially, it is still lower than 0.95. In the next chapter, we learn the reason for this. To motivate this, make a plot of the error, the difference between each poll’s estimate and the actual d = 0.021. Stratify by pollster.\nRedo the plot that you made for exercise 9, but only for pollsters that took five or more polls.\n\n\n\n25 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>23</span>  <span class='chapter-title'>Interval Estimation Practice</span>"
    ]
  },
  {
    "objectID": "slr_practice.html",
    "href": "slr_practice.html",
    "title": "26  Simple Linear Regression Practice",
    "section": "",
    "text": "27 Practice Problems\n\nResearchers studying anthropometry collected body and skeletal diameter measurements, as well as age, weight, height and sex for 507 physically active individuals. The scatterplot below shows the relationship between height and shoulder girth (circumference of shoulders measured over deltoid muscles), both measured in centimeters.\n\n\nif(!require(openintro)){\ninstall.packages(\"openintro\")\nlibrary(openintro)\n}\n\nLoading required package: openintro\n\n\nLoading required package: airports\n\n\nLoading required package: cherryblossom\n\n\nLoading required package: usdata\n\nplot(x=bdims$sho_gi, y=bdims$hgt, xlab=\"Shoulder girth (cm)\", ylab = \"Height(cm)\")\n\n\n\n\n\n\n\n\n\nDescribe the relationship between shoulder girth and height.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a positive, seemingly linear relationship between shoulder girth and height. It seems to be a moderately strong association.\n\n\n\n\nHow would the relationship change if shoulder girth was measured in inches while the units of height remained in centimeters?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe relationship would be the same if the units of shoulder girth were changed; the only change to the plot would be the labeling of the X axis. The scatterplot itself would appear unchanged.\n\n\n\n\nThe Coast Starlight Amtrak train runs from Seattle to Los Angeles. The scatterplot below displays the distance between each stop (in miles) and the amount of time it takes to travel from one stop to another (in minutes).\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(x=coast_starlight$dist, y=coast_starlight$travel_time, xlab=\"Distance(miles)\", ylab=\"Travel Time (minutes)\")\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the relationship between distance and travel time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nAs distance tends to increase the travel time tends to increase as well, but there is a great deal of variability.\n\n\n\n\nHow would the relationship change if travel time was instead measured in hours, and distance was instead measured in kilometers?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIf the units were changed linearly (miles to km, and minutes to hours) the association would not be affected.\n\n\n\n\nCorrelation between travel time (in miles) and distance (in minutes) is r = 0.636. What is the correlation between travel time (in kilometers) and distance (in hours)?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation between travel time in km and distance in hours would be the exact same as the correlation with the original units. Correlation is a standardized statistic and is unaffected by affine (linear) transformations in the variables.\n\n\n\n\nWrite the equation of the regression line for predicting travel time.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nstarlight.lm &lt;- lm(travel_time ~ 1 + dist, data=coast_starlight)\nstarlight.lm\n\n\nCall:\nlm(formula = travel_time ~ 1 + dist, data = coast_starlight)\n\nCoefficients:\n(Intercept)         dist  \n    50.8842       0.7259  \n\n\nThe predicted model is \\(\\hat{time} = 50.8842 + 0.7259\\cdot dist\\)\n\n\n\n\nInterpret the slope and the intercept in this context.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe slope can be interpreted as the average increase in travel time (.7259 minutes) for a 1 mile increase in travel distance. The slope does not have a meaningful interpretation as a zero distance trip would not take 50.88 minutes.\n\n\n\n\nCalculate \\(R^2\\) of the regression line for predicting travel time from distance traveled for the Coast Starlight, and interpret \\(R^2\\) in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(starlight.lm)$r.sq\n\n[1] 0.4044083\n\n\nThe \\(R^2\\) statistic equals 0.4044, which can be interpreted as variation in travel distance accounts for 40.44% of the variation in travel time along the Amtrak Starlight train line.\n\n\n\n\nThe distance between Santa Barbara and Los Angeles is 103 miles. Use the model to estimate the time it takes for the Starlight to travel between these two cities.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsum(coef(starlight.lm)*c(1, 103))\n\n[1] 125.6537\n\n#or\npredict(starlight.lm, newdata=data.frame(dist=103))\n\n       1 \n125.6537 \n\n\nUsing our model we would predict the travel time between Santa Barbara and Los Angeles to be 125.65 minutes.\n\n\n\n\nIt actually takes the Coast Starlight about 168 mins to travel from Santa Barbara to Los Angeles. Calculate the residual and explain the meaning of this residual value.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n168 - 125.65\n\n[1] 42.35\n\n\nThe residual is 42.35 minutes; That means that our model under-estimates the travel time between Santa Barbara and Los Angeles by 42.35 minutes.\n\n\n\n\nSuppose Amtrak is considering adding a stop to the Coast Starlight 500 miles away from Los Angeles. Would it be appropriate to use this linear model to predict the travel time from Los Angeles to this point?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nrange(coast_starlight$dist)\n\n[1]  10 352\n\n\nBecause the dataset that generated this model included distances from 10 to 352 miles, it would only be appropriate to use this model to predict travel time for distances in this range. We lack any evidence from the dataset that the modeled relationship exists for distances beyond 352 miles.\n\n\n\n\nWhat would be the correlation between the ages of partners if people always dated others who are\n\n\n\n3 years younger than themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, it would be close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a -3\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\n2 years older than themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a +2\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\nhalf as old as themselves?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe correlation would be positive, and it would again be close to 1.\n\nage.a &lt;- runif(1000,40,100)\nage.b &lt;- age.a / 2\ncor(age.a, age.b)\n\n[1] 1\n\n\n\n\n\n\nSuppose we fit a regression line to predict the shelf life of an apple based on its weight. For a particular apple, we predict the shelf life to be 4.6 days. The apple’s residual is -0.6 days. Did we over or under estimate the shelf-life of the apple? Explain your reasoning.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe residual is \\(y-\\hat{y}\\) or \\(observed-predicted\\). A negative residual indicates that \\(\\hat{y} &gt; y\\), so our model would have over-estimated shelf life for this apple.\n\n\n\n\nExplore the starbucks dataset in the openintro package, specifically the relationship between the number of calories and the amount of protein (in grams) in Starbucks food. Since Starbucks only lists the number of calories on the display items, we might be interested in predicting the amount of protein a menu item has based on its calorie content.\n\n\n\nCreate a scatter plot with protein as the Y variable and calories as the X variable.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(y=starbucks$protein, x=starbucks$calories, xlab=\"Calories\", ylab=\"Protein (g)\", main=\"Starbucks menu items\")\n\n\n\n\n\n\n\n\n\n\n\n\nDescribe the relationship between number of calories and amount of protein (in grams) that Starbucks food menu items contain. b.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThere is a general trend that foods with higher calorie levels tend to have higher amounts of protein, but there is a great deal of variability. The positive trend is apparent though.\n\n\n\n\nIn this scenario, what are the predictor and outcome variables\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe predictor variable is calories, since that is the one piece of information labeled on the menu items. The outcome variable is protien (g), since we would try to predict that based on calories.\n\n\n\n\nWhy might we want to fit a regression line to these data?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe relationship between calories and protein seems like it might be linear, and a straight relationshup lends itself to OLS regression.\n\n\n\n\nThe following linear model is fit to the data:\n\n\nstarbucks.lm &lt;- lm(protein ~ 1 + calories, data=starbucks)\nplot(y=starbucks$protein, x=starbucks$calories, xlab=\"Calories\", ylab=\"Protein (g)\", main=\"Starbucks menu items\")\nabline(starbucks.lm, col=\"red\")\n\n\n\n\n\n\n\nplot(starbucks.lm, which=1)\n\n\n\n\n\n\n\n\n\\(\\hat{protein} = -1.18211 + 0.03147 \\cdot calories\\)\nWhat does the residuals vs. predicted plot tell us about the variability in our prediction errors based on this model for items with lower vs. higher predicted protein?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis plot demonstrates very clearly that the variability is not constant; we have much higher variation in protein for foods with higher predicted protein than in foods with lower predicted protein. This indicates a violation in the assumption of constant variance. In order to fit a linear model perhaps the data should be transformed first, for example a log-transformation to the variables.\n\n\n\n\nThe scatterplot shows the relationship between socioeconomic status measured as the percentage of children in a neighborhood receiving reduced-fee lunches at school (lunch) and the percentage of bike riders in the neighborhood wearing helmets (helmet). The average percentage of children receiving reduced-fee lunches is 30.833% with a standard deviation of 26.724% and the average percentage of bike riders wearing helmets is 30.883% with a standard deviation of 16.948%.\n\n\nlibrary(openintro)\nplot(helmet$helmet ~ helmet$lunch, xlab=\"Rate of Receiving a Reduced-Fee Lunch (%)\", ylab=\"Rate of Wearing a Helmet (%)\")\n\n\n\n\n\n\n\n\n\nIf the \\(R^2\\) for the least-squares regression line for these data is 72%, what is the correlation between lunch and helmet?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nCorrelation is precisely the square root of \\(R^2\\), the coefficient of determination. Therefore, the correlation would be\n\nsqrt(.72)\n\n[1] 0.8485281\n\n\n\n\n\n\nCalculate the slope and intercept for the least-squares regression line for these data.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nhelmet.lm &lt;- lm(helmet ~ lunch, data=helmet)\ncoef(helmet.lm)\n\n(Intercept)       lunch \n 47.4904464  -0.5386091 \n\n\nThe slope is -.5386 and the intercept is 47.49.\n\n\n\n\nInterpret the intercept of the least-squares regression line in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe intercept would be interpreted that in a neighborhood where 0% of the children receive reduced fare lunches, we predict that 47.49% of bike riders wear helmets.\n\n\n\n\nInterpret the slope of the least-squares regression line in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nFor every percentage point increase in rate of children receiving reduced fare lunches, we predict the rate of bike-riders who wear helmets to decrease on average by 0.5386 percentage points.\n\n\n\n\nWhat would the value of the residual be for a neighborhood where 40% of the children receive reduced-fee lunches and 40% of the bike riders wear helmets? Interpret the meaning of this residual in the context of the application.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n40-predict(helmet.lm, newdata=data.frame(lunch=40))\n\n       1 \n14.05392 \n\n\nThe residual comes to 14.05392, which means our model under-estimates the helmet rate in this neighborhood by 14.04392 percentage points.\n\n\n\n\n\n28 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>26</span>  <span class='chapter-title'>Simple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html",
    "href": "mlr_practice.html",
    "title": "30  Multiple Linear Regression Practice",
    "section": "",
    "text": "31 Practice Problems",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#predicting-salary",
    "href": "mlr_practice.html#predicting-salary",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.1 Predicting salary",
    "text": "31.1 Predicting salary\nSuppose we have a data set with five predictors, \\(X_1 =\\) GPA, \\(X_2 =\\) IQ, $X_3 =$ Level ( \\(1\\) for College and \\(0\\) for High School), \\(X_4\\) = Interaction between GPA and IQ, and \\(X_5\\) = Interaction between GPA and Level. The response is starting salary after graduation (in thousands of dollars). Suppose we use least squares to fit the model, and get \\(\\hat{\\beta}_0=50, \\hat{\\beta}_1=20, \\hat{\\beta}_2=0.07, \\hat{\\beta}_3=35, \\hat{\\beta}_4=0.01\\) and \\(\\hat{\\beta}_5 = -10\\).\n\n\nWhich answer is correct, and why?\n\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates.\nFor a fixed value of IQ and GPA, high school graduates earn more, on average, than college graduates provided that the GPA is high enough.\nFor a fixed value of IQ and GPA, college graduates earn more, on average, than high school graduates provided that the GPA is high enough.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIt helps to write out the estimated model with variable names:\n\\[\\widehat{Salary} = 50 + 20(GPA)+0.07(IQ) + 35(1_{college})+0.01(GPA \\times IQ) -10 (GPA \\times 1_{college})\\]\nIf we fix IQ and GPA, the models for salaries are:\n\\[\\widehat{Salary}_{HS} = 50 + 20(GPA)+0.07(IQ) +0.01(GPA \\times IQ)\\]\n\\[\\widehat{Salary}_{C} = 85 + 10(GPA)+0.07(IQ) +0.01(GPA \\times IQ)\\]\nIf we compare these two you can see that the effect of higher GPA is more substantial for high school students. What is the difference in salary?\n\\[ \\widehat{Salary}_{C}-\\widehat{Salary}_{HS} = 35-10(GPA)\\] If \\(GPA &lt; 3.5\\) this difference is positive (i.e. college grads earn a higher salary). If \\(GPA &gt; 3.5\\) then the difference is negative (i.e. high school grads earn higher salary)\nThus iii is the correct choice.\n\n\n\n\nPredict the salary of a college graduate with IQ of 110 and a GPA of 4.0.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n85+ 10*4 + .07*110 + 0.01*4*110\n\n[1] 137.1\n\n\nThe predicted salary is $137.1\n\n\n\n\nTrue or false: Since the coefficient for the GPA/IQ interaction term is very small, there is very little evidence of an interaction effect. Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe magnitude of the coefficient has nothing to do with the level of evidence of the interaction. It’s all relative to the standard error of this coefficient. A small magnitude coefficient with a very small standard error would have very little uncertainty. If the standard error is big that would mean the uncertainty is great. This is why we take the ratio of \\(\\hat{\\beta}_j/se(\\hat{\\beta}_j)\\)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#cubic-model",
    "href": "mlr_practice.html#cubic-model",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.2 Cubic Model",
    "text": "31.2 Cubic Model\nI collect a set of data ($n = 100$ observations) containing a single predictor and a quantitative response. I then fit a linear regression model to the data, as well as a separate cubic regression, i.e. \\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon\\).\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is linear, i.e. \\(Y = \\beta_0 + \\beta_1X + \\epsilon\\). Consider the training residual sum of squares (RSS) for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nIn expectation (in the sense of expected value) we would expect \\(\\hat{\\beta}_2=\\hat{\\beta}_3=0\\) and then RSS would be the same for both the linear and the cubic model. However in practice the data will not exhibit a perfect straight line relationship and thus these coefficients will be nonzero. That would mean the cubic model will fit slightly better and RSS will be lower for the cubic model.\n\n\n\n\nAnswer (a) using test rather than training RSS.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA cubic model would presumably slightly overfit to the training data, and could actually perform worse on test data, data that is drawn from a truly linear model. So we would expect test RSS to be lower with the cubic model.\n\n\n\n\nSuppose that the true relationship between \\(X\\) and \\(Y\\) is not linear, but we don’t know how far it is from linear. Consider the training RSS for the linear regression, and also the training RSS for the cubic regression. Would we expect one to be lower than the other, would we expect them to be the same, or is there not enough information to tell? Justify your answer.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nWithout knowing what the relationship is, this would be hard to answer. However if there is any curve to the true relationship a cubic model will most likely fit the training data better and will give us a lower RSS than a linear model.\n\n\n\n\nAnswer (c) using test rather than training RSS.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#auto-i",
    "href": "mlr_practice.html#auto-i",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.3 Auto I",
    "text": "31.3 Auto I\nThis question involves the use of simple linear regression on the Auto data set.\n\n\nUse the lm() function to perform a simple linear regression with mpg as the response and horsepower as the predictor. Use the summary() function to print the results. Comment on the output.\n\nFor example:\ni. Is there a relationship between the predictor and the response?\nii. How strong is the relationship between the predictor and the response?\niii. Is the relationship between the predictor and the response positive or negative?\niv. What is the predicted mpg associated with a horsepower of 98? What are the associated 95% confidence and prediction intervals?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nlibrary(ISLR)\n\nWarning: package 'ISLR' was built under R version 4.3.3\n\nauto.fit &lt;- lm(mpg~1+horsepower, data=Auto)\nsummary(auto.fit)\n\n\nCall:\nlm(formula = mpg ~ 1 + horsepower, data = Auto)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-13.5710  -3.2592  -0.3435   2.7630  16.9240 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 39.935861   0.717499   55.66   &lt;2e-16 ***\nhorsepower  -0.157845   0.006446  -24.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.906 on 390 degrees of freedom\nMultiple R-squared:  0.6059,    Adjusted R-squared:  0.6049 \nF-statistic: 599.7 on 1 and 390 DF,  p-value: &lt; 2.2e-16\n\n\nThere is a very strong relationship between the predictor and response. The p-value is close to zero meaning this data would be virtually impossible if the relationship was not real. The relationship is negative, clearly by the sign of the coefficient. Higher horsepower vehicles tend on average to have lower fuel efficiency.\n\npredict(auto.fit, newdata=data.frame(horsepower=98), interval=\"confidence\")\n\n       fit      lwr      upr\n1 24.46708 23.97308 24.96108\n\npredict(auto.fit, newdata=data.frame(horsepower=98), interval=\"prediction\")\n\n       fit     lwr      upr\n1 24.46708 14.8094 34.12476\n\n\nA vehicle with 98 horsepower has a predicted fuel efficiency of 24.47 mpg. The 95% confidence interval says that we are 95% confident that the average mpg of cars with 98 horsepower is within 23.97 and 24.96 mpg. The 95% prediction interval tells us that if we pick one arbitrary car with a horsepower of 98, we are 95% confident that its mpg will be within 13.81 and 34.12 mpg. You can tell that there is quite a bit of variation among cars.\n\n\n\n\nPlot the response and the predictor. Use the abline() function to display the least squares regression line.\nUse the plot() function to produce diagnostic plots of the least squares regression fit. Comment on any problems you see with the fit.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#auto-ii",
    "href": "mlr_practice.html#auto-ii",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.4 Auto II",
    "text": "31.4 Auto II\nThis question involves the use of multiple linear regression on the Auto data set.\n\n\nProduce a scatterplot matrix which includes all of the variables in the data set.\nCompute the matrix of correlations between the variables using the function cor(). You will need to exclude the name variable, cor() which is qualitative.\nUse the lm() function to perform a multiple linear regression with mpg as the response and all other variables except name as the predictors. Use the summary() function to print the results. Comment on the output. For instance:\n\ni. Is there a relationship between the predictors and the response?\nii. Which predictors appear to have a statistically significant relationship to the response?\niii. What does the coefficient for the year variable suggest?\n\nUse the plot() function to produce diagnostic plots of the linear regression fit. Comment on any problems you see with the fit. Do the residual plots suggest any unusually large outliers? Does the leverage plot identify any observations with unusually high leverage?\nUse the * and : symbols to fit linear regression models with interaction effects. Do any interactions appear to be statistically significant?\nTry a few different transformations of the variables, such as \\(log(X)\\), \\(\\sqrt{X}\\), \\(X^2\\). Comment on your findings.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#carseats",
    "href": "mlr_practice.html#carseats",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.5 Carseats",
    "text": "31.5 Carseats\nThis question should be answered using the Carseats data set.\n\n\nFit a multiple regression model to predict Sales using Price, Urban, and US.\nProvide an interpretation of each coefficient in the model. Be careful—some of the variables in the model are qualitative!\nWrite out the model in equation form, being careful to handle the qualitative variables properly.\nFor which of the predictors can you reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\nOn the basis of your response to the previous question, fit a smaller model that only uses the predictors for which there is evidence of association with the outcome.\nHow well do the models in (a) and (e) fit the data?\nUsing the model from (e), obtain 95% confidence intervals for the coefficient(s).\nIs there evidence of outliers or high leverage observations in the model from (e)?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#simulated-data",
    "href": "mlr_practice.html#simulated-data",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.6 Simulated Data",
    "text": "31.6 Simulated Data\nIn this exercise you will create some simulated data and will fit simple linear regression models to it. Make sure to use set.seed(1) prior to starting part (a) to ensure consistent results.\n\n\nUsing the rnorm() function, create a vector, x, containing \\(100\\) observations drawn from a \\(N(0, 1)\\) distribution. This represents a feature, \\(X\\).\nUsing the rnorm() function, create a vector, eps, containing \\(100\\) observations drawn from a \\(N(0, 0.5^2)\\) distribution—a normal distribution with mean zero and variance 0.25.\nUsing x and eps, generate a vector y according to the model \\(Y = −1 + 0.5X + \\epsilon\\). What is the length of the vector y? What are the values of \\(\\beta_0\\) and \\(\\beta_1\\) in this linear model?\nCreate a scatterplot displaying the relationship between x and y. Comment on what you observe.\nFit a least squares linear model to predict y using x. Comment on the model obtained. How do \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) compare to \\(\\beta_0\\) and \\(\\beta_1\\)?\nDisplay the least squares line on the scatterplot obtained in (d). Draw the population regression line on the plot, in a different color. Use the legend() command to create an appropriate legend.\nNow fit a polynomial regression model that predicts y using x and x^2. Is there evidence that the quadratic term improves the model fit? Explain your answer.\nRepeat (a)–(f) after modifying the data generation process in such a way that there is less noise in the data. The model should remain the same. You can do this by decreasing the variance of the normal distribution used to generate the error term \\(\\epsilon\\) in (b). Describe your results.\nRepeat (a)–(f) after modifying the data generation process in such a way that there is more noise in the data. The model should remain the same. You can do this by increasing the variance of the normal distribution used to generate the error term in (b). Describe your results.\nWhat are the confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\) based on the original data set, the noisier data set, and the less noisy data set? Comment on your results.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#collinearity",
    "href": "mlr_practice.html#collinearity",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.7 Collinearity",
    "text": "31.7 Collinearity\nThis problem focuses on the collinearity problem.\n\n\nPerform the following commands in R:\n\n\nset.seed (1)\nx1 &lt;- runif(100)\nx2 &lt;- 0.5 * x1 + rnorm(100) / 10\ny &lt;- 2 + 2 * x1 + 0.3 * x2 + rnorm(100)\n\nThe last line corresponds to creating a linear model in which y is a function of x1 and x2. Write out the form of the linear model. What are the regression coefficients?\n\nWhat is the correlation between x1 and x2? Create a scatterplot displaying the relationship between the variables.\nUsing this data, fit a least squares regression to predict y using x1 and x2. Describe the results obtained. What are \\(\\hat{\\beta}_0\\), \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)? How do these relate to the true \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\)? Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)? How about the null hypothesis \\(H_0 : \\beta_2 = 0\\)?\nNow fit a least squares regression to predict y using only x1. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)?\nNow fit a least squares regression to predict y using only x2. Comment on your results. Can you reject the null hypothesis \\(H_0 : \\beta_1 = 0\\)?\nDo the results obtained in (c)–(e) contradict each other? Explain your answer.\nNow suppose we obtain one additional observation, which was unfortunately mismeasured.\n\n\nx1 &lt;- c(x1 , 0.1)\nx2 &lt;- c(x2 , 0.8)\ny &lt;- c(y, 6)\n\nRe-fit the linear models from (c) to (e) using this new data. What effect does this new observation have on the each of the models? In each model, is this observation an outlier? A high-leverage point? Both? Explain your answers.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#boston",
    "href": "mlr_practice.html#boston",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.8 Boston",
    "text": "31.8 Boston\nThis problem involves the Boston data set. We will now try to predict per capita crime rate using the other variables in this data set. In other words, per capita crime rate is the response, and the other variables are the predictors.\n\n\nFor each predictor, fit a simple linear regression model to predict the response. Describe your results. In which of the models is there a statistically significant association between the predictor and the response? Create some plots to back up your assertions.\nFit a multiple regression model to predict the response using all of the predictors. Describe your results. For which predictors can we reject the null hypothesis \\(H_0 : \\beta_j = 0\\)?\nHow do your results from (a) compare to your results from (b)? Create a plot displaying the univariate regression coefficients from (a) on the \\(x\\)-axis, and the multiple regression coefficients from (b) on the \\(y\\)-axis. That is, each predictor is displayed as a single point in the plot. Its coefficient in a simple linear regression model is shown on the \\(x\\)-axis, and its coefficient estimate in the multiple linear regression model is shown on the \\(y\\)-axis.\nIs there evidence of non-linear association between any of the predictors and the response? To answer this question, for each predictor X, fit a model of the form\n\n\\(Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\beta_3X^3 + \\epsilon.\\)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#rock",
    "href": "mlr_practice.html#rock",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.9 Rock",
    "text": "31.9 Rock\nThe rock data set, which comes packaged with R, describes 48 rock samples taken at a petroleum reservoir. Each sample has four measurements, all related to measuring permeability of the rock (the geological details are not so important, here):\n\narea: the area of the pores (measured in a number of pixels out of a 256-by-256 image that were “pores”)\nperi: perimeter of the “pores” part of the sample\nshape: perimeter(peri) of the pores part divided by square root of the area (area)\nperm: a measurement of permeability (in milli-Darcies, a unit of permeability, naturally)\n\n\nplotting the data\n\nSuppose that our goal is to predict permeability (perm) from other characteristics of the rock.\nFor each of the three other variables area, peri and shape, fit a linear regression model that predicts perm from this variable and an intercept term. That is, you should fit three models, each using one of area, peri and shape.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\narea_lm &lt;- lm(perm ~ 1 + area, data=rock)\nperi_lm &lt;- lm(perm ~ 1 + peri, data=rock)\nshape_lm &lt;- lm(perm ~ 1 + shape, data=rock)\n\n\n\n\nPart b: comparing fits\nCompute the RSS for each of the three models fitted in Part a. Which is best?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmy_rss &lt;- function( fitted_model ) {\n  return( sum( (fitted_model$residuals)**2 ) )\n}\nmapply( my_rss, list(area_lm, peri_lm, shape_lm) )  \n\n[1] 7591852 4092864 6216896\n\n\nThe model using peri to predict perm achieves the smallest RSS.\n\n\n\nPart c: multiple regression\nNow, fit a model that predicts perm from the other three variables (you should not include any interaction terms).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n# TODO: replace NA with model-fitting code\nfull_lm &lt;- lm( perm ~ 1 + area + peri + shape, data=rock )\n\nsummary(full_lm)\n\n\nCall:\nlm(formula = perm ~ 1 + area + peri + shape, data = rock)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-750.26  -59.57   10.66  100.25  620.91 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 485.61797  158.40826   3.066 0.003705 ** \narea          0.09133    0.02499   3.654 0.000684 ***\nperi         -0.34402    0.05111  -6.731 2.84e-08 ***\nshape       899.06926  506.95098   1.773 0.083070 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 246 on 44 degrees of freedom\nMultiple R-squared:  0.7044,    Adjusted R-squared:  0.6843 \nF-statistic: 34.95 on 3 and 44 DF,  p-value: 1.033e-11\n\n\n\n\n\nPart d: interpreting model fits\nConsider the coefficient of peri in the model from Part c.\n\nGive an interpretation of this estimated coefficient.\nIs this coefficient statistically significantly different from zero at the \\(\\alpha=0.01\\) level?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe estimated coefficient of peri is statistically significantly different from zero at the \\(0.01\\) level (the p-value is approximately \\(3*10^{-8}\\)).\nThe estimate of the coefficient indicates that holding area and shape constant, a unit increase in peri is associated with a decrease of \\(0.344\\) in perm.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "mlr_practice.html#fitting-and-interpreting-a-linear-regression-model",
    "href": "mlr_practice.html#fitting-and-interpreting-a-linear-regression-model",
    "title": "30  Multiple Linear Regression Practice",
    "section": "31.10 Fitting and Interpreting a Linear Regression Model",
    "text": "31.10 Fitting and Interpreting a Linear Regression Model\nThe trees data set in R contains measurements of diameter (in inches), height (in feet) and the amount of timber (volume, in cubic feet) in each of 31 trees. See ?trees for additional information.\nThe code below loads the data set. Note that the column of the data set encoding tree diameter is mistakenly labeled Girth (girth is technically a measure of circumference, not a diameter).\n\ndata(trees)\nhead(trees)\n\n  Girth Height Volume\n1   8.3     70   10.3\n2   8.6     65   10.3\n3   8.8     63   10.2\n4  10.5     72   16.4\n5  10.7     81   18.8\n6  10.8     83   19.7\n\n\nPart a: examining correlations\nIt stands to reason that the volume of timber in a tree should scale with both the height of the tree and the diameter. However, it also stands to reason that height and diameter are highly correlated. Use the cor function to compute the pairwise correlations among the three variables.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncor(trees)\n\n           Girth    Height    Volume\nGirth  1.0000000 0.5192801 0.9671194\nHeight 0.5192801 1.0000000 0.5982497\nVolume 0.9671194 0.5982497 1.0000000\n\n\n\n\n\nSuppose that we had to choose either tree diameter (labeled Girth in the data) or tree height with which to predict volume. Based on these correlations, Which would you choose? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nGirth has a much higher correlation with Volume than does Height, so it is likely to be a better predictor.\n\n\n\nPart b: comparing model fits\nWell, let’s put the above to a test. Use lm to fit two linear regression models from this data set (both should, of course, include intercept terms):\n\nPredicting volume from height\nPredicting volume from diameter (labeled Girth in the data set)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nht_lm &lt;- lm( Volume ~ 1 + Height, data=trees );\ngr_lm &lt;- lm( Volume ~ 1 + Girth, data=trees );\n\nc( my_rss(ht_lm), my_rss(gr_lm) )\n\n[1] 5204.8950  524.3025\n\n\n\n\n\nCompare the sum of squared residuals of these two models. Which is better? Does this agree with your observations in Part a?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe model predicting volume from girth is indeed a better model, achieving a factor of ten decrease in RSS compared to the model using height. This is in agreement with the prediction in part a.\n\n\n\nExamining the model outputs above (or extracting information again here), what do each of your fitted models conclude about the null hypothesis that the slope is equal to zero?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary( ht_lm )\n\n\nCall:\nlm(formula = Volume ~ 1 + Height, data = trees)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-21.274  -9.894  -2.894  12.068  29.852 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -87.1236    29.2731  -2.976 0.005835 ** \nHeight        1.5433     0.3839   4.021 0.000378 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.4 on 29 degrees of freedom\nMultiple R-squared:  0.3579,    Adjusted R-squared:  0.3358 \nF-statistic: 16.16 on 1 and 29 DF,  p-value: 0.0003784\n\n\n\nsummary( gr_lm )\n\n\nCall:\nlm(formula = Volume ~ 1 + Girth, data = trees)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.065 -3.107  0.152  3.495  9.587 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -36.9435     3.3651  -10.98 7.62e-12 ***\nGirth         5.0659     0.2474   20.48  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.252 on 29 degrees of freedom\nMultiple R-squared:  0.9353,    Adjusted R-squared:  0.9331 \nF-statistic: 419.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nBoth models conclude that the coefficient of the single predictor is statistically significantly different from zero at level \\(0.001\\).\n\n\n\nPart c: volume and diameter\nThinking back to fourth grade geometry with Mrs. Galvin, you remember that the area of a circle grows like the square of the diameter (that is, if we double the diameter, the area of the circle quadruples). It follows that timber volume, which is basically the volume of a cylinder, should scale linearly with the square of the diameter.\nCreate a scatter plot of volume as a function of diameter. Does the “geometric” intuition sketched above agree with what you see in the plot? Why or why not?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(trees$Girth, trees$Volume)\n\n\n\n\n\n\n\n\nAt least visually, this looks like a simple linear relationship. It doesn’t appear that using squared diameter should change anything in terms of prediction performance.\n\n\n\nPart d: incorporating non-linearities\nFit a linear regression model predicting volume from the squared diameter (and an intercept term, of course). Compute the residual sum of squares of this model.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndiam2_lm &lt;- lm( Volume ~ 1 + I(Girth^2), data=trees)\n\nmy_rss( diam2_lm )\n\n[1] 329.3191\n\n\n\n\n\nCompare the RSS of this model with that obtained in Part b using the linear diameter. Which is better, the quadratic model or the linear model? Does this surprise you? Why or why not? If you are surprised, what might explain the observation?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThis is indeed notably smaller than the RSS achieved by the linear model. This is surprising given that there is no clear nonlinearity in the scatterplot, but it is in keeping with our “geometric” intuition.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>30</span>  <span class='chapter-title'>Multiple Linear Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html",
    "href": "logistic_practice.html",
    "title": "33  Logistic Regression Practice",
    "section": "",
    "text": "34 Practice Problems",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#getting-an-a",
    "href": "logistic_practice.html#getting-an-a",
    "title": "33  Logistic Regression Practice",
    "section": "34.1 Getting an A",
    "text": "34.1 Getting an A\nSuppose we collect data for a group of students in a statistics class with variables \\(X_1 =\\) hours studied, \\(X_2 =\\) undergrad GPA, and \\(Y =\\) receive an A. We fit a logistic regression and produce estimated coefficient, \\(\\hat{\\beta}_0 = −6\\), \\(\\hat{\\beta}_1 = 0.05\\), \\(\\hat{\\beta}_2 = 1\\).\n\n\nEstimate the probability that a student who studies for 40 h and has an undergrad GPA of 3.5 gets an A in the class.\nHow many hours would the student in part (a) need to study to have a 50% chance of getting an A in the class?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#mens-basketball",
    "href": "logistic_practice.html#mens-basketball",
    "title": "33  Logistic Regression Practice",
    "section": "34.2 Men’s Basketball",
    "text": "34.2 Men’s Basketball\nThe code below reads the Xavier men’s basketball 20-21 team game results and creates a dummy variable “win” that takes the numeric value of 1 if XU outscored their opponent and 0 otherwise. In this application we’ll use logistic regression to model wins/losses.\n\nxub &lt;- read.csv(\"https://remiller1450.github.io/data/xubball2021.csv\") \nxub$win &lt;- ifelse(xub$Margin &gt; 0, 1, 0)\n\n\nFit the logistic regression model win ~ X3P, which predicts whether a game was won or lost based upon the number of made 3-point shots. How does the number of made 3-point shots impact the odds of Xavier winning? Is the effect of “3XP” statistically meaningful?\nFit the logistic regression model win ~ X3P + X3P., which adjusts for the percentage of three point attempts that are made. How does the effect of “3XP” in this model differ from the effect you described in the previous question? Briefly explain why the adjusted effect is so different from the unadjusted effect.\nUse a likelihood ratio test to compare the two models described in (a) and (b). Does the test provide statistically compelling evidence that the larger model provides a better fit?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#spam-filter",
    "href": "logistic_practice.html#spam-filter",
    "title": "33  Logistic Regression Practice",
    "section": "34.3 Spam Filter",
    "text": "34.3 Spam Filter\nClassifying emails as “spam” is a classical data-science application that involves modeling a binary outcome. The following questions will use the spam dataset contained in the kernlab package, which contains a sample of 4601 emails that were manually labeled as either “spam” or “nonspam” (recorded as the variable “type”), along with various predictors that describe the normalized frequencies of certain words/symbols within each email message. We will focus on the following predictors:\n\n“capitalAve” - The average number of capitalized letters (per sentence) contained in the email.\n“charDollar” - The normalized frequency of the dollar sign character\n“charExclamation” - The normalized frequency of the exclamation point character\n\n\n#install.packages(\"kernlab\")\nlibrary(kernlab) \n\nWarning: package 'kernlab' was built under R version 4.3.3\n\ndata(\"spam\") \ntable(spam$type)\n\n\nnonspam    spam \n   2788    1813 \n\n\n\nUse boxplots to show the distribution of each of the three variables listed above in “spam” and “nonspam” emails. Based upon a visual inspection of these plots, which variable appears to be the strongest predictor of an email being “spam”? (Hint: Your choie of predictor is somewhat subjective, but try “zooming in” on your boxplots using the “xlim” or “ylim” arguments to help you make your selection)\nFit a logistic regression model that uses the variable you chose in (a) to predict “spam”. Then, use this model to intpret the effect of this variable on the odds of an email being “spam”.\nCreate a graph that displays the expected probability, along with 90% confidence intervals, of an email being spam in response to the predictor you identified in Question #13. (Hint: this graph should resemble the ones from the “Confidence and Prediction Intervals” section)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#paying-dividentds",
    "href": "logistic_practice.html#paying-dividentds",
    "title": "33  Logistic Regression Practice",
    "section": "34.4 Paying Dividentds",
    "text": "34.4 Paying Dividentds\nSuppose that we wish to predict whether a given stock will issue a dividend this year (“Yes” or “No”) based on \\(X\\), last year’s percent profit. We examine a large number of companies and discover that the mean value of $X$ for companies that issued a dividend was \\(\\bar{X} = 10\\), while the mean for those that didn’t was \\(\\bar{X} = 0\\). In addition, the variance of \\(X\\) for these two sets of companies was \\(\\hat{\\sigma}^2 = 36\\). Finally, 80% of companies issued dividends. Assuming that \\(X\\) follows a normal distribution, predict the probability that a company will issue a dividend this year given that its percentage profit was \\(X = 4\\) last year.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#odds",
    "href": "logistic_practice.html#odds",
    "title": "33  Logistic Regression Practice",
    "section": "34.5 Odds",
    "text": "34.5 Odds\nThis problem has to do with odds.\n\n\nOn average, what fraction of people with an odds of 0.37 of defaulting on their credit card payment will in fact default?\nSuppose that an individual has a 16% chance of defaulting on her credit card payment. What are the odds that she will default?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#weekly",
    "href": "logistic_practice.html#weekly",
    "title": "33  Logistic Regression Practice",
    "section": "34.6 Weekly",
    "text": "34.6 Weekly\nThis question should be answered using the Weekly data set, which is part of the ISLR2 package. This data is similar in nature to the Smarket data from this chapter’s lab, except that it contains 1,089 weekly returns for 21 years, from the beginning of 1990 to the end of 2010.\n\n\nProduce some numerical and graphical summaries of the Weekly data. Do there appear to be any patterns?\nUse the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to print the results. Do any of the predictors appear to be statistically significant? If so, which ones?\nCompute the confusion matrix and overall fraction of correct predictions. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.\nNow fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Compute the confusion matrix and the overall fraction of correct predictions for the held out data (that is, the data from 2009 and 2010).\nExperiment with different combinations of predictors, including possible transformations and interactions. Report the variables and associated confusion matrix that appears to provide the best results on the held out data.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#logistic-auto",
    "href": "logistic_practice.html#logistic-auto",
    "title": "33  Logistic Regression Practice",
    "section": "34.7 Logistic Auto",
    "text": "34.7 Logistic Auto\nIn this problem, you will develop a model to predict whether a given car gets high or low gas mileage based on the Auto data set.\n\n\nCreate a binary variable, mpg01, that contains a 1 if mpg contains a value above its median, and a 0 if mpg contains a value below its median. You can compute the median using the median() function. Note you may find it helpful to use the data.frame() function to create a single data set containing both mpg01 and the other Auto variables.\nExplore the data graphically in order to investigate the association between mpg01 and the other features. Which of the other features seem most likely to be useful in predicting mpg01? Scatterplots and boxplots may be useful tools to answer this question. Describe your findings.\nSplit the data into a training set and a test set.\nPerform logistic regression on the training data in order to predict mpg01 using the variables that seemed most associated with mpg01 in (b). What is the test error of the model obtained?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#boston-logistic",
    "href": "logistic_practice.html#boston-logistic",
    "title": "33  Logistic Regression Practice",
    "section": "34.8 Boston Logistic",
    "text": "34.8 Boston Logistic\nUsing the Boston data set, fit classification models in order to predict whether a given census tract has a crime rate above or below the median. Explore logistic regression and describe your findings.\nHint: You will have to create the response variable yourself, using the variables that are contained in the Boston data set. # Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "logistic_practice.html#function-practice",
    "href": "logistic_practice.html#function-practice",
    "title": "33  Logistic Regression Practice",
    "section": "34.9 Function Practice",
    "text": "34.9 Function Practice\nThis problem involves writing functions.\n\n\nWrite a function, Power(), that prints out the result of raising 2 to the 3rd power. In other words, your function should compute \\(2^3\\) and print out the results.\n\nHint: Recall that x^a raises x to the power a. Use the print() function to output the result.\n\nCreate a new function, Power2(), that allows you to pass any two numbers, x and a, and prints out the value of x^a. You can do this by beginning your function with the line\n\nPower2 &lt;- function(x, a){}\nYou should be able to call your function by entering, for instance, Power2(3,8) on the command line. This should output the value of \\(3^8\\), namely, 6,561.\n\nUsing the Power2() function that you just wrote, compute \\(10^3\\), \\(8^{17}\\), and \\(131^3\\).\nNow create a new function, Power3(), that actually returns the result x^a as an R object, rather than simply printing it to the screen. That is, if you store the value x^a in an object called result within your function, then you can simply return() this result, using the following line:\n\nreturn(result)\nThe line above should be the last line in your function, before the } symbol.\n\nNow using the Power3() function, create a plot of \\(f(x) = x^2\\). The \\(x\\)-axis should display a range of integers from 1 to 10, and the y-axis should display \\(x^2\\). Label the axes appropriately, and use an appropriate title for the figure. Consider displaying either the \\(x\\)-axis, the \\(y\\)-axis, or both on the log-scale. You can do this by using log = \"x\", log = \"y\", or log = \"xy\" as arguments to the plot() function.\nCreate a function, PlotPower(), that allows you to create a plot of x against x^a for a fixed a and for a range of values of x. For instance, if you call PlotPower(1:10, 3) then a plot should be created with an \\(x\\)-axis taking on values \\(1, 2, \\ldots , 10\\), and a \\(y\\)-axis taking on values \\(1^3, 2^3, \\ldots , 10^3\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>33</span>  <span class='chapter-title'>Logistic Regression Practice</span>"
    ]
  },
  {
    "objectID": "cv_practice.html",
    "href": "cv_practice.html",
    "title": "22  Cross Validation Practice",
    "section": "",
    "text": "23 Practice Problems\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "cv_practice.html#boston-polynomial-regression",
    "href": "cv_practice.html#boston-polynomial-regression",
    "title": "22  Cross Validation Practice",
    "section": "23.1 Boston Polynomial Regression",
    "text": "23.1 Boston Polynomial Regression\nUse 5-fold validation to fit polynomials of order 1-5 on the Boston dataset, predicting MDEV from LSTAT"
  },
  {
    "objectID": "cv_practice.html#logistic-cv-on-mtcars",
    "href": "cv_practice.html#logistic-cv-on-mtcars",
    "title": "22  Cross Validation Practice",
    "section": "23.2 Logistic CV on mtcars",
    "text": "23.2 Logistic CV on mtcars\nLet’s revisit our dear old friend the mtcars data set once more. The am column of this data frame indicates whether a particular model of car has an automatic (0) or manual (1) transmission. This problem will consider how to go about predicting this trait based on the other available variables.\nPart a: fitting logistic regression\nFit a logistic regression model to predict am based on the read axle ratio (drat) and an intercept term.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ndrat_lr &lt;- glm( am ~ 1 + drat, data=mtcars, family='binomial')\n\nsummary(drat_lr)\n\n\nCall:\nglm(formula = am ~ 1 + drat, family = \"binomial\", data = mtcars)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-1.5495  -0.2609  -0.1505   0.5382   1.7453  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -21.021      7.838  -2.682  0.00732 **\ndrat           5.577      2.063   2.704  0.00685 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 43.23  on 31  degrees of freedom\nResidual deviance: 21.65  on 30  degrees of freedom\nAIC: 25.65\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\n\nPart b: interpreting logistic regression\nLooking at your fitted model from Part a, is the estimated coefficient for drat statistically significantly different from zero?\nGive a 95% confidence interval for the fitted coefficient.\nGive an interpretation of what this coefficient means.\nDoes an increase in drat tend to result in a higher or lower probability of a car having a manual transmission?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nconfint( drat_lr )\n\nWaiting for profiling to be done...\n\n\n                 2.5 %    97.5 %\n(Intercept) -42.327246 -9.555062\ndrat          2.526815 11.158615\n\n\nA unit increase in drat is associated with an increase of 5.577 in the log-odds of a car having a manual transmission.\nThus, an increase in drat is associated with a higher probability of a car having a manual transmission.\n\n\n\nPart c: model comparison with LOOCV\nNow, suppose we are considering adding another variable to our model from Part a. As discussed in lecture and your readings, adding additional variables to a model will always increase our performance when measured on the training data, so to make a fair comparison between this new model and the model from Part a, we need to compare their performances on data not seen in the training procedure.\nUse leave-one-out cross validation (LOOCV) to compare the performance of your model from Part a against a model that predicts am from drat and mpg (and an intercept term).\nNote: in our discussion of LOOCV, we mostly discussed linear regression, where the natural way to assess a model’s performance was its squared error in predicting the held-out observations. In logistic regression, squared error is not such a natural choice. For this problem, you should fit the model using the standard maximum likelihood approach as implemented in glm, but you should assess the model’s performance on held-out data by making a prediction with the model (i.e., predicting 0 or 1), and then checking whether or not this prediction matches the true value of am in the held-out observation. Recall that our trained logistic regression model outputs a probability based on the given predictor(s). You should turn this probability into a prediction by rounding the probability to 0 or 1 (you may break a tie at probability \\(0.5\\) as you see fit).\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- nrow(mtcars);\nsinglevar_performances &lt;- rep(NA, n);\ntwovar_performances &lt;- rep(NA, n);\nfor( i in 1:n ) {\n  heldout &lt;- mtcars[i,];\n  heldout_correct_answer &lt;- heldout$am\n  trainset &lt;- mtcars[-c(i),]\n  singlevar_model &lt;- glm( am ~ 1 + drat, data=trainset, family='binomial')\n  twovar_model &lt;- glm( am ~ 1 + drat + mpg, data=trainset, family='binomial')\n  # Make predictions on the held-out point.\n  # Need to turn a probability into a 0 or 1, which we do with round().\n  singlevar_pred &lt;- round( predict( singlevar_model,\n                                        type='response', newdata=heldout ) )\n  twovar_pred &lt;- round( predict( twovar_model,                            \n                                 type='response', newdata=heldout ) )\n  # Record whether we predicted correctly.\n  singlevar_performances[i] &lt;- singlevar_pred==heldout_correct_answer;\n  twovar_performances[i] &lt;- twovar_pred==heldout_correct_answer;\n}\n\n# Now, average over the n different held-out data points.\nc( mean(singlevar_performances), mean(twovar_performances) )\n\n[1] 0.8125 0.8125\n\n\n\n\n\nWhich model is better according to your implementation of leave-one-out cross validation?\n\n\n\n\n\n\nSolution\n\n\n\n\n\nBoth models perform identically, according to LOOCV. In light of this, we should prefer the model that uses only drat, since we prefer simpler models (i.e., those with fewer variables), all else being equal."
  },
  {
    "objectID": "cv_practice.html#defaulting-on-loans",
    "href": "cv_practice.html#defaulting-on-loans",
    "title": "22  Cross Validation Practice",
    "section": "23.3 Defaulting on loans",
    "text": "23.3 Defaulting on loans\nwe used logistic regression to predict the probability of default using income and balance on the Default data set. We will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before beginning your analysis.\n\n\nFit a logistic regression model that uses income and balance to predict default.\nUsing the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:\n\n\n\nSplit the sample set into a training set and a validation set.\nFit a multiple logistic regression model using only the training observations.\nObtain a prediction of default status for each individual in the validation set by computing the posterior probability of default for that individual, and classifying the individual to the default category if the posterior probability is greater than 0.5.\nCompute the validation set error, which is the fraction of the observations in the validation set that are misclassified.\n\n\n\nRepeat the process in (b) three times, using three different splits of the observations into a training set and a validation set. Comment on the results obtained.\nNow consider a logistic regression model that predicts the probability of default using income, balance, and a dummy variable for student. Estimate the test error for this model using the validation set approach. Comment on whether or not including a dummy variable for student leads to a reduction in the test error rate."
  },
  {
    "objectID": "cv_practice.html#defaulting-on-loans-ii",
    "href": "cv_practice.html#defaulting-on-loans-ii",
    "title": "22  Cross Validation Practice",
    "section": "23.4 Defaulting on loans II",
    "text": "23.4 Defaulting on loans II\nWe continue to consider the use of a logistic regression model to predict the probability of default using income and balance on the Default data set. In particular, we will now compute estimates for the standard errors of the income and balance logistic regression coefficients in two different ways: (1) using the bootstrap, and (2) using the standard formula for computing the standard errors in the glm() function. Do not forget to set a random seed before beginning your analysis.\n\n\nUsing the summary() and glm() functions, determine the estimated standard errors for the coefficients associated with income and balance in a multiple logistic regression model that uses both predictors.\nWrite a function, boot.fn(), that takes as input the Default data set as well as an index of the observations, and that outputs the coefficient estimates for income and balance in the multiple logistic regression model.\nUse the boot() function together with your boot.fn() function to estimate the standard errors of the logistic regression coefficients for income and balance.\nComment on the estimated standard errors obtained using the glm() function and using your bootstrap function."
  },
  {
    "objectID": "cv_practice.html#looping-loocv-logistic",
    "href": "cv_practice.html#looping-loocv-logistic",
    "title": "22  Cross Validation Practice",
    "section": "23.5 Looping LOOCV Logistic",
    "text": "23.5 Looping LOOCV Logistic\nWe saw that the cv.glm() function can be used in order to compute the LOOCV test error estimate. Alternatively, one could compute those quantities using just the glm() and predict.glm() functions, and a for loop. You will now take this approach in order to compute the LOOCV error for a simple logistic regression model on the Weekly data set. Recall that in the context of classification problems, the LOOCV error is given in (5.4).\n\n\nFit a logistic regression model that predicts Direction using Lag1 and Lag2.\nFit a logistic regression model that predicts Direction using Lag1 and Lag2 using all but the first observation.\nUse the model from (b) to predict the direction of the first observation. You can do this by predicting that the first observation will go up if P(Direction = “Up”|Lag1, Lag2) &gt; 0.5. Was this observation correctly classified?\nWrite a for loop from i = 1 to i = n, where n is the number of observations in the data set, that performs each of the following steps:\n\n\n\nFit a logistic regression model using all but the ith observation to predict Direction using Lag1 and Lag2.\nCompute the posterior probability of the market moving up for the ith observation.\nUse the posterior probability for the ith observation in order to predict whether or not the market moves up.\nDetermine whether or not an error was made in predicting the direction for the ith observation. If an error was made, then indicate this as a 1, and otherwise indicate it as a 0.\n\n\n\nTake the average of the n numbers obtained in (d)iv in order to obtain the LOOCV estimate for the test error. Comment on the results."
  },
  {
    "objectID": "cv_practice.html#simulated-data-cv",
    "href": "cv_practice.html#simulated-data-cv",
    "title": "22  Cross Validation Practice",
    "section": "23.6 Simulated Data CV",
    "text": "23.6 Simulated Data CV\n\nGenerate a simulated data set as follows:\n\n\nset.seed (1)\nx &lt;- rnorm (100)\ny &lt;- x - 2 * x^2 + rnorm (100)\n\nIn this data set, what is n and what is p? Write out the model used to generate the data in equation form. (b) Create a scatterplot of X against Y . Comment on what you find. (c) Set a random seed, and then compute the LOOCV errors that result from fitting the following four models using least squares: i. Y = β0 + β1X + ϵ ii. Y = β0 + β1X + β2X2 + ϵ iii. Y = β0 + β1X + β2X2 + β3X3 + ϵ iv. Y = β0 + β1X + β2X2 + β3X3 + β4X4 + ϵ. Note you may find it helpful to use the data.frame() function to create a single data set containing both X and Y . (d) Repeat (c) using another random seed, and report your results. Are your results the same as what you got in (c)? Why? (e) Which of the models in (c) had the smallest LOOCV error? Is this what you expected? Explain your answer. (f) Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (c) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?"
  },
  {
    "objectID": "cv_practice.html#simulated-data-best-subset",
    "href": "cv_practice.html#simulated-data-best-subset",
    "title": "22  Cross Validation Practice",
    "section": "23.7 Simulated Data Best Subset",
    "text": "23.7 Simulated Data Best Subset\nIn this exercise, we will generate simulated data, and will then use this data to perform best subset selection.\n\n\nUse the rnorm() function to generate a predictor X of length n = 100, as well as a noise vector ϵ of length n = 100.\nGenerate a response vector Y of length n = 100 according to the model Y = β0 + β1X + β2X2 + β3X3 + ϵ, where β0, β1, β2, and β3 are constants of your choice.\nUse the regsubsets() function to perform best subset selection in order to choose the best model containing the predictors X,X2, . . . ,X10. What is the best model obtained according to Cp, BIC, and adjusted R2? Show some plots to provide evidence for your answer, and report the coefficients of the best model obtained. Note you will need to use the data.frame() function to create a single data set containing both X and Y .\nRepeat (c), using forward stepwise selection and also using backwards stepwise selection. How does your answer compare to the results in (c)?\nNow fit a lasso model to the simulated data, again using X,X2, . . . , X10 as predictors. Use cross-validation to select the optimal value of λ. Create plots of the cross-validation error as a function of λ. Report the resulting coefficient estimates, and discuss the results obtained.\nNow generate a response vector Y according to the model Y = β0 + β7X7 + ϵ, and perform best subset selection and the lasso. Discuss the results obtained."
  },
  {
    "objectID": "cv_practice.html#college-applications",
    "href": "cv_practice.html#college-applications",
    "title": "22  Cross Validation Practice",
    "section": "23.8 College Applications",
    "text": "23.8 College Applications\nIn this exercise, we will predict the number of applications received using the other variables in the College data set.\n\n\nSplit the data set into a training set and a test set.\nFit a linear model using least squares on the training set, and report the test error obtained.\nFit a ridge regression model on the training set, with λ chosen by cross-validation. Report the test error obtained.\nFit a lasso model on the training set, with λ chosen by crossvalidation. Report the test error obtained, along with the number of non-zero coefficient estimates.\nComment on the results obtained. How accurately can we predict the number of college applications received? Is there much difference among the test errors resulting from these three approaches?"
  },
  {
    "objectID": "cv_practice.html#features-and-error",
    "href": "cv_practice.html#features-and-error",
    "title": "22  Cross Validation Practice",
    "section": "23.9 Features and Error",
    "text": "23.9 Features and Error\nWe have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.\n\n\nGenerate a data set with p = 20 features, n = 1,000 observations, and an associated quantitative response vector generated according to the model Y = Xβ + ϵ, where β has some elements that are exactly equal to zero.\nSplit your data set into a training set containing 100 observations and a test set containing 900 observations.\nPerform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.\nPlot the test set MSE associated with the best model of each size.\nFor which model size does the test set MSE take on its minimum value? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.\nHow does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the coefficient values.\nCreate a plot displaying \\(\\sqrt{\\sum_{j=1}^p(\\beta_j − \\hat{\\beta}_j^r)^2}\\) for a range of values of \\(r\\), where \\(\\hat{\\beta}_j^r\\) \\(j\\) is the \\(j\\)th coefficient estimate for the best model containing r coefficients. Comment on what you observe. How does this compare to the test MSE plot from (d)?"
  },
  {
    "objectID": "bootstrap_practice.html",
    "href": "bootstrap_practice.html",
    "title": "23  Bootstrap Practice",
    "section": "",
    "text": "24 Practice Problems\n\nA researcher is studying salamanders in western Massachusetts, and wishes to estimate the average length, measured in centimeters, of adult female salamanders living in wetlands there. She and her team collect an independent sample of 36 salamanders, measuring each one (in centimeters) from head to tail and recording the measurement. The data is reproduced below.\n\n\nsalamander_lengths &lt;- c( 14.5, 13.9, 14.1, 14.2, 14.7, 13.8, 14.6, 16.0, 14.7,\n                         14.8, 15.1, 14.6, 14.4, 13.7, 12.1, 14.8, 11.0, 12.0,\n                         13.8, 14.3, 13.9, 13.4, 14.6, 14.5, 15.6, 15.2, 16.1,\n                         15.2, 13.8, 16.6, 13.6, 15.4, 12.5, 12.8, 14.1, 15.2);\n\nUse the bootstrap to construct a 95% confidence interval for the population mean height. You should use at least 200 bootstrap replicates.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nn &lt;- length(salamander_lengths)\nB &lt;- 200;\nbootreps &lt;- rep( NA, B );\nfor(bb in 1:B ) {\n  bootsamp &lt;- sample(salamander_lengths, size=n, replace=TRUE)\n  bootreps[bb] &lt;- mean(bootsamp)\n}\n\nSEboot &lt;- sd(bootreps)\n\nmuhat &lt;- mean(salamander_lengths)\nc( muhat - 1.96*SEboot, muhat+1.96*SEboot )\n\n[1] 13.88357 14.64977\n\n\n\n\n\n\nWe will now derive the probability that a given observation is part of a bootstrap sample. Suppose that we obtain a bootstrap sample from a set of \\(n\\) observations.\n\n\n\nWhat is the probability that the first bootstrap observation is not the \\(j\\)th observation from the original sample? Justify your answer.\nWhat is the probability that the second bootstrap observation is not the \\(j\\)th observation from the original sample?\nArgue that the probability that the \\(j\\)th observation is not in the bootstrap sample is \\((1 − 1/n)^n\\).\nWhen \\(n = 5\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 100\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nWhen \\(n = 10,000\\), what is the probability that the \\(j\\)th observation is in the bootstrap sample?\nCreate a plot that displays, for each integer value of \\(n\\) from 1 to 100,000, the probability that the \\(j\\)th observation is in the bootstrap sample. Comment on what you observe.\nWe will now investigate numerically the probability that a bootstrap sample of size \\(n = 100\\) contains the \\(j\\)th observation. Here \\(j = 4\\). We repeatedly create bootstrap samples, and each time we record whether or not the fourth observation is contained in the bootstrap sample.\n\n\nstore &lt;- rep(NA, 10000)\nfor(i in 1:10000){\nstore[i] &lt;- sum(sample (1:100 , rep=TRUE) == 4) &gt; 0\n}\nmean(store)\n\n[1] 0.6355\n\n\nComment on the results obtained.\n\nWe will now consider the Boston housing data set, from the ISLR2 library.\n\n\n\nBased on this data set, provide an estimate for the population mean of medv. Call this estimate ˆμ.\nProvide an estimate of the standard error of ˆμ. Interpret this result. Hint: We can compute the standard error of the sample mean by dividing the sample standard deviation by the square root of the number of observations.\nNow estimate the standard error of ˆμ using the bootstrap. How does this compare to your answer from (b)?\nBased on your bootstrap estimate from (c), provide a 95% confidence interval for the mean of medv. Compare it to the results obtained using t.test(Boston$medv). Hint: You can approximate a 95% confidence interval using the formula [ˆμ − 2SE(ˆμ), ˆμ + 2SE(ˆμ)].\nBased on this data set, provide an estimate, ˆμmed, for the median value of medv in the population.\nWe now would like to estimate the standard error of ˆμmed. Unfortunately, there is no simple formula for computing the standard error of the median. Instead, estimate the standard error of the median using the bootstrap. Comment on your findings.\nBased on this data set, provide an estimate for the tenth percentile of medv in Boston census tracts. Call this quantity ˆμ0.1. (You can use the quantile() function.)\nUse the bootstrap to estimate the standard error of ˆμ0.1. Comment on your findings.https://ritsokiguess.site/pasias/the-bootstrap.html\n\nhttps://ritsokiguess.site/pasias/the-bootstrap.html\n\n\n25 Beyond STAT 340\nThese problems are excellent practice but they are beyond the material we cover in STAT 340."
  },
  {
    "objectID": "R08_prediction_examples.html#creating-linear-data",
    "href": "R08_prediction_examples.html#creating-linear-data",
    "title": "23  Simple Linear Regression - Examples",
    "section": "23.1 Creating Linear Data",
    "text": "23.1 Creating Linear Data\nWe can simulate data from a linear model \\[Y_i = 5 + 5X+\\epsilon\\] where \\(\\epsilon \\overset{iid}{\\sim}N(\\mu,\\sigma^2)\\).\n\n#There is no assumption that X follows any particular distribution\nX &lt;- rpois(30, lambda=3)\ne &lt;- rnorm(30, mean=0, sd=4)\n\nY &lt;- 5 + 5*X + e\n\nplot(X, Y)"
  },
  {
    "objectID": "R08_prediction_examples.html#atlanta-lead-example",
    "href": "R08_prediction_examples.html#atlanta-lead-example",
    "title": "23  Simple Linear Regression - Examples",
    "section": "23.2 Atlanta Lead Example",
    "text": "23.2 Atlanta Lead Example\n\nlead_data &lt;- read.csv(\"data/lead.csv\")\natlanta_lead &lt;- subset(lead_data, city==\"Atlanta\")\nsummary(atlanta_lead)\n\n     city           air.pb.metric.tons aggr.assault.per.million\n Length:36          Min.   : 421.0     Min.   : 431.0          \n Class :character   1st Qu.: 603.8     1st Qu.: 924.5          \n Mode  :character   Median : 813.5     Median :1295.5          \n                    Mean   : 919.9     Mean   :1399.2          \n                    3rd Qu.:1247.8     3rd Qu.:1850.2          \n                    Max.   :1516.0     Max.   :2301.0          \n\n# if we call lm without storing it to a object\nlm(aggr.assault.per.million ~ 1 + air.pb.metric.tons, data=atlanta_lead)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nCoefficients:\n       (Intercept)  air.pb.metric.tons  \n           107.943               1.404  \n\n\n\natlanta_lead_lm &lt;- lm(aggr.assault.per.million ~ 1 + air.pb.metric.tons, data=atlanta_lead)\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nScatter Plot using Base R\n\nplot(aggr.assault.per.million ~ air.pb.metric.tons, data=atlanta_lead)\nabline(atlanta_lead_lm, col=\"red\")\n\npoints(y=fitted(atlanta_lead_lm), x=atlanta_lead$air.pb.metric.tons, pch=16 )\n\n\n\n\n\natlanta_lead_lm$residuals\n\n          1           2           3           4           5           6 \n 330.080025  226.850054  155.793859 -216.454844  256.930171  102.756395 \n          7           8           9          10          11          12 \n-356.355996 -149.487118  382.879164 -273.899218 -268.626594 -280.180195 \n         13          14          15          16          17          18 \n 175.359863 -294.175006   23.787530  109.576291  -97.442440  -80.249933 \n         19          20          21          22          23          24 \n  -8.530910   45.337967  -43.830619  116.062179  -55.317646  -70.964906 \n         25          26          27          28          29          30 \n-129.344731   14.387834  -58.799484  143.529335  148.229626    9.807148 \n         31          32          33          34          35          36 \n 199.737410  -64.487372   16.648940   14.188998  -27.773538    3.977759 \n\n# or resid(atlanta_lead_lm)\n# or residuals(atlanta_lead_lm)\n\nA proper residual plot\n\nplot(y=atlanta_lead_lm$residuals,\n     x=fitted(atlanta_lead_lm))\n\n\n\n\nThe wrong way to make a residual plot\n\nplot(y=atlanta_lead_lm$residuals,\n     x=atlanta_lead$aggr.assault.per.million)\n\n\n\n\nChecking for auto-correlation among residuals\nThis is when the residual values tend to be associated with the ‘next’ residual.\n\n#we look at the correlation between residuals and the \n# next residual\nplot(x=resid(atlanta_lead_lm)[-length(resid(atlanta_lead_lm))], y= resid(atlanta_lead_lm)[-1])\n\n\n\n#we're looking for independence between the residual values - no shape \n\nQQ plot\n\nn &lt;- nrow(atlanta_lead)\n\n# Let's look at what 36 equally spaced quantiles would be\n(1:36)/(37)\n\n [1] 0.02702703 0.05405405 0.08108108 0.10810811 0.13513514 0.16216216\n [7] 0.18918919 0.21621622 0.24324324 0.27027027 0.29729730 0.32432432\n[13] 0.35135135 0.37837838 0.40540541 0.43243243 0.45945946 0.48648649\n[19] 0.51351351 0.54054054 0.56756757 0.59459459 0.62162162 0.64864865\n[25] 0.67567568 0.70270270 0.72972973 0.75675676 0.78378378 0.81081081\n[31] 0.83783784 0.86486486 0.89189189 0.91891892 0.94594595 0.97297297\n\nqs &lt;- qnorm((1:36)/(37))\nstripchart(qs)\n\n\n\n\nThe 4 pre-fab plots of the linear model object\n\nplot(atlanta_lead_lm)\n\n\n\n\n\n\n\n\n\n\n\n\n#You can run this to get just the first two plots\nplot(atlanta_lead_lm, which=1:2)\n\n\n\n\n\n\n\n\n23.2.1 looking at qqplots\nLet’s just explore the type of variation we see with normal QQ plots for this sample size so we know what tolerable deviation from the diagonal looks like\n\npar(mar=c(0,0,0,0))\npar(mfrow=c(3,3))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\nqqnorm(rnorm(n))\n\n\n\npar(mfrow=c(1,1))\n\n#also the qqPlot function in the car library adds a 95% tolerance band\nlibrary(car)\n\nLoading required package: carData\n\nqqPlot(residuals(atlanta_lead_lm))\n\n\n\n\n[1] 9 7"
  },
  {
    "objectID": "R08_prediction_examples.html#inference-on-the-coefficients",
    "href": "R08_prediction_examples.html#inference-on-the-coefficients",
    "title": "23  Simple Linear Regression - Examples",
    "section": "23.3 Inference on the coefficients",
    "text": "23.3 Inference on the coefficients\n\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nIf for example I want to test \\(H_0: \\beta_1 = 1.3\\) vs \\(H_A: \\beta_1 &gt; 1.3\\) The t statistic would be \\[t = \\dfrac{1.40375 - 1.3}{.08112}=1.278969\\]\n\nt &lt;- (1.40375-1.3)/.08112\n\nThe \\(p\\)-value is going to be the right -tail probability from a T distribtuion with 34 degrees of freedom\n\npt(t, df=34, lower.tail=FALSE)\n\n[1] 0.1047842"
  },
  {
    "objectID": "R08_prediction_examples.html#prediction",
    "href": "R08_prediction_examples.html#prediction",
    "title": "23  Simple Linear Regression - Examples",
    "section": "23.4 Prediction",
    "text": "23.4 Prediction\nPoint estimate\n\npredict(atlanta_lead_lm, newdata=data.frame(air.pb.metric.tons=1300))\n\n       1 \n1932.813 \n\n\nA confidence interval for \\(E(Y|X=1300)\\) This estimates the location of the regression line with 95% confidence.\n\npredict(atlanta_lead_lm, newdata=data.frame(air.pb.metric.tons=1300), interval=\"confidence\", level=.95)\n\n       fit      lwr      upr\n1 1932.813 1845.229 2020.397\n\n\nA ‘prediction’ interval for \\(\\hat{Y}|X=1300)\\) This estimates the range of values for a individual data point line with 95% confidence.\n\npredict(atlanta_lead_lm, newdata=data.frame(air.pb.metric.tons=1300), interval=\"prediction\")\n\n       fit      lwr      upr\n1 1932.813 1555.393 2310.233\n\n\nDemonstration of the difference between the two types of intervals.\n\nNMC &lt;- 1000\nb0 &lt;- 0\nb1 &lt;- 1\nplot(x=NA, y=NA, xlim=c(400,1400), ylim=c(0, 2000))\nfor(i in 1:NMC){\n  #Generate data from a linear model\n  X &lt;- runif(30, 400, 1400)\n  err &lt;- rnorm(30, 0, sd=100)\n  Y &lt;- 100 + 1.4 * X + err\n\n  myFit &lt;- lm(Y ~ X)\n  b0[i] &lt;- unname(coefficients(myFit)[1])\n  b1[i] &lt;- unname(coefficients(myFit)[2])\n  abline(myFit, col=rgb(0,0,0,.25))\n}\n\n\n\ny.est &lt;- b0 + 1300*b1\nquantile(y.est, c(.025, .975))\n\n    2.5%    97.5% \n1861.168 1979.547 \n\n\nFor the second type of confidence interval\n\nX &lt;- rep(1300, 1000)\ny.pred &lt;- 100 + 1.4 * X + rnorm(1000, 0, sd=100)\nquantile(y.pred, c(.025, .975))\n\n    2.5%    97.5% \n1732.464 2115.994"
  },
  {
    "objectID": "R08_prediction_examples.html#uncertainty-in-prediction",
    "href": "R08_prediction_examples.html#uncertainty-in-prediction",
    "title": "23  Simple Linear Regression - Examples",
    "section": "23.5 Uncertainty in Prediction",
    "text": "23.5 Uncertainty in Prediction\nConsider a linear relationship \\[y = 10 + 2x + \\epsilon\\]\nConsider if X ranges from .1 to 3, 1 to 30 or 10 to 300, and \\(\\sigma = .5, 5, 50\\)\n\npar(mfrow=c(3,3))\npar(mar=c(2,2,1,0))\nscalar &lt;- c(.1,1,10)\nfor(i in 1:3){\n  for(j in 1:3){\n    X &lt;- seq(scalar[i]*1, scalar[i]*30, length.out=30)\n    Y &lt;- 10 + 2 * X + rnorm(length(X), 0, sd=5*scalar[j])\n    plot(X, Y, main=ifelse(i==1, paste0(\"sigma=\",5*scalar[j]),\"\"))\n    abline(lm(Y~X))    \n  }\n}\n\n\n\n\nSo you can see that the fit of the linear model is related to the range of the \\(X\\) data as well as the variance of the error term.\n\n23.5.1 Uncertainty of the linear model estimate\nLet’s look at the uncertainty of the line of best fit. We’ll sample 30 points from this population and fit a line and repeat.\n\\(Y = 10 + 2X + \\epsilon\\)\n\nslopes &lt;- 0; intercepts &lt;- 0; #empty vectors\nset.seed(1)\nfor(i in 1:500){\n  X &lt;- runif(30, min=10, max=20)\n  Y &lt;- 10 + 2 * X + rnorm(30, mean=0, sd=5)\n  \n  if(i==1){\n    plot(X,Y)\n  }\n  linearFit &lt;- lm(Y~X)\n  abline(linearFit, col=rgb(0,0,0,.1))\n  slopes[i] &lt;- coef(linearFit)[2]\n  intercepts[i] &lt;- coef(linearFit)[1]\n}\nabline(a=10, b=2, col=\"red\")\n\n\n\n\nWhat is the standard deviation of the intercept and slope estimates?\n\nsd(intercepts)\n\n[1] 5.022983\n\nsd(slopes)\n\n[1] 0.332713\n\n\nLook at the summary output from the linear model (this is the very last model that was fit):\n\nsummary(linearFit)$coefficients\n\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 18.66863  6.6534833 2.805844 0.009027105\nX            1.46134  0.4236239 3.449615 0.001796701\n\n\nThese are not the exact same as the standard errors, but they are in the ballpark. These standard errors are related to this particular model fit with one set of \\((X,Y)\\) data. If we averaged the standard error over many model fits we’d probably see the values match much better.\n\n\n23.5.2 Two types of interval estimates for our linear model: Confidence intervals and prediction intervals.\nThe standard error (i.e. uncertainty) of the slope and intercept estimates is captured by the idea of a “confidence interval” in prediction. Using the last sampled \\((X,Y)\\) data we fit\n\nYlm &lt;- lm(Y~X)\nxrng &lt;- seq(10,20,.5)\n\nUsing the predict function we can estimate \\(E(Y|X=x)\\) for some particular value of \\(X\\). For example, if \\(X=16\\) a point estimate is given by\n\npredict(Ylm, newdata=data.frame(X=16))\n\n       1 \n42.05007 \n\n\nA confidence interval for \\(E(Y|X=16)\\) is given by adding the interval=\"confidence\" option\n\npredict(Ylm, newdata=data.frame(X=16), interval=\"confidence\")\n\n       fit      lwr      upr\n1 42.05007 39.96838 44.13175\n\n\nWe would say “I am 95% confident that the expected value of \\(Y\\) when \\(x=16\\) is between 39.97 and 44.13”.\nWe can actually plot the confidence interval band over the data range to see it visualized.\n\nCIconf &lt;- predict(Ylm, newdata=data.frame(X=xrng), interval=\"confidence\")\nplot(X,Y)\nabline(Ylm)\nabline(10,2, col=\"red\")\nlines(x=xrng, y=CIconf[,2], col=\"black\", lty=2)\nlines(x=xrng, y=CIconf[,3], col=\"black\", lty=2)\n\n\n\n\nIn a sense, we are 95% certain that the true line is within this interval, at least within the range of data that we have observed. But the better way to interpret it is that for any particular \\(X\\) value, we are 95% certain that the the expected value of \\(Y\\) (true line value in red) is within the interval.\nFor example, take \\(x=16\\). What is the 95% confidence interval for \\(E(\\hat Y|X=16)\\)?\n\npredict(Ylm, newdata=data.frame(X=16), interval=\"confidence\")\n\n       fit      lwr      upr\n1 42.05007 39.96838 44.13175\n\n\nAnd what is the true \\(E(Y|X=16)\\)?\n\n10+2*16\n\n[1] 42\n\n\nThe 95% confidence manifests itself if we were to repeat this many times, sampling a different sample each time and checking whether our interval covers the true expected value of 42.\n\ncontains42 &lt;- 0\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  X &lt;- runif(30, min=10, max=20)\n  Y &lt;- 10 + 2 * X + rnorm(30, mean=0, sd=5)\n  Ylm &lt;- lm(Y~X)\n  CI &lt;- predict(Ylm, newdata=data.frame(X=16), interval=\"confidence\")[,2:3]\n  contains42[i] &lt;- CI[1]&lt;=42 & CI[2]&gt;=42\n}\nmean(contains42) #proportion of TRUEs in the results\n\n[1] 0.95\n\n\nOn the other hand, a prediction interval is wider. Let’s just plot it and then talk about what it actually means.\n\nYlm &lt;- lm(Y~X)\nxrng &lt;- seq(10,20,.5)\nCIconf &lt;- predict(Ylm, newdata=data.frame(X=xrng), interval=\"confidence\")\nCIpred &lt;- predict(Ylm, newdata=data.frame(X=xrng), interval=\"prediction\")\nplot(X,Y)\nabline(Ylm)\nabline(10,2, col=\"red\")\nlines(x=xrng, y=CIconf[,2], col=\"black\", lty=2)\nlines(x=xrng, y=CIconf[,3], col=\"black\", lty=2)\nlines(x=xrng, y=CIpred[,2], col=\"blue\", lty=2)\nlines(x=xrng, y=CIpred[,3], col=\"blue\", lty=2)\n\n\n\n\nThis interval predicts the next observed point from the population. For example, suppose we observe 1000 more points with \\(X=16\\)\n\nnewX &lt;- rep(16,1000)\nnewY &lt;- 10 + 2 * newX + rnorm(1000, mean=0, sd=5)\n\nYlm &lt;- lm(Y~X)\nxrng &lt;- seq(10,20,.5)\nCIpred &lt;- predict(Ylm, newdata=data.frame(X=xrng), interval=\"prediction\")\nplot(X,Y, ylim=range(newY))\nabline(Ylm)\nabline(10,2, col=\"red\")\nlines(x=xrng, y=CIpred[,2], col=\"blue\", lty=2)\nlines(x=xrng, y=CIpred[,3], col=\"blue\", lty=2)\npoints(newX, newY, col=rgb(0,0,1,.1))\n\n\n\n\nMost of these new observations are within the prediction interval, but not all of them. What proportion of them?\n\nCI &lt;- predict(Ylm, newdata=data.frame(X=16), interval=\"prediction\")\nmean(newY &gt;= CI[2] & newY &lt;= CI[3])\n\n[1] 0.963\n\n\nNot a huge surprise - It’s a 95% prediction interval. How high or low this proportion is is dependent on the particular 30 data points that we used to fit the data - if our slope and intercept estimates are better, we’ll get closer to 95%, and if our standard error of residuals is higher this will increase the percentage because each prediction interval will be wider.\nA better way to measure the predictive power of the interval is to repeat with new data on every model fit:\n\ncontainsY &lt;- 0\nNMC &lt;- 1000\nfor(i in 1:NMC){\n  X &lt;- runif(30, min=10, max=20)\n  Y &lt;- 10 + 2 * X + rnorm(30, mean=0, sd=5)\n  Ylm &lt;- lm(Y~X)\n  newY &lt;- 10 + 2 * 16 + rnorm(1, mean=0, sd=5)\n  CI &lt;- predict(Ylm, newdata=data.frame(X=16), interval=\"prediction\")[,2:3]\n  containsY[i] &lt;- CI[1]&lt;=newY & CI[2]&gt;=newY\n}\nmean(containsY) #proportion of TRUEs in the results\n\n[1] 0.956\n\n\n\n\n23.5.3 Contrasting the interpretations\nOur interpretation of a prediction interval is like this: “I am 95% confident (certain) that for a new observation with \\(X=16\\), \\(Y\\) will be within the interval”\nOur interpretation of a confidence interval is : “I am 95% confident that the mean \\(Y\\) value for all observations with \\(X=16\\) is within the interval”\n\n\n23.5.4 Why the difference in size?\nThe standard error of a confidence interval for \\(E(Y|x)\\) is \\[\nSE(\\overline{Y}|x) = s_e \\sqrt{\\frac1n + \\frac{(x-\\bar{x})^2}{SS_{xx}}}\n\\] The standard error of a prediction interval for \\(\\hat{Y}|x\\) is \\[\nSE(\\hat{Y}|x) = s_e \\sqrt{1+\\frac1n + \\frac{(x-\\bar{x})^2}{SS_{xx}}}\n\\] Thus the prediction interval will be wider (notice the “1+” in the formula).\nWhy is the prediction interval wider than the confidence interval? There are two sources of uncertainty. The first is that we only estimated the placement of the model line - the slope and intercept are both estimates. As you saw before, new data would give us new estimates and there is some natural uncertainty.\nThe second source of uncertainty is that there are factors not in the model which give variation captured in the \\(\\sigma^2_\\epsilon\\) term in the model."
  },
  {
    "objectID": "estimation1.html#learning-objectives",
    "href": "estimation1.html#learning-objectives",
    "title": "19  Estimation Part 1",
    "section": "",
    "text": "Explain the statistical task of estimation and give examples of real-world estimation problems.\nDefine the concept of a statistic and explain how and why we view a statistic as a random quantity.\nExplain the difference between an estimate and an estimator.\nUse the law of large numbers to explain why larger sample sizes are generally preferable when performing estimation.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#statistical-estimation",
    "href": "estimation1.html#statistical-estimation",
    "title": "19  Estimation Part 1",
    "section": "19.2 Statistical Estimation",
    "text": "19.2 Statistical Estimation\nThe goal of estimation is to (approximately) determine the value of a quantity out there in the world. Often, we identify this quantity with a parameter in a model, such as the mean of a normal.\nExample: Human heights\nLet’s think back to our human height example from our first lecture. Recall that our goal was to determine the average human height, \\(\\mu\\).\nWe said that it was infeasible to measure the height of every human, but we could measure the heights \\(X_1,X_2,\\dots,X_n\\) of a few thousand humans and report the mean of that sample (the “sample mean”), \\[\n\\hat{\\mu} = \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i,\n\\] where \\(n\\) is the number of humans in our sample.\nThus, we might report the value of \\(\\hat{\\mu}\\) (say, 172.1 cm) and state that “We estimate the average human height to be \\(172.1\\) cm.”\nThis value \\(\\hat{\\mu}\\) is called a point estimate. We make our “best guess” as to the true value of \\(\\mu\\).\nAside: in case you haven’t seen it before, this “hat” notation, where we write \\(\\hat{\\mu}\\) for our estimate is common in statistics. If we are trying to esitmate a parameter, say, \\(\\mu\\), then we will often write our estimate of that parameter as the same symbol with a hat (technically a circumflex) on it, like \\(\\hat{mu}\\).\nFacts from probability theory (specifically, the law of large numbers, which we’ll talk about soon) state that this sample mean \\(\\hat{\\mu}\\) is close to the true population mean \\(\\mu\\).\nBut how close is close?\nIn addition to our estimate \\(\\hat{\\mu}\\), we would like to have some kind of notion of how certain we are in our estimate.\nSaid another way, if we say that “we estimate the average human height to be 172.1 cm”, we might also be willing to say that \\(172.3\\) cm or \\(171.8\\) cm are also reasonable estimates.\nIf you have seen confidence intervals (CIs) before, both of these ideas should sound somewhat familiar. If you haven’t seen CIs before, not to worry– we’ll discuss them in great detail next week. This week, we’ll talk primarily about point estimates, but we’re also building the groundwork needed to talk about confidence intervals next week.\n\n19.2.1 Example: Universal Widgets of Madison\nThe Universal Widgets of Madison (UW-Madison) company manufactures widgets Their widget machine produces widgets all day.\nUnfortunately, making widgets is hard, and not all widgets produced by the machine are functional. Due to randomness in the manufacturing process, a widget is functional with probability \\(p\\), and dysfunctional with probability \\(1-p\\). The engineers on the UW-Madison production line are quite confident that widgets are independent of one another– that is, whether or not one widget is dysfunctional has no bearing on whether or not any other widgets coming off the production line are dysfunctional.\nUW ships widgets in batches, and they want to ensure that every batch ships with at least 5 functional widgets in it.\nThus, we have two (related) questions to answer:\n\nWhat is a good estimate for \\(p\\) (i.e., a point estimate for \\(p\\))?\nHow many widgets should be in a batch to ensure that (with high probability) a batch ships with at least \\(5\\) functional widgets in it?\n\nWe will focus on the first of these two questions, since if we have a good estimate for \\(p\\), we can get a decent answer to question (2) using Monte Carlo methods. Still, in the course of these lectures, you will see how to address the second question quite easily.\nStep 1: Specify a model\nAll of statistics starts with choosing a model for the world, so let’s start there.\nWhat would be a good model for this setting?\nSince the outcome of interest here is binary (i.e., it is a yes/no or success/failure outcome), it is natural to model whether a widget is functional or dysfunctional as a Bernoulli random variable with success probability \\(p\\).\nThat is, we model each widget as being functional with probability \\(p\\) and dysfunctional with probability \\(1-p\\).\nThe production engineers are condfident that we are safe assuming that widget are independent. Of course, in the real world, the independence assumption is probably unrealistic, but we’ll let this slide becuase if we tried to account for dependency we would have a rough time creating a model.\nSo, we will make the following assumption: widgets are functional independently with probability \\(p\\).\nWe’ll imagine that we take a sample of widgets from the production line at UW-Madison, and use that sample to try and estimate \\(p\\).\nHaving chosen a model for our data, the first thing we need to do is implement it in R. For now, we’ll make arbitrary choices for the number of widgets n and the probability p of a widget being functional.\n\nn &lt;- 200; # We will examine n=200 widgets\np &lt;- 0.8; # Suppose that 80% of widgets are functional\nfunctional_widgets &lt;- rbinom(1, size=n, p); # Draw one sample of widgets.\nfunctional_widgets; # How many of the n widgets are functional?\n\n[1] 156\n\n\nQuestion: why is the binomial distribution the right thing to use, here?\n\n# Let's wrap that up in a function for use later.\ngenerate_widgets &lt;- function(n,p) {\n  return( rbinom(1, size=n, p) );\n}\n\nStep 2: Estimating \\(p\\)\nSuppose that we can collect data by observing widgets \\(1,2,\\dots,n\\).\nLet’s denote our data by \\(X_1,X_2,\\dots,X_n\\), where \\(X_i=1\\) if the \\(i\\)-th widget is functional and \\(X_i=0\\) if it is dysfunctional. That is, recalling our indicator function notation, \\[\nX_i = 1_{\\large \\text{ widget } i \\text{ is functional } }\n\\]\nIf we examine enough widgets, we know that we can estimate \\(p\\) very well using the sample mean\n\\[\\overline{X} = \\frac{1}{n} \\sum_{i=1}^n X_i = \\frac{ \\text{# of functional widgets} }{ n }.\\]\nAgain, the law of large numbers (which we’ll discuss more formally soon), says that once \\(n\\) is big, this estimate will be really close to \\(\\mathbb{E} \\bar{X} = p\\). More specifically, the more widgets we examine, the more accurate our estimate will be (on average).\nUnfortunately, widgets aren’t free. So, here are two questions:\n\nIf we are willing to tolerate an error of, say, 2%, how many widgets do we need to examine?\nSuppose we examine 1000 widgets and observe that 882 of them are functional, so we estimate \\(p\\) to be \\(882/1000 = 0.882\\). How close is this to the true value of \\(p\\), on average?\n\nQuestion 1 is a question about experiment design. Specifically, it is a question about sample size. How many observations (i.e., how much data) do we need to collect in order to get a certain level of estimation accuracy?\nQuestion 2 is a question about the accuracy of a specific estimate, namely the sample mean. We will see below that these two questions are, in a certain sense, two sides of the same coin.\nSo, to start, what do we mean when we say that our estimate will be close to \\(p\\)?\nLet’s see this in action with a simulation.\n\n# Still n=200 widgets, 80% of which are functional.\nn &lt;- 200; p &lt;- 0.8;\n\n# This time, we'll generate lots of iterations\n# of our experiment, and we'll make a histogram of our\n# estimates of p.\n# This is going to look a lot like Monte Carlo!\nNMC &lt;- 1000;\nfunctional_widgets &lt;- rep(NA, NMC);\nfor(i in 1:NMC) {\n  functional_widgets[i] &lt;- generate_widgets(n,p)\n}\n# Plot estimates of p, #functional/#observations\nhist( functional_widgets/n );\n# Draw a vertical line at the true value of p\nabline( v=p, col='red', lwd=4 );\n\n\n\n\n\n\n\n\nLet’s pause and make sure we understand the experiment we just ran.\nEach data point in the above histogram corresponds to a single instance of our experiment, in which we observe \\(n=200\\) widgets, each of which is functional independently with probability \\(p=0.8\\) (indicated in red in the plot).\nTo estimate \\(p\\), we count up what fraction of the \\(200\\) widgets in our sample are functional.\nSince the data are random, our estimate of \\(p\\) is also random. The histogram above illustrates that randomness.\nSometimes our estimate is a bit higher than the true value of \\(p\\), sometimes it is lower. But as we can see, most of the time our estimate is close to \\(p\\), within about \\(0.06\\).",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#aside-estimators-estimates-and-statistics",
    "href": "estimation1.html#aside-estimators-estimates-and-statistics",
    "title": "19  Estimation Part 1",
    "section": "19.3 Aside: Estimators, Estimates and Statistics",
    "text": "19.3 Aside: Estimators, Estimates and Statistics\nBefore continuing our investigation of widgets, let’s take a moment to discuss things in more generality and establish some vocabulary.\nSuppose we have our data \\(X_1,X_2,\\dots,X_n\\). If we performed another experiment, we would presumably see a different set of values for our data. That is reflected in the fact that we model the observations \\(X_1,X_2,\\dots,X_n\\) as being random variables.\nSo, in our example above, \\(X_i\\) is a Bernoulli random variable representing whether or not widget \\(i\\) is functional.\nWe might observe that six our of ten widgets are functional, but that could be entirely due to chance– on another day we might observe that seven out of ten are functional, or four out of ten or… etc.\nWe typically summarize our data with a statistic, say \\(S(X_1,X_2,\\dots,X_n)\\). This should remind you of our test statistics from hypothesis testing. Remember, a statistic is just a function of our data– it takes our data as input and spits out a number (or collection of numbers) summarizing our data.\nIn our example above, we summarized the data with the sample mean \\(S(X_1,X_2,\\dots,X_n) = n^{-1} \\sum_{i=1}^n X_i\\), but this statistic \\(S\\) can be any function of your data. We usually choose the function \\(S\\) to be so that \\(S(X_1,X_2,\\dots,X_n)\\) will tend to be close to our quantity of interest (e.g., the mean \\(\\mu\\) in our human heights example, or our probability \\(p\\) in our widgets example).\nWe call this function \\(S\\) an estimator for that quantity of interest.\nIn our widgets example, we are estimating the probability \\(p\\), and we chose our statistic to be the sample mean of the data (i.e., the fraction of widgets that were functional). That is, we used the sample mean as our estimator for \\(p\\).\nWe call a particular value of this estimator (i.e., \\(S\\) applied to a particular choice of data) an estimate of our quantity. So, if we observe 162 functional widgets in our sample of \\(n=200\\) widgets, our estimate of \\(p\\) is \\(162/200 = 0.81\\).\nNow, since the data \\(X_1,X_2,\\dots,X_n\\) are random, and \\(S = S(X_1,X_2,\\dots,X_n)\\) is a function of the data, that means that our statistic \\(S\\) is also random. So, in just the same way that \\(X_i\\) has a distribution (e.g., \\(X_i \\sim \\operatorname{Bernoulli}(p)\\) above), \\(S\\) also has a distribution.\nWe usually call this distribution the sampling distribution, because it describes the behavior of our statistic, which is a function of the sample.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#more-data-better-accuracy",
    "href": "estimation1.html#more-data-better-accuracy",
    "title": "19  Estimation Part 1",
    "section": "19.4 More data, better accuracy",
    "text": "19.4 More data, better accuracy\nSo, let’s turn back to the first of our two questions: If we are willing to tolerate an error of, say, 2%, how many widgets do we need to examine?\nWell, let’s start by looking at the histogram of estimates from \\(2000\\) different runs with \\(n=200\\) and \\(p=0.8\\).\n\nn &lt;- 200; p &lt;- 0.8; # Still n=200 widgets, 80% of which are functional.\n\n# We'll generate lots of iterations\nNMC &lt;- 2000;\nfunctional_widgets &lt;- rep(NA, NMC);\nfor(i in 1:NMC) {\n  functional_widgets[i] &lt;- generate_widgets(n,p)\n}\n# Plot estimates of p, #functional/#observations\nhist( functional_widgets/n );\n# Draw a vertical line at the true value of p\nabline( v=p, col='red', lwd=4 );\n\n\n\n\n\n\n\n\nMost of the estimates are between \\(0.72\\) and \\(0.88\\).\nLet’s try increasing \\(n\\) from \\(n=200\\) to \\(n=500\\). That is, let’s try gathering more data, in the form of more widgets.\n\n# n=500 widgets instead of 200, but still 80% are functional.\nn &lt;- 500; p &lt;- 0.8;\n\n# We'll generate lots of iterations\nNMC &lt;- 2000;\nfunctional_widgets_larger &lt;- rep(NA, NMC);\nfor(i in 1:NMC) {\n  functional_widgets_larger[i] &lt;- generate_widgets(n,p)\n}\n# Plot estimates of p, #functional/#observations\nhist( functional_widgets_larger/n );\n# Draw a vertical line at the true value of p\nabline( v=p, col='red', lwd=4 );\n\n\n\n\n\n\n\n\nIf you compare this plot to the one above, you’ll see that the values are more tightly concentrated about \\(p=0.8\\). In fact, let’s just display them both in one plot.\n\np &lt;- 0.8; # Still n=200 widgets, 80% of which are functional.\n\n# Put the data into a data frame to pass to ggplot2.\nphat &lt;- c(functional_widgets/200, functional_widgets_larger/500 ); # \"p hat\", i.e., estimate of p.\nn &lt;- c( rep(200, 2000), rep(500, 2000) );\ndf &lt;- data.frame( 'n'=as.factor(n), 'phat'=phat);\n\nlibrary(ggplot2)\npp &lt;- ggplot( df, aes(x=phat, color=n, fill=n));\npp &lt;- pp + geom_histogram( aes(), position='identity', alpha=0.5, binwidth=0.01);\npp &lt;- pp + geom_vline( xintercept=p, color='red');\npp\n\n\n\n\n\n\n\n\nLooking at the plot, we see that the \\(n=500\\) estimates (blue) tend to cluster more tightly around the true value of \\(p\\) (\\(p=0.8\\), indicated by the vertical red line), when compared with the \\(n=200\\) estimates (orange).\nGathering more data (i.e., observing more widgets) gives us a more accurate (on average!) estimate of \\(p\\).\nJust to drive this home, let’s increase \\(n\\) even more.\n\np &lt;- 0.8; # Still using 80% functional rate.\n\n# Note: there are \"cleaner\" ways to build this data frame,\n# but those ways are harder to understand on a first glance.\n# At this stage of your career, \"clumsy but easy to read\"\n# is better than \"short but cryptic\"\nwidgets_100 &lt;- rbinom(1000, size=100, p);\nwidgets_200 &lt;- rbinom(1000, size=200, p);\nwidgets_400 &lt;- rbinom(1000, size=400, p);\nwidgets_800 &lt;- rbinom(1000, size=800, p);\n\n# Compute \"p hat\", i.e., estimate of p\nphat &lt;- c(widgets_100/100, widgets_200/200,\n          widgets_400/400, widgets_800/800 );\nn &lt;- c( rep(100, 1000), rep(200, 1000), rep(400, 1000), rep(800, 1000) );\n\n# Put the data into a data frame to pass to ggplot2.\ndf &lt;- data.frame( 'n'=as.factor(n), 'phat'=phat);\npp &lt;- ggplot( df, aes(x=phat, color=n ));\n# Using a smoothed density instead of histogram for easy comparison\npp &lt;- pp + geom_density( size=2 );\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp &lt;- pp + geom_vline( xintercept=p, color='red', size=1.5);\npp",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#more-data-how-much-better-accuracy",
    "href": "estimation1.html#more-data-how-much-better-accuracy",
    "title": "19  Estimation Part 1",
    "section": "19.5 More data, how much better accuracy?",
    "text": "19.5 More data, how much better accuracy?\nThe plot above certainly seems to indicate that as we increase the number of samples (i.e., the number of widgets \\(n\\)), our estimate becomes more accurate, in the sense that it is closer to the true value of \\(p\\) (on average, anyway).\nBut when we increase our sample from, say \\(n=100\\) to \\(n=800\\), like in the experiment above, just how much better does our estimate become?\nAs a reminder, we are denoting our data by \\(X_1,X_2,\\dots,X_n\\), where \\(X_i=1\\) if the \\(i\\)-th widget is functional and \\(X_i=0\\) if it is dysfunctional. That is, recalling our indicator function notation yet again, \\[\nX_i = 1_{\\large \\text{ widget } i \\text{ is functional } }.\n\\]\nWe are using the sample mean as our estimator, \\[\n\\hat{p} = \\hat{p}(X_1,X_2,\\dots,X_n) = \\frac{1}{n} \\sum_{i=1}^n X_i.\n\\]\nLet’s consider the variance of this estimator.\nWhy the variance? Well, remember that for a random variable \\(Z\\), \\[\n\\operatorname{Var} Z = \\mathbb{E}( Z - \\mathbb{E}Z)^2.\n\\] That is, the variance describes how close a variable is on average to its expectation.\nNow, the expectation of our estimator is \\[\n\\mathbb{E} \\hat{p}\n= \\mathbb{E} \\frac{1}{n} \\sum_{i=1}^n X_i\n= \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E} X_i\n= \\frac{1}{n} \\sum_{i=1}^n p = p,\n\\]\nwhere we used\n\nthe definition of our estimator \\(\\hat{p}\\)\nlinearity of expectation: \\(\\mathbb{E}(aX+bY) = a \\mathbb{E} X + b \\mathbb{E} Y\\)\nthe fact that the expectation of an indicator is the probability of the event in the indicator: \\(\\mathbb{E} 1_{A} = \\Pr[ A ]\\).\nbasic facts about summation: \\(\\sum_{i=1}^n p = np\\).\n\nSo on average, our estimator \\(\\hat{p}\\) is equal to the thing we are trying to estimate. That’s good! In fact, it’s so good that statisticians have a special name for this property: we say that \\(\\hat{p}\\) is an unbiased estimator of \\(p\\).\nBut the fact that our estimator is on average equal to \\(p\\) doesn’t tell us about how close it is to \\(p\\). For example, suppose that we have an estimator that is equal to \\(p+100\\) half the time and \\(p-100\\) the other half of the time. On average, our estimate is equal to \\(p\\): \\[\n\\frac{1}{2}(p+100) + \\frac{1}{2}(p-100) = \\frac{p}{2} + 50 + \\frac{p}{2} - 50 = p,\n\\] but our estimate is never particularly close to \\(p\\)…\nSo how close is our sample mean \\[\n\\hat{p} = \\frac{1}{n} \\sum_{i=1}^n X_i\n\\]\nto \\(p\\), on average (as measured by the average squared distance)?\nLet’s try to compute it:\n\\[\n\\operatorname{Var} \\hat{p}\n=\n\\operatorname{Var} \\sum_{i=1}^n \\frac{ X_i }{n}.\n\\]\nNow, our widget indicators \\(X_1,X_2,\\dots,X_n\\) are independent, so we can use the fact that the variance of a sum of independent random variables is just the sum of their variances: \\[\n\\operatorname{Var} \\hat{p}\n=\n\\sum_{i=1}^n \\operatorname{Var} \\frac{ X_i }{n}.\n\\]\nNow, since all of the \\(X_i\\) have the same distribution, we just have to compute \\[\n\\sigma^2_n = \\operatorname{Var} \\frac{ X_1 }{n},\n\\]\nand we’ll be done, since \\(\\operatorname{Var} \\hat{p} = n \\sigma^2_n\\).\nSo what is \\(\\sigma^2_n\\)?\n\\[\n\\operatorname{Var} \\frac{ X_1 }{n}\n= \\mathbb{E} \\left( \\frac{X_1 - \\mathbb{E} X_1}{n} \\right)^2\n= \\mathbb{E} \\left( \\frac{X_1 - p}{n} \\right)^2.\n\\]\nNow, \\(X_1\\) has a discrete distribution: \\(X_1 = 1\\) with probability \\(p\\) and \\(X_1=0\\) with probability \\(1-p\\). So, \\[\n\\begin{aligned}\n\\operatorname{Var} \\frac{ X_1 }{n}\n&= \\mathbb{E} \\left(\\frac{X_1 - p}{n} \\right)^2 \\\\\n&= \\frac{ (1-p)^2 }{n^2} \\Pr[ X_1 = 1] + \\frac{ (-p)^2 }{ n^2 } \\Pr[ X_1 = 0 ] \\\\\n&= \\frac{ (1-p)^2 p + p^2(1-p) }{ n } \\\\\n&= \\frac{ p(1-p)[ (1-p) + p ] }{ n^2 } \\\\\n&= \\frac{ p(1-p) }{ n^2 }.\n\\end{aligned}\n\\] If you’ve played around with Bernoulli random variables before, that numerator should look familiar– that’s the variance of a Bernoulli with success parameter \\(p\\). The \\(n^2\\) in the denominator is indicative of a basic fact about variance that you may have seen before, depending on your background: \\(\\operatorname{Var} aX = a^2 \\operatorname{Var} X\\).\nOkay, so we have found that \\[\n\\sigma^2_n = \\operatorname{Var} \\frac{ X_1 }{n} = \\frac{ p(1-p) }{ n^2 },\n\\]\nand we said that \\(\\operatorname{Var} \\hat{p} = n \\sigma^2_n\\), so we have found the variance of our estimator: \\[\n\\operatorname{Var} \\hat{p} = n \\frac{ p(1-p) }{ n^2 } = \\frac{ p(1-p) }{ n}.\n\\]",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#taking-stock-more-data-less-variance",
    "href": "estimation1.html#taking-stock-more-data-less-variance",
    "title": "19  Estimation Part 1",
    "section": "19.6 Taking stock: more data less variance",
    "text": "19.6 Taking stock: more data less variance\nDang, that was a lot of math. What did we learn as a result of it?\nWell, the variance of our estimator \\(\\hat{p}\\), based on a sample of size \\(n\\), is \\[\n\\operatorname{Var} \\hat{p} = \\frac{ p(1-p) }{ n}.\n\\]\n\\(p\\) doesn’t depend on \\(n\\)m so as the sample size \\(n\\) increases, the variance decreases like \\(1/n\\). That is, if we want to decrease the variance of our estimate by 1/2, we need to double our sample size.\nThere’s one small flaw here. The variance isn’t quite the right measure of “how close” our estimate is to our target \\(p\\). The more appropriate choice is the standard deviation.\nIn later courses you’ll see in detail why this is really the right quantity to care about here, but for now, think of it this way:\nKind of like a physics problem, the “units” of variance is “squared stuff”, where “stuff” is the unit that your variable is measured in.\nBut the “right” way to measure how close we are to something isn’t in squared units– that’s like an area, not a distance. So we have to take the square root of the variance to get a sensible answer.\nSo, \\[\n\\operatorname{sd} \\hat{p} = \\sqrt{ \\operatorname{Var} \\hat{p} }\n= \\frac{ \\sqrt{ p(1-p) } }{ \\sqrt{n} }.\n\\]\nNow, suppose that we have a sample of size \\(n\\), and we want to cut “how close our estimate is on average” in half (i.e., decrease the standard deviation by a factor of two).\nMultiplying \\(n\\) by \\(4\\) decreases \\(\\operatorname{sd} \\hat{p}\\) by \\(\\sqrt{4} = 2\\). So to halve our standard deviation, we have to increase our sample size by 4. To decrease our standard deviation by a factor of ten, we need to increase our sample size by a factor of 100.\nThat’s going to get our of hand quickly, especially if samples are challenging or expensive to get (e.g., subjects in a medical study)…",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#aside-probability-of-bad-events",
    "href": "estimation1.html#aside-probability-of-bad-events",
    "title": "19  Estimation Part 1",
    "section": "19.7 Aside: probability of “bad” events",
    "text": "19.7 Aside: probability of “bad” events\nSo more data (increasing \\(n\\)) gives us a more accurate estimate (i.e., makes our estimate concentrate closer to the true \\(p\\) on average).\nBut we started our widgets example asking about how to guarantee that our estimate is close to the probability \\(p\\).\nThere is a problem with this, though. Our data is random, and sometimes we get unlucky. So we can never guarantee that our estimate is close.\nLet’s take a short aside to make this more precise.\nWe saw in our simulation above that our estimate \\(\\hat{p}\\) of \\(p\\) was usually close to \\(p\\), and making \\(n\\) bigger (i.e., collecting more data) meant that \\(\\hat{p}\\) was closer to \\(p\\), on average.\nCan we guarantee that, if \\(n\\) is big enough, then \\(\\hat{p}\\) will be arbitrarily close to \\(p\\)?\nUnfortunately, the answer is no.\nTo see what this is the case, let’s consider a very specific event: the event that all \\(n\\) of our widgets are functional.\nWhen this happens, our estimate of \\(p\\) is \\[\n\\hat{p} = n^{-1} \\sum_{i=1}^n X_i = n^{-1} n = 1.\n\\]\nThis event has probability (we’re going to use independence of the widgets to write the probability of \\(X_1=X_2=\\cdots=X_n=1\\) as a product of probabilities) \\[\n\\Pr[ X_1=1, X_2=1, \\dots, X_n = 1 ]\n= \\prod_{i=1}^n \\Pr[ X_i = 1 ] = p^n.\n\\] Now, unless \\(p=0\\), this means that the event \\(X_1=X_2=\\cdots=X_n=1\\) occurs with some positive probability, albeit very small.\nThat is, no matter how large \\(n\\) is, there is still some small but positive probability that our estimate is simply \\(\\hat{p} =1\\). What that means is that we can never give a 100% guarantee that our estimate is arbitrarily close to the true value of \\(p\\)– there’s always a vanishingly small chance that \\(\\hat{p}=1\\).\nNow, with that said, notice that as \\(n\\) gets larger, the probability of this bad “all widgets are functional” event gets smaller and smaller. Roughly speaking, this is what we mean when we say that more data gives us a more accurate estimate. The probability that our estimate is far from the true value of \\(p\\) gets smaller and smaller as we increase \\(n\\).\nThe law of large numbers, which we will (finally) discuss soon, will let us say something both stronger and more precise than this, but the above example is a good illustration of the core idea.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#more-data-better-accuracy-part-ii",
    "href": "estimation1.html#more-data-better-accuracy-part-ii",
    "title": "19  Estimation Part 1",
    "section": "19.8 More data, better accuracy part II",
    "text": "19.8 More data, better accuracy part II\nInstead of trying to do more math, let’s try and code up an experiment to get a handle on this.\nLet’s simplify things a bit by writing a function that will generate a random copy of \\(S(X_1,X_2,\\dots,X_n) = \\hat{p}\\) given a choice of \\(n\\) and the true value of \\(p\\).\n\nsimulate_S &lt;- function( n, p ) {\n  functional_widgets &lt;- rbinom(1, size=n, prob=p);\n  # Our statistic is the fraction of the n widgets\n  # that are functional.\n  return(functional_widgets/n);\n}\n\n# Simulate n=200 widgets with functional probability p=0.8\nsimulate_S(200, 0.8) \n\n[1] 0.8\n\n\nNow, we want to use this function to estimate the probability that our estimate is within \\(0.02\\) of \\(p\\).\nThat is, we want to estimate \\[\n\\Pr\\left[ S \\in (p-0.02, p+0.02) \\right]\n=\n\\Pr\\left[ | S(X_1,X_2,\\dots,X_n) - p | &lt; 0.02 \\right]\n\\] We could explicitly compute this number. After all, we know how to compute the probability distribution of the Bernoulli and/or Binomial distributions.\nBut instead, let’s just use Monte Carlo estimation.\n\n# Here's a function that will take our estimate S (= phat) and check if it is within 0.02 of p or not.\ncheck_if_S_is_good &lt;- function( S, p ) {\n    return( abs(S-p) &lt; 0.02)\n}\n\n# Now, let's simulate a lot of instances of our experiment\n# and count up what fraction of the time our estimate is \"good\"\nN_MC &lt;- 2000; # Repeat the experiment 2000 times. N_MC = \"number of Monte Carlo (MC) replicates\"\nn &lt;- 200; p &lt;- 0.8; # Still using n=200, p=0.8\n\n# Create a data frame to store the outcome of our experiment.\n# We are initially filling entries with NAs, which we will fill in as we run.\nmonte_carlo &lt;- data.frame(replicate = 1:N_MC, S = rep(NA, N_MC), S_good = rep(NA, N_MC));\n\n# Let's just check what the data frame looks like before we populate it.\nhead( monte_carlo )\n\n  replicate  S S_good\n1         1 NA     NA\n2         2 NA     NA\n3         3 NA     NA\n4         4 NA     NA\n5         5 NA     NA\n6         6 NA     NA\n\n\n\n# For each replicate, run the experiment and record results.\n# We want to keep track of the value of S and whether or not S was good.\nfor(i in 1:N_MC){\n    monte_carlo$S[i] &lt;- simulate_S( n, p );\n    monte_carlo$S_good[i] &lt;- check_if_S_is_good(monte_carlo$S[i], p)\n}\n\nsum( monte_carlo$S_good )/N_MC\n\n[1] 0.4995\n\n\nSo about half of our estimates were within \\(0.02\\) of \\(p\\).\nOur experiments above suggested that we could improve this by increasing \\(n\\), so let’s try that.\n\n# This is the exact same setup except we're changing n from 200 to 400.\nN_MC &lt;- 2000; n &lt;- 400; p &lt;- 0.8; # Still using p=0.8 and 2000 Monte Carlo trials.\n\n# Note that we don't really have to create the data frame again.\n# We could, if we wanted, just overwrite it,\n# but this is a good habit to be in\n# to make sure we don't accidentally \"reuse\" old data.\nmonte_carlo &lt;- data.frame( replicate = 1:N_MC,\n                           S = rep(NA, N_MC),\n                           S_good = rep(NA, N_MC));\n\nfor(i in 1:N_MC){\n  monte_carlo$S[i] &lt;- simulate_S( n, p );\n  monte_carlo$S_good[i] &lt;- check_if_S_is_good(monte_carlo$S[i], p)\n}\n\nsum( monte_carlo$S_good )/N_MC\n\n[1] 0.676\n\n\nThat’s an improvement! But still about 30% of the time we’re going to be more than 0.02 away from \\(p\\)…\n\n# Just as a check, let's plot a histogram again.\npp &lt;- ggplot(monte_carlo, aes(x = S)) + geom_histogram(bins = 30) + geom_vline( xintercept=p, col='red' )\n\npp\n\n\n\n\n\n\n\n\nExercise: play around with \\(n\\) in the above code to find how large our sample size has to be so that \\(\\Pr[ |S-p| \\le 0.02 ] \\approx 0.95\\).\nSo we can never have a perfect guarantee that our estimate is within, say, \\(0.02\\) of the truth. Instead, we have to settle for guarantees like “with probability 0.95, our estimate is within \\(0.02\\) of the truth.”\nThis is the idea behind confidence intervals, which we’ll discuss in more detail next week.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#what-are-we-trying-to-estimate",
    "href": "estimation1.html#what-are-we-trying-to-estimate",
    "title": "19  Estimation Part 1",
    "section": "19.9 What are we trying to estimate?",
    "text": "19.9 What are we trying to estimate?\nIn our discussions above, we are interested in\n\nEstimating \\(p\\) and\nKnowing how good our estimate is (i.e., “how likely is it that we are within 0.02 of the truth?”).\n\nGenerally, once we compute the statistic \\(S\\), we could just report it and be done with it. “We estimate \\(p\\) to be \\(0.785\\)”, and leave it at that. That’s the problem of point estimation. But this leaves open the question of how close our estimate is to the truth.\nIf we knew \\(p\\), like in the examples above, we could say how close we are, but we don’t know \\(p\\). So, how can we say how close we are without knowing the true value of the thing we are estimating?\nAbove, \\(p\\) was defined as a parameter in a model. However, often times, “parameters” can be imagined as something different. Here are two other ways:\n\nImagine getting an infinite amount of data. What would be the value of \\(S\\) with an infinite amount of data?\nImagine repeating the whole experiment lots of different times and on experiment \\(i\\) you created statistic \\(S_i\\). What is the average of those statistics \\(S_1,S_2,\\dots\\)?\n\nFor most functions of the data \\(S\\), these two values are the same thing (though the first one might be a bit easier to think about). However, if they are different (and sometimes they are), it is the second one that we are actually going to use. That second value is in fact the expected value of the statistic, \\(\\mathbb{E} S(X_1,X_2,\\dots,X_n)\\), which we will often shorten to just \\(\\mathbb{E} S\\), with it being understood that \\(S\\) depends on the data \\(X_1,X_2,\\dots,X_n\\).\nExample: The maximum of the \\(X_i\\) is one statistic for which those two notions are not the same. So is the minimum. Why?\nSo, to recap, here is the problem:\n\n\\(S\\) is a random variable.\nWe only observe one example of it, but we want to estimate \\(\\mathbb{E} S\\).",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#point-estimation-a-good-place-to-begin",
    "href": "estimation1.html#point-estimation-a-good-place-to-begin",
    "title": "19  Estimation Part 1",
    "section": "19.10 Point estimation: a good place to begin",
    "text": "19.10 Point estimation: a good place to begin\nSo, we want an estimate of \\(\\mathbb{E} S\\). Well, what better estimate than \\(S\\) itself? This isn’t an arbitrary decision– there are good mathematical reasons behind this. The simplest of these reasons comes from the definition of the expectation: “on average”, \\(S = \\mathbb{E} S\\), and \\(S\\) is “usually” close to \\(\\mathbb{E} S\\).\nWe’ve mentioned the law of large numbers (LLN) a couple of times already this semester. Let’s look at it a bit closer.\nThe weak law of large numbers states that if \\(X_1,X_2,\\dots\\) are i.i.d. with mean \\(\\mu\\), then for any \\(\\epsilon &gt; 0\\), \\[\n\\lim_{n \\rightarrow \\infty} \\Pr\\left[ \\left| \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu \\right| &gt; \\epsilon \\right] = 0.\n\\] Restating that in language from calculus class, for every \\(\\epsilon &gt; 0\\) and every \\(\\delta &gt; 0\\), there exists an \\(n_0\\) such that if \\(n \\ge n_0\\), then\n\\[\n\\Pr\\left[ \\left| \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu \\right| &gt; \\epsilon \\right] \\le \\delta.\n\\]\nStated more simply, for any fixed \\(\\epsilon &gt; 0\\), we can guarantee that the sample mean \\(\\bar{X}\\) is within \\(\\epsilon\\) of \\(\\mu\\) with arbitrarily high probability, so long as our sample size \\(n\\) is large enough.\nIt turns out that this can be extended to include more complicated functions than the sample mean. In fact, so long as \\(S\\) is a “nice” function of the data (and so long as \\(S\\) is constructed in an appropriate way), then \\(S\\) will be close to the parameter that we want to estimate with high probability. That is, for “nice” estimators \\(S = S(X_1,X_2,\\dots,X_n)\\), letting \\(\\theta\\) denote our parameter of interest, \\[\n\\Pr\\left[ \\left| S - \\theta \\right| &gt; \\epsilon \\right] \\le \\delta.\n\\]\n\n19.10.1 Example: Estimating the rate parameter in the exponential distribution.\nLet’s see an example that will also illustrate a useful trick for constructing new estimators.\nSuppose that we have data \\(X_1,X_2,\\dots,X_n\\) i.i.d. from an exponential distribution with rate parameter \\(\\lambda\\). You can look up on Wikipedia that the mean of an exponential with rate \\(\\lambda\\) is \\(\\mathbb{E} X_1 = 1/\\lambda\\). Suppose that we want to estimate \\(\\lambda\\). How can we do that?\nWell, we know that \\[\n\\mathbb{E} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n \\mathbb{E} X_i\n= \\frac{1}{\\lambda}.\n\\]\nSo the law of large numbers states that for large sample size, the sample mean \\(\\bar{X}\\) should be close to \\(1/\\lambda\\). But if \\(\\bar{X}\\) is close to \\(1/\\lambda\\), then \\(1/\\bar{X}\\) is close to \\(\\lambda\\) (exercise: prove this! Show that if \\(|\\bar{X} - 1/\\lambda| \\le \\epsilon\\) for some small \\(\\epsilon &gt; 0\\), then \\(|1/\\bar{X} - \\lambda| \\le \\epsilon(\\lambda+\\epsilon)\\)).\nIn this case, then, our estimator is \\[\nS(X_1,X_2,\\dots,X_n) = \\frac{1}{ \\bar{X} },\n\\]\nand for large \\(n\\), \\(S\\) will be close to \\(\\lambda\\). That is, as \\(n\\) gets large, \\(\\bar{X}\\) is close to \\(\\mathbb{E} \\bar{X}\\), and therefore \\(\\frac{1}{\\bar{X}}\\) is close to \\(1/ \\mathbb{E} \\bar{X}\\).\nOkay, but this isn’t a math class. Let’s try simulating and see what happens.\n\nrun_exprate_expt &lt;- function( n, rate ) {\n  data &lt;- rexp(n=n, rate=5);\n  Xbar &lt;- mean( data )\n  return( 1/Xbar )\n}\n\nM &lt;- 1e4;\nreplicates &lt;- rep(NA, M);\nfor (i in 1:M ) {\n  replicates[i] &lt;- run_exprate_expt(200, rate=5);\n}\nhist(replicates);\nabline(v=5, col=\"red\", lwd=4);\n\n\n\n\n\n\n\n\nThis is actually an example of a general technique for doing estimation, called the method of moments. We express the parameter we want to estimate (in this case, the rate \\(\\lambda\\)), and we express it in terms of the moments of our data, \\(\\mathbb{E} X\\), \\(\\mathbb{E} X^2\\), \\(\\mathbb{E} X^3\\), …\nThis is a nice technique, because we know how to estimate moments using the law of large numbers– the sample mean is close to \\(\\mathbb{E} X\\), the sample mean of the squares \\(n^{-1} \\sum_i X_i^2\\) is close to \\(\\mathbb{E} X^2\\), the mean \\(n^{-1} \\sum_i X_i^3\\) is close to \\(\\mathbb{E} X^3\\), and so on– and we get all of that for free from the law of large numbers.\nIn the case above, we only needed the first moment. The parameter of interest obeys \\(\\lambda = 1/\\mathbb{E} X\\).\nThere are other ways of deriving estimators– most famously maximum likelihood estimation, which we’ll talk about a bit later this semester.\nFor now, the important thing is just that you’ve seen this and recognize that there are methods out there for constructing an estimator once we’ve written down a model and chosen a parameter of interest in it.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#how-many-samples-are-enough",
    "href": "estimation1.html#how-many-samples-are-enough",
    "title": "19  Estimation Part 1",
    "section": "19.11 How many samples are enough?",
    "text": "19.11 How many samples are enough?\nSo we can appeal to the law of large numbers to support our assumption that \\(S\\) is close to \\(\\mathbb{E} S\\). But we said above that we also want to communicate how certain we are about this estimate.\nThere’s a problem there– the law of large numbers doesn’t tell us anything about how close \\(S\\) is to \\(\\mathbb{E} S\\) for finite sample size \\(n\\). It just says that if \\(n\\) is suitably large, then the probability that \\(S\\) is “far” from \\(\\mathbb{E} S\\) is arbitrarily small.\nThis is the… limitation? of limit results (I’m so sorry).\nSo, \\(S\\) is close to \\(\\mathbb{E} S\\), but we don’t know how close.\nThe best we can do is to create an interval of values that is “usually right”, in that it “usually” contains the true value of \\(\\mathbb{E} S\\). This is the motivation for confidence intervals and uncertainty quantification, the subject of next week’s lectures.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation1.html#review",
    "href": "estimation1.html#review",
    "title": "19  Estimation Part 1",
    "section": "19.12 Review",
    "text": "19.12 Review\n\nCreate a parametric model for simulation\nUnderstand the difference between an estimator, an estimate and a statistic\nSimulate the distribution of an estimator\nSee the effect of sample size on the variance of an estimator\nThe expected value and variance of \\(\\hat{P}\\)\nPoint estimates for model parameters",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "testing2.html#learning-objectives",
    "href": "testing2.html#learning-objectives",
    "title": "16  Statistical Testing, Continued",
    "section": "",
    "text": "Explain the meaning of Type 1 and Type 2 errors in statistical hypothesis testing.\nExplain the significance level of a test.\nTest a simple parametric null hypothesis",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#testing-and-types-of-errors",
    "href": "testing2.html#testing-and-types-of-errors",
    "title": "16  Statistical Testing, Continued",
    "section": "16.2 Testing and Types of Errors",
    "text": "16.2 Testing and Types of Errors\nWhen we perform a statistical hypothesis test and produce a p-value, this p-value denotes a probability that, if the null hypothesis were true, we would see results at least as extreme as the observed data. Except in some really extreme circumstances, this p-value is not zero. Thus, there is some probability that, even if the null hypothesis is true, we observe very unlikely data and, as a result, reject the null hypothesis incorrectly. Similarly, even if the null hypothesis is false, the data that we observe may not be “weird” enough to constitute sufficient evidence against the null hypothesis, and we may conclude, incorrectly, that the null hypothesis is true.\nThe possible outcomes are summarized in the table below. Something like this table is probably familiar to you from other classes, either in statistics or elsewhere:\n\\[\n\\begin{aligned}\n                   & ~~~~ {\\bf H_0 \\text{ true }} & {\\bf H_0 \\text{ false} } \\\\\n{\\bf \\text{Do not reject } H_0 } & ~~~ \\text{ True negative }  & \\text{ False negative } \\\\\n{\\bf \\text{Reject } H_0 } & ~~~ \\text{ False positive } & \\text{ True positive }\n\\end{aligned}\n\\]\nThe bottom-left and top-right entries of this table are the kinds of errors we can make. In statistics, we often call them “Type I” and “Type II” errors, respectively.\nSaid another way:\n\nA Type I error corresponds to rejecting the null hypothesis when it is in fact true. That is, type I errors correspond to “false alarms”.\nA Type II error corresponds to accepting the null hypothesis when it is not true. That is, type II errors correspond to “misses”.\n\nExample: coin flipping\nLet’s suppose that we are flipping a coin and we want to assess whether or not the coin is fair. We model coin flips as being drawn from a Bernoulli with success parameter \\(p\\). A fair coin corresponds to \\(p=1/2\\), so our null hypothesis is \\[\nH_0 : p = 1/2,\n\\]\nwhere \\(p \\in [0,1]\\) is the probability of a coinflip landing heads.\nWe read that as something like “H nought is that \\(p\\) is 1/2” or “the null hypothesis is that \\(p=1/2\\)”.\nA Type I error would correspond to the case where \\(p=1/2\\) but we conclude incorrectly) that the coin is not fair, i.e., we conclude that \\(p \\neq 1/2\\).\nA Type II error would correspond to the case where \\(p \\neq 1/2\\) (i.e., the null hypothesis is not true), but we conclude (incorrectly!) that the coin is fair.\nExample: lady tasting more tea\nRecall our lady tasting tea example, where our null hypothesis was \\[\nH_0 : \\text{ Muriel Bristol is guessing at random }\n\\]\nA Type I error would correspond to the case where Bristol is guessing completely randomly (i.e., cannot tell the milk-first cups from the milk-second cups), but we conclude, incorrectly, that she can tell the difference.\nA Type II error would correspond to the case where Bristol really can tell the difference, but we incorrectly conclude that she is guessing at random.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#balancing-errors",
    "href": "testing2.html#balancing-errors",
    "title": "16  Statistical Testing, Continued",
    "section": "16.3 Balancing errors",
    "text": "16.3 Balancing errors\nOf course, there are trade-offs involved in this Type I and Type II business.\nOne way to avoid committing a Type I error altogether is to just conduct a test wherein we always accept the null hypothesis. This is great, except that if the null hypothesis is not true, then we will commit a Type II error with probability 1.\nIn the other direction, we could design a test that always rejects the null hypothesis. Then, if the null hypothesis is true, our probability of a Type I error is \\(1\\), but if \\(H_0\\) is not true, we will always be right!\nThis makes it clear that unless we are okay with a totally useless test, we need to balance these two types of errors against one another.\nFor a particular test, the probability that we commit a Type I error is called the level or size of the test, and is denoted by \\(\\alpha\\).\nThe standard hypothesis testing approach is to decide, ahead of time, how large we are willing to let \\(\\alpha\\) be, and then choose the “rejection threshold” for our test statistic accordingly. That is, we specify the probability of a Type I error that we are willing to tolerate, and then adjust our test accordingly so that we reject \\(H_0\\) (when it is true) with probability \\(\\alpha\\).",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#testing-at-level-alpha",
    "href": "testing2.html#testing-at-level-alpha",
    "title": "16  Statistical Testing, Continued",
    "section": "16.4 Testing at level \\(\\alpha\\)",
    "text": "16.4 Testing at level \\(\\alpha\\)\nThe “standard” hypothesis testing framework says that we should set our acceptable Type I error probability \\(\\alpha\\) and then conduct our test in such a way that the probability of a Type I error is indeed \\(\\alpha\\). Well, fair enough. Let’s use the standard \\(\\alpha=0.05\\).\nHow do we ensure that our statistical test has level \\(\\alpha=0.05\\)?\nThe “standard” way is as follows. Suppose that our data is \\(D\\) and our test statistic is \\(T(D)\\), and “extreme” or “unusual” or “weird” outcomes (i.e., observed data) correspond to larger values of \\(T(D)\\).\nSo our test will reject the null hypothesis for especially large values of \\(T(D)\\).\nThen our aim is to find a number \\(t_\\alpha\\) such that \\[\n\\Pr[ T(D) \\ge t_\\alpha ; H_0 ] = \\alpha,\n\\]\nwhere we recall that the semicolon notation is just to denote that the probability is under the null, i.e., under the situation where \\(H_0\\) is true.\nHow do we find this magic number \\(t_\\alpha\\), which we usually call the “critical value” or “rejection threshold”?\nWell, if we know the distribution of \\(T = T(D)\\) under \\(H_0\\), then this is simple. If \\(F_0\\) denotes the cumulative distribution function of \\(T\\) under the null, then by definition, \\[\n\\Pr[ T &gt; t_\\alpha; H_0 ] = 1 -\\Pr[ T \\le t_\\alpha; H_0 ]  = 1- F_0( t_\\alpha ).\n\\]\nNote: you might have noticed that we’re using \\(T &gt; t_\\alpha\\) instead of \\(T \\ge t_\\alpha\\), here. For continuous distributions, this distinction doesn’t really matter. But for the binomial, which is discrete, the difference does matter… For now the computation/calculation is more important. We’ll come back to the \\(T &gt; t_\\alpha\\) vs \\(T \\ge t_\\alpha\\) issue.\nSo we just need to choose the critical value \\(t_\\alpha\\) in such a way that \\[\nF_0(t_\\alpha) = 1-\\alpha.\n\\]\nLet’s take a look at this in action.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#coinflips-revisited",
    "href": "testing2.html#coinflips-revisited",
    "title": "16  Statistical Testing, Continued",
    "section": "16.5 Coinflips revisited",
    "text": "16.5 Coinflips revisited\nReturning to our coinflipping example, suppose that we observe a sample of 200 coinflips, which we model as \\(200\\) independent Bernoulli random variables with success probability \\(p\\), and we want to test the null hypothesis \\[\nH_0 : p=1/2.\n\\]\nThe first thing we need to do is choose a measure of how “unusual” a particular observation is. That is, we need to specify our test statistic.\nA natural choice of test statistic is \\(T = \\text{ # of heads }\\). That is, we just count how many of our 200 coin flips landed heads.\nImportant point: we have assumed a parametric model for our data, so the test that we are about to develop is a parametric test of our null hypothesis. We are going to make specific use of our model assumption that the coinflips are distributed as independent Bernoullis, and use the fact that the total number of heads from those coinflips will be distributed as a Binomial random variable.\nUnder our model, \\(T\\) is a sum of \\(n=200\\) independent Bernoulli random variables, which means that \\(T\\) is distributed according to a Binomial random variable with size parameter \\(200\\) and success probability \\(p\\).\nLet’s just assume (obviously incorrectly, but let’s do it for simplicity, just for the time being!) that larger \\(T\\) corresponds to more “extreme” or “unusual” data. That is, small values of \\(T\\) are not (for now!) considered evidence against the null hypothesis.\nFollowing our discussion above, we need to find the value of \\(t\\) that solves \\[\nF_T( t ) = 1-\\alpha,\n\\]\nand reject if \\(T \\ge t\\). Well, lucky for us, R already knows how to solve this! The qRV function in R, where RV is the name of a random variable, computes quantiles of a distribution. For \\(q \\in [0,1]\\), the \\(q\\)-th quantile of a distribution is the value \\(t\\) such that \\(F(t) = q\\).\n\n# size=200, prob=0.5 because those are the parameters of our model \nqbinom(0.95, size=200, prob=0.5)\n\n[1] 112\n\n\nLet’s just verify that this makes sense. We should have that \\(F_T(112) = 1- \\alpha = 0.95\\). We can compute that in R using pbinom.\n\npbinom( 112, size=200, prob=0.5)\n\n[1] 0.9615812\n\n\nHmm… That’s not 0.95. What’s up with that?\nWell, the binomial distribution is discrete, so there may not be an exact solution to \\(F_T(t) = 1-\\alpha\\). Let’s try one click smaller:\n\npbinom( 111, size=200, prob=0.5)\n\n[1] 0.9481805\n\n\nHmm… well, that’s a little smaller than we’d like ideally– we want this number to be exactly \\(0.95 = 1-\\alpha\\), but it’s good enough! Let’s use a critical value of \\(t=111\\) and reject if \\(T &gt; 111\\), while bearing in mind that our test is technically speaking only approximately level-\\(\\alpha\\) (but really the approximation is so close that it shouldn’t bother us…)\nAside: there are ways to correct this issue that we just saw and make it so that we have exactly level-\\(\\alpha\\) probability of Type I error. Unfortunately, those tricks are a bit beyond the scope of our course this semester. Stay tuned in your later stats courses, or come ask at office hours!\nOkay, so this says that if we want to test at level \\(\\alpha=0.05\\), we should reject \\(H_0\\) any time that we see \\(T &gt; 111\\) heads.\nLet’s try that a few times, and count how often we incorrectly reject the null hypothesis. If all goes well, we should reject close to \\(0.05*100\\) = 1/20 = 5%$ of the time.\n\nrun_coinflip_trial &lt;- function(pheads=0.5) {\n  # run one iteration of our experiment.\n  # Return 0 if we accept the null hypothesis and return 1 if we reject\n  \n  # Generate data under the null hypothesis.\n  number_of_heads &lt;- rbinom(n=1, size=200, prob=pheads);\n  \n  # number_of_heads is already equal to our test statistic T.\n  # Just need to check whether or not it is above our rejection threshold.\n  if( number_of_heads &lt;= 111) {\n    return( 0 ); # 0 like H_0, i.e., accept the null.\n  } else { # test statistic is larger than rejection threshold}\n    return( 1 );\n  }\n}\n\nNow, let’s run the experiment a bunch of times.\n\nNMC &lt;- 1e4; # Repeat our experiment 10K times.\nnreject &lt;- 0; # keep track of how often we reject\nfor( i in 1:NMC ) {\n  # conveniently, run_coinflip_trial returns 0 if we accept\n  # and 1 if we reject, so to count how many rejections we get,\n  # we can just sum them up.\n  nreject &lt;- nreject + run_coinflip_trial()\n}\n\n# Now, compute our (empirical) probability of rejection\nnreject/NMC\n\n[1] 0.0534\n\n\nThat number should be close to 0.05. Of course, there’s randomness in our experiment, so it will probably not be exact.\nNow, let’s see what happens when the null isn’t true. Suppose that in truth, the coin is bent, and \\(\\Pr[\\text{heads}] = 0.75\\). How often do we (correctly) reject the null hypothesis?\n\nNMC &lt;- 1e4; # Repeat our experiment 10K times.\nnreject &lt;- 0; # keep track of how often we reject\nfor( i in 1:NMC ) {\n  # conveniently, run_coinflip_trial returns 0 if we accept\n  # and 1 if we reject, so to count how many rejections we get,\n  # we can just sum them up.\n  nreject &lt;- nreject + run_coinflip_trial(pheads=0.75);\n}\n\n# Now, compute our (empirical) probability of rejection\nnreject/NMC\n\n[1] 1\n\n\nThat number should be close to 1, if not equal to it. When the true value of \\(p = \\Pr[ \\text{heads}]\\) is far from \\(1/2\\), our test manages to detect this fact and correctly reject the null hypothesis.\nOkay, but how about a different way that our null hypothesis could be incorrect: suppose that \\(p=0.25\\).\n\nNMC &lt;- 1e4; # Repeat our experiment 10K times.\nnreject &lt;- 0; # keep track of how often we reject\nfor( i in 1:NMC ) {\n  # conveniently, run_coinflip_trial returns 0 if we accept\n  # and 1 if we reject, so to count how many rejections we get,\n  # we can just sum them up.\n  nreject &lt;- nreject + run_coinflip_trial(pheads=0.25);\n}\n\n# Now, compute our (empirical) probability of rejection\nnreject/NMC\n\n[1] 0\n\n\nHmm… that’s not great. Our test never rejects the null hypothesis, even though \\(0.25=1/4\\) is pretty far from \\(1/2\\). We had no problem rejecting when \\(p=0.75\\), so what’s up?\nWell, if we think a moment, this shouldn’t be so surprising. Our test rejects when the number of heads is larger than 111. If \\(p = 0.25\\), we expect that most of the time we’ll see about \\(200 p = 50\\) heads, which is much smaller than 111. Thus, we should expect that our test will (almost) never reject the null hypothesis.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#one-sided-vs-two-sided-tests",
    "href": "testing2.html#one-sided-vs-two-sided-tests",
    "title": "16  Statistical Testing, Continued",
    "section": "16.6 One-sided vs two-sided tests",
    "text": "16.6 One-sided vs two-sided tests\nThis problem arises because our test above is what is a called a one-sided test. Our test statistic is good for detecting when the true value of \\(p\\) is larger than \\(1/2\\), but ill-suited to detecting when the true value of \\(p\\) is smaller than \\(1/2\\).\nRemember how we said above that we were going to set aside the fact that an especially small number of heads could also be considered “weird”?\nLet’s consider a different statistical test, still based on the number of heads \\(T\\), but this time we’ll devise a two-sided test, wherein we will reject if \\(T\\) is too large or too small.\nThe trick is that now we need to be a bit more clever about our rejection thresholds. We want to devise a statistical test wherein we reject if \\(T\\) is smaller than some number, say \\(t_1\\), or larger than some other number \\(t_2 &gt; t_1\\). The trick is, we still want to make sure that we have significance level \\(\\alpha\\). That is, we want it to be the case that \\[\n\\Pr[ \\text{reject}; H_0 ] = \\Pr[ \\{T &lt; t_{\\alpha,1} \\} \\cup \\{T &gt; t_{\\alpha,2} \\} ] = \\alpha\n\\]\nThere are a lot of ways we can do this, but the easiest (and most pleasingly symmetric) is to choose \\(t_{\\alpha,1}\\) and \\(t_{\\alpha,2}\\) so that \\(\\Pr[ T &lt; t_{\\alpha,1} ] = \\alpha/2\\) and \\(\\Pr[ T &gt; t_{\\alpha,2} ] = \\alpha/2\\). Then, since the events \\(\\{ T&lt;t_{\\alpha,1} \\}\\) and \\(\\{ T &gt; t_{\\alpha,2} \\}\\) are disjoint, \\[\n\\Pr[ \\{T &lt; t_{\\alpha,1} \\} \\cup \\{T &gt; t_{\\alpha,2} \\} ]\n= \\Pr[ T &lt; t_{\\alpha,1} ] + \\Pr[ T &gt; t_{\\alpha,2} ] = \\frac{\\alpha}{2} + \\frac{\\alpha}{2} = \\alpha.\n\\]\nThe set \\[\n\\{ x : x &lt; t_{\\alpha,1} \\text{ or } x &gt; t_{\\alpha,2} \\}\n\\]\nIs called a rejection region, because it is the set of values for which we reject the null hypothesis.\nNote: different books and articles will follow different conventions around whether the region should have \\(x \\le t_{\\alpha,1}\\) or \\(x &lt; t_{\\alpha,1}\\), and similarly for the upper limit. Just something to be careful of. Which convention we use doesn’t really matter– the important thing is that we are 1) consistent and 2) we make sure that we choose the rejection threshold so that our probability of rejecting under the null is \\(\\alpha\\).\nHere’s a picture of the situation:\n\n# Draw the PMF of the Binomial\nplot(0:200, dbinom(0:200, p=0.5, size=200) )\n# Draw a line indicating a lower rejection threshold at 87\n# (we'll see below how we choose 86! For now, it's just for the sake of\n# choosing some lower limit to draw the picture!)\nabline(v=87, col='red')\n# and similarly draw a line for the right-hand rejection threshold.\n# again, how we choose \nabline(v=114, col='red')\n\n\n\n\n\n\n\n\nThe dots indicate the distribution of our test statistic (i.e., the number of heads). Our test is going to reject the null hypothesis if the statistic \\(T\\) falls outside the two red lines.\nThe dots in the plot trace out the PMF of the Binomial, so the “area under the curve” is 1.\nBonus question: why did I put “area under the curve” in quotes, there?\nTo keep our test at level-\\(\\alpha\\) (i.e., to ensure that our probability of a Type I error is \\(\\alpha\\), or, equivalently, that we incorrectly reject under the null with probability \\(\\alpha\\)), we need to choose these two lines so that the area under the curve outside the two lines is equal to \\(\\alpha\\).\nEquivalently, we need to choose the two red lines so that the area under the curve between the two lines is equal to \\(1-\\alpha\\).\n\n16.6.1 Choosing the rejection region\nSo how do we actually figure out \\(t_{\\alpha,1}\\) and \\(t_{\\alpha,2}\\)?\nWell, we still know the distribution of \\(T\\) under the null distribution– it is a Binomial with size parameter \\(200\\) and success probability \\(p\\).\nSo, just as with \\(t_\\alpha\\) above, we can use qbinom to find \\(t_{\\alpha,1}\\) solving \\[\nF_T( t_{\\alpha,1} ) = \\alpha/2.\n\\]\nOkay, so let’s do that.\n\n# Reminder: we need the quantile of \\alpha/2 = 0.025.\nqbinom(0.025, size=200, prob=0.5);\n\n[1] 86\n\n\nAnd let’s just check that with pbinom. We want this to evaluate to (close to) \\(0.025\\).\n\npbinom(86, size=200, prob=0.5)\n\n[1] 0.02798287\n\n\nHmm… that is a touch high… Let’s try one click smaller, again, just to make sure that the discrete nature of our data isn’t playing tricks on us.\n\npbinom(85, size=200, prob=0.5)\n\n[1] 0.0200186\n\n\nOkay, that’s way smaller, so let’s stick with rejecting when \\(T \\le 86\\), i.e., \\(T&lt;87\\). So \\(t_{\\alpha,1} = 87\\).\nWhat about \\(t_{\\alpha,2}\\)? We need to choose it so that \\(\\Pr[ T &gt; t_{\\alpha,2}; H_0 ] = 0.025 = \\alpha/2.\\) Well, \\[\n\\Pr[ T &gt; t_{\\alpha,2}; H_0 ] = 1- \\Pr[ T \\le t_{\\alpha,2}; H_0 ].\n\\]\nSo we just need to find \\(t_{\\alpha,2}\\) satisfying \\[\n\\Pr[ T \\le t_{\\alpha,2}; H_0 ] = 1-\\alpha/2 = 1-0.025 = 0.975.\n\\]\nOnce again, now that we have an event of the form \\(T \\le t_{\\alpha,2}\\), we have a plain old cumulative distribution function, and we can use R to solve this.\n\nqbinom(0.975, size=200, prob=0.5)\n\n[1] 114\n\n\nAnd let’s just verify that this is reasonably close to what we want, in light of the discrete nature of our data.\n\npbinom(114, size=200, prob=0.5)\n\n[1] 0.9799814\n\n\nHmm… just to double check, let’s try choosing that threshold a bit lower.\n\npbinom(113, size=200, prob=0.5)\n\n[1] 0.9720171\n\n\nOkay, neither of these is great– we want exactly \\(0.975\\), remember. Let’s stick with \\(114\\).\nSo we are going to use \\(t_{1,\\alpha} = 87\\) and \\(t_{2,\\alpha} = 114\\) as demarcating our rejection region.\nLet’s implement our two-sided hypothesis test.\n\nrun_coinflip_trial2 &lt;- function(pheads=0.5) {\n  # run one iteration of our experiment for our two-sided test.\n  # Return 0 if we accept the null hypothesis and return 1 if we reject\n  \n  # Generate data under the null hypothesis.\n  number_of_heads &lt;- rbinom(n=1, size=200, prob=pheads);\n  \n  # number_of_heads is already equal to our test statistic T.\n  # Just need to check whether or not it is in the rejection region\n  if( number_of_heads &lt; 87 | number_of_heads &gt; 114 ) {\n    return(1); # Reject the null hypothesis.\n  } else {\n    return(0); # Accept H_0, hence returning 0.\n  }\n}\n\nAnd once again, let’s try running our experiment a large number of times, under the setting where the null hypothesis \\(p=1/2\\) is in fact true, and track how often we incorrectly reject. It should be about \\(\\alpha=0.05\\), once again.\n\nNMC &lt;- 1e4; # Repeat our experiment 10K times.\nnreject &lt;- 0; # keep track of how often we reject\nfor( i in 1:NMC ) {\n  # conveniently, run_coinflip_trial returns 0 if we accept\n  # and 1 if we reject, so to count how many rejections we get,\n  # we can just sum them up.\n  nreject &lt;- nreject + run_coinflip_trial2(pheads=0.5);\n}\n\n# Now, compute our (empirical) probability of rejection\nnreject/NMC\n\n[1] 0.0473\n\n\nOnce again, this should be close to \\(0.05\\), though again not exact, since our experiments are random.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#one-sided-vs-two-sided-tests-1",
    "href": "testing2.html#one-sided-vs-two-sided-tests-1",
    "title": "16  Statistical Testing, Continued",
    "section": "16.7 One-sided vs two-sided tests",
    "text": "16.7 One-sided vs two-sided tests\nSo, at this point, we’ve seen two different tests of our null hypothesis, \\[\nH_0 : p = \\frac{1}{2}.\n\\]\nBoth have the same level \\(\\alpha = 0.05\\) (at least approximately, but let’s ignore that!), but they have different rejection regions.\nOur first test was one-sided, and we only rejected for large values of \\(T = \\text{number of heads}\\).\nOur second test was two-sided, and rejected for small or large values of \\(T\\).\nNotice, however, that the “large” threshold for these two different tests are different. Compare\n\\[\n\\{ t : t &gt;114 \\} ~ \\text{ versus } ~ \\{  t : t &gt; 111 \\}.\n\\]\nOur two-sided test, which rejects values of \\(T\\) bigger than 114, requires “more extreme” values on the large side to reject the null hypothesis when compared with our one-sided test. This means that there should be values of \\(p\\) for which our one-sided test rejects reasonably frequently while our two-sided test is less likely to reject. Let’s see if we can find such a value.\nFirst, let’s write code to compare our two different tests.\n\ncompare_reject_rates &lt;- function( p ) {\n  # Run 10K experiments under the setting where P[heads]=p.\n  # Use both the one-sided and two-sided tests and track how often\n  # each one rejects.\n  NMC &lt;- 1e4; # Repeat our experiment 10K times.\n\n  # keep track of how often our two different tests reject.\n  nreject_twosided &lt;- 0;\n  nreject_onesided &lt;- 0;\n\n  for( i in 1:NMC ) {\n\n    # conveniently, run_coinflip_trial returns 0 if we accept\n    # and 1 if we reject, so to count how many rejections we get,\n    # we can just sum them up.\n    nreject_onesided &lt;- nreject_onesided + run_coinflip_trial(pheads=p)\n    nreject_twosided &lt;- nreject_twosided + run_coinflip_trial2(pheads=p);\n  }\n  return( c( nreject_onesided/NMC, nreject_twosided/NMC ) );\n}\n\nNow, let’s try some values of \\(p\\).\n\npseq &lt;- seq(0.5, 1, 0.01);\n\n# Reminder: the mapply function takes a function and a vector of values\n# and applies that function to each of those values.\n# See ?mapply for details.\n# You've probably seen functions from the tidyverse (e.g., dplyr)\n# for doing similar things. I'm showing you mapply to stress that there are\n# lots of different ways for doing the (more or less) same thing in R.\nrej_rates &lt;- mapply( compare_reject_rates, pseq )\n\n# Take a careful look at how the output is shaped:\nrej_rates\n\n       [,1]   [,2]   [,3]   [,4]   [,5]   [,6]   [,7]   [,8]   [,9]  [,10]\n[1,] 0.0533 0.0866 0.1462 0.2090 0.3149 0.4213 0.5312 0.6369 0.7379 0.8271\n[2,] 0.0472 0.0536 0.0779 0.1142 0.1790 0.2673 0.3675 0.4891 0.5811 0.7000\n      [,11]  [,12]  [,13]  [,14]  [,15]  [,16]  [,17]  [,18] [,19]  [,20]\n[1,] 0.8887 0.9386 0.9629 0.9838 0.9937 0.9963 0.9988 0.9995 1.000 0.9999\n[2,] 0.7892 0.8605 0.9176 0.9537 0.9762 0.9868 0.9950 0.9979 0.999 0.9996\n      [,21] [,22] [,23] [,24] [,25] [,26] [,27] [,28] [,29] [,30] [,31] [,32]\n[1,] 1.0000     1     1     1     1     1     1     1     1     1     1     1\n[2,] 0.9998     1     1     1     1     1     1     1     1     1     1     1\n     [,33] [,34] [,35] [,36] [,37] [,38] [,39] [,40] [,41] [,42] [,43] [,44]\n[1,]     1     1     1     1     1     1     1     1     1     1     1     1\n[2,]     1     1     1     1     1     1     1     1     1     1     1     1\n     [,45] [,46] [,47] [,48] [,49] [,50] [,51]\n[1,]     1     1     1     1     1     1     1\n[2,]     1     1     1     1     1     1     1\n\n\nLet’s make a plot of the two different rejection rates as a function of \\(p\\).\n\nonesided &lt;- rej_rates[1,]\ntwosided &lt;- rej_rates[2,]\n# We're going to make a plot in ggplot2 because it's prettier.\n# ggplot2 wants a dataframe, so we have to build it.\n# We'll make a column of our probabilities,\n# a column of our rejections rates,\n# and a column labeling one- vs two-sided.\n# We needtwo copies of the pseq sequence, one for each of our tests,\n# hence the p=rep(pseq,2)\nrejrate &lt;- c(onesided,twosided);\nsidedness &lt;- c( rep('One-sided',length(onesided)),\n                rep('Two-sided',length(twosided) ) );\ndf &lt;- data.frame( p=rep(pseq,2), RejectionRate=rejrate, Test=sidedness );\n\nlibrary(ggplot2)\npp &lt;- ggplot( df, aes(x=p, y=RejectionRate, color=Test) );\npp &lt;- pp + geom_line(aes(color=Test)) + geom_hline(yintercept=0.05, color='red');\npp\n\n\n\n\n\n\n\n\nSo we see that the one-sided test is more likely to (correctly) reject compared to the two-sided test when \\(p &gt; 1/2\\).\nBut this plot doesn’t tell the whole story– let’s look at the whole range of \\(p \\in [0,1]\\).\n\npseq &lt;- seq(0, 1, 0.01);\n\nrej_rates &lt;- mapply( compare_reject_rates, pseq )\n\nonesided &lt;- rej_rates[1,]\ntwosided &lt;- rej_rates[2,]\n# We're going to make a plot in ggplot2 because it's prettier.\n# ggplot2 wants a dataframe, so we have to build it.\n# We'll make a column of our probabilities,\n# a column of our rejections rates,\n# and a column labeling one- vs two-sided.\n# We need two copies of the pseq sequence, one for each of our tests,\n# hence the p=rep(pseq,2)\nrejrate &lt;- c(onesided,twosided);\nsidedness &lt;- c( rep('One-sided',length(onesided)),\n                rep('Two-sided',length(twosided) ) );\ndf &lt;- data.frame( p=rep(pseq,2), RejectionRate=rejrate, Test=sidedness );\n\npp &lt;- ggplot( df, aes(x=p, y=RejectionRate, color=Test) );\npp &lt;- pp + geom_line(aes(color=Test)) + geom_hline(yintercept=0.05, color='red');\npp\n\n\n\n\n\n\n\n\nAn important point: both of these test have the same level. That is, their probability of rejecting the null when \\(H_0\\) is true (i.e., \\(p=1/2\\)) is the same (up to the approximations that we had to make in dealing with the fact that the Binomial is a discrete distribution):\n\ncompare_reject_rates(0.5)\n\n[1] 0.0534 0.0499",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#test-statistics-and-where-to-find-them",
    "href": "testing2.html#test-statistics-and-where-to-find-them",
    "title": "16  Statistical Testing, Continued",
    "section": "16.8 Test statistics and where to find them",
    "text": "16.8 Test statistics and where to find them\nCrucial to our discussion above was that we have our function that measured how “unusual” or “extreme” our data is. This function is called a test statistic because it is a function of the data (hence a “statistic”) and because we use it in the test of our hypothesis.\nSometimes it’s really obvious what our test statistic should be– like counting how many cups our tea taster got right. Sometimes it’s less obvious. We’ll see examples like that later in the semester.\nLet’s look at this in the context of our coinflipping example above. We’ll why the “right” test statistic isn’t always obvious.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#example-flipping-a-possibly-biased-coin-continued",
    "href": "testing2.html#example-flipping-a-possibly-biased-coin-continued",
    "title": "16  Statistical Testing, Continued",
    "section": "16.9 Example: flipping a (possibly biased) coin, continued",
    "text": "16.9 Example: flipping a (possibly biased) coin, continued\nReminder: our null hypothesis was that the coin is fair, which we would write as \\[\nH_0 : p = \\frac{1}{2}.\n\\]\nHere is the result of flipping this coin 200 times:\n\nflips &lt;- \"HTTHTHHTTHTTTHTHHTTHHTHHTHHHHHTTTTHHHHHHHHTHHTHHHTTTHTTTHHHTHHHHHHTHTTHTTHTHTTHHHHTHHHTHHTTHTTTTTTHHHTTHTHTHHTTHHHTTTHHTHHHTTHHTTTTTHTHHTTTHTTHTTHHTTTTTHTTTHTHHHHTTTTTHTTTHHHHHTHHTHTHHTTTTTTHHHTTTHTTT\"\n\nHow might we go about testing our null hypothesis that \\(p=1/2\\)? Well, we have to start by asking what we expect the data to “look like” if the probability of heads is actually \\(1/2\\), and then come up with a test statistic that captures that.\nAbove, we chose the number of heads as our test statistic, which is certainly a reasonable choice, but let’s pause to appreciate that there are other test statistics we could have chosen.\nLet’s implement that test statistic, this time in a way that it takes a string of coinflips like our data above.\n\ncount_heads &lt;- function( coinflips ) {\n  # We're implicitly assuming coinflips is a vector of one-character strings.\n  # Just a reminder that in non-demo code, we usually want to include some\n  # error checking to make sure that our function's arguments are\n  # as we expect.\n  return( sum( coinflips==rep('H', length(coinflips)) ) );\n}\n\nOkay, let’s apply this statistic to our flips data.\n\n# We need to turn our string of coin flips into a vector before\n# we pass it into `count_heads()`\n# strsplit( x, split=s) splits the string x on the separating character s,\n# and returns a list structure. See ?strsplit for more.\n# unlist( v ) turns a list structure into a flat vector that is easier\n# to work with. See ?unlist for details.\ncount_heads( unlist( strsplit( flips, split='' ) ) );\n\n[1] 99\n\n\nOkay, now, we’ve chosen our test statistic, which measures how “unusual” or “extreme” our data is. But we don’t know how unusual is unusual, or how extreme is extreme. That is, we don’t know what this number would look like, if the null hypothesis were true.\nLet’s simulate some data under the null hypothesis to get an idea.\n\nsimulate_coinflips &lt;- function(n, p) {\n  flips &lt;- sample( c('H','T'), size=n, replace=TRUE, prob=c(p, 1-p) );\n  # flips is now a vector of 'H' and 'T's,\n  # which is what count_heads expects, so let's return it.\n  # If we were going to print this or something, I might prefer to\n  # return a string using paste( flips, sep='')\n  return( flips );\n}\n\n# Now let's simulate a bunch of coinflips\nNMC &lt;- 500;\n# We're going to make a histogram of the number of heads that show up\n# in each experiment run, so create a vector to store those in.\ncounts &lt;- rep( 0, NMC );\nfor( i in 1:NMC ) {\n  counts[i] &lt;- count_heads( simulate_coinflips( 200, 0.5 ) );\n}\n\n# Now, make a histogram. As usual, we could use ggplot2 to make things\n# look a bit nicer, but we just want something quick and simple, here.\nhist( counts ) \n\n\n\n\n\n\n\n\nThe vast majority of the time, the number of heads in 200 fair coin flips is between 80 and 120. Once in a while, of course, it’s more than that or less than that, but an “unusual” result would presumably correspond to a number of heads being much larger or much smaller than \\(100\\). This corresponds to our two-sided test discussed above– we are concerned about the true value of \\(p\\) being either higher or lower than our null value \\(p = 1/2\\).\nFor now, let’s just note that our data observed in flips, has a number of coin flips well within our “usual” range of 80 to 120.\n\ncount_heads( unlist( strsplit(flips, split='')) )\n\n[1] 99\n\n\nNow, here’s a different sequence of coin flips:\n\nmore_flips &lt;- 'HHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHHTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTT'\n\nThis sequence of flips has 100 heads and 100 tails:\n\ncount_heads( unlist( strsplit( more_flips, split='')))\n\n[1] 100\n\n\nAccording to our test statistic, this is not an unusual outcome at all– in 200 flips of a fair coin, we expect there to be around 100 heads and 100 tails. Yet I think we can all agree that there’s something unusual about the sequence of coin flips above.\nIf you’ve seen the play Rosencrantz and Guildenstern Are Dead (or watched the movie based on the play), this might ring a bell. For those not familiar, see this clip.\nSo here we see that our test statistic doesn’t capture all of the ways that our data could be “extreme” or “unusual”. This is precisely why devising a “good” test statistic can be hard!\nOne example of a good statistic here may be to examine run lengths, e.g. using a function like this.\nlongestRun = function(x,target){\n    max(0,with(rle(x), lengths[values==target]))\n}\nThere are a lot of different ways that our data can be “weird”, and we need to be careful that we capture the right notion of weirdness!",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#choosing-alpha-trade-offs",
    "href": "testing2.html#choosing-alpha-trade-offs",
    "title": "16  Statistical Testing, Continued",
    "section": "16.10 Choosing \\(\\alpha\\): trade-offs",
    "text": "16.10 Choosing \\(\\alpha\\): trade-offs\nThe standard choice in (most of) the sciences is to set \\(\\alpha=0.05\\). You probably already remember this number from STAT240 or other courses.\nThis really is a pretty arbitrary choice, based largely on some writings by early statistics researchers, but for some reason it has stuck. You’ll sometimes see researchers use \\(\\alpha=0.01\\), also, but the “right” choice of \\(\\alpha\\) really depends on the nature of the research problem you are asking and on how “bad” it would be to commit a false positive.\nExample: consider a screening test for cancer, where the null hypothesis corresponds to the patient being healthy (i.e., not having cancer). Then a Type I error corresponds to mistakenly declaring that a test subject has cancer when they in fact do not. Of course, this isn’t ideal– this false positive may cause the patient to worry and would trigger expensive follow-up tests to confirm a cancer diagnosis.\nOn the other hand, compare this risk against the risk of committing a Type II error. This would correspond to a patient who comes to us with cancer, but our test does not detect the cancer. Our patient goes home, mistakenly thinking they do not have cancer. This is certainly a much worse outcome than a Type I error, and we may want to spend more resources guarding against this, even if it means a higher chance of committing a Type I error.\nGenerally speaking, reducing \\(\\alpha\\) (i.e., reducing the probability of a Type I error) incurs an increase in the probability of a Type II error, and vice versa. The details of this trade-off will have to wait for your later classes, but it’s good to be aware of it!",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "testing2.html#review",
    "href": "testing2.html#review",
    "title": "16  Statistical Testing, Continued",
    "section": "16.11 Review",
    "text": "16.11 Review\nIn this lecture we covered:\n\nType 1 and Type 2 Errors\nBalance between error types\nSignificance Level \\(\\alpha\\)\nRejection rules (critical value)\nOne-sided vs two-sided tests\nRejection region\nPower of a test statistic\nPower function (curve)\nComparing test statistics\nChoosing \\(\\alpha\\)",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "prediction1.html#learning-objectives",
    "href": "prediction1.html#learning-objectives",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "",
    "text": "Explain both simple and multiple linear regression\nUse R to run linear regression on a given data set and interpret the resulting coefficient estimates\nExplain what it means to associate a p-value to an estimated coefficient",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#prediction-an-overview",
    "href": "prediction1.html#prediction-an-overview",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.2 Prediction: an overview",
    "text": "25.2 Prediction: an overview\nIn a prediction problem, we are given data pairs \\((X_1, Y_1), (X_2, Y_2), \\dots, (X_n, Y_n)\\) and we want to use \\(X_i\\) to predict \\(Y_i\\).\nWe call the \\(X_i\\) values the predictors (also called the independent variables), and we call our \\(Y_i\\) values the responses (or the dependent variables or the outcomes).\nLet’s look at an example that we discussed in our very first lecture.\n\nHere, our \\((X_i,Y_i)\\) pairs correspond to years of education (\\(X_i\\)) and income (\\(Y_i\\)). That is, our predictors are years of education, and our responses are income.\nOur goal is to use this data to learn a function that maps years of education to income. That is, we want a function that takes years of education as input and outputs a prediction as to how much income we predict for a person with that income.\nNow, the very first problem we run into is what we mean by learning a function– there are lots of functions out there!\nLinear regression makes a particular choice: we will learn a linear function that maps our predictors to (predicted) responses.\nYou discussed this problem at some length in STAT240, in the setting where our \\(X_i\\) and \\(Y_i\\) variables were all real-valued. That is, you learned about simple linear regression, in which we model our data as \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,\n\\]\nwhere \\(\\beta_0\\) is the intercept term, \\(\\beta_1\\) is the coefficient associated with our predictors, and \\(\\epsilon_i\\) is an error term.\nOne way to think of this is that we imagine that given a value for the predictor \\(X_i\\), say \\(X_i = x_i\\), the “true” value of \\(Y_i\\) would be \\(\\beta_0 + \\beta_1 x_i\\). However, we don’t see this quantity exactly. Instead, due either to uncertainty in our data collection or to random factors (or both), we see a noisy version of this quantity, namely \\(Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\).\nExample: income and education\nWe might predict that a person with \\(X_i\\) years of education makes \\[\nY_i = 20 + 4 X_i\n\\] thousands of dollars in salary. But there are lots of other factors in addition to years of education that might influence how much money someone makes. Including the error \\(\\epsilon_i\\) is a way to account for these unmeasured variables.\nExample: astronomy\nIn the early days of astronomy, scientists like Johannes Kepler and Tycho Brahe were trying to fit the trajectories of planets in the sky to curves. When an astronomer takes a measurement of where a planet is, there are many sources of noise that cause them to make the measurement imprecisely. These sources of noise include both human error (e.g., imprecision in positioning the telescope, misreading a sight) and external sources (e.g., variations in humidity in the atmosphere change the way light bends as it reaches the observer’s eye). If we model the position of a planet at time \\(X_i\\) as \\[\nY_i = \\beta_0 + \\beta_1 X_i\n\\] for some choice of coefficients \\(\\beta_0\\) and \\(\\beta_1\\), we might account for these sources of measurement error via our error term \\(\\epsilon_i\\). Of course, this linear model is incorrect– the planets follow quite complicated trajectories in the night sky, not simple straight lines. We’ll come back to the matter of when linear models are or are not appropriate.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#writing-down-the-model",
    "href": "prediction1.html#writing-down-the-model",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.3 Writing down the model",
    "text": "25.3 Writing down the model\nTo recap, a simple linear model has an outcome \\(y\\) and single predictor \\(x\\). It is defined by the following equation:\n\\[\n\\begin{aligned}\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n    &= ~~~~~~~\\hat{y}_i ~~~~~~ + \\epsilon_i\n\\end{aligned}\n\\]\nwhere \\(i = 1,2, \\dots, n\\), and the error terms \\(\\epsilon_i \\sim N(0,\\sigma^2)\\) are independent over \\(i=1,2,\\dots,n\\). The \\(\\hat{y}_i\\) notation is to stress that once we have chosen values for the coefficients \\(\\beta_0\\) and \\(\\beta_1\\), our prediction of the response of the \\(i\\)-th data point is \\(\\hat{y}_i = \\beta_0 + \\beta_1 x_i\\).\nThis equation represents our model, not the truth! We want to choose \\(\\beta_0\\) and \\(\\beta_1\\) so that this model describes our observed data as well as possible, but we have to bear in mind that this linearity assumption, that \\(y_i\\) is (exactly or approximately) expressible as \\(\\beta_0 + \\beta_1 x_i\\), is an assumption. Our model will be good at predicting outcomes only in so far as this model agrees with reality.\nThe subscript \\(i\\) in our regression equation indexes the \\(n\\) observations in the dataset. Think of \\(i\\) as a row number. So another way to think about our model \\[\ny_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\n\\]\nis as a system of \\(n\\) equations, \\[\n\\begin{aligned}\ny_1 &= \\beta_0 + \\beta_1 x_1 + \\epsilon_1 \\\\\ny_2 &= \\beta_0 + \\beta_1 x_2 + \\epsilon_2 \\\\\n    &\\vdots \\\\\ny_i &= \\beta_0 + \\beta_1 x_i + \\epsilon_i \\\\\n&\\vdots \\\\\ny_{n-1} &= \\beta_0 + \\beta_1 x_{n-1} + \\epsilon_{n-1} \\\\\ny_{n} &= \\beta_0 + \\beta_1 x_{n} + \\epsilon_{n}.\n\\end{aligned}\n\\]\nThe error terms \\(\\epsilon_i\\) in a linear model correspond, essentially, to the part of the variation in the data that remains unexplained by the deterministic portion of the model (encoded in the linear function \\(\\beta_0 + \\beta_1 x\\)).\nOne of the key assumptions of a linear model is that the residuals are independent and have mean zero, \\(\\mathbb{E} \\epsilon_i = 0\\). Most typically, we further assume that they are normally distributed with mean \\(0\\) and variance \\(\\sigma^2\\). We’ll do that here in these notes, but this choice can sometimes be “relaxed”, in the sense that we may not need to assume that the errors are normal for linear regression to work, depending on what we want to do downstream. Later in your studies, when you take your mathematical statistics course, you’ll put that statement on firmer ground; for now, we’ll have to leave it vague.)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#interpreting-simple-linear-regression",
    "href": "prediction1.html#interpreting-simple-linear-regression",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.4 Interpreting simple linear regression",
    "text": "25.4 Interpreting simple linear regression\nSo let’s suppose that we’re using the linear model \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i.\n\\]\nTo simplify things, let’s just ignore the error term for a moment. After all, \\(\\epsilon_i\\) just captures uncertainty in our measurements. In the ideal world of no measurement error, our model predicts that for a particular choice of predictor \\(x\\), we will measure the response \\[\ny = \\beta_0 + \\beta_1 x.\n\\]\nNow, let’s first consider what happens when \\(x=0\\). Then \\(y=\\beta_0\\). Said another way, if we plotted the line \\(y = \\beta_0 + \\beta_1 x\\), \\(\\beta_0\\) would be the intercept of our model.\nFor example, here’s the function with \\(\\beta_0 = -2\\) and \\(\\beta_1 = 1.\\)\n\nx &lt;- seq(-2,8,0.1)\nbeta0 &lt;- -2;\nbeta1 &lt;- 1;\ny &lt;- beta0 + beta1 * x;\nplot(x,y)\nabline(h=0);\nabline(v=0);\n\n\n\n\n\n\n\n\nEquivalently, since we know that this function forms a line:\n\n# Pass NULL to plot to create an empty plot with axes.\nplot(NULL, xlab=\"\", ylab=\"\", xlim=c(-2, 8), ylim=c(-4, 6))\nabline(a=-2, b=1, col='red', lw=3);\nabline(h=0);\nabline(v=0);\n\n\n\n\n\n\n\n\nLooking at those two plots, it’s clear that \\(\\beta_0 = -2\\) is indeed the intercept of our function. But to reiterate, the “typical” interpretation of the parameter \\(\\beta_0\\) is as describing what would happen if we observed a data point for which our predictor \\(x\\) were equal to zero.\nNow, it’s pretty obvious that \\(\\beta_1\\) is the slope of our function. But how do we interpret it? Well, let’s suppose that we take one measurement with predictor \\(x\\). Our model says that (again, ignoring the error term for now) we will see a response \\[\ny = \\beta_0 + \\beta_1 x.\n\\]\nNow, let’s suppose we take another measurement, this time at predictor value \\(x+1\\). Our model predicts that we will measure the response \\[\ny' = \\beta_0 + \\beta_1 (x+1).\n\\]\nIf we subtract one from the other, we have \\[\ny' - y = \\beta_0 + \\beta_1 (x+1) - (\\beta_0 + \\beta_1 x)\n= \\beta_1.\n\\]\nIn other words, \\(\\beta_1\\) is the change in response that our model predicts if we increase the value of our predictor by one unit.\nExample: income and education\nLet’s come back to our model predicting income (in tens of thousands of dollars) from education, and suppose that we have fit a model of the form \\[\ny = 20 + 4 x.\n\\]\nSo our coefficients are \\(\\beta_0 = 20\\) and \\(\\beta_1 = 4\\). Thus, our model predicts that an increase in education by \\(1\\) year is associated with in an increase of $40K in salary (4 times our unit of measurement, $10K/year).\nSimilarly, since \\(\\beta_0=20\\), our model “predicts” that a person with zero years of education will receive a salary of $20K per year.\nExample: a cautionary tale\nInterpreting the intercept as describing the response at \\(x=0\\) can get a little bit weird if we push the idea too far. Let’s consider a similar problem, this time of predicting income from height. Suppose that we fit a model that predicts income (in thousands of dollars) from height (in centimeters), \\[\ny = 10 + 0.4 x,\n\\]\nwhere \\(x\\) is height in centimeters (note that the units on this example don’t really make sense– don’t let that bother you; it’s not the point).\nThe intercept of this model is \\(\\beta_0 = 10\\). So our model “predicts” that a person with height \\(x=0\\) would make a salary of $10,000 per year. Now, that’s all fine and good, except that I, for one, have never encountered a person with height 0 cm.\nSo our model makes a prediction, but it is making a prediction on an input that we don’t really every expect to encounter in the real word.\nThe high-level point is that often our linear regression model really only makes sense over a certain range of values, and we should be careful when using our model to extrapolate to “strange” values of \\(x\\).\nEven though we might be able to associate a response with any particular input \\(x\\), that doesn’t mean that every such input is realistic. These matters will mostly have to wait for later courses on modeling, but it’s a point we’ll come back to a couple of times over the next few weeks, and it’s a common pitfall in interpreting linear regression models, so it’s worth bringing it to your attention now.\n\n25.4.1 Caution: causality\nIt is always tempting in talking about models like this to say, having fit a model, that “increasing the value of the predictor by one unit causes an increase in the response by one unit” or that “increasing the predicor by one unit results in an increase of such and such amount”. Indeed, many statisticians will say something like this when speaking informally. Still, we should be careful to avoid giving a causal interpretation to our findings.\nFor example, suppose that we fit a linear regression model to predict the number of cancer cases per year in Wisconsin based on pollution levels (measured in, say, PM2.5), and we estimate \\(\\beta_1 = 10.2\\). We might be tempted to say that a unit increase of PM2.5 causes, on average*, an additional 10.2 cancer cases.\nFor better or worse, this is a stronger statement than what we can conclude from a linear model fitted in this way. We can only say that a unit increase in PM2.5 is associated with an increase of 10.2 cancer cases. This is the old “correlation is not causation” saying, wearing a slightly different hat.\nThere is a whole area in statistics called causal inference that attempts to use statistics to make causal statements, but it is, unfortunately, outside the scope of the course.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#fitting-the-model",
    "href": "prediction1.html#fitting-the-model",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.5 Fitting the model",
    "text": "25.5 Fitting the model\nSuppose that we have chosen values of our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) in our regression model. How do we decide how “good” or “bad” this choice of coefficients is? We need a function that takes a particular choice of \\(\\beta_0\\) and \\(\\beta_1\\) and outputs a number that measures how well or poorly the resulting model describes our data.\nIn the setting where larger values of this function correspond to worse model fit, we call this kind of a function a loss function: it takes a choice of model parameters (i.e., coefficients \\(\\beta_0\\) and \\(\\beta_1\\), in our case), and outputs a number that measures how poorly our model fits the data.\n\n25.5.1 Choosing a loss: sum of squares\nThere are lots of functions we could choose to use as our loss function, but by far the most common choice is the residual sum of squares (RSS), sometimes called the sum of squared errors (SSE): \\[\n\\ell( \\beta_0, \\beta_1 )\n= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n= \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2.\n\\]\nThe terms \\(y_i - \\hat{y}_i = y_i - (\\beta_0 + \\beta_1x_i)\\) are called the residuals. The word residual comes from the word residue (cue flashback to chem lab?), which refers to something that is left over. The residuals are what is left over after we try to predict the responses from the predictors \\(x_1,x_2,\\dots,x_n\\).\nOur goal is then to choose our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to minimize the sum of squared residuals loss in the equation above. We call this ordinary least squares (OLS) regression. “Least squares” because, well, we’re minimizing the sum of squares. “Ordinary” because there are other sums of squares we could look at that would be a little less ordinary (see here for details, if you’re curious).\nLet’s note that the sum of squared errors is not the only possible loss we could choose. For example, we might try to minimize the sum of absolute deviations, \\[\n\\sum_{i=1}^n |y_i - \\hat{y}_i|\n= \\sum_{i=1}^n \\left|y_i - (\\beta_0 + \\beta_1x_i) \\right|.\n\\]\nAs we’ve seen in recent lectures, though, trying to minimize this loss with respect to our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) can be challenging.\n\n\n25.5.2 Minimizing the loss\nSo we have a loss function \\[\n\\ell(\\beta_0,\\beta_1) = \\sum_{i=1}^n (y_i - (\\beta_0 + \\beta_1x_i))^2,\n\\] and we want to choose \\(\\beta_0\\) and \\(\\beta_1\\) to minimize this quantity.\nTo do that, we are going to dust off our calculus textbooks, take derivatives, set those derivatives equal to zero, and solve for \\(\\beta_0\\) and \\(\\beta_1\\). That is, we want to solve \\[\n\\frac{ \\partial \\ell( \\beta_0, \\beta_1 )}{ \\partial \\beta_0 } = 0\n~~~\\text{ and } ~~~\n\\frac{ \\partial \\ell( \\beta_0, \\beta_1 ) }{ \\partial \\beta_1 }\n= 0.\n\\]\nI’ll spare you the mathematical details; if you’re curious, you can find a derivation of the solution in any introductory regression book, or here\nThe important point is that we find that our estimates should be \\[\n\\begin{aligned}\n\\hat{\\beta}_0 &= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\\\\n\\hat{\\beta}_1 &= \\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\sum_{i=1}^n (x_i - \\bar{x})^2 },\n\\end{aligned}\n\\]\nwhere \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of the predictors and responses, respectively: \\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^n x_i\n~~~\\text{ and }~~~\n\\bar{y} = \\frac{1}{n} \\sum_{i=1}^n y_i.\n\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#interpreting-the-estimates",
    "href": "prediction1.html#interpreting-the-estimates",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.6 Interpreting the estimates",
    "text": "25.6 Interpreting the estimates\nLet’s pause and try to interpret what these two estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) actually mean. Let’s start with \\(\\hat{\\beta}_1\\).\n\n25.6.1 Interpreting the estimated slope\nBy our definition of \\(\\hat{\\beta}_1\\), we have\n\\[\n\\hat{\\beta}_1\n=\n\\frac{ \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\sum_{i=1}^n (x_i - \\bar{x})^2 }\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n                      { \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2 },\n\\] where we multiplied the numerator and denominator by \\(1/n\\).\nNow, let’s notice that the denominator is just the (uncorrected) sample variance of the predictors: \\[\ns_x^2 = \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\] If we define the analogous quantity for the predictors, \\[\ns_y^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\bar{y})^2,\n\\] we have \\[\n\\hat{\\beta}_1\n= \\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{s_x^2}\n=\n\\frac{ s_y }{ s_x }\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ s_x s_x }.\n\\]\nNow, let’s look at the other sum, \\[\n\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}),\n\\] and notice that it is the sample covariance of the predictors and responses. Recalling our definition of the correlation as \\[\n\\rho_{x,y}\n= \\frac{ \\mathbb{E} (X - \\mathbb{E} X)( Y - \\mathbb{E}Y)}\n{\\sqrt{ (\\operatorname{Var} X)( \\operatorname{Var} Y) }},\n\\]\nwe notice that \\[\n\\hat{\\rho}_{x,y}\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ s_x s_x }\n=\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }\n{ \\sqrt{ s_x^2 s_y^2 }}\n\\]\nis the sample correlation between our predictors and responses– we plugged in the sample versions of the covariance and the variances.\nSo, our estimated coefficient \\(\\hat{\\beta}_1\\) can be expressed as \\[\n\\hat{\\beta}_1\n=\n\\frac{ s_y }{ s_x } \\hat{\\rho}_{x,y}.\n\\]\nIn other words, the slope of our model is the ratio of the standard deviations, scaled by the correlation between our predictors and responses.\nAn interesting case to think about is when the predictors and responses are perfectly correlated (i.e., the predictors and responses form a perfect line, with no “jitter”). Then our estimated slope is \\(\\hat{\\beta}_1 = \\sqrt{ s_y^2/s_x^2 } = s_y/s_x\\). In other words, the slope of our model is just the ratio of the standard deviation of the responses to that of the predictors. Think of this as like a “change of units” from predictors to responses. If our predictors are measured in, say, years of education, and our responses are measured in dollars per year, then the ratio of the standard deviations has units \\[\n\\frac{ \\text{dollars per year} }{ \\text{years of education}},\n\\]\nand multiplying this by our predictor, which is measured in “years of education”, we get \\[\n\\text{ response }\n=\n\\frac{ \\text{dollars per year} }{ \\text{years of education}}\n\\cdot\n\\text{ years of education }\n=\n\\text{ dollars per year },\n\\] which is what we expect.\nThis kind of “dimension analysis”, which you may have seen before in physics, can be a useful way to make sure that you’re performing calculations correctly!\n\n\n25.6.2 Interpreting the intercept term \\(\\hat{\\beta}_0\\)\nTurning our attention to \\(\\hat{\\beta}_0\\), we have\n\\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\] Why does this choice make sense?\nWell, let’s suppose that we decided to make our lives even harder by restricting our choice of prediction function to be a constant. That is, suppose we wanted to choose a prediction function that returns the same output, say, \\(\\hat{y}\\), no matter the input \\(x\\).\nIf we wanted to choose this output using the same least-squares approach that we used above, we would want to choose \\(\\hat{y}\\) so that it minimizes \\[\n\\sum_{i=1}^n (y_i - y)^2.\n\\] A little bit of calculus (seriously, this one’s easy– try it!) shows that the way to minimize this is to choose the output \\[\n\\hat{y}= \\frac{1}{n} \\sum_{i=1}^n y_i = \\bar{y}.\n\\] Now, thankfully, we are not actually trying to predict our data with a constant function. We are allowed to choose a slope!\nHaving chosen our slope \\(\\hat{\\beta}_1\\), our model without an intercept term predicts that the \\(i\\)-th observation should have response \\(\\hat{\\beta}_1 x_i\\). If we add an intercept term to the model, sticking with our least squares loss, we would like to choose \\(\\beta_0\\) so as to minimize \\[\n\\sum_{i=1}^n \\left( y_i - \\hat{\\beta}_1 x_i - \\beta_0 \\right)^2.\n\\]\nThe exact same kind of calculus argument (taking a derivative with respect to \\(\\beta_0\\), this time– again, give it a try!), gets us \\[\n\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x}.\n\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#variance-of-estimates",
    "href": "prediction1.html#variance-of-estimates",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.7 Variance of estimates",
    "text": "25.7 Variance of estimates\nAfter fitting, we can find our predicted \\(\\hat{y_i}\\), i.e. the \\(y\\) values on the line.\n\\[\n\\hat{y_i}=\\hat{\\beta_0}+\\hat{\\beta_1}x_i\n\\]\nas well as our model residuals \\(\\hat{\\epsilon}_i\\)\n\\[\n\\hat{\\epsilon_i}=y_i-\\hat{y_i}\n\\]\nFrom this, we also get for free an estimate of the variance of the residuals \\(\\sigma^2\\), which happens to be very useful in computing other statistics. The reason is that the larger the residuals’ variance, the less precisely we can estimate our regression coefficients, which should make a lot of sense.\n\\[\n\\hat{\\sigma}^2=\\text{mean squared error}=\\frac{SSE}{n-2}=\\frac1{n-2}\\sum_i(y_i-\\hat{y_i})^2\n\\]\nWe can also easily derive the variance of the slope. First, observe that\n\\[\\begin{align}\n\\sum_i (x_i - \\bar{x})\\bar{y}\n&= \\bar{y}\\sum_i (x_i - \\bar{x})\\\\\n&= \\bar{y}\\left(\\left(\\sum_i x_i\\right) - n\\bar{x}\\right)\\\\\n&= \\bar{y}\\left(n\\bar{x} - n\\bar{x}\\right)\\\\\n&= 0\n\\end{align}\\]\nThis means that\n\\[\\begin{align}\n\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})\n&= \\sum_i (x_i - \\bar{x})y_i - \\sum_i (x_i - \\bar{x})\\bar{y}\\\\\n&= \\sum_i (x_i - \\bar{x})y_i\\\\\n&= \\sum_i (x_i - \\bar{x})(\\beta_0 + \\beta_1x_i + \\epsilon_i )\\\\\n\\end{align}\\]\nUsing this, we can easily derive \\(\\text{Var}(\\hat{\\beta_1})\\) as follows:\n\\[\\begin{align}\n\\text{Var}(\\hat{\\beta_1})\n& = \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_i (x_i - \\bar{x})^2} \\right) \\\\\n&= \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})(\\beta_0 + \\beta_1x_i + \\epsilon_i )}{\\sum_i (x_i - \\bar{x})^2} \\right), \\;\\;\\;\\text{substituting in the above} \\\\\n&= \\text{Var} \\left(\\frac{\\sum_i (x_i - \\bar{x})\\epsilon_i}{\\sum_i (x_i - \\bar{x})^2} \\right), \\;\\;\\;\\text{noting only $\\epsilon_i$ is a random variable} \\\\\n&=  \\frac{\\sum_i (x_i - \\bar{x})^2\\text{Var}(\\epsilon_i)}{\\left(\\sum_i (x_i - \\bar{x})^2\\right)^2} , \\;\\;\\;\\text{independence of } \\epsilon_i \\text{ and, Var}(cX)=c^2\\text{Var}(X) \\\\\n&= \\frac{\\sigma^2}{\\sum_i (x_i - \\bar{x})^2} \\\\\n\\end{align}\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#running-simple-linear-regression",
    "href": "prediction1.html#running-simple-linear-regression",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.8 Running simple linear regression",
    "text": "25.8 Running simple linear regression\nOkay, that’s enough abstraction. Let’s apply this to some real data and see how things go.\nIn the 1920s, Thomas Midgley Jr. discovered that adding tetraethyllead to gasoline decreased engine knocking (i.e., when fuel doesn’t fully ignite in an engine cylinder, which may damage the engine). He won the 1923 Nichols medal, a prestigious prize in chemistry, for his discovery.\nThe result of burning tetraethyllead in gasoline resulted in high levels of lead in the atmostphere. By the 1950s to 70s, researchers started to suspect that increased lead levels in the atmosphere was causing widespread lead poisoning, with symptoms ranging from depression, loss of appetite, and amnesia to anemia, insomnia, slurred speech, and cognitive impairment. Starting in the 1980s, the use of tetraethyllead in gasoline started to be phased out. At most gas stations in the United States, you’ll notice that gasoline is still marked as being “unleaded”, just in case you were worried!\nIn more recent years, a more controversial theory has emerged, suggesting that exposure to lead (be it in the atmosphere or in paint in older buildings) correlates with incidents of violent crime later in life 1 2. This study was first conducted in the US, but it was soon replicated in other countries and the similar results have been found elsewhere in the world.\n\n\n\nLet’s look at a dataset that contains atmospheric lead content levels and aggravated assault rates for several cities in the US and see if we can build a simple linear regression model to explain the trend and make predictions.\n\nlead &lt;- read.csv('data/lead.csv')\n# First things first: let's look at the data.\nhead(lead)\n\n     city air.pb.metric.tons aggr.assault.per.million\n1 Atlanta                421                     1029\n2 Atlanta                429                      937\n3 Atlanta                444                      887\n4 Atlanta                457                      533\n5 Atlanta                461                     1012\n6 Atlanta                454                      848\n\n\nThe variables we are interested in are lead levels in the atmosphere (measured in metric tons of lead emitted) and the aggravated assault rate per million 22 years later. We want to predict the assault rate from lead levels, so our predictor (or explanatory variable or independent variable, if you prefer) is lead levels, and our response (or dependent variable) is the assault rate. This data is available for a number of cities:\n\nlevels( as.factor(lead$city) )\n\n[1] \"Atlanta\"      \"Chicago\"      \"Indianapolis\" \"Minneapolis\"  \"New Orleans\" \n[6] \"San Diego\"   \n\n\nFor simplicity, let’s focus on the city of Atlanta.\n\natlanta_lead &lt;- lead[ lead$city=='Atlanta', ];\n## Alternative approach, using filter in dplyr:\n# library(dplyr)\n# atlanta_lead  lead %&gt;% filter(city == \"Atlanta\")\n\n# Plot the data to get a look at what's going on.\nlibrary(ggplot2)\npp &lt;- ggplot( atlanta_lead,\n          aes(x=air.pb.metric.tons, y=aggr.assault.per.million));\npp &lt;- pp + geom_point();\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\n\n\n\n\nVisually, it’s quite clear that assaults are well-predicted as a linear function of lead levels. One thing that is already perhaps of concern is that the data appears to be a bit more “spread out” in the vertical direction for lower lead levels. This is a bit concerning in light of our assumption that the error terms were all distributed according to a normal with mean \\(0\\) and variance \\(\\sigma^2\\). We’ll come back to this point below. For now, let’s press on.\nTo fit a linear model in R, we use the lm() function (lm for “linear model”). The syntax is as simple as lm(y ~ 1 + x, data=dframe), where dframe is the data frame containing our data, and y ~ 1 + x means to regress the variable y (i.e., the column y in the dataframe dframe) against the variable x and an intercept term (that’s the 1 in the model formula): \\[\ny = \\beta_1 x + \\beta_0\n\\]\nNote that the 1 in the model formula y ~ 1 + x is completely optional– R will include an intercept term automatically. Still, I recommend including it for the sake of clarity.\nThe function lm returns an object of the class lm. This is an object that contains a bunch of information about our fitted model. We’ll see some of that information below.\nSo in our case, we want to regress aggr.assault.per.million against air.pb.metric.tons. That is, we want a model like \\[\n\\text{agg.assault} = \\beta_0 + \\beta_1 \\cdot \\text{air.pb}\n\\]\nSo we’ll write that as aggr.assault.per.million ~ 1+ air.pb.metric.tons. Let’s try fitting the model and then we’ll ask R to summarize our model. Running summary() on the model object gives us a variety of useful summary statistics and other information about the fitted model.\n\natlanta_lead_lm &lt;- lm(aggr.assault.per.million ~ 1 + air.pb.metric.tons,\n                      data=atlanta_lead)\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nWe’ll come back a little later to talk about some of these numbers in more detail. For now, let’s notice in particular that we have estimated coefficients, accessible in the coefficients attribute of our model object:\n\natlanta_lead_lm$coefficients\n\n       (Intercept) air.pb.metric.tons \n        107.942757           1.403746 \n\n\nSo our model predicts that in Atlanta, an increase of one metric ton of lead in the atmosphere is associated with an increase of about 1.4 aggravated assaults per million people. Similarly, the intercept indicates that our model predicts that in the absence of any lead in the atmosphere, there would be about 108 aggravated assaults per million people.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#working-with-lm-output-diagnostics",
    "href": "prediction1.html#working-with-lm-output-diagnostics",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.9 Working with lm() output: diagnostics",
    "text": "25.9 Working with lm() output: diagnostics\nLet’s look at how our fitted model tracks the data by overlaying the fitted line on our scatterplot above. One way to do this would be to use abline with the entries of atlanta_lead_lm$coefficients to specify the slope and intercept, but lm is so common that ggplot2 has this same basic functionality built-in, in the form of geom_smooth().\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\n\n\n\n\nLooks like a pretty good fit! Let’s look at some of the other information included in the output of lm().\n\n# gets fitted y-values (i.e., points on line of best fit)\nfitted(atlanta_lead_lm)\n\n        1         2         3         4         5         6         7         8 \n 698.9200  710.1499  731.2061  749.4548  755.0698  745.2436  787.3560  836.4871 \n        9        10        11        12        13        14        15        16 \n 849.1208  990.8992 1038.6266 1096.1802 1118.6401 1169.1750 1183.2125 1187.4237 \n       17        18        19        20        21        22        23        24 \n1194.4424 1197.2499 1302.5309 1351.6620 1414.8306 1529.9378 1597.3176 1689.9649 \n       25        26        27        28        29        30        31        32 \n1757.3447 1782.6122 1852.7995 1879.4707 1991.7704 2000.1929 2101.2626 2185.4874 \n       33        34        35        36 \n2209.3511 2231.8110 2217.7735 2236.0222 \n\n\n\n# residuals( model ) gets residuals (the difference between the\n# observed response y and the response predicted by our model)\nresiduals(atlanta_lead_lm)  # We can also use resid()\n\n          1           2           3           4           5           6 \n 330.080025  226.850054  155.793859 -216.454844  256.930171  102.756395 \n          7           8           9          10          11          12 \n-356.355996 -149.487118  382.879164 -273.899218 -268.626594 -280.180195 \n         13          14          15          16          17          18 \n 175.359863 -294.175006   23.787530  109.576291  -97.442440  -80.249933 \n         19          20          21          22          23          24 \n  -8.530910   45.337967  -43.830619  116.062179  -55.317646  -70.964906 \n         25          26          27          28          29          30 \n-129.344731   14.387834  -58.799484  143.529335  148.229626    9.807148 \n         31          32          33          34          35          36 \n 199.737410  -64.487372   16.648940   14.188998  -27.773538    3.977759 \n\n\nThese residuals reflect the error between our model’s prediction and the true observations, and they are often quite informative.\nRecall that our model assumes that the observation errors \\(\\epsilon_i\\) are normally distributed about zero, with a shared variance \\(\\sigma^2\\). To check that this assumption is (approximately) true, we can plot the residuals:\n\nresids &lt;- residuals(atlanta_lead_lm)\nhist(resids)\n\n\n\n\n\n\n\n\nThat looks… okay, at any rate. The residuals are (approximately) symmetric about zero, and the histogram looks normal-ish to me. We’ll come back to this point, and later in your studies (e.g., if you take our department’s regression course) you’ll learn lots of ways for assessing model fit (e.g., checking if the normal errors assumption is correct), but for the time being, we’ll be satisfied with the “ocular inspection” method.\n\n25.9.1 Homoscedasticity\nAnother important point, far more important that the normality assumption, is that the variance of the errors \\(\\epsilon_i\\) does not depend on the predictor \\(X_i\\). This is referred to as “homogeneity of variance”, more commonly called homoscedasticity. Its absence, heteroscedasticity, wherein the variance of the error terms varies with \\(X_i\\), can be a big problem for linear regression.\nSo let’s check for it, just visually for now. We want to plot the residuals as a function of their predictor values. If our errors are homoscedastic, we should observe the variance of the residuals about the horizontal line \\(y=0\\) to be more or less constant along the x-axis. R will do this for us automatically if we call plot on our model object. In fact, R will make several plots for us automatically, and return those plots in a list-like object. The residuals as a function of the x values is the first of these, and we can access it with the which keyword to plot().\n\nplot(atlanta_lead_lm, which=1)\n\n\n\n\n\n\n\n\nThe red line is fitted by R; if our residuals are reasonably well-behaved, this line should be horizontal. Inspecting this plot, it looks as we suspected– the residuals for smaller lead atmospheric levels have slightly higher variance, and tend to be biased toward positive values. Still (and this is an intuition that you’ll develop as you perform more analyses), this doesn’t look especially extreme.\n\n\n25.9.2 Assessing normality of the residuals\nThe stronger assumption, not required by linear regression per se, but a good assumption to check for use in downstream testing procedures (we’ll talk about that soon!), is that the errors are normal with mean zero. We saw that their histogram above looked pretty reasonable. A better check for fit is to construct a Q-Q plot (still no relation to the Chinese restaurant on University Ave, sadly).\n\nplot(atlanta_lead_lm, which=2)\n\n\n\n\n\n\n\n\nLet’s recall that a Q-Q-plot displays the quantiles (i.e., the percentiles) of our observed data against the quantiles of the normal distribution. If our data were perfectly normally distributed, then the Q-Q plot would look like a straight line with slope 1 (up to randomness in the data, of course). If our data is not normal, the Q-Q plot will look far different.\nJust to see an example of this, let’s generate some data from a t-distribution (which looks normal, but has “heavier tails”), and look at the Q-Q plot.\n\ndata &lt;- rt(n=36, df=3, ncp=0);\nhist(data)\n\n\n\n\n\n\n\n\n\nqqnorm(data);\n# Add a line to the plot to indicate the behavior we would\n# expect to see if the data were normal.\nqqline(data, col='red');\n\n\n\n\n\n\n\n\nSo this is an example of the kind of behavior we would expect to see if our data were not well-described by a normal. Here’s our lead level data again.\n\n# We could also call qqnorm(atlanta_lead_lm$residuals)\nplot(atlanta_lead_lm, which=2)\n\n\n\n\n\n\n\n\nThere are some slightly concerning spots there, especially in the bottom-left, but it’s not too extreme (in my opinion, anyway). Once again, later on you’ll learn more rigorous ways of checking model assumptions like this. We’re just trying to get an intuition for now.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#testing-and-confidence-intervals-for-coefficients",
    "href": "prediction1.html#testing-and-confidence-intervals-for-coefficients",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.10 Testing and confidence intervals for coefficients",
    "text": "25.10 Testing and confidence intervals for coefficients\nSo we’ve established that our model is a reasonably good fit for the lead data, at least in the sense that the trend in the data follows our plotted line and such.\nCan we conclude from this that the association between lead and aggravated assault rate is “real”? It’s possible, after all, that the observed association is merely due to chance.\nWell, in our discussions of hypothesis testing we saw a number of tools for checking if observations were merely due to chance or not. Linear regression has its own set of tools for checking whether observed coefficient estimates are “merely due to chance”.\nLet’s look back at our model summary and let’s pay particular attention to the “coefficients” part of the output.\n\nsummary(atlanta_lead_lm)\n\n\nCall:\nlm(formula = aggr.assault.per.million ~ 1 + air.pb.metric.tons, \n    data = atlanta_lead)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-356.36  -84.55    6.89  122.93  382.88 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        107.94276   80.46409   1.342    0.189    \nair.pb.metric.tons   1.40375    0.08112  17.305   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 180.6 on 34 degrees of freedom\nMultiple R-squared:  0.898, Adjusted R-squared:  0.895 \nF-statistic: 299.4 on 1 and 34 DF,  p-value: &lt; 2.2e-16\n\n\nThe Coefficients table includes our estimates for the coefficients, standard errors for those estimates, t-values (i.e., a test statistic) for those statistics and, at the far right of the table, a column headed Pr[&gt;|t|]. Hey… that’s a p-value!\nNotice that in this case, the p-value associated to our estimate of the coefficient of lead levels is quite small. That indicates that an estimate this extreme (or more extreme) is highly unlikely to have arisen entirely by chance.\nNow, it is in dealing with these p-values that we need to be a bit careful about our model assumptions. If the assumption of normal, homoscedastic errors is violated, things can go wrong with these p-values. But since our Q-Q plot indicated that our residuals were reasonably normal-looking, we can be somewhat confident that this is reflecting a real effect (well, and it’s pretty clear just from the plot that there’s a linear relationship…).\nThis p-value arises, in essence, from a \\(t\\)-test. This t-test is designed to test the null hypothesis \\[\nH_0 : \\beta_1 = 0.\n\\]\nIn this case, our p-value associated with \\(\\beta_1\\) is quite small, indicating a correlation between lead levels and aggravated assault levels. It does not imply causation, though, as you well know by now. Nonetheless, this seems to be fairly convincing evidence that there is an association.\nThinking back to our brief discussion of the connection between confidence intervals and testing, you won’t be surprised to learn that we can also compute confidence intervals for the true value of the coefficients.\n\nconfint(atlanta_lead_lm, level=0.95)\n\n                        2.5 %     97.5 %\n(Intercept)        -55.579958 271.465472\nair.pb.metric.tons   1.238891   1.568602\n\n\nOf course, we also tested the hypothesis \\(H_0 : \\beta_0 = 0\\), and the p-value is not especially small (also reflected in the fact that the confidence interval includes \\(0\\)). This indicates that our intercept term was not statistically significantly different from zero.\nNote, however, that just because our p-value associated to the intercept term isn’t especially small, that doesn’t mean that the intercept isn’t useful for prediction. Let’s turn our attention to that matter.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#making-predictions",
    "href": "prediction1.html#making-predictions",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.11 Making predictions",
    "text": "25.11 Making predictions\nSuppose that tomorrow a chemical company near Atlanta has an incident, and lead is released into the atmosphere. Suppose that the new atmospheric levels of lead are found to be 1300 metric tons. What would you predict the approximate aggravated assault rate to be 22 years later?\nWell, let’s start by just looking at a plot of our model again.\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\n\n\n\n\nJust looking at the plot, we see that at x-value 1300, our fitted line is just about exactly at 2000 aggravated assaults per million people.\nHopefully you can imagine how annoying it would be to perform this exercise by hand every time we need a new prediction. Luckily, R model objects (including linear regression) support a function called predict, which does exactly what it sounds like. We pass our model, and some data (i.e., x-values), and predict() outputs our model’s predicted responses at those values.\n\npredict(atlanta_lead_lm, newdata=data.frame(air.pb.metric.tons=1300))\n\n       1 \n1932.813 \n\n\nSuppose the company continues to release more lead into the atmosphere, and next year, the levels are measured to be 2000 metric tons. Can we use our model to predict what aggravated assault rates might look like 22 years later?\nWell, looking at the plot again, 2000 metric tons is rather far outside the range of our observed predictor values.\n\npp &lt;- ggplot( atlanta_lead,\n              aes(x=air.pb.metric.tons,y=aggr.assault.per.million));\n# The argument `se` specifies whether or not to include a\n# confidence interval around the plotted line.\n# We'll talk about that later.\n# For now we'll just suppress the CI with se=FALSE\npp &lt;- pp +geom_point() + geom_smooth(method=\"lm\",\n                                     formula=\"y~x\",\n                                     se=FALSE);\npp &lt;- pp + labs( x=\"Lead levels (metric tons)\",\n                 y=\"Agg'd assaults (per million)\",\n                 title=\"Violent crime and atmospheric lead (22 year lag)\" )\npp\n\n\n\n\n\n\n\n\nAs we’ve alluded to earlier in these lecture notes, predictions made far outside the range of observed predictors have to be treated carefully They may be reliable, but they also may not. The reliability of a prediction usually decreases the further away it is from your data.\nFor example, perhaps once lead levels reach a certain point, there just isn’t much more damage they can do to human development. Then we would see the linear trend flatten out at higher lead levels. Our function would cease to be linear, and naively applying our linear models to those values would result in poor prediction performance.\nJust to drive the point home, here’s a cautionary tale:\n\n\n\n|t|)\n(Intercept) 17.965050 0.849663 21.144 &lt; 2e-16  disp -0.006622 0.004166 -1.590 0.12317\nhp -0.022953 0.004603 -4.986 2.88e-05  wt 1.485283 0.429172 3.461 0.00175 ** — Signif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1\nResidual standard error: 1.062 on 28 degrees of freedom Multiple R-squared: 0.6808, Adjusted R-squared: 0.6466 F-statistic: 19.91 on 3 and 28 DF, p-value: 4.134e-07\n\n\n:::\n:::\n\n\n\n\n\n\nOnce again, before we get too eager about interpreting the model, we should check that our residuals are reasonable.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist(mtc_model$residuals)\n\n\n\n\n\n\n\n:::\nHmm… that isn’t amazing, but on the other hand, there aren’t very many observations to begin with, so we shouldn’t expect a particularly normal-looking histogram.\nChecking heteroscedasticity isn’t so easy now, but we can still do things like compare the residuals with a normal via a Q-Q plot:\n\nplot(mtc_model, which=2)\n\n\n\n\n\n\n\n\nIn my opinion, this Q-Q plot would likely lead me to question the assumption of normal errors. That doesn’t mean that we can’t proceed with using our linear model, but it will mean that we should be a bit careful with how much credence we give to any quantities that depend on our normality assumption (e.g., our p-values).\nLet’s press on regardless, for now, mostly for the sake of demonstration of what we would do, if we were reasonably happy with our model assumptions. Still, we should bear in the back of our minds that perhaps our normality assumptions perhaps aren’t exactly true.\nLet’s return to our model output.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nLet’s start at the bottom. The residual standard error (RSE) is listed as being 1.062. This is related to the sum of squared errors we discussed earlier, and it’s exactly what it sounds like– the standard error of the model’s residuals. Ideally, we want this number to be small– after all, it measures the error in our model. We’ll come back to this matter later in the semester.\nThe output lists this as being the residual standard error on 28 degrees of freedom. Remember that as a rule of thumb, the degrees of freedom will be the number of data points less the number of parameters we estimate. In this case, there are 32 data points\n\nnrow(mtcars)\n\n[1] 32\n\n\nand our model has four parameters: the intercept and our three predictors’ coefficients, so 28 degrees of freedom checks out!\nMuch more on that whole matter soon, but let’s keep looking at that model summary.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nThe F-statistic associated with our residuals has a very small p-value associated to it, indicating, in essence, that our model fit is much better than would be expected by chance. You’ll learn a lot more about the F-distribution when you learn about analysis of variance (ANOVA) in a later class.\nScanning our way up the model summary, let’s look at the table of coefficient estimates. We see that our intercept term and the coefficients for horsepower (hp) and weight (wt) are flagged as being significant. Thus, briefly putting on our testing hats, we would reject the null hypotheses \\(H_0 : \\beta_0=0\\), \\(H_0: \\beta_{\\text{hp}} = 0\\) and \\(H_0: \\beta_{\\text{wt}} = 0\\). On the other hand, there is insufficient evidence to reject the null \\(H_0 : \\beta_{\\text{wt}} = 0\\).\nIn other words, it appears that horsepower and weight are associated with changes in quarter-mile time, but displacement is not.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#assessing-model-fit",
    "href": "prediction1.html#assessing-model-fit",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.12 Assessing model fit",
    "text": "25.12 Assessing model fit\nOnce we’ve fit a model to the data, how do we tell if our model is good or not? We started talking about this above, and it is a trickier question that it might seem at first. We’ll have lots more to say about the problem in coming weeks. For now, though, let’s consider the most obvious answer to this question.\nWe fit our model to data by minimizing the sum of squares (we’re sticking with simple linear regression here for simplicity– this idea extends to multiple linear regression in the obvious way), \\[\n\\ell( \\beta_0, \\beta_1 )\n= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n= \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2.\n\\]\nSo what better way to measure how good our model is than using precisely this quantity? We define the residual sum of squares (RSS; also called the sum of squared errors, SSE) to be the sum of squared errors of our model. That is, letting \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) be our estimates of the coefficients, \\[\n\\operatorname{RSS}\n=\n\\operatorname{SSE}\n= \\sum_{i=1}^n \\epsilon_i^2 = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - (\\hat{beta}_0 + \\hat{\\beta}_1 x_i) \\right)^2\n\\]\nThe number of degrees of freedom will have bearing on the distribution of this error term- Uunder the model where the errors are indeed normally distributed, the residual sum of squares (RSS) will have an F-distribution with degrees of freedom given by the number of observations minus the number of parameters (like the t-distribution, the F-distribution has the degrees of freedom as one of its parameters; the other, the 3 in the summary above, comes from the number of coefficients less one). Knowing this fact lets us build a rejection region for using the RSS as a test statistic, and that’s exactly where the overall p-value at the bottom of the summary comes from.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nAnother useful quantity in describing how well our model describes the data is the Coefficient of Determination, or \\(R\\)-squared, which can be interpreted as measuring the proportion (between 0 and 1) of the variation in \\(Y\\) that is explained by the variation in \\(X\\).\n\\[\nR^{2}=\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}}=1-\\frac{\\text{RSS}}{\\text{TSS}},\n\\]\nwhere \\[\n\\operatorname{TSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n\\]\nis the total sum of squares.\nIn the case of simple linear regression, things simplify so that \\(R^{2}=r^{2}\\), where \\(r\\) is the correlation coefficient between \\(X\\) and \\(Y\\),\n\\[\nr\n=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}\n{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n\\]\nWhen this quantity is close to 1, we can be confident that our linear model is accurately capturing a trend in the data.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#looking-ahead-model-selection",
    "href": "prediction1.html#looking-ahead-model-selection",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.13 Looking ahead: model selection",
    "text": "25.13 Looking ahead: model selection\nOne important point that we’ve ignored in our discussion above is how we go about choosing what predictors to include in our model. For example, the mtcars data set has columns\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nIn our example above, we just chose a few of these to use as predictors. But suppose that we didn’t know ahead of time which predictors to use. How do we choose which ones to include in our model? Are there downsides to just including all of them?",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#review",
    "href": "prediction1.html#review",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "25.14 Review",
    "text": "25.14 Review\n\nThe simple linear regression model\npredictor / response variables\nassumptions of SLR model\ninterpretation of SLR components\ncorrelation is not causation\nresiduals\nloss function of OLS: Sum of squared residuals\nslope estimate formula and interpretation\nintercept estimate formula and interpretation\nunits of \\(\\beta\\) estimates\nmean squared error, estimate of \\(\\sigma^2_\\epsilon\\)\nvariance of \\(\\beta\\) estimates\nOLS in R, and summary output\nOLS diagnostics (residual plot, residual QQ plot)\ninference and confidence intervals for coefficients\npredictions & prediction intervals\n\\(R^2\\) and \\(r\\)",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction1.html#footnotes",
    "href": "prediction1.html#footnotes",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "",
    "text": "https://ir.lawnet.fordham.edu/ulj/vol20/iss3/1↩︎\nhttps://doi.org/10.1016/j.envres.2007.02.008↩︎",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction2.html#multiple-regression",
    "href": "prediction2.html#multiple-regression",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "",
    "text": "28.1.1 Specifying multiple predictors\nHow do we go about adding more variables to our model?\nWell, it’s basically as simple as you would imagine. We just add more predictors (and more coefficients) to our linear function. If we have \\(p\\) predictors plus an intercept, we predict the response \\(y\\) according to \\[\ny = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_{p-1} x_{p-1} + \\beta_p x_p,\n\\]\nwhere \\(x_1,x_2,\\dots,x_p \\in \\mathbb{R}\\) are predictors.\n\nSimilarly, our model now takes the form that for each \\(i=1,2,\\dots,n\\), we observe \\[\nY_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_{p-1} X_{i,p-1} + \\beta_p X_{i,p} + \\epsilon_i,\n\\]\nwhere \\(\\epsilon_i\\) is an error term (again, assumed normally distributed, independent over \\(i\\), etc) and \\(X_i = (X_{i,1},X_{i,2},\\dots,X_{i,p})^T \\in \\mathbb{R}^p\\) is a vector of predictors. If you haven’t taken linear algebra or you just don’t like vectors, not to worry– it’s perfectly safe to think of this as just a list of numbers in this class. But linear algebra is awesome and super useful if you want to understand machine learning methods, so I recommend that you take linear algebra if you haven’t already!\nNote: some resources will use this notation “backward”, instead writing \\((X_{1,i}, X_{2,i}, \\dots, X_{p-1,i}, X_{p,i})\\) for the predictors. The distinction doesn’t matter much, so long as you’re consistent.\nA convenient way to think about our data set, then, is to make an array of numbers (i.e., a matrix), in which each row corresponds to an observation, and each column corresponds to a predictor: \\[\n\\bf{X} =\n\\begin{bmatrix}\nX_{1,1} & X_{1,2} & \\cdots & X_{1,p} \\\\\nX_{2,1} & X_{2,2} & \\cdots & X_{2,p} \\\\\n\\vdots & \\vdots   & \\ddots & \\vdots \\\\\nX_{n-1,1} & X_{n-1,2} & \\cdots & X_{n-1,p} \\\\\nX_{n,1} & X_{n,2} & \\cdots & X_{n,p}\n\\end{bmatrix}\n\\]\nThe nice thing about this is that we can tack on a column of ones corresponding to our intercept term, \\[\n\\bf{X} =\n\\begin{bmatrix}\n1 & X_{1,1} & X_{1,2} & \\cdots & X_{1,p} \\\\\n1 & X_{2,1} & X_{2,2} & \\cdots & X_{2,p} \\\\\n\\vdots & \\vdots & \\vdots  & \\ddots & \\vdots \\\\\n1 & X_{n-1,1} & X_{n-1,2} & \\cdots & X_{n-1,p} \\\\\n1& X_{n,1} & X_{n,2} & \\cdots & X_{n,p}\n\\end{bmatrix}\n\\]\nand then (if you don’t know linear algebra, it’s okay to just skip this! It won’t be on an exam, it’s just a useful connection to another area that some students have seen before!), we can write our regression formula \\[\ny_i = \\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\cdots + \\beta_p x_{i,p}~~~\\text{ for } i = 1,2,\\dots,n\n\\] as a matrix-vector equation, \\[\n\\bf{y} = \\bf{X} \\bf{\\beta}\n\\] where \\(\\bf{y} = (y_1,y_2,\\dots, y_n)^T\\) is a vector of our responses, and \\(\\bf{\\beta} = (\\beta_0,\\beta_1, \\beta_2, \\dots, \\beta_p)^T\\) is a vector of our coefficients.\nAgain, if you haven’t taken linear algebra before, you can safely ignore the above. It won’t be on an exam or a homework or anything like that, I’ve just included it for students who are familiar with linear algebra to illustrate a connection to another area of math!\n\n\n28.1.2 Example: the mtcars dataset\nLet’s recall the mtcars data set, which includes a number of variables describing the specifications and performance of a collection of car brands.\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nLet’s suppose that we are interested in predicting the quarter mile time (qsec, the time it takes the car to go 1/4 mile from a dead stop) based on its engine displacement (disp, measured in cubic inches), horsepower (hp, measured in… horsepower) and weight (wt, measured in 1000s of pounds).\nThat is, we want to build a multiple linear regression model of the form \\[\n\\text{qsec} = \\beta_0 + \\beta_1 \\text{disp} + \\beta_2 \\text{hp} + \\beta_3 \\text{wt} + \\epsilon\n\\]\nTo fit such a model in R, the syntax is quite similar to the simple linear regression case. The only thing that changes is that we need to specify this model in R’s notation. We do that via qsec ~ 1 + disp + hp + wt.\nLet’s fit the model in R and see what happens.\n\nmtc_model &lt;- lm( qsec ~ 1 + disp + hp + wt, data=mtcars);\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nOnce again, before we get too eager about interpreting the model, we should check that our residuals are reasonable.\n\nhist(mtc_model$residuals)\n\n\n\n\n\n\n\n\nHmm… that isn’t amazing, but on the other hand, there aren’t very many observations to begin with, so we shouldn’t expect a particularly normal-looking histogram.\nChecking heteroscedasticity isn’t so easy now, but we can still do things like compare the residuals with a normal via a Q-Q plot:\n\nplot(mtc_model, which=2)\n\n\n\n\n\n\n\n\nIn my opinion, this Q-Q plot would likely lead me to question the assumption of normal errors. That doesn’t mean that we can’t proceed with using our linear model, but it will mean that we should be a bit careful with how much credence we give to any quantities that depend on our normality assumption (e.g., our p-values).\nLet’s press on regardless, for now, mostly for the sake of demonstration of what we would do, if we were reasonably happy with our model assumptions. Still, we should bear in the back of our minds that perhaps our normality assumptions aren’t exactly true.\nLet’s return to our model output.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nScanning our way up the model summary, let’s look at the table of coefficient estimates. We see that our intercept term and the coefficients for horsepower (hp) and weight (wt) are flagged as being significant. Thus, briefly putting on our testing hats, we would reject the null hypotheses \\(H_0 : \\beta_0=0\\), \\(H_0: \\beta_{\\text{hp}} = 0\\) and \\(H_0: \\beta_{\\text{wt}} = 0\\). On the other hand, there is insufficient evidence to reject the null \\(H_0 : \\beta_{\\text{wt}} = 0\\).\nIn other words, it appears that horsepower and weight are associated with changes in quarter-mile time, but displacement is not.\n\n\n28.1.3 Interpreting estimated coefficients\nIn the case of simple linear regression, our interpretation of an estimated slope \\(\\hat{\\beta}_1\\) was that an increase of one unit in our predictor was associated with an increase of \\(\\hat{\\beta}_1\\) in our response.\nWhat do our estimated coefficients mean when we have multiple predictors instead of just one?\nWell, the interpretation is almost exactly the same. Let’s consider one of the coefficients in our model.\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nOur estimate for the wt coefficient is about \\(1.5\\) (we’re rounding here just to avoid writing a bunch of numbers again and again).\nIf this were simple linear regression, we would say that a unit increase in weight is associated with an increase of \\(1.5\\) seconds in qsec time. But this isn’t simple linear regression– we have other predictors in our model. In particular, we have coefficients corresponding to engine size (disp) and horsepower (hp). That is, our estimated coefficient of wt is the increase in qsec associated with a unit increase of wt while holding hp and disp fixed.\nTypically, say something like “controlling for hp and disp, a unit increase of wt is associated with an increase of \\(1.5\\) in qsec”.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#assessing-model-fit",
    "href": "prediction2.html#assessing-model-fit",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.2 Assessing model fit",
    "text": "28.2 Assessing model fit\nOnce we’ve fit a model to the data, how do we tell if our model is good or not? It’s a trickier question that it might seem at first, and we’ll have lots more to say about the problem in coming weeks. For now, though, let’s consider the most obvious answer to this question.\nWe fit our model to the data by minimizing the sum of squares (we’re sticking with simple linear regression here for simplicity– this idea extends to multiple linear regression in the obvious way), \\[\n\\ell( \\beta_0, \\beta_1 )\n= \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n= \\sum_{i=1}^n \\left( y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2.\n\\]\nSo what better way to measure how good our model is than using precisely this quantity, as achieved by our fitted model?\n\n28.2.1 Assessing model fit with RSS\nWe define the residual sum of squares (RSS; also called the sum of squared errors, SSE) to be the sum of squared residuals between our model and the true responses. That is, letting \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) be our estimates of the coefficients, \\[\n\\operatorname{RSS}\n=\n\\operatorname{SSE}\n=\n\\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n=\n\\sum_{i=1}^n \\left(y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_i) \\right)^2.\n\\]\nOkay, but suppose that we fit our model and we get a particular RSS, say, 55. How do we tell whether a particular RSS is large or small? Well, before we even get to that, there are two problems, here.\n\nWe really ought to renormalize that sum– otherwise, models fit with more observations will tend to trivially have larger RSS. Further, if we have more parameters (i.e., more coefficients, i.e., more predictors) in our model, we are going to be able to trivially reduce the RSS. This is a subtle point, and we’ll discuss it in detail when we talk about model selection and cross validation in a few weeks. The important point for now is that instead of looking at the RSS, we adjust the RSS by dividing it by the degrees of freedom of our model: \\(n-(p+1)\\): \\[\n\\frac{ \\operatorname{RSS} }{ \\operatorname{df} }\n=\n\\frac{1}{n-(p+1)} \\sum_{i=1}^n \\left( \\hat{y}_i - y_i \\right)^2\n\\] Remember, generally speaking, our degrees of freedom are the number of data points, less the number of parameters: \\(n-(p+1)\\), if \\(p\\) is the number of predictors (and an additional \\(1\\) for the intercept term). So a model with more parameters will have a smaller denominator in that expression, and will have a larger RSS. That is, the denominator is smaller when we have more parameters available to our model.\nRSS is a sum of squares. So, like a variance, it is, in a certain sense, of the wrong units to be measuring the “size” of our error. So let’s take the square root of our (renormalized) RSS: \\[\n\\sqrt{ \\frac{ \\operatorname{RSS} }{ \\operatorname{df} } }\n=\n\\sqrt{ \\frac{ \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 }{ n-(p+1) } }\n\\] This quantity is the residual standard error (RSE), and it is reported in the chunk of information at the bottom of our model summary:\n\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nThe residual standard error is listed as being 1.062, and as being the residual standard error on 28 degrees of freedom. Remember that most typically, the degrees of freedom of a model (any model, not just linear regression) will be the number of data points less the number of parameters we estimate. In this case, there are 32 data points\n\nnrow(mtcars)\n\n[1] 32\n\n\nand our model has four parameters: the intercept and our three predictors’ coefficients, so 28 degrees of freedom checks out!\n\n\n28.2.2 What constitutes a good fit?\nIdeally, we want the RSE to be small– after all, it measures the error in our model. But how small is small? What number should we compare it to? Well, in a certain sense, the sum of squared residuals is a measure of how much variance is in the responses that is not explained by our model: \\[\n\\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2\n\\]\nWe need a number to compare this against. But what would this be? Well, ideally, we would like to compare this error against that of another model, and why not (almost) the dumbest model of all: the model with no predictors, just an intercept.\nRecall from last week’s lecture that we determined that if we couldn’t use any predictors, and were only allowed an intercept term, then we should choose \\[\n\\hat{\\beta}_0 = \\bar{y}.\n\\]\nIn other words, the “dumbest” model (okay, I’m sure we could come up with even sillier models if we put our minds to it, but bear with me!) is one that just predicts the sample mean of the responses regardless of the value of the predictor(s).\nIf we used this model, we would obtain a sum of squared residuals given by \\[\n\\sum_{i=1}^n \\left(y_i - \\bar{y} \\right)^2.\n\\]\nThis quantity is called the total sum of squares (TSS), and you’ve seen it before, though it was kind of hiding…\n\n\n28.2.3 Assessing model fit with \\(R^2\\)\nWith the RSS and TSS in hand, we can define the coefficient of determination, or \\(R\\)-squared:\n\\[\nR^2=\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}}=1-\\frac{\\text{RSS}}{\\text{TSS}},\n\\]\nwhere, as a reminder, \\[\n\\operatorname{TSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}\n\\]\nis the total sum of squares and \\[\n\\operatorname{RSS}=\\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_i \\right)^{2}\n\\] is the residual sum of squares.\nIf we think of 1. RSS as being the amount of variation in the data not captured by our model, and 2. TSS as being the amount of variation in the data (once we get rid of the structure explained by the “dumbest” model),\nthen \\(1 - \\operatorname{RSS}/\\operatorname{TSS}\\) is the proportion of the variation that is explained by our model.\nIn the case of simple linear regression, things simplify so that \\(R^{2}=r^{2}\\), where \\(r\\) is the correlation coefficient between the predictors and responses: \\[\nr\n=\\frac{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)\\left(y_{i}-\\bar{y}\\right)}\n{\\sqrt{\\sum_{i=1}^{n}\\left(x_{i}-\\bar{x}\\right)^{2}} \\sqrt{\\sum_{i=1}^{n}\\left(y_{i}-\\bar{y}\\right)^{2}}}\n\\]\nWhen \\(R^2\\) is close to 1, we can be confident that our linear model is accurately capturing the structure (i.e., variation) in the data.\n\n\n28.2.4 Assessing fit another way: MSS\nLet’s consider a different kind of sum of squares: the sum of squares between our model and the “dumbest” model: \\[\n\\sum_{i=1}^n \\left( \\hat{y}_i - \\bar{y} \\right)^2.\n\\]\nThis quantity is often called the model sum of squares (MSS) or the explained sum of squares (ESS). These names will make more sense when you revisit these quantities in a mathematical statistics course– the short version is that this measures the amount of variation in the data explained by our model– it’s like a variance of our model’s predictions.\nAs an aside, we can show (if you’re bored, give it a try) that \\[\n\\operatorname{TSS} = \\operatorname{RSS} + \\operatorname{MSS},\n\\] so that \\[\nR^2=\\frac{\\text{TSS}-\\text{RSS}}{\\text{TSS}}=\\frac{\\text{MSS}}{\\text{TSS}},\n\\] Again, that means we can interpret \\(R^2\\) as measuring the proportion (between 0 and 1) of the variation in the responses (TSS) that is explained by our model.\nIf our model is a good fit to the data, the MSS should be large, while the RSS should be small. So a sensible number to look at is the ratio of these two different sums of squared errors, each adjusted for their degrees of freedom: \\[\n\\frac{ \\operatorname{MSS}/p }{ \\operatorname{RSS}/(n-p-1) }.\n\\]\nIn fact, in the setting where our noise terms \\(\\epsilon_i\\) are independent normals with shared mean \\(0\\) and shared variance \\(\\sigma^2\\), this ratio follows a specific distribution: the F-distribution\n\n\n28.2.5 Assessing model fit with the F-distribution\nIf we look at the very bottom of our model summary, we’ll see a mention of an F-statistic:\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nThe F-distribution is another one of those classic random variables that arise when we look at estimating variance. Just as the T-distribution arises when we look at a ratio of a normal to an estimated variance, something like \\[\n\\frac{ \\bar{X} - \\mu }{ \\frac{1}{n}\\sum_i( X_i - \\bar{X})^2 },\n\\] the F-distribution arises when we look at a ratio of variance estimates, like \\[\n\\frac{ \\frac{1}{n} \\sum_i (U_i - \\bar{U})^2}{ \\frac{1}{m}\\sum_j(V_j - \\bar{V})^2 }.\n\\]\nIts behavior is governed by two degree-of-freedom parameters, one describing the numerator and one describing the denominator. You’ll learn a lot more about the F-distribution when you learn about analysis of variance (ANOVA) in later classes. In our case, we are looking at the ratio \\[\n\\frac{ \\operatorname{MSS}/p }\n  { \\operatorname{RSS}/(n-p-1) }\n= \\frac{ \\frac{1}{p} \\sum_{i=1}^n \\left( \\hat{y}_i - \\bar{y}\\right)^2 }\n{  \\frac{1}{n-p-1} \\sum_{i=1}^n \\left( y_i - \\hat{y}_i \\right)^2 },\n\\] which is a ratio of “means” of squared errors (i.e., variance-like things!). After a bit of algebra (which will have to wait for a mathematical statistics class, I’m afraid), we can show that this ratio is equivalent to a ratio of sample variances.\nThe important part is that under the null hypothesis in which all of the coefficients predictors are zero, \\[\nH_0 : \\beta_0 = \\beta_1 = \\cdots = \\beta_p = 0,\n\\]\nour ratio of squared error terms will follow an F-distribution with parameters given by \\((n-p-1)\\) and \\(p\\). Knowing this distribution, we can test the null hypothesis above, and associate a p-value with the null hypothesis that our model has no explanatory power.\nLooking at our model summary again,\n\nsummary(mtc_model)\n\n\nCall:\nlm(formula = qsec ~ 1 + disp + hp + wt, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8121 -0.3125 -0.0245  0.3544  3.3693 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.965050   0.849663  21.144  &lt; 2e-16 ***\ndisp        -0.006622   0.004166  -1.590  0.12317    \nhp          -0.022953   0.004603  -4.986 2.88e-05 ***\nwt           1.485283   0.429172   3.461  0.00175 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.062 on 28 degrees of freedom\nMultiple R-squared:  0.6808,    Adjusted R-squared:  0.6466 \nF-statistic: 19.91 on 3 and 28 DF,  p-value: 4.134e-07\n\n\nthe F-statistic associated with our residuals has a very small p-value associated to it, indicating, in essence, that our model fit is much better than would be expected by chance. Said another way, we can be fairly confident that our model has captured a trend present in our data.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#looking-ahead-model-selection",
    "href": "prediction2.html#looking-ahead-model-selection",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.3 Looking ahead: model selection",
    "text": "28.3 Looking ahead: model selection\nOne important point that we’ve ignored in our discussion above is how we go about choosing what predictors to include in our model. For example, the mtcars data set has columns\n\nnames(mtcars)\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nIn our example above, we just chose a few of these to use as predictors. But suppose that we didn’t know ahead of time which predictors to use. How do we choose which ones to include in our model? Are there downsides to just including all of them? We’ll discuss this in great detail in a couple of weeks when we discuss model selection and cross validation, a general set of tools for deciding what predictors to keep in a model and which to discard.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#mo-predictors-mo-problems",
    "href": "prediction2.html#mo-predictors-mo-problems",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.4 Mo’ predictors mo’ problems",
    "text": "28.4 Mo’ predictors mo’ problems\nWe’ve seen that extending simple linear regression to multiple linear regression opens up a realm of possibilities for us to incorporate additional information into our models, but this comes at the cost of a few possible pitfalls, which we’ll briefly outline below.\nBefore we turn to that, let’s take a quick detour to discuss an important trick that lets us include categorical random variables in our regressions.\n\n28.4.1 Handling categorical predictors\nSuppose that our long-standing client Catamaran wants to predict spending habits of its customers. For each customer in their database, they know whether or not that customer owns one or more cats, and whether or not they own one or more dogs. This indication of whether or not a customer owns cats (or dogs) is categorical, and it isn’t obvious at first how to incorporate this information into a linear regression model.\nThe trick is to do something very simple: let’s include two predictors: \\[\nx_{i,1} = \\begin{cases} 1 & \\mbox{ if customer } i \\text{ owns one or more cats} \\\\\n        0 & \\mbox { otherwise. }\n        \\end{cases}\n\\] and \\[\nx_{i,2} = \\begin{cases} 1 & \\mbox{ if customer } i \\text{ owns one or more dogs} \\\\\n        0 & \\mbox { otherwise. }\n        \\end{cases}\n\\]\nThen, our model might be something like \\[\ny_i =\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2}.\n\\] Our predicted spending by a customer who owns cats but no dogs would be \\[\n\\beta_0 + \\beta_1,\n\\] while our prediction for a customer who owns dogs but not cats would be \\[\n\\beta_0 + \\beta_2,\n\\] and our prediction for a customer who owns both cats and dogs would be \\[\n\\beta_0 + \\beta_1 + \\beta_2.\n\\]\nThese binary variables encoding our categorical variables are often called dummy variables in the literature, especially in the social sciences.\nThe especially nice thing here is that our interpretation of the coefficients as “the change in response to a unit increase” can be retained: \\(\\beta_1\\) is the (expected) change in spending associated with a “unit increase” in the “customer owns cats” variable. A unit increase in this variable (i.e., from 0 to 1) is just going from “no cats” to “owns one or more cats”.\nExample: mtcars revisited. Returning to the mtcars dataset, notice that there are a couple of binary predictors: vs (the engine shape, i.e., cylinder configuration; 0 for “V”-shaped, 1 for “straight”) and am (transmission type; 0 for automatic, 1 for manual). Let’s plot the mtcars data, and see how qsec varies with these two categories:\n\nlibrary(ggplot2)\npp &lt;- ggplot(mtcars, aes( x=as.factor(vs), y=qsec,\n                          color=as.factor(am),\n                          fill=as.factor(am))) + geom_point();\npp\n\n\n\n\n\n\n\n\nLet’s see what happens when we fit a model using these two binary predictors.\n\nmtc_bin &lt;- lm( qsec ~ 1 + am + vs, data=mtcars );\nsummary(mtc_bin)\n\n\nCall:\nlm(formula = qsec ~ 1 + am + vs, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.77902 -0.37789  0.01687  0.65157  2.91187 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.1303     0.2754  62.195  &lt; 2e-16 ***\nam           -1.3091     0.3791  -3.453  0.00172 ** \nvs            2.8579     0.3753   7.614 2.15e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.038 on 29 degrees of freedom\nMultiple R-squared:  0.6842,    Adjusted R-squared:  0.6624 \nF-statistic: 31.41 on 2 and 29 DF,  p-value: 5.519e-08\n\n\nLooks like both engine shape and transmission type are useful predictors!\nWhat if we have a category with more than two levels, like demographic information? For example, what if we have survey data in which respondents list what state they live in, and we would like to include that information in our regression?\nWell, for each possible category, we can create a binary predictor. So, for example, for each state we could create a predictor that is 1 if the person lives in that state, and zero otherwise. Of course, this is going to result in creating an awful lot of predictors, but that’s a matter that we’ll handle in a few weeks when we discuss model selection and cross validation.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#interactions",
    "href": "prediction2.html#interactions",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.5 Interactions",
    "text": "28.5 Interactions\nLet’s look at that mtcars plot again with our two binary predictors.\n\npp &lt;- ggplot(mtcars, aes( x=as.factor(vs), y=qsec,\n                          color=as.factor(am),\n                          fill=as.factor(am))) + geom_point();\npp\n\n\n\n\n\n\n\n\nIt looks to me like among cars with vs=0, the cars with am=0 have better qsec than those with am=1, but that the transmission type doesn’t make very much difference for qsec among the cars with vs=1 (i.e., straight cylinder configuration).\nThis is an example of an interaction: the effect of one predictor seems to depend on another predictor.\nThe similarity to the term “drug interaction” is perhaps the best analogy here. Certain medications will have much stronger or much different effects if they are taken in combination with other medications. For example, people taking Warfarin, a blood thinner invented here at UW-Madison, need to be very careful when taking NSAIDs such as ibuprofen, because the two drugs interact to cause gastrointestinal bleeding. Either of these two medications in isolation does not drastically increase this risk, but they interact to yield a much higher risk.\nSo it looks like our two binary predictors have an interaction. How might we include this interaction in our model? Well, the natural way is to create another predictor, given by the product of these two predictors: \\[\ny_i =\\beta_0 + \\beta_1 x_{i,1} + \\beta_2 x_{i,2} + \\beta_{1,2} x_{i,1} x_{i,2}.\n\\]\nExample: raining cats and dogs Returning to our Catamaran example, consider the fact that people who own both cats and dogs are also more likely to own a lot of other animals (rats, hamsters, horses, goats… whatever), and this might manifest as an interaction between cat ownership and dog ownership wherein people who own cats and dogs spend far more than would be predicted by just summing the effect of cat ownership and the effect of dog ownership.\nWe can include interactions in our linear models in R by writing the interaction into our model formula. We just write a colon (:) between the two predictors whose interaction we want to include:\n\nmtc_interact &lt;- lm( qsec ~ 1 + vs + am + vs:am, data=mtcars );\nsummary(mtc_interact);\n\n\nCall:\nlm(formula = qsec ~ 1 + vs + am + vs:am, data = mtcars)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.80000 -0.35429  0.03786  0.66687  2.93286 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 17.14250    0.30497  56.210  &lt; 2e-16 ***\nvs           2.82464    0.50245   5.622 5.09e-06 ***\nam          -1.34583    0.52823  -2.548   0.0166 *  \nvs:am        0.07869    0.77325   0.102   0.9197    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.056 on 28 degrees of freedom\nMultiple R-squared:  0.6843,    Adjusted R-squared:  0.6505 \nF-statistic: 20.23 on 3 and 28 DF,  p-value: 3.556e-07\n\n\nLooks like in this case, what looked like an interaction to my eye turned out not to be. We could try only including the interaction term, i.e., fit a model of the form \\[\ny_i =\\beta_0 + \\beta_{1,2} x_{i,1} x_{i,2},\n\\]\nbut it turns out there’s still no significant effect:\n\nmtc_inter &lt;- lm( qsec ~ 1 + am:vs, data=mtcars);\nsummary(mtc_inter)\n\n\nCall:\nlm(formula = qsec ~ 1 + am:vs, data = mtcars)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.1104 -0.7829 -0.1400  0.7097  5.2896 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  17.6104     0.3512  50.146   &lt;2e-16 ***\nam:vs         1.0896     0.7509   1.451    0.157    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.756 on 30 degrees of freedom\nMultiple R-squared:  0.06559,   Adjusted R-squared:  0.03444 \nF-statistic: 2.106 on 1 and 30 DF,  p-value: 0.1571\n\n\nLet’s look at an example where an interaction is present, just to make sure we understand what this usually looks like. This example is due to Jim Frost, whose statistics tutorials are an especially good resource.\nLet’s consider a survey in which we ask people about how much they enjoy different foods and different condiments, and we want to build a model to predict how much people will enjoy their meal given a certain food and a certain condiment. That is, our model looks like \\[\n\\text{satisfaction}\n=\n\\beta_0\n+ \\beta_1 * \\text{food}\n+ \\beta_2 * \\text{condiment}.\n\\]\nFor simplicity, let’s assume that our predictors food and condiment each only take two values: \\[\n\\text{food} \\in \\{ \\text{hot dog}, \\text{ice cream} \\}\n~~~\\text{ and }~~~\n\\text{condiment} \\in \\{ \\text{mustard}, \\text{chocolate} \\}.\n\\]\nNow, someone who gives a high rating for the combination of hot dogs and mustard, and a high rating for ice cream and chocolate may not be so enthusiastic about ice cream with mustard or a hot dog with chocolate. In other words, whether the addition of a condiment increases or decreases a person’s enjoyment of a food depends on the value of the food predictor.\nThis “it depends” property is almost the definition of an interaction effect.\nLet’s download this dataset from Jim Frost’s website to have a look.\n\n# Note: I've downloaded the file and renamed it as frost_example.csv\nfrost &lt;- read.csv('data/frost_example.csv');\n\nhead(frost)\n\n  Enjoyment    Food Condiment\n1  81.92696 Hot Dog   Mustard\n2  84.93977 Hot Dog   Mustard\n3  90.28648 Hot Dog   Mustard\n4  89.56180 Hot Dog   Mustard\n5  97.67683 Hot Dog   Mustard\n6  83.61713 Hot Dog   Mustard\n\n\nLet’s start by plotting this data so we can see the interaction effect for ourselves.\n\npp &lt;- ggplot(frost, aes(x=Food, y=Enjoyment, color=Condiment, fill=Condiment));\npp &lt;- pp + geom_point()\npp\n\n\n\n\n\n\n\n\nIt is pretty clear than when Food=Hot Dog, changing the Condiment predictor from Chocolate to Mustard increases the Enjoyment response, while the opposite pattern holds when Food=Ice Cream. This is the hallmark of an interaction effect.\nLet’s wee what happens if we ignore this interaction:\n\nenjoy_nointer &lt;- lm(Enjoyment ~ 1 + Food + Condiment, data=frost)\nsummary(enjoy_nointer)\n\n\nCall:\nlm(formula = Enjoyment ~ 1 + Food + Condiment, data = frost)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-23.0067 -14.3016   0.5382  13.4187  27.0218 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       79.3237     2.9278  27.093   &lt;2e-16 ***\nFoodIce Cream     -0.2826     3.3807  -0.084    0.934    \nCondimentMustard  -3.7251     3.3807  -1.102    0.274    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 15.12 on 77 degrees of freedom\nMultiple R-squared:  0.01561,   Adjusted R-squared:  -0.009958 \nF-statistic: 0.6105 on 2 and 77 DF,  p-value: 0.5457\n\n\nNeither the Food nor the Condiment predictor has a significant effect!\nAside: note that R has changed the names of our predictors a bit! lm() noticed that we had passed categorical data (e.g., Hot Dog, Ice Cream) into the linear regression, and it automatically created a dummy encoding for it. The name FoodIce Cream indicates that R created a dummy variable that is \\(1\\) when Food=Ice Cream and \\(0\\) when Food=Hot Dog.\nNow, let’s include the interaction effect:\n\nenjoy_nointer &lt;- lm(Enjoyment ~ 1 + Food + Condiment + Food:Condiment, data=frost)\nsummary(enjoy_nointer)\n\n\nCall:\nlm(formula = Enjoyment ~ 1 + Food + Condiment + Food:Condiment, \n    data = frost)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.068 -3.068 -0.407  2.802 13.015 \n\nCoefficients:\n                               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      65.317      1.120   58.34   &lt;2e-16 ***\nFoodIce Cream                    27.731      1.583   17.52   &lt;2e-16 ***\nCondimentMustard                 24.289      1.583   15.34   &lt;2e-16 ***\nFoodIce Cream:CondimentMustard  -56.028      2.239  -25.02   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.007 on 76 degrees of freedom\nMultiple R-squared:  0.8935,    Adjusted R-squared:  0.8892 \nF-statistic: 212.4 on 3 and 76 DF,  p-value: &lt; 2.2e-16\n\n\nWow! Suddenly all our predictors are significant!\nYou may find it interesting to play around with including or not including other terms here (e.g., what happens if you include the interaction but only one of the Food or Condiment predictors?). You’ll learn plenty more about this in your regression courses and when you discuss analysis of variance (ANOVA) in your later classes.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#nonlinear-transformations",
    "href": "prediction2.html#nonlinear-transformations",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.6 Nonlinear transformations",
    "text": "28.6 Nonlinear transformations\nLet’s come back once more to the mtcars data set, and let’s look in particular at mpg (miles per gallon) and hp (horsepower).\n\nplot( mtcars$hp, mtcars$mpg )\n\n\n\n\n\n\n\n\nLet’s fit a linear model to this data and see how things look.\n\nmtc_lm &lt;- lm( mpg ~ 1 + hp, data=mtcars );\n\npp &lt;- ggplot(mtcars, aes(x=hp, y=mpg) ) + geom_point() + geom_smooth(method='lm', formula='y~1+x', se=FALSE );\npp\n\n\n\n\n\n\n\n\nThat looks okay, but let’s check the residuals.\n\nplot( mtc_lm, which=1)\n\n\n\n\n\n\n\n\nVisually, it looks as though our residuals tend to be positive for small and larger values of our predictor, but negative for mid-range values. This isn’t so surprising if we look a little more closely at our original plotted model overlaid on the observations:\n\nmtc_lm &lt;- lm( mpg ~ 1 + hp, data=mtcars );\n\npp &lt;- ggplot(mtcars, aes(x=hp, y=mpg) ) + geom_point() + geom_smooth(method='lm', formula='y~1+x', se=FALSE );\npp\n\n\n\n\n\n\n\n\nIt looks a bit like there is a non-linear trend in our data– as horsepower increases, mpg decreases very quickly at first, and then levels off. That isn’t what we would see if the relationship were simply linear– it’s more in keeping with a nonlinear trend.\nIt looks to me like\nThat isn’t the end of the story, though: the curve looks a bit like a quadratic function. So what if we add a squared term to our predictors?\n\nmtc_lm &lt;- lm( mpg ~ 1 + hp + I(hp^2), data=mtcars );\n\npp &lt;- ggplot(mtcars, aes(x=hp, y=mpg) ) + geom_point() + geom_smooth(method='lm', formula='y~1+x + I(x^2)', se=FALSE );\npp\n\n\n\n\n\n\n\n\nDefinitely a better fit!\nNotice that to get our non-linear term hp^2 into the model, we had to write our formula as mpg ~ 1 + hp + I(hp^2). If we just wrote mpg ~ 1 + hp + hp^2, R would parse hp^2 as just hp. I(...) prevents this and ensures that R doesn’t clobber the expression inside.\nGenerally speaking, if you see a non-linear trend in the data, replacing one or more predictors with a nonlinear transformation thereof is usually the easiest solution. Of course, the question of what nonlinear transformation to use is another matter. Almost always, replacing a predictor x with x^2 or log(x) will do the trick, but there is a whole toolbox of methods for these kinds of things that you’ll see if you take a regression course later.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "prediction2.html#review",
    "href": "prediction2.html#review",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "28.7 Review",
    "text": "28.7 Review\nIn these notes we covered\n\nThe general multiple regression model\nInterpretation of estimated coefficients\nAssessing Model fit with \\(R^2\\)\nMSS and the F Statistic for overall fit\nCategorical predictors\nInteraction Terms\nNonlinear Transformations",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "logistic.html#learning-objectives",
    "href": "logistic.html#learning-objectives",
    "title": "31  Logistic Regression",
    "section": "",
    "text": "Explain the motivation for logistic regression\nFit a logistic regression model to data with one or more predictors and a binary response\nInterpret estimated coefficients in a logistic regression model",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#logistic-regression-motivation",
    "href": "logistic.html#logistic-regression-motivation",
    "title": "31  Logistic Regression",
    "section": "31.2 Logistic regression: motivation",
    "text": "31.2 Logistic regression: motivation\nConsider the following data, which you’ll play with more in discussion section: the Pima Indian data set.\n\nlibrary(MASS)\nhead(Pima.te)\n\n  npreg glu bp skin  bmi   ped age type\n1     6 148 72   35 33.6 0.627  50  Yes\n2     1  85 66   29 26.6 0.351  31   No\n3     1  89 66   23 28.1 0.167  21   No\n4     3  78 50   32 31.0 0.248  26  Yes\n5     2 197 70   45 30.5 0.158  53  Yes\n6     5 166 72   19 25.8 0.587  51  Yes\n\n\nThis data was collection by the US National Institute of Diabetes and Digestive and Kidney Diseases. Each row of this data set corresponds to a woman of Pima Indian descent living near Phoenix Arizona. The columns include a number of biomarkers (e.g., glucose levels, blood pressure and age), as well as a column indicating whether or not each subject had diabetes (as measured according to measures laid out by the World Health Organization). See ?Pima.te for more information.\nCan we predict whether or not a given person has diabetes based on their biomarkers? It is natural to cast this as a prediction problem just like the linear regression problems we have discussed in recent weeks: we are given pairs \\((X_i, Y_i)\\), where \\(X_i\\) is a vector of predictors and \\(Y_i\\) is a response. The difference is that now, \\(Y_i\\) is more naturally thought of as a binary label (in this case, indicating whether or not a subject has diabetes), instead of a continuous number.\nExample: image classification Suppose that we have a collection of images, and our goal is to detect whether or not a given image has a cat in it. Our data takes the form of a collection of pairs \\((X_i, Y_i)\\), \\(i=1,2,\\dots,n\\) where \\(X_i\\) is an image (e.g., a bunch of a pixels) and \\[\nY_i = \\begin{cases} 1 &\\mbox{ if image } i \\text{ contains a cat }\\\\\n                    0 &\\mbox{ if image } i \\text{ does not contain a cat. }\n                    \\end{cases}\n\\] Example: fraud detection An online banking or credit card service might like to be able to detect whether or not a given transaction is fraudulent. Predictors \\(X_i\\) might take the form of things like transaction amount, location and time, and the binary label \\(Y_i\\) corresponds to whether or not the transaction is fraudulent.\nExample: brain imaging In own work in neuroscience, a common task is to detect whether or not a subject has a disease or disorder (e.g., Parkinson’s or schizophrenia) based on brain imaging data obtained via, for example, functional magnetic resonance imaging (fMRI). Our labels \\(Y_i\\) are given by \\[\nY_i = \\begin{cases} 1 &\\mbox{ if subject } i \\text{ has the disease/disorder }\\\\\n                    0 &\\mbox{ if subject } i \\text{ does not have the disease/disorder. }\n                    \\end{cases}\n\\] Our predictors \\(X_i\\) for subject \\(i \\in \\{1,2,\\dots,n\\}\\) might consist of a collection of features derived from the brain imagining (e.g., average pixel values or correlations between different locations in the brain). In my own research, \\(X_i\\) is a network that describes brain structure, constructed from the imaging data. In other studies, \\(X_i\\) might even be the brain imaging data itself. Our goal is to predict \\(Y_i\\) based on the observed imaging data (or image-derived features) \\(X_i\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#formulating-a-model",
    "href": "logistic.html#formulating-a-model",
    "title": "31  Logistic Regression",
    "section": "31.3 Formulating a model",
    "text": "31.3 Formulating a model\nA natural approach to this problem would be to just take our existing knowledge of linear regression and apply it here. If our predictor \\(X_i\\) has \\(p\\) dimensions, \\[\nX_i = (X_{i,1}, X_{i,2}, X_{i,3}, \\dots, X_{i,p} )^T \\in \\mathbb{R}^p,\n\\] then we might try to fit a model of the form \\[\nY_i = \\beta_0 + \\beta_1 X_{i,1} + \\beta_2 X_{i,2} + \\cdots + \\beta_p X_{i,p}\n\\]\nThe obvious problem with this is that the right-hand side of this equation is an arbitrary real number, whereas the left-hand side (i.e., \\(Y_i\\)) is \\(0\\) or \\(1\\).\n\n31.3.1 Example: linear regression on the Pima data set\nFor easier coding later, let’s add a column to the Pima.te data set. The type column is Yes or No according to whether or not each subject has diabetes. Let’s add a column with a more straightforward name and which directly encodes this diabetes status as \\(0\\) or \\(1\\).\n\nPima.te$diabetes &lt;- ifelse( Pima.te$type=='Yes', 1, 0)\nhead(Pima.te)\n\n  npreg glu bp skin  bmi   ped age type diabetes\n1     6 148 72   35 33.6 0.627  50  Yes        1\n2     1  85 66   29 26.6 0.351  31   No        0\n3     1  89 66   23 28.1 0.167  21   No        0\n4     3  78 50   32 31.0 0.248  26  Yes        1\n5     2 197 70   45 30.5 0.158  53  Yes        1\n6     5 166 72   19 25.8 0.587  51  Yes        1\n\n\nNow, let’s pick a predictor and fit a simple linear regression model.\n\npima_lm &lt;- lm( diabetes ~ 1 + glu, data=Pima.te );\nsummary(pima_lm)\n\n\nCall:\nlm(formula = diabetes ~ 1 + glu, data = Pima.te)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.9516 -0.2701 -0.1138  0.2408  1.0025 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.6278036  0.0892443  -7.035 1.16e-11 ***\nglu          0.0080171  0.0007251  11.057  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4023 on 330 degrees of freedom\nMultiple R-squared:  0.2703,    Adjusted R-squared:  0.2681 \nF-statistic: 122.3 on 1 and 330 DF,  p-value: &lt; 2.2e-16\n\n\nLooks like we managed to capture some signal there, at least according to the F-statistic! Let’s look at a plot.\n\nlibrary(ggplot2)\npp &lt;- ggplot(data=Pima.te, aes(x=glu, y=diabetes)) + geom_point()\npp &lt;- pp + geom_smooth(method='lm', se=FALSE, formula='y ~ 1 + x')\npp\n\n\n\n\n\n\n\n\nHmm… so our fitted model takes in a number (i.e., glu) and outputs a number that is our prediction for \\(Y_i\\). The trouble is that our responses are binary, but our outputs can be anything. If our goal is to predict \\(y\\), and we predict \\(y\\) to be \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\), then we can (potentially) predict arbitrary large or small values for \\(y\\), if \\(x\\) is suitably large or small. That’s a bit of an awkward fact– after all, we want our prediction to be \\(0\\) or \\(1\\). This isn’t the end of the world. We could do something like “clipping” our output \\(\\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) to whichever of \\(0\\) or \\(1\\) is closest or something like that. But this feels clumsy, at best. Is there a more appropriate approach?\nWell, there are many, but an especially simple one is logistic regression, which seeks to keep the “linear combination of predictors” idea that we like so much from linear regression, but modify how we make predictions to be better-suited to the “binary response” setting.\n\n\n31.3.2 Logistic regression\nThe core problem with applying linear regression to a binary response is that the output of linear regression can be any number, while we are really only interested in outputs in \\(\\{0,1\\}\\). How might we transform our linear prediction, \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\), into something more sensible?\nLogistic regression solves this problem by taking our linear prediction \\(\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\) and turning it into a probability, i.e., a number between \\(0\\) and \\(1\\). We do that using the logistic function, \\[\n\\sigma( z ) = \\frac{ 1 }{1 + e^{-z}} = \\frac{ e^{z} }{ 1 + e^{z} }.\n\\]\nHere is a plot of the function:\n\nz&lt;- seq(-4,4,0.01);\nplot( z, 1/(1+exp(-z)), col='blue' );\nabline(h=0); abline(h=1);\nabline(v=0)\n\n\n\n\n\n\n\n\nThe logistic function is an example of a sigmoid function (fancy Greek for “S-shaped”).\nUsing the logistic function, we can modify the output of our model by passing our linear regression model’s prediction, which is a real number, as the input of the logistic function so that it outputs a number between zero and one. \\[\n\\sigma\\left( \\hat{y} \\right)\n=\n\\sigma\\left( \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\right)\n=\n\\frac{ 1 }{1 + \\exp\\{ -(\\hat{\\beta}_0 + \\hat{\\beta}_1 x) \\} }\n=\n\\frac{ \\exp\\{ \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\} }{1 + \\exp\\{ \\hat{\\beta}_0 + \\hat{\\beta}_1 x \\} }\n\\]\nThe especially nice thing about this is that since this number is between \\(0\\) and \\(1\\), we can interpret this as a probability: \\[\n\\Pr[ Y_i = 1 ; X_i=x, \\beta_0, \\beta_1 ]\n=\n\\sigma\\left( \\beta_0 + \\beta_1 x \\right)\n=\n\\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n\\]\nThe semi-colon notation there is just to stress that we are treating \\(\\beta_0\\) and \\(\\beta_1\\) as parameters and the data \\(x\\) as an input, but not as things that we are conditioning on.\nAside: there is a whole branch of statistics called Bayesian statistics that seeks to treat parameters like our coefficients \\(\\beta_0\\) and \\(\\beta_1\\) as variables that we can condition on. It’s a very cool area, especially useful in machine learning, but it will have to wait for later courses!\nNow, when it comes time to make a prediction on input \\(x\\), we can pass \\(\\beta_0 + \\beta_1 x\\) into the sigmoid function, and predict \\[\n\\hat{y} = \\begin{cases} 1 &\\mbox{ if } \\sigma( \\beta_0 + \\beta_1 x ) &gt; \\frac{1}{2} \\\\\n                        0 &\\mbox{ if } \\sigma( \\beta_0 + \\beta_1 x ) \\le \\frac{1}{2}. \\end{cases}\n\\]\nIn the multivariate setting, where our \\(i\\)-th predictor \\(X_i\\) takes the form \\[\nX_i = \\left( X_{i,1}, X_{i,2}, \\dots, X_{i,p} \\right),\n\\]\nmultiple logistic regression is the straight-forward extension of this idea. Given (estimated) coefficients \\(\\beta_0, \\beta_1, \\beta_2, \\dots, \\beta_p\\), we output a probability \\[\n\\begin{aligned}\n\\Pr[ Y_i = 1 &; X_i=(x_1,x_2,\\dots,x_p), \\beta_0, \\beta_1, \\dots, \\beta_p ] \\\\\n&=\n\\sigma\\left( \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p \\right) \\\\\n&=\n\\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\cdots + \\beta_p x_p) \\} }\n\\end{aligned}\n\\]",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#logistic-regression-in-r",
    "href": "logistic.html#logistic-regression-in-r",
    "title": "31  Logistic Regression",
    "section": "31.4 Logistic regression in R",
    "text": "31.4 Logistic regression in R\nLet’s ignore, for now, the question of how we fit this model (spoiler alert: least squares isn’t going to make a lot of sense anymore; we’ll need something more clever), let’s ask R to fit a logistic regression model to our problem of predicting diabetes from the glu variable in the Pima data set.\nThe function that we use in R is glm. “GLM” stands for generalized linear model. That is, we are generalizing linear regression. In particular, we are generalizing linear regression by doing linear regression, but then passing the linear regression prediction \\(\\beta_0 + \\beta_1 x\\) through another function.\nTo perform logistic regression, we need to specify to R that we are using a “binomial” family of responses– our response \\(Y\\) is binary, so it can be thought of as a Binomial random variable, with \\(\\Pr[ Y = 1] = \\sigma( \\beta_0 + \\beta_1 X)\\).\nOther than that, fitting a model with glm is basically the same as using plain old lm. Even the model summary output looks about the same:\n\npima_logistic &lt;- glm( diabetes ~ 1 + glu, data=Pima.te, family=binomial );\nsummary(pima_logistic)\n\n\nCall:\nglm(formula = diabetes ~ 1 + glu, family = binomial, data = Pima.te)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -5.946808   0.659839  -9.013   &lt;2e-16 ***\nglu          0.042421   0.005165   8.213   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 420.30  on 331  degrees of freedom\nResidual deviance: 325.99  on 330  degrees of freedom\nAIC: 329.99\n\nNumber of Fisher Scoring iterations: 4\n\n\nBut notice now that our model’s outputs are always between \\(0\\) and \\(1\\):\n\npp &lt;- ggplot( Pima.te, aes(x=glu, y=diabetes) ) + geom_point();\npp &lt;- pp + geom_smooth(formula='y ~ 1+x', se=FALSE,\n                       method='glm',\n                       method.args=list(family = \"binomial\") );\n# Note that geom_smooth needs an extra list() of arguments to specify the\n# extra arguments that we passed to the glm() function above.\n# In particular, we need to tell the GLM to use a binomial response.\npp\n\n\n\n\n\n\n\n\nIf we interpret our model’s output as a probability, namely the probability that we observe label \\(Y\\) for predictors \\(X\\), then it is clear that as glu increases, our model thinks it is more likely that a person has diabetes.\nJust as with linear regression, our logistic model object will make predictions for new, previously unseen predictors if we use the predict function.\n\n# Reminder: we pass a data frame into the predict function, and our model\n# will produce an output for ease row of that data frame.\n# So this is making a prediction for a subject with `glu=200`.\n# Looking at our scatter plot above, it's clear that `glu=200` is at the\n# far upper end of our data distribution, so we should expect our model to\n# produce an output close to 1.\npredict(pima_logistic, type='response', newdata = data.frame(glu=200) )\n\n        1 \n0.9267217 \n\n\nGenerally speaking, as glu increases, our model is more confident that the subject has diabetes.\n\nglu_vals &lt;- seq(50,200,10);\nlogistic_outputs &lt;- predict(pima_logistic, type='response',\n                            newdata = data.frame(glu=glu_vals) )\nplot( glu_vals, logistic_outputs)\n\n\n\n\n\n\n\n\nAgain, these are our model’s predicted outputs for given input values for glucose levels. As glu increases, our model assigns higher and higher probabilities to the possibility that a subject has diabetes.\nYou’ll play around with logistic regression and the Pima data set more in discussion section, where you’ll find that if we choose our predictors more carefully (and use more predictors), we can build a pretty good model!",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#interpreting-model-coefficients",
    "href": "logistic.html#interpreting-model-coefficients",
    "title": "31  Logistic Regression",
    "section": "31.5 Interpreting model coefficients",
    "text": "31.5 Interpreting model coefficients\nIn linear regression, our model coefficients had a simple interpretation: \\(\\beta_1\\) was the change in response associated with a unit change in the corresponding predictor. Is there an analogous interpretation for logistic regression?\nWell, there is, but it requires a bit of extra work to define some terms so that we can talk sensibly about what changes in response to a change in a predictor.\nThe odds of an event \\(E\\) with probability \\(p\\) are given by \\[\n\\operatorname{Odds}(E) = \\frac{ p }{ 1-p }.\n\\]\nThat is, the odds of an event is the ratio of the probability of the event happening to the probability that it doesn’t happen. So, for example, if we have an event that occurs with probability \\(1/2\\), the odds of that event are \\((1/2)/(1/2) = 1\\), which we usually say as “one to one odds”. Some of this kind of vocabulary may be familiar to you from sports betting, if you’ve been to Las Vegas or watched the Kentucky Derby.\nSo, let’s suppose that we have a logistic regression model with a single predictor, that takes predictor \\(x\\) and outputs a probability \\[\np(x) = \\frac{ 1 }{ 1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }.\n\\] and note that \\[\n1-p(x) = \\frac{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}}\n\\] The odds associated with this probability are \\[\n\\operatorname{Odds}(x)\n= \\frac{ p(x) }{1-p(x)}\n= \\frac{1 }{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }\n= \\exp\\{ \\beta_0 + \\beta_1 x \\}.\n\\]\nSince the odds can get very large or very small, especially once we start working with small probabilities, it is often much easier to work with the logarithm of the odds, usually called the “log-odds” for short. This quantity is especially important in statistics associated with biology (e.g., drug trials and studies of risks associated with diseases). If we look at the log odds associated with our logistic regression model, \\[\n\\operatorname{Log-Odds}(x)\n= \\log \\operatorname{Odds}(x)\n= \\log \\exp\\{ \\beta_0 + \\beta_1 x \\}\n= \\beta_0 + \\beta_1 x.\n\\]\nIn other words, under our logistic regression model, the “slope” \\(\\beta_1\\) is the increase in the log-odds of the response being \\(1\\) associated with a unit increase in \\(x\\).\nSaid yet another way, logistic regression is what happens if we use a linear regression model to predict the log-odds of the event that the response is \\(1\\), i.e., the log-odds of the event \\(Y_i = 1\\).",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#fitting-the-model",
    "href": "logistic.html#fitting-the-model",
    "title": "31  Logistic Regression",
    "section": "31.6 Fitting the model",
    "text": "31.6 Fitting the model\nWe said previously that we were going to ignore, for the time being, the matter of how we fit out logistic regression model. Well, let’s come back to that question.\nLet’s start by recalling ordinary least squares regression, where we had the following loss function:\n\\[\n\\ell( \\beta_0 ,\\beta_1 )\n= \\sum_{i=1}^n \\left(y_i - (\\beta_0 + \\beta_1 x_i) \\right)^2\n= \\sum_{i=1}^n (\\hat{y}_i - y_i)^2,\n\\]\nwhere \\(\\hat{y_i}=\\beta_0+\\beta_1 x_i\\) is the predicted \\(y_i\\) based on the coefficients \\(\\beta_0,\\beta_1\\). This function measures how “wrong” our model is if we use coefficients \\(\\beta_0\\) and \\(\\beta_1\\), and we choose these coefficients in such a way as to make thie loss function as small as possible. We used the square of the loss because it makes a lot of the math easier.\nNow that we’re working with logistic regression instead of linear, how do we decide what makes a good choice of coefficient? We could try and force a way to use least squares, but things would get a bit complicated.\nLet’s try something different. Our logistic regression model takes a predictor and outputs a probability that the response is \\(1\\). That is, for our \\(i\\)-th predictor, our model outputs (we’re sticking with the case of one predictor for simplicity, but the idea extends to multiple predictors in the natural way): \\[\n\\Pr[ Y_i=1; X_i=x, \\beta_0, \\beta_1 ]\n= \\frac{1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}}.\n\\]\nIn other words, this is the probability that our model gives to the event that the \\(i\\)-th response is \\(1\\). On the other hand, our model assigns probability to the event that the \\(i\\)-th response is \\(0\\) given by \\[\n\\begin{aligned}\n\\Pr[ Y_i=0; X_i=x, \\beta_0, \\beta_1 ]\n&=\n1-\\Pr[ Y_i=1; X_i=x, \\beta_0, \\beta_1 ] \\\\\n&= \\frac{ \\exp\\{ -(\\beta_0 + \\beta_1 x) \\} }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\\\\n&= \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}}\n.\n\\end{aligned}\n\\]\nNow, if the \\(i\\)-th response really is \\(0\\), then we want this number to be big. And if the \\(i\\)-th response really is \\(1\\), we want \\(1/(1+\\exp\\{ -(\\beta_0 + \\beta_1 x )\\})\\) to be big.\nFurther, we want this pattern to hold for all of our data. That is, we want our model to give a “high probability” to all of our data. If our \\(i\\)-th observation takes the form \\((x_i,y_i)\\), then the probability of our data under the model, assuming the observations are independent, is \\[\n\\prod_{i=1}^n \\Pr[ Y_i=y_i; X_i=x_i, \\beta_0, \\beta_1 ].\n\\]\nFor one of our data points, using the fact that \\(y_i\\) is either \\(0\\) or \\(1\\), we can write \\[\n\\Pr[ Y_i=y_i; X_i=x_i, \\beta_0, \\beta_1 ]\n=\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}.\n\\] So the probability of our whole data set is \\[\n\\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}.\n\\] So this is the probability of all of our data under our model, for a particular choice of parameters \\(\\beta_0\\) and \\(\\beta_1\\). We call this the likelihood of the data. One way to pick our model coefficients is to choose them so that this quantity is large– if this model is large, that means our data agrees with the model.\nSo we want to choose \\(\\beta_0\\) and \\(\\beta_1\\) to make this probability, the likelihood, large. How do we do that? Well, our likelihood is a product of a bunch of things, and products are hard to work with. Let’s take a logarithm. Remember, logs turn products into sums, and sums are easy to work with. \\[\n\\begin{aligned}\n\\log\n& \\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\sum_{i=1}^n \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i} \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i}\n\\end{aligned}\n\\]\nNow, let’s notice that an individual term in this sum is a log of a product, and that \\(\\log a^b = b \\log a\\), so that: \\[\n\\begin{aligned}\n\\log &\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i} \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n+ \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= y_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)\n\\end{aligned}\n\\]\nSo the logarithm of our likelihood is \\[\n\\begin{aligned}\n\\log\n& \\prod_{i=1}^n\n\\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)^{y_i}\n\\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)^{1-y_i} \\\\\n&= \\sum_{i=1}^n\ny_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right).\n\\end{aligned}\n\\] Notice that since the logarithm function is monotone, making the likelihood large is the same as making the likelihood large and vice versa.\nLogistic regression chooses the coefficients \\(\\beta_0\\) and \\(\\beta_1\\) to make this log-likelihood large. Equivalently, we minimize the negative log likelihood, \\[\n\\ell(\\beta_0, \\beta_1)\n=\n-\\sum_{i=1}^n \\left[\ny_i \\log \\left( \\frac{ 1 }{1 + \\exp\\{ -(\\beta_0 + \\beta_1 x) \\}} \\right)\n+ (1-y_i) \\log \\left( \\frac{ 1 }{1 + \\exp\\{ \\beta_0 + \\beta_1 x \\}} \\right)\n\\right].\n\\]\nThat is, while our loss function for linear regression is the sum of squared errors, our loss function for logistic regression is the negative log likelihood.\nUnlike in the least squares case, these optimized coefficients do not have nice closed form solutions. Still, when we conduct logistic regression in R, this minimization problem is solved for us using tools from optimization.\n\n31.6.1 A bit of philosophy: maximum likelihood\nIt turns out that this idea of choosing our parameters to maximize the probability of the data is so popular in statistics that it has a name: maximum likelihood estimation. When we need to estimate the value of a parameter, we choose the one that maximizes the likelihood of the data.\nInterestingly, in many situations, the least squares estimate and the maximum likelihood estimate of a parameter are, in fact, the same. In many situations, the sample mean is the least squares estimate of our parameter, and it is also the estimate that we would obtain if we maximize the likelihood. Examples of this kind of situation include: linear regression, estimating the mean of a normal, and estimating the rate parameter of the Poisson.\nYou’ll see many more connections between these ideas, and establish some of the interesting properties of maximum likelihood estimation in your more advanced statistics classes. For now, let’s very quickly establish that the sample mean of the normal is both the least squares estimator and the maximum likelihood estimator.\n\n\n31.6.2 Example: mean of a normal\nConsider the setting in which we observe data \\(X_1,X_2,\\dots,X_n\\) drawn independently and identically distributed according to a normal distribution with unknown mean \\(\\mu\\) and known variance \\(\\sigma^2\\) (assuming the variance is known is just for the sake of making some things simpler– a similar story is true if we have to estimate the variance, as well).\nSo let’s suppose that we observe \\(X_1=x_1, X_2=x_2, \\dots, X_n=x_n\\). Let’s consider two different ways to estimate the mean \\(\\mu\\).\nLeast squares estimation. In least squares, we want to choose the number that minimizes the sum of squares between our estimate and the data: \\[\n\\min_{m} \\sum_{i=1}^n ( x_i - m )^2.\n\\]\nLet’s dust off our calculus and solve this. We’ll take the derivative and set it equal to zero. First, let’s find the derivative: \\[\n\\frac{d}{d m} \\sum_{i=1}^n ( x_i - m )^2\n= \\sum_{i=1}^n \\frac{d}{d m} ( x_i - m )^2\n= 2 \\sum_{i=1}^n (x_i - m),\n\\]\nwhere we used the fact that the derivative is linear (so the derivative of a sum is the sum of the derivatives) to get the first equality.\nNow, let’s set that derivative equal to zero and solve for \\(m\\). \\[\n\\begin{aligned}\n0 &= 2 \\sum_{i=1}^n (x_i - m) \\\\\n0 &= \\sum_{i=1}^n x_i - nm \\\\\nn m &= \\sum_{i=1}^n x_i \\\\\nm &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{aligned}\n\\]\nSo the least squares estimate of \\(\\mu\\) is just the sample mean, \\[\n\\hat{\\mu} = \\frac{1}{n} \\sum_{i=1}^n x_i.\n\\]\nMaximum likelihood estimation. Okay, now let’s try a maximum likelihood approach. Remember, maximum likelihood says that we should choose our estimate to be the number that makes our data as “likely” as possible– that is, the number that makes the probability of the data large.\nUnder the normal, the “probability” (it’s actually a density, not a probability, but that’s okay) of our data is \\[\n\\begin{aligned}\n\\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n&= \\prod_{i=1}^n \\Pr[ X_i = x_i; \\mu, \\sigma^2 ] \\\\\n&= \\prod_{i=1}^n \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\},\n\\end{aligned}\n\\] where we used the fact that our data is independent to write the “probability” as a product.\nOkay, now how are we going to choose \\(\\mu\\) to minimize this? Well, we have a product of things, and products are annoying. Let’s take the log of this, instead, to make it a sum: \\[\n\\begin{aligned}\n\\log \\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n&= \\log \\prod_{i=1}^n \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\} \\\\\n&= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n            \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\} \\\\\n&= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  + \\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\},\n\\end{aligned}\n\\]\nwhere we made repeated use of the fact that the logarithm of a product is the sum of the logarithms.\nTaking the log to simplify the likelihood is so common that we have a name for this quantity: the log likelihood. Notice that because the logarithm is a monotone function, maximizing the likelihood and maximizing the log-likelihood are equivalent. The maximizer of one will be the maximizer of the other, so there’s no problem here.\nSo we have \\[\n\\log \\Pr[ X_1=x_1, X_2=x_2, \\dots, X_n=x_n; \\mu, \\sigma^2 ]\n= \\sum_{i=1}^n \\log  \\frac{1}{\\sqrt{ 2 \\pi \\sigma^2 } }\n  + \\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\}.\n\\]\nWe’re almost there!\nNotice that the first of these two sums doesn’t depend on \\(\\mu\\). So for the purposes of finding which value of \\(\\mu\\) makes the log-likelihood large, we can ignore it, and just concentrate on maximizing \\[\n\\sum_{i=1}^n \\log \\exp\\left\\{ \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 } \\right\\}.\n\\]\nSince \\(\\log \\exp x = x\\), this simplifies to \\[\n\\sum_{i=1}^n \\frac{ -(x_i - \\mu)^2}{ 2\\sigma^2 }\n= \\frac{-1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\n\\]\nNow, maximizing \\(-f(\\mu)\\) is the same as minimizing \\(f(\\mu)\\). In other words, the maximum likelihood estimate for the mean of our normal distribution is the value of \\(\\mu\\) that minimizes \\[\n\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nIf we take derivatives and set the result equal to zero, we’re going to get the very same least squares problem we had before, and we know that the minimizer is obtained by taking \\[\n\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i,\n\\]\nthe sample mean!\nSo, to recap, two different ways to measure what is a “good” estimate give us the same answer: least squares and maximum likelihood both lead us to estimate the mean of the normal as the sample mean!\nThis “specialness” of the sample mean is actually very common in the “nice” distributions that we see a lot in our early statistics courses, but it’s worth bearing in mind that this isn’t generally the case: in more “interesting” settings, least squares and maximum likelihood estimators can be very different! Stay tuned in your later courses to learn more.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#prediction-versus-feature-selection-the-two-cultures",
    "href": "logistic.html#prediction-versus-feature-selection-the-two-cultures",
    "title": "31  Logistic Regression",
    "section": "31.7 Prediction versus feature selection: the two cultures",
    "text": "31.7 Prediction versus feature selection: the two cultures\nWe have motivated most of our discussion of regression the last few weeks by talking about prediction. We see data \\(X_i\\), and our goal is to predict (i.e., guess) an associated response or label \\(Y_i\\). A “good” model is one that does well at predicting these labels or responses.\nExample: image recognition Consider once again the problem of detecting whether or not an image contains a cat. We observe \\((X_i, Y_i)\\) pairs in which each image \\(X_i\\) has an associated label \\[\nY_i = \\begin{cases} 1 &\\mbox{ if image } i \\text{ contains a cat }\\\\\n                    0 &\\mbox{ if image } i \\text{ does not contain a cat. }\n                    \\end{cases}\n\\]\nOur goal is then to build a model that takes in an image \\(x\\) and produces a prediction (i.e., a best guess) \\(\\hat{y}\\) based on \\(X\\) as to the true label \\(y\\). A good model is one that correctly guesses \\(y\\) most of the time. Something like \\[\n\\Pr[ \\hat{y} = y ] \\text{ is close to } 1\n\\]\nExample: housing prices Consider once again the problem of predicting how much a house will sell for. We get predictors in the form of a vector \\(x\\) containing the house’s age, square footage, proximity to parks, quality of the local school system, and so on, and our goal is to predict the price \\(y\\) of that house. A “good” model is one whose prediction \\(\\hat{y}\\) is “close” to the true value \\(y\\) “on average”. Something like \\[\n\\mathbb{E} \\left( \\hat{y} - y \\right)^2 \\text{ is small. }\n\\]\nNow, if our goal is only to detect whether or not a picture contains a cat, or only to predict how much a house will sell for, then that’s kind of the end of the story.\nBut suppose that we have a really good predictor for housing price, and someone asks us “what is it that makes a house expensive?”. Just because we have a good predictive model doesn’t mean that we can answer this question easily.\nFor example, large complicated neural nets are very good at image detection, but it’s very hard to take a trained neural net and figure out what it is about a particular image that causes the neural net to “believe” that there is or isn’t a cat in the image. There is no easy way to determine what the “meaningful” or “predictive” features of the image are. Indeed, this problem is a major area of research currently in machine learning and statistics. See here if you’re curious to learn more.\nOne of the good things about linear and logistic regression is that we can very easily determine which features were “useful” or “meaningful” for prediction– the estimated coefficients and associated p-values tell us quite a lot about which predictors (i.e., “features” in the language of machine learning) are (probably) useful.\nIndeed, sometimes the determination of which predictors are “useful” or “meaningful” is the important part of the statistical question.\nExample: fMRI data and schizophrenia. In my own research, I work a lot with functional magnetic resonance imagine (fMRI) data obtained from studies in which the subjects are either healthy controls or are diagnosed with schizophrenia. It’s tempting to want to build a model that can predict, given an fMRI scan of a person’s brain, whether or not this person has schizophrenia.\nThat’s all fine and good, but doctors already have a very good way of telling whether or not a person has schizophrenia, and it’s way cheaper than doing an MRI: just talk to them for five minutes! If you doubt this, search around for videos of interviews with schizophrenics.\nSo being able to predict whether or not someone has schizophrenia based on an fMRI scan is interesting and perhaps impressive, but it isn’t at all useful scientifically or clinically: we have a cheaper simpler way of telling if someone is schizophrenic.\nWhat is important to my colleagues in neuroscience and neurology is a more subtle question: what is it that is different between the brains of schizophrenic and healthy controls?\nIn other words, oftentimes we don’t just want a model that can perform prediction well, but we want a model that explains what it is in our data that generally explains the differences we see in responses or labels. Further, we usually want to be able to use that model to make inferences about the world– to estimate things like how levels of coffee consumption correlate with health outcomes.\nThis tension, between prediction and modeling, was discussed extensively in a famous paper from 2001 by Leo Breiman, titled Statistical Modeling: The Two Cultures.\nNow, it’s not as though there’s some huge fight between “prediction people” and “modeling people”. We work together all the time! But it’s useful to understand the distinction between these two different kinds of problems and to be able to distinguish when one or the other is the more appropriate “hat” to be wearing. Keep an eye out for these kinds of things as you progress in your data science career!",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "logistic.html#review",
    "href": "logistic.html#review",
    "title": "31  Logistic Regression",
    "section": "31.8 Review",
    "text": "31.8 Review\n\nRelationship between probability, odds and log-odds\nSigmoid function \\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\)\nSimple logistic model \\(\\text{log-odds}(Y=1)=\\beta_0 + \\beta_1 X_1\\)\nPredicting \\(y\\) from a logistic model\ninterpreting the coefficients of a logistic model\nZ tests of significance of logistic coefficients\nfitting a logistic model (maximize log-likelihood / minimize residual deviance)\nlikelihood / log likelihood / residual deviance\nconfusion matrix for a logistic model, prediction statistics",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "cv.html#learning-objectives",
    "href": "cv.html#learning-objectives",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.1 Learning objectives",
    "text": "32.1 Learning objectives\nAfter this lecture, you will be able to\n\nExplain the problem of variable selection in the context of linear regression\nExplain and apply cross-validation methods, including leave-one-out cross-validation and \\(K\\)-fold cross-validation.\nExplain subset selection methods, including forward and backward stepwise selection.\nExplain and apply regularization and shrinkage methods, including ridge regression and the LASSO."
  },
  {
    "objectID": "cv.html#model-selection-overview",
    "href": "cv.html#model-selection-overview",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.2 Model selection: overview",
    "text": "32.2 Model selection: overview\nOur focus this week will be on model selection for regression problems. Still, we should note that similar ideas apply in many other situations. For example, when clustering data, we use model selection to choose how many clusters to group the data into.\nThe unifying idea is that we have to choose among many different similar ways of describing the data. That’s what model selection helps us do."
  },
  {
    "objectID": "cv.html#variable-selection",
    "href": "cv.html#variable-selection",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.3 Variable selection",
    "text": "32.3 Variable selection\nSuppose that we have a collection of \\(p\\) predictors, and that \\(p\\) is very large (say, \\(p \\approx n\\)). If we try to fit a model using all of these predictors, we will end up over-fitting to the data, a point that we have discussed briefly a few times this semester.\nIf you’ve taken linear algebra (skip this paragraph if not!): in linear regression (a similar story holds for other prediction models), we have \\(n\\) equations in \\(p\\) unknowns. When \\(p\\) is of a similar size to the number of observations \\(n\\), the system is over-determined (or close to it)..\nIn situations like this, we would like to choose just a few of these predictors for inclusion in a statistical model (e.g., linear regression). This is an example of model selection: we have a bunch of different models under consideration (i.e., a different possible model for each set of variables we might choose), and we want to pick the best one.\nThe natural question, then, is: how do we compare models?\n\n32.3.1 Example: mtcars\nLet’s consider a very simple example, adapted from Section 3.3.2 and Section 5.1 in ISLR, revisiting our old friend the mtcars data set.\n\ndata('mtcars');\nhead(mtcars);\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nLet’s confine our attention to the mpg (miles per gallon) and hp (horsepower) variables. Can we predict gas mileage from the horsepower?\nLet’s try fitting linear regression.\n\nmodel1 &lt;- lm(mpg ~ 1 + hp, mtcars);\nintercept1 &lt;- model1$coefficients[1];\nslope1 &lt;- model1$coefficients[2];\n\n# Plot the data itself\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\npp &lt;- pp + geom_abline(intercept=intercept1, slope=slope1, colour='blue' );\npp\n\n\n\n\nOkay, it looks reasonable, but you might notice that the residuals have a bit of a weird behavior. Let’s plot them to see what I mean.\n\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg-(slope1*hp+intercept1))) + geom_point()\npp\n\n\n\n\nThe residuals have a kind of U-shape. This suggests that there is a non-linearity in the data that we are failing to capture. Let’s try adding another predictor: the squared horsepower.\n\nmodel2 &lt;- lm(mpg ~ 1 + hp + I(hp^2), mtcars);\nintercept2 &lt;- model2$coefficients[1];\nslope2_1 &lt;- model2$coefficients[2];\nslope2_2 &lt;- model2$coefficients[3];\n\n# Plot the data itself\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\n# As usual, there are cleaner ways to do this plot, but this is the quick and easy way to make itt.\n# If we were doing this more carefully, we would evaluate the cureve in the plot at more x-values than just the ones in the data to smooth things out.\npp &lt;- pp + geom_line( aes(x=hp, y=intercept2 + slope2_1*hp + slope2_2*I(hp^2) ), colour='red' );\npp\n\n\n\n\nThat looks like quite an improvement! Just for comparison:\n\npp &lt;- ggplot( mtcars, aes(x=hp, y=mpg)) + geom_point();\npp &lt;- pp + geom_abline(intercept=intercept1, slope=slope1, colour='blue' );\npp &lt;- pp + geom_line( aes(x=hp, y=intercept2 + slope2_1*hp + slope2_2*I(hp^2) ), colour='red' );\npp\n\n\n\n\nWe can also compare the squared residuals to confirm that adding the feature hp^2 actually decreased our error:\n\nc( sum( model1$residuals^2 ), sum( model2$residuals^2 ) );\n\n[1] 447.6743 274.6317\n\n\nWhy stop there? Why not add hp^3 as well, or even hp^4? Well, funny enough, that is precisely the idea behind polynomial regression, which you can learn more about in ISLR (Section 3.3.2; more substantial discussion in Chapter 7) or in a regression course.\nBut that raises the question: how do we know when to stop?\n-You’ll find that if you add hp^3 to the model above, that the sum of squared residuals does indeed improve.\n-But how do we know if that improvement is worth it?\nOne approach to this problem would be to examine the \\(p\\)-values associated to the coefficients (see ISLR Chapter 3 for a discussion of that approach). In these notes, we will see a different, arguably more principled approach.\n\n\n32.3.2 Overfitting and Unseen Data\nIf we keep adding more predictors to our model, the residuals will continue to decrease, but this will not actually mean that our model is better. Instead, what we will be doing is over-fitting to the data. That is, our model will really just be “memorizing” the data itself rather than learning a model.\nThe true test of model quality is how well it does at predicting for data that we didn’t see.\nThat is, if we fit our model on data \\((X_i,Y_i)\\) for \\(i=1,2,\\dots,n\\), how well does our model do on a previously unseen data point \\((X_{n+1},Y_{n+1})\\)?\nSpecifically, in the case of regression, we want our model to minimize \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2,\n\\]\nwhere \\(\\hat{Y}_{n+1}\\) is our model’s prediction based on coefficients estimated from our \\(n\\) training observations."
  },
  {
    "objectID": "cv.html#validation-sets",
    "href": "cv.html#validation-sets",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.4 Validation Sets",
    "text": "32.4 Validation Sets\nSo rather than focusing on how well our model fits our training data, we should be trying to determine how well our model does when it gets applied to data that we haven’t seen before.\nSpecifically, we would like to know the mean squared error (MSE), \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2.\n\\]\nNote: The name is hopefully clear– it is the expectation (mean) of the squared error between our prediction and the truth.\nIf you’ve taken an ML course, this idea should already be quite familiar. We always train (“fit” in the language of statistics) our model on a training set, and then assess how well the model performs on a test set that our model hasn’t seen before.\nThe trouble is that in most statistical problems, we have at most a few hundred data points to work with. As a result, we can’t really afford to set aside some of our data just to use as a test set.\nNote that this is in contrast to many machine learning settings (e.g., training a neural net), where we often have tens or hundreds of thousands of data points to work with.\nFollowing the logic of the train/test split idea in ML, though, a natural approach is to do the following:\n\nSplit our data into two parts, say \\(S_1,S_2\\), such that \\(S_1 \\cup S_2 = \\{1,2,\\dots,n\\}\\) and \\(S_1 \\cap S_2 = \\emptyset\\).\nObtain estimate \\(\\hat{\\beta}_1\\) by fitting a model on the observations in \\(S_1\\)\nEvaluate the error of our fitted model on \\(S_2\\), \\[\n\\hat{E}_1\n=\n\\frac{1}{|S_2|} \\sum_{i \\in S_2} \\left( Y_i - \\hat{\\beta}_1 X_i \\right)^2.\n\\]\n\nTypically, we call \\(S_2\\), the set that we make predictions for, the validation set, because it is validating our model’s performance.\n\n32.4.1 Example: mtcars revisited\nLet’s see this in action on the mtcars data set.\nWe randomly split the data set into two groups. For each model order 1, 2, 3, 4 and 5, we fit the model to the training set and then measure the sum of squared residuals of that model when applied to the validation set.\nOne run of this experiment is summarized in resids_onerun. For details, refer to mtcars_poly.R, which is included among the supplementary files for this lecture.\n\nsource('r_scripts/mtcars_poly.R');\n\nhead(resids_onerun);\n\n  Order       Error\n1     1   20.105302\n2     2    9.170243\n3     3   26.532305\n4     4  292.739805\n5     5 3210.927214\n\n\n\n# Plot these results\npp &lt;- ggplot(resids_onerun, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\npp\n\n\n\n\nLet’s pause to make sure that we understand what this plot actually shows. We split the mtcars dataset randomly into two sets, a “training” set and a “validation” set. For each order (1, 2, 3, 4, 5), we fit a model of that order to the training set. Then we use that model to try and predict the outcomes (mpg) on the validation set. So this is the performance of five different models, each trained on the same data \\(S_1\\) and evaluated on the same data \\(S_2\\), different from the training data.\nLooking at the plot, we see that as we add higher-order powers of hp, we don’t really gain much in terms of the error (i.e., sum of squared residuals) beyond order 2. Indeed, past the order-3 model, the error gets worse again!\nAside: this deteriorating performance is due largely to the fact that the mtcars data set is so small. Once we split it in half, we are fitting our model to just 16 observations. Estimating four or five coefficients from only about 15 observations is asking for trouble! This is a tell-tale sign of over-fitting of a model. This would be a good occasion for some kind of regularization, but we’ll come back to that.\n\n\n32.4.2 Variance in the residuals\nThere’s one problem, though, beyond matters of sample size. That plot shows the residuals as a function of model order for one particular random set \\(S_1\\). Let’s plot the same residuals for a few different random sets.\nNote: the data frame resids contains multiple replicates of the above experiment. Once again, refer to the code in mtcars_poly.R for details.\n\nhead(resids)\n\n  Rep Order    Error\n1   1     1 20.10530\n2   2     1 12.67520\n3   3     1 25.22882\n4   4     1 19.04291\n5   5     1 30.66274\n6   6     1 21.86813\n\n\n\npp &lt;- ggplot(resids, aes(x=Order, y=log(Error), color=as.factor(Rep) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\nHmm. There’s quite a lot of variance among our different estimates of the prediction error. Note that the y-axis is on a log scale, so an increase from, say, 2 to 3 is an order of magnitude increase in error.\nEach of these is supposed to be estimating the error \\[\n\\mathbb{E} \\left( \\hat{Y}_{n+1} - Y_{n+1} \\right)^2,\n\\]\nbut there’s so much variation among our estimates that it’s hard to know if we can trust any one of them in particular!\nIndeed, the variance is so high that we needed to plot the error on a log scale! Once in a while, we get unlucky and pick an especially bad train/validate split, and the error is truly awful!\nQuestion: what explains the variance among the different lines in that plot?\nQuestion: How might we reduce that variance?"
  },
  {
    "objectID": "cv.html#reducing-the-variance-leave-one-out-cross-validation",
    "href": "cv.html#reducing-the-variance-leave-one-out-cross-validation",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.5 Reducing the variance: Leave-one-out cross-validation",
    "text": "32.5 Reducing the variance: Leave-one-out cross-validation\nOne source of variance in our cross-validation plots above was the fact that each replicate involved splitting the data in half and training on only one of the two halves.\nThat means that on average, from one replicate to another, the data used to train the model changes quite a lot, and hence our estimated model changes a lot. That’s where the variance comes from in the plot we just looked at!\nThere is also the related problem that we are training on only half of the available data. As statisticians and/or machine learners, we don’t like not using all of our data!\nSo, here’s one possible solution: instead of training on half the data and validating (i.e., evaluating the model) on the other half, let’s train on all of our data except for one observation, then evaluate our learned model on that one held-out data point.\nThat is, instead of splitting our data into two halves, we\n\nTake one observation and set it aside (i.e., hold it out)\nTrain our model on the other \\(n-1\\) observations\nEvaluate our model on the held-out observation.\n\nThis is called leave-one-out cross-validation (LOO-CV).\n\n# This R file implements the same experiment as we saw above,\n# but this time doing LOO-CV instead of a naive two-set split.\nsource('r_scripts/mtcars_poly_loocv.R');\n\npp &lt;- ggplot(resids_onerun, aes(x=Order, y=Error ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\nBut once again, that’s just one run. Let’s display several of them in one plot.\n\npp &lt;- ggplot(resids, aes(x=Order, y=log(Error), color=as.factor(Rep) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\nFor each of our replicates, we are estimating our model based on \\(n-1\\) of the observations, and then evaluating our prediction on the one held-out observation.\nBut now we have a different kind of variance: our estimate of the error is at the mercy of the one observation that we chose to hold out. If we chose an especially “bad” or “challenging” observation to hold out, then our error might be especially high.\nLeave-one-out cross-validation (LOO-CV) tries to bridge this gap (i.e., balancing the better stability of leaving one observation out with the variability induced by evaluating on a single point) by:\nFor each \\(i=1,2,\\dots,n\\):\n\nTrain the model on \\(\\{ (X_j, Y_j) : i \\neq i \\}\\).\nEvaluate on \\((X_i, Y_i)\\).\nAverage the model error over all \\(i =1,2,\\dots,n\\).\n\nThis illustration from ISLR should give you the general idea.\n\n\n\nSchematic of LOO-CV (Credit: ISLR2e fig. 5.3)\n\n\nLet’s see that in action. As we have done many times this semester, this code is optimized for clarity and readability, not for concision or “cleverness”. There are much more “graceful” ways of doing this, and shortly we’ll see R’s built-in CV tools, which are what we would normally use for this. But here the goal is to illustrate the core ideas in a really obvious way, hence the “clumsy” code.\n\ndata('mtcars'); # Still using mtcars data; reloading it just to remind us.\n\nnrows &lt;- nrow(mtcars); # Number of observations in the data\nnorder &lt;- 5;\n# For each choice of observation to hold out, we need to record the score\n# (i.e., squared erro) for each of the five model orders.\nerrors &lt;- data.frame( 'Row'=rep(1:nrows, each=norder),\n                     'Order'=rep(1:norder, times=nrows),\n                                 'Error'=rep(NA, nrows*norder));\n\nfor ( i in 1:nrow(mtcars) ) {\n  train_data &lt;- mtcars[-c(i),]; # Leave out the i-th observation\n  leftout &lt;- mtcars[c(i),]; # the row containing the left-out sample.\n  \n  # Remember, we are fitting five different models\n  # So that we can compare them.\n  \n  # Fit the linear model, then evaluate.\n  m1 &lt;- lm(mpg ~ 1 + hp, train_data );\n  m1.pred &lt;- predict( m1, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==1); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m1.pred - leftout$mpg)^2 ; \n  \n  # Fit the quadratic model, then evaluate.\n  m2 &lt;- lm(mpg ~ 1 + hp + I(hp^2), train_data );\n  m2.pred &lt;- predict( m2, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==2); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m2.pred - leftout$mpg)^2; \n\n  # Fit the cubic model, then evaluate.\n  m3 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3), train_data );\n  m3.pred &lt;- predict( m3, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==3); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m3.pred - leftout$mpg)^2; \n  \n  # Fit the 4-th order model, then evaluate.\n  m4 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3) + I(hp^4), train_data );\n  m4.pred &lt;- predict( m4, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==4); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m4.pred - leftout$mpg)^2; \n  \n  # Fit the 5-th order model, then evaluate.\n  m5 &lt;- lm(mpg ~ 1 + hp + I(hp^2) + I(hp^3) + I(hp^4) + I(hp^5), train_data );\n  m5.pred &lt;- predict( m5, leftout );\n  idx &lt;- (errors$Row==i & errors$Order==5); # Pick out row of the errors df.\n  # record squared error btwn predict and truth\n  errors[idx,]$Error &lt;- (m5.pred - leftout$mpg)^2;\n}\n\nOkay, so let’s make sure that we understand what is going on, here.\nThe data frame errors now has nrows*norders rows. So for each observation in the cars data set, there are five entries in the table errors, recording the squared error for the models of order 1, 2, 3, 4 and 5 when that data point was held out.\nWe said that when we do CV, we want to average across the \\(n\\) observations, so let’s do that. We’re going to use the aggregate function, which is one of the ways to perform “group-by” operations in R.\nGroup-by operations are where we pool our observations into subsets according to some criterion, and then compute a summary statistic over all of the observations in the same subset (i.e., the same “group”).\nUsing that language, we want to group the rows of errors according to model order, and take the average squared error within each order.\n\n# Error ~ Order tells R to group the data according to the Order column\n# and that we want to summarize the Error column within observations\n# of the same Order.\n# Passing the FUN=mean argument tells R that the summary statistic we want to use\n# is the function mean().\n# We could pass other summary statistic functions in this argument.\n# For example, we could use median, sd, var, max, etc.,\n# though those would be a bit silly here.\nerr_agg &lt;- aggregate(Error ~ Order, data=errors, FUN=mean);\n\nhead(err_agg)\n\n  Order     Error\n1     1  17.25330\n2     2  10.56143\n3     3  10.57458\n4     4  61.21760\n5     5 641.19551\n\n\nAnd we can plot that just to drive the point home.\n\npp &lt;- ggplot(err_agg, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\n\n32.5.1 Recap: single split versus LOO-CV\nSo far we have seen two different ways of estimating a model’s performance on unseen data.\nThe first was to randomly split the data into two sets, train on one and evaluate on the other.\nPro: Only have to fit a model once (or just a few times, if we are going to repeat the operation and average)\nCon: Only have half of the data available to fit the model, which leads to less accurate prediction (and thus high variance in estimated model).\nThe second is leave-one-out cross-validation.\nPro: Use all but one observation to fit the model, so model fit is almost as good as if we had used all of the data\nCon: Have to fit the model anew for each held-out data point, results in fitting the model \\(n\\) different times, which can be expensive.\nCon: Because any two training sets overlap in all but one of their elements, our fitted models are very highly correlated with one another, so we’re doing a lot of work (\\(n\\) model fits) to get a bunch of highly correlated measurements.\nSo, the natural question is: can we bridge the gap between these two extremes.\n\n\n32.5.2 The happy medium: \\(K\\)-fold cross validation\nWell, there are a few different ways to bridge this gap, for example using Monte Carlo methods. Let’s discuss the most popular one here.\nWe’ll borrow a bit from the LOO-CV idea, while lessening the correlatedness of the models fits.\n\\(K\\)-fold CV randomly divides the data into \\(K\\) subsets, called folds. Then, one at a time, we hold out one of the folds, train our model on the \\(K-1\\) remaining folds, and evaluate our model’s prediction error on the held-out fold. Then, we can average the errors across the \\(K\\) folds.\nThat is, the “recipe” for \\(K\\)-fold cross-validation is\n\nRandomly partition the data into \\(K\\) (approximately) same-sized subsets, \\(S_1,S_2,\\dots,S_K\\) such that \\(\\cup_k S_k = \\{1,2,\\dots,n\\}\\) and \\(S_k \\cap S_\\ell = \\emptyset\\) for all \\(k \\neq \\ell\\)\nFor each \\(k=1,2,\\dots,K\\), train a model on the observations indexed by \\(i \\in \\cup_{\\ell \\neq k} S_\\ell\\) and compute the prediction error \\[\n\\hat{E}_k =  \\frac{1}{|S_k|} \\sum_{i \\in S_k} (\\hat{y}_i - y_i)^2\n\\]\nEstimate the true error \\(\\mathbb{E} (\\hat{y}_{n+1} - y_{n+1})^2\\) as \\[\n\\frac{1}{K} \\sum_{k=1}^K \\hat{E}_k,\n\\] Schematically, this looks something like this (with \\(K=5\\)):\n\n\n\n\nSchematic of \\(K\\)-fold CV (Credit: ISLR2e fig. 5.5)\n\n\nLet’s implement this in R, just for the practice. Once again, R has built-in tools for making this easier, which we will discuss later, but this is a good opportunity to practice our R a bit.\n\ndata('mtcars'); # We'll continue to use the mtcars data set\nK &lt;- 5; # 5-fold regularization. K between 5 and 10 is a fairly standard choice\n\n# The first thing we need to do is partition the data into K folds.\n# There are many different ways to do this,\n# including using functions from other packages\n# (e.g., https://www.rdocumentation.org/packages/caret/versions/6.0-90/topics/trainControl)\n# But here's an approach using the R function split() that I like\nn &lt;- nrow(mtcars);\n# sample(n,n,replace=FALSE) really just randomly permutes the data.\n# Then, passing that into the split function assigns these to the K different\n# factors defined by as.factor(1:K).\n# See ?split for more information.\nKfolds &lt;- split( sample(1:n, n,replace=FALSE), as.factor(1:K));\n\nWarning in split.default(sample(1:n, n, replace = FALSE), as.factor(1:K)): data\nlength is not a multiple of split variable\n\n# Note that this will throw a warning in the event that K does not divide n\n# evenly. That's totally fine!\n\nKfolds\n\n$`1`\n[1] 22 14 28  8 31  4 25\n\n$`2`\n[1] 13 19 15  9 17 27 10\n\n$`3`\n[1]  2 11 16 18 23  7\n\n$`4`\n[1] 30 32 12  1  3 20\n\n$`5`\n[1]  5 26  6 21 24 29\n\n\nNow, for each of these \\(K=5\\) folds, we’ll set it aside, train on the remaining data, and evaluate on the fold.\n\n# The file mtcars_Kfold.R defines a function that trains the five different-order\n# models and evaluates each one according to the given holdout set.\n# It largely repeats the structure of the LOO-CV code implemented above,\n# hence why it is relegated to a file for your later perusal.\nsource('r_scripts/mtcars_Kfold.R');\n\n# Set up a data frame to hold our residuals.\nnorder &lt;- 5;\nKfold_resids &lt;- data.frame( 'Order'=rep(1:norder, each=K),\n                            'Fold'=rep(1:K, norder ),\n                            'Error'=rep(NA, K*norder) );\n\nfor (k in 1:K ) {\n  heldout_idxs &lt;- Kfolds[[k]]; # The indices of the k-th hold-out set.\n  \n  # Now train the 5 different models and store their residuals.\n  idx &lt;- (Kfold_resids$Fold==k);\n  Kfold_resids[idx, ]$Error &lt;- mtcars_fit_models( heldout_idxs );\n  \n}\n\nhead(Kfold_resids)\n\n  Order Fold     Error\n1     1    1 29.190348\n2     1    2  9.894237\n3     1    3 20.834760\n4     1    4 14.021542\n5     1    5  8.658949\n6     2    1 29.452228\n\n\nNow, we need to aggregate over the \\(K=5\\) folds, and then we can plot the errors. Once again, we need to use a log scale for the errors, because the higher-order models cause some really bad prediction errors on a handful of “bad” examples.\n\nKF_agg &lt;- aggregate(Error ~ Order, data=Kfold_resids, FUN=mean);\n\npp &lt;- ggplot(KF_agg, aes(x=Order, y=log(Error) ) );\npp &lt;- pp + geom_line( size=1)\npp\n\n\n\n\nOnce again, the order-2 model, mpg ~ 1 + hp + hp^2, does best (usually, anyway– occasionally the order-3 model is slightly better due to randomness on this small data set)."
  },
  {
    "objectID": "cv.html#aside-the-bias-variance-decomposition",
    "href": "cv.html#aside-the-bias-variance-decomposition",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.6 Aside: the bias-variance decomposition",
    "text": "32.6 Aside: the bias-variance decomposition\nNote: This subsection includes a lot of math, including a lot of expectation and variance terms and taking expectations with respect to some variables but not others. You are not responsible for these details on an exam. The important thing to take away from this subsection is the concept of the bias-variance decomposition of the means squared error (MSE), in which we can write the MSE as a variance term plus a squared bias.\nSuppose that we have a quantity \\(\\theta\\) that we want to estimate, and we have an estimator \\(\\hat{\\theta}\\), the mean squared error is defined as \\[\n\\operatorname{MSE}(\\hat{\\theta}, \\theta)\n= \\mathbb{E} \\left( \\hat{\\theta} - \\theta \\right)^2.\n\\]\nFor example, in our CV examples above, we wanted to estimate the squared error on a previously unseen data point, \\(\\mathbb{E}( \\hat{Y}_{n+1} - Y_{n+1} )^2\\). Note that even though this looks kind of like MSE, it is not. This quantity is \\(\\theta\\) in our MSE expression above. It is a thing we want to estimate. Our love of squared errors has caused us to have a whole mess of colliding notation. Such is life.\nImportant point: we are taking expectation here with respect to the random variable \\(\\hat{theta}\\). Its randomness comes from the data itself (which we usually assume to depend on the true parameter \\(\\theta\\) in some way).\nNow, let’s expand the MSE by adding and subtracting \\(\\mathbb{E} \\hat{\\theta}\\) inside the square: \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\begin{aligned}\n\\operatorname{MSE}\n&= \\E \\left( \\thetahat - \\theta \\right)^2 \\\\\n&= \\E \\left( \\thetahat - \\E \\thetahat + \\E \\thetahat - \\theta \\right)^2 \\\\\n&= \\E\\left[  \\left( \\thetahat - \\E \\thetahat \\right)^2\n           + 2\\left( \\thetahat - \\E \\thetahat \\right)\\left( \\E \\thetahat - \\theta \\right)\n           + \\left( \\E \\thetahat - \\theta \\right)^2 \\right] \\\\\n&= \\E \\left( \\thetahat - \\E \\thetahat \\right)^2\n  +  \\E 2\\left( \\thetahat - \\E \\thetahat \\right)\n                \\left( \\E \\thetahat - \\theta \\right)\n  + \\E \\left( \\E \\thetahat - \\theta \\right)^2.\n\\end{aligned}\n\\] Now, let’s notice that \\(\\theta\\) and \\(\\mathbb{E} \\hat{\\theta}\\) are not random, so they can get pulled out of the expectation (along with the factor of \\(2\\), which is also not random!). we can write (again, remember that the expectation is over \\(\\hat{\\theta}\\), while \\(\\theta\\) is non-random) \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\E 2\\left( \\thetahat - \\E \\thetahat \\right) \\left( \\E \\thetahat - \\theta \\right)\n= 2 \\left( \\E \\thetahat - \\theta \\right) \\E \\left( \\thetahat - \\E \\thetahat \\right)\n    = 0,\n\\] because \\[\n\\mathbb{E}\\left(\\hat{\\theta} - \\mathbb{E} \\hat{\\theta} \\right)\n= \\mathbb{E} \\hat{\\theta} - \\mathbb{E} \\hat{\\theta}\n= 0.\n\\]\nPlugging this into our equation above, we conclude that \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\newcommand{\\thetahat}{\\hat{\\theta}}\n\\operatorname{MSE}\n= \\E \\left( \\thetahat - \\E \\thetahat \\right)^2\n+ \\E \\left( \\E \\thetahat - \\theta \\right)^2.\n\\]\nThe first term on the right is just a variance– like \\(\\mathbb{E}(X - \\mathbb{E}X)^2\\).\nThe second term on the right is the expectation of \\((\\mathbb{E} \\hat{\\theta} - \\theta)^2\\). But this term isn’t random at all– \\(\\theta\\) is a fixed parameter, and \\(\\mathbb{E} \\hat{\\theta}\\) is just an expected value (i.e., not random!), so \\[\n\\E \\left( \\E \\thetahat - \\theta \\right)^2\n= \\left( \\E \\thetahat - \\theta \\right)^2,\n\\] and notice that this is just the squared bias– the square of the difference between the expectation of our estimator and the thing it is supposed to estimate.\nSo, to recap, we have shown that we can decompose the MSE as \\[\n\\operatorname{MSE}(\\hat{\\theta}, \\theta)\n= \\operatorname{Var} \\hat{\\theta} + \\operatorname{Bias}^2(\\hat{\\theta}, \\theta).\n\\]\nIn general, there will be many different estimators (i.e., many different choices of \\(\\hat{\\theta}\\)) that all obtain (approximately) the same MSE. The above equation means that once we are choosing among these different “similar” estimators (i.e., estimators that have similar MSE), we are really just trading off between bias and variance. That is, an estimator with smaller bias will have to “pay” for it with more variance. This is often referred to as the bias-variance tradeoff."
  },
  {
    "objectID": "cv.html#cv-and-the-bias-variance-tradeoff",
    "href": "cv.html#cv-and-the-bias-variance-tradeoff",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.7 CV and the bias-variance tradeoff",
    "text": "32.7 CV and the bias-variance tradeoff\nNow, the purpose of cross-validation is to estimate the model error \\(\\E(\\hat{Y}_{n+1}-Y_{n+1})^2\\). The bias-variance tradeoff says that, roughly speaking, different “reasonable” ways of estimating this quantity will all have about the same MSE, but will involve balancing bias against variance.\n\n32.7.1 Bias in CV\nLet’s think back to the “naive” cross-validation approach, in which we split the data into two sets of similar sizes, train on one and evaluate on the other. When we do that, we train our model on a much smaller data set than if we used the full data. The result is that we (accidentally) over-estimate the error of our model, because models trained on less data simply tend to be less accurate.\nThat is to say, the “naive” cross-validation approach tends to yield a biased estimate of the true error of the model. Specifically, our estimate is biased upward.\nOn the other hand, LOOCV should be approximately unbiased as an estimate of the model error, because the difference between training on \\(n\\) and \\(n-1\\) data points should not be especially large (at least once \\(n\\) is reasonably large).\nIt stands to reason that \\(K\\)-fold CV should sit at a kind of “happy medium” level of bias between LOOCV and “naive” CV.\n\n\n32.7.2 Variance in CV\nSo LOOCV is the least biased estimate of model error, but the bias-variance trade-off predicts that we must “pay” for this in variance. It turns out that LOOCV has the most variance out of the three methods LOOCV, \\(K\\)-fold CV (for \\(K &lt; n\\)) and “naive” CV.\nIntuitively, the variance in LOOCV comes from the following fact: recall that for each \\(i=1,2,\\dots,n\\), we hold out the \\(i\\)-th data point and train a model on the rest.\nThis means that we have \\(n\\) different trained models, each trained on \\(n-1\\) data points, but each pair of training sets overlap in \\(n-2\\) of their data points. The result is that the trained models are highly correlated with one another. Changing just one data point in our data set doesn’t change the fitted model much!\nThe result is that these estimated model errors are highly correlated with one another, with the result that our overall estimate of the model error has high variance.\nThe \\(K\\) models trained in \\(K\\)-fold CV are less correlated with one another, and hence we have (comparatively) less variance. It turns out in this case that \\(K\\) less-correlated error estimates have smaller correlation than \\(n\\) highly-correlated ones.\n\n\n32.7.3 \\(K\\)-fold CV: the happy medium\nThus, \\(K\\)-fold CV is a popular choice both because it is computationally cheaper than LOOCV (\\(K\\) model fits compared to \\(n\\) of them) and because it strikes a good balance between bias and variance."
  },
  {
    "objectID": "cv.html#variable-selection-for-real-this-time",
    "href": "cv.html#variable-selection-for-real-this-time",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.8 Variable selection, for real this time",
    "text": "32.8 Variable selection, for real this time\nIn our examples above, we concentrated on choosing among a family of linear regression models that varied in their orders, in the sense that they included as predictors all powers of the horsepower variable hp, up to some maximum power, to predict gas mileage. Hopefully it is clear how we could modify our approach to, say, choose which variables we do and don’t include in a model (e.g., as in the Pima diabetes data set that we’ve seen a few times this semester).\nUltimately, our goal was to choose, from among a set of predictors that we could include in our model (e.g., powers of hp, in the case of the mtcars example), which predictors to actually include in the model. Again, this task is variable selection.\nOne thing that might be bugging us so far is that any way we slice it, cross-validation doesn’t use all of the available data: we are always holding something out of our fitted model for the sake of estimating our error on unseen data.\nLet’s look at a few different approaches to variable selection that do not rely on cross-validation. These alternative methods have the advantage of not trying to estimate the unknown model error on unseen data. On the other hand, these methods can be more computationally intensive and tend to come with fewer theoretical guarantees.\nThis is not to suggest, however, that these methods are at odds with cross-validation. In actual research papers and in industry applications, you’ll often see both CV and some of the methods presented below used in tandem to select the best model for the job."
  },
  {
    "objectID": "cv.html#setup-linear-regression-and-fitting",
    "href": "cv.html#setup-linear-regression-and-fitting",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.9 Setup: linear regression and fitting",
    "text": "32.9 Setup: linear regression and fitting\nLet’s continue to focus on linear regression, bearing in mind that the ideas introduced here apply equally well to other regression and prediction methods (e.g., logistic regression). Let’s recall that multiple linear regression models a response \\(Y \\in \\mathbb{R}\\) as a linear (again, technically affine– linear plus an intercept!) function of a set of \\(p\\) predictors plus normal noise: \\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\epsilon.\n\\]\nHere, \\(\\epsilon\\) is mean-zero normal with unknown variance \\(\\sigma^2 &gt; 0\\), and the variables \\(X_1,X_2,\\dots,X_p\\) are the predictors. We often refer to \\(p\\), the number of predictors, as the dimension of the problem, because the data (well, the vector of predictors, anyway), lies in \\(p\\)-dimensional space. Collecting the coefficients into a vector \\((\\beta_0,\\beta_1,\\dots,\\beta_p) \\in \\mathbb{R}^{p+1}\\) and creating a vector \\(X=(1,X_1,X_2,\\dots,X_p) \\in \\mathbb{R}^p\\), we can write this more succinctly as (if you have not taken linear algebra, you can safely ignore this, we’re just including it because it’s a common notation) \\[\nY = \\beta^T X + \\epsilon.\n\\]\nIn multiple linear regression, we observe a collection of predictor-response pairs \\((X_i,Y_i)\\) for \\(i=1,2,\\dots,n\\), with \\[\nX_i = (1,X_{i,1},X_{i,2},\\dots,X_{i,p}) \\in \\mathbb{R}^{p+1}.\n\\]\nNote that here we are including the intercept term \\(1\\) in the vector of predictors for ease of notation. This is a common notational choice, so we’re including it here to get you used to seeing this. Of course, this is not universal– it’s one of those conventions that you have to be careful of and check what you are reading.\n\n32.9.1 Recap: variable selection\nSo we have \\(p\\) variables (plus an intercept term), and we want to select which ones to include in our model. There are many reasons to want to do this, but let’s just highlight three of them:\n\nIf there are many “useless” variables (i.e., ones that are not good predictors of the response), then including them in the model can make our predictions less accurate. Thus, we would like to proactively identify which variables are not useful, and avoid including them in the model in the first place.\nA model with fewer variables is simpler, and we like simple models! Explaining, say, heart attack risk as a function of two or three factors is a lot easier to use than a model that uses ten or twenty factors.\nIf the number of predictors \\(p\\) is too large (say, larger than \\(n\\)– a common occurrence in genomic studies, for example), our estimates of the coefficients are very unstable. Variable selection and related tools give us a way to introduce stability in the form of regularization, which we will talk about below."
  },
  {
    "objectID": "cv.html#best-subset-selection",
    "href": "cv.html#best-subset-selection",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.10 Best subset selection",
    "text": "32.10 Best subset selection\nSo, we have \\(p\\) predictor variables available to us, and we want to choose which of them to actually include in our model.\nWell, the most obvious solution is to just try all possible combinations of features, train a model using each combination, and keep the best one (measured by, say, residual sum of squares).\nThis would have an obvious drawback: we have already seen that we can trivially improve the RSS of our model by adding variables. So the models that include more variables would do better, even if those variables did not actually lead to better model error on unseen data.\nThe solution to this is to do the following:\n\nFor each \\(k=1,2,\\dots,p\\), for every set of \\(k\\) different variables, fit a model and keep the model that best fits the data (measured by RSS). Call this model \\(M_k\\).\nUse CV (or some other tool like AIC or adjusted \\(R^2\\), which we’ll discuss below) to select among the models \\(M_1,M_2,\\dots,M_p\\).\n\nThis is called best subset selection. It is implemented in R in, for example, the leaps library the function regsubsets, which gets called in more or less the same way as lm. See (here)[https://cran.r-project.org/web/packages/leaps/index.html] for documentation if you’re interested.\nThere is one rather glaring problem with best subset selection, though:\nQuestion: if there are \\(p\\) predictors, how many models does best subset selection fit before it makes a decision?\nSo once \\(p\\) is even moderately large, best subset selection is computationally expensive, and we need to do something a little more clever."
  },
  {
    "objectID": "cv.html#stepwise-selection",
    "href": "cv.html#stepwise-selection",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.11 Stepwise selection",
    "text": "32.11 Stepwise selection\nSo best subset selection is expensive because we have to try every possible model, and then choose among the best “size-\\(k\\)” model for each \\(k=1,2,\\dots,p\\). How might we cut down on the computational expense?\nStepwise selection methods avoid exhaustively checking all \\(2^p\\) possible models by starting with a particular model and adding or removing one variable at a time (i.e., in “steps”).\nThe important part is in how we decide which predictor to add or remove from the model at a particular time.\n\n32.11.1 Forward stepwise selection\nThe most obvious (to me, anyway) way to avoid checking every possible model is to start with a “null” model (i.e., no predictors, just an intercept term), then repeatedly add the “best” predictor not already in the model. That is,\n\nStart by fitting the “null” model, with just an intercept term. Call it \\(M_0\\).\nFor each \\(k=1,2,\\dots,p\\), among the \\(p-k\\) predictors not already in the model, add the one that yields the biggest improvement in RSS. Call this model, which includes \\(k\\) predictors and the intercept term, \\(M_k\\).\nUse CV or some other method (e.g., an information criterion; see ISLR Section 6.1) to choose among \\(M_0,M_1,M_2,\\dots,M_p\\).\n\nThe important thing is that in Step 2 above, for each \\(k=1,2,\\dots,p\\), we need to fit \\(p-k\\) different models. Thus, in total (i.e., summing over \\(k=0,1,2,\\dots,p\\)), we end up fitting \\[\n1+ \\sum_{k=0}^{p-1} (p-k)\n= 1+p^2 - \\frac{(p-1)p}{2} = 1+\\frac{ 2p^2 - p^2 + p }{2}\n= 1+ \\frac{ p(p+1)}{2}\n\\] different models.\nTo get a sense of what a big improvement this is, when \\(p\\) is large, this right-hand side is approximately \\(p^2/2\\). Compare that with \\(2^p\\), which is a MUCH larger number. For example, when \\(p=10\\), \\(2^{10} \\approx 1000\\), while \\(10^2/2 \\approx 50\\). When \\(p=20\\), \\(2^{20} \\approx 1,000,000\\) while \\(20^2/2 \\approx 200\\).\nOf course, the drawback is that forward stepwise selection might “miss” the optimal model, since it does not exhaustively fit every possible model the way that best subset selection does.\n\n\n32.11.2 Backward stepwise selection\nWell, if we can do forward stepwise selection, why not go backwards?\nIn backward stepwise selection, we start with the full model (i.e., a model with all \\(p\\) predictors), and iteratively remove one predictor at a time, always removing the predictor that decreases RSS the least.\nJust like forward stepwise regression, this decreases the number of models we have to fit from \\(2^p\\) to something more like (approximately) \\(p^2/2\\).\nCautionary note: backward selection will only work if the number of observations \\(n\\) is larger than \\(p\\). If \\(n &lt; p\\), the “full” model cannot be fit, because we have an overdetermined system of linear equations– \\(n\\) equations in \\(p\\) unknowns, and \\(p &gt; n\\). This is a setting where regularization can help a lot (see below), but the details are best left to your regression course(s).\n\n\n32.11.3 Hybrid approaches: the best of both worlds?\nIt is outside the scope of this course, but there do exist stepwise selection methods that try to combine forward and backward stepwise selection. For example, we can alternately add and remove variables as needed. This can be helpful when, for example, a predictor is useful “early” in the selection process, but becomes a less useful predictor once other predictors have been included."
  },
  {
    "objectID": "cv.html#model-comparison-statistics-adjusted-r2-aic-and-bic",
    "href": "cv.html#model-comparison-statistics-adjusted-r2-aic-and-bic",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.12 Model Comparison Statistics: Adjusted \\(R^2\\), AIC and BIC",
    "text": "32.12 Model Comparison Statistics: Adjusted \\(R^2\\), AIC and BIC\nRather than comparing the RSS of two models, which only compares the reduction to residuals with no regards to the number of predictors (the complexity) in the model, there are some statistics that are often used. Note that these statistics are relevant when comparing models of different complexity - two models with the same number of predictors would just as well be compared using \\(RSS\\). These statistics are useful to balance the benefit of reduced \\(RSS\\) with the cost of additional model complexity.\nIn each case, \\(k\\) is the number of parameters being estimated - including the intercept.\n\n32.12.1 Adjusted \\(R^2\\)\n\\[R^2_{adj} = 1 − \\frac{RSS/(n-k)}{TSS/(n-1)}=1-\\frac{RSS}{TSS}\\left(\\frac{n-1}{n-k}\\right)\\] The fraction multiplied will be \\(&gt;1\\), and grows with model complexity. In an extreme case, this penalty can result in a negative \\(R^2_{adj}\\), so it’s important to remember that this statistic is not meaningful by itself, only when used to compare models. Thus \\(R^2_{adj} &lt; R^2\\), and applies a penalty that grows with the number of predictors. When comparing two models using \\(R^2_{adj}\\) we prefer the model that has the higher value.\n\n\n32.12.2 Akaike information criterion (AIC)\n\\[AIC = −2 \\ln(L) + 2k\\] If you work out the math (we won’t here) for a linear model this can be expressed in terms of \\(RSS\\) \\[AIC = n\\ln(RSS/n) + 2k\\]\n\n\n32.12.3 Bayesian information criterion (BIC)\n\\[BIC = -2 \\ln(L)+\\ln(n)k =  n\\ln(RSS/n) + \\ln(n)k\\] The first term in AIC and BIC is the residual deviance, which we want to be as low as possible. While a more complex model will reduce residual deviance, both AIC and BIC add a penalty. BIC adds a more severe penalty per predictor (if \\(n &gt; e^2\\approx7.4\\)).\nThe bottom line: between these three model comparison statistics, BIC more heavily favors simpler models, \\(R^2_{adj}\\) allows for more complex models and AIC is somewhere in the middle."
  },
  {
    "objectID": "cv.html#shrinkage-and-regularization",
    "href": "cv.html#shrinkage-and-regularization",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.13 Shrinkage and Regularization",
    "text": "32.13 Shrinkage and Regularization\nThe variable selection methods we just discussed involved trying out different subsets of the predictors and seeing how the model performance changed as a result.\nLet’s consider an alternative approach. What if instead of trying lots of different models with different numbers of predictors, we went ahead and fit a model with all \\(p\\) available predictors, but we modify our loss function in such a way that we will set the coefficients of “unhelpful” predictors to zero? This is usually called “shrinkage”, because we shrink the coefficients toward zero. You will also often hear the term regularization, which is popular in machine learning, and means more or less the same thing.\nLet’s briefly discuss two such methods, undoubtedly two of the most important tools in the statistical toolbox: ridge regression and the LASSO.\n\n32.13.1 Ridge regression\nBy now you are bored to death of seeing the linear regression least squares objective, but here it is again: \\[\n\\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j X_{i,j} \\right)^2\n\\]\nHere we are assuming that we have \\(p\\) predictors, so each \\((X_i,Y_i)\\) pair has a vector of predictors \\(X_i = (X_{i,1},X_{i,2},\\dots,X_{i,p}) \\in \\mathbb{R}^p\\) and response \\(Y_i \\in \\mathbb{R}\\).\nRemember, we’re trying to minimize this RSS by choosing the coefficients \\(\\beta_j\\), \\(j=0,1,2,\\dots,p\\) in a clever way.\nRidge regression shrinks these estimated coefficients toward zero by changing the loss slightly. Instead of minimizing the RSS alone, we add a penalty term: \\[\n\\sum_{i=1}^n \\left( Y_i - \\beta_0 - \\sum_{j=1}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n= \\operatorname{RSS} + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] where \\(\\lambda \\ge 0\\) is a tuning parameter (which we have to choose– more on that soon).\nOur cost function now has two different terms:\n\nOur old friend RSS, which encourages us to choose coefficients that reproduce the observed responses accurately\nThe shrinkage penalty \\(\\lambda \\sum_{j=1}^p \\beta_j^2\\), which encourages us to choose all our coefficients (other than \\(\\beta_0\\)) equal to zero. That is, it shrinks our solution toward the origin.\n\nThe tuning parameter \\(\\lambda\\) controls how much we care about this shrinkage penalty compared to the RSS term. When \\(\\lambda\\) is big, we “pay” more for large coefficients, so we will prefer coefficients closer to zero. When \\(\\lambda=0\\), we recover plain old least squares regression.\nFor each value of \\(\\lambda\\) that we choose, we get a different solution to our (regularized) regression, say, \\(\\hat{\\beta}^{(\\lambda)}\\). In this sense, whereas least squares linear regression gives us just one solution \\(\\hat{\\beta}\\), shrinkage methods give us a whole family of solutions, corresponding to different choices of \\(\\lambda\\).\nFor this reason, choosing the tuning parameter \\(\\lambda\\) is crucial, but we will have only a little to say about this matter, owing to time constraints. Luckily, you already know a family of methods for choosing \\(\\lambda\\)– cross validation is a very common appraoch!\n\n32.13.1.1 Ridge regression on the mtcars data set\nLet’s try this out on the mtcars data set, trying to predict mpg from all the of the available predictors, this time. One thing to bear in mind: the data set is only 32 observations, so our fits are going to be a little unstable (but this is precisely why we use regularization!).\n\nnames(mtcars);\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\nRidge regression is available in the MASS library in R.\n\nlibrary(MASS);\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nlambda_vals &lt;- c(0,1,2,5,10,20,50,100,200,500); # Choose lambdas to try.\n# lm.ridge needs:\n# 1) a model (mpg~. says to model mpg as an intercept\n#         plus a coefficient for every other variable in the data frame)\n# 2) a data set (mtcars, of course)\n# 3) a value for lambda. lambda=0 is the default,\n#         and recovers classic linear regression.\n#         But we can also pass a whole vector of lambdas, like we are about to do,\n#         and lm.ridge will fit a separate model for each.\n# See ?lm.ridge for details.\nridge_models &lt;- lm.ridge(mpg~., mtcars, lambda=lambda_vals);\n\n# Naively plotting this object shows us how the different coefficients\n# change as lambda changes.\nplot( ridge_models );\n\n\n\n\nEach line in the above plot represents the coefficient of one of our predictors. The x-axis is our choice of \\(\\lambda\\) (lambda in the code) and the y-axis is the actual value of the coefficients.\nActually extracting those predictor labels to make a legend for this plot is annoying, and beside the point– refer to the documentation in ?lm.ridge). The important point is that as we change \\(\\lambda\\), the coefficients change. Generally speaking, as \\(\\lambda\\) gets bigger, more coefficients are closer to zero.\nIndeed, if we make \\(\\lambda\\) big enough, all of the coefficients will be zero (except the intercept, because it isn’t multiplied by \\(\\lambda\\) in the loss). That’s shrinkage!\nJust as a sanity check, let’s fit plain old linear regression and verify that the coefficients with \\(\\lambda=0\\) match.\n\nlm_sanity_check &lt;- lm(mpg~., mtcars);\nlm_sanity_check$coefficients\n\n(Intercept)         cyl        disp          hp        drat          wt \n12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393 \n       qsec          vs          am        gear        carb \n 0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925 \n\n\nAnd compare that with\n\nhead( coef( ridge_models), 1 ); # the first row is the lambda=0.0 setting.\n\n                    cyl       disp          hp     drat        wt      qsec\n  0 12.30337 -0.1114405 0.01333524 -0.02148212 0.787111 -3.715304 0.8210407\n           vs       am     gear       carb\n  0 0.3177628 2.520227 0.655413 -0.1994193\n\n\nThey’re the same, up to several digits of precision, anyway. Good!\n\n\n32.13.1.2 Shrinkage and RSS\nNow, for each value of \\(\\lambda\\), we get a different fitted model. How do these different models do in terms of their fit (as measured by RSS)?\nWell, annoyingly, the object returned by lm.ridge does not include a residuals attribute the same way that the lm object does:\n\nmean( lm_sanity_check$residuals^2 );\n\n[1] 4.609201\n\n\nMore annoyingly still, the object returned by lm.ridge also does not include a predict method, so we can just call something like predict( model, data) the way we would with the output of lm:\n\nmean( (predict( lm_sanity_check, mtcars) - mtcars$mpg )^2 )\n\n[1] 4.609201\n\n\nSo, we have to roll our own predict/residuals computation. This is going to be a bit complicated, but it’s worth the detour to get some programming practice.\nOur ridge regression model has coefficients: one set of coefficients for each valu of lambda that we passed in.\n\nlength( lambda_vals )\n\n[1] 10\n\n\nThose estimated coefficients are stored in a matrix. Each column of this matrix corresponds to a coefficient (including the intercept, the first column. Each row corresponds to one \\(\\lambda\\) value.\n\ncoef( ridge_models )\n\n                    cyl          disp           hp      drat         wt\n  0 12.30337 -0.1114405  0.0133352399 -0.021482119 0.7871110 -3.7153039\n  1 16.53766 -0.1624028  0.0023330776 -0.014934856 0.9246313 -2.4611460\n  2 18.55460 -0.2212878 -0.0007347273 -0.013481440 0.9597173 -2.0619305\n  5 20.72198 -0.3079969 -0.0036206803 -0.012460649 1.0060841 -1.6219933\n 10 21.27391 -0.3563891 -0.0048700433 -0.011966312 1.0409643 -1.3618221\n 20 20.88322 -0.3792044 -0.0054324669 -0.011248991 1.0545238 -1.1389693\n 50 19.85752 -0.3614183 -0.0052401829 -0.009609992 0.9828882 -0.8673112\n100 19.37410 -0.3092845 -0.0044742424 -0.007814946 0.8353896 -0.6656981\n200 19.29829 -0.2337711 -0.0033702943 -0.005732613 0.6286426 -0.4706006\n500 19.54099 -0.1331434 -0.0019134597 -0.003201881 0.3567082 -0.2559721\n          qsec        vs        am      gear        carb\n  0 0.82104075 0.3177628 2.5202269 0.6554130 -0.19941925\n  1 0.49258752 0.3746517 2.3083758 0.6857159 -0.57579125\n  2 0.36772540 0.4389536 2.1835666 0.6545252 -0.64772938\n  5 0.23220486 0.5749017 1.9622086 0.5933014 -0.65548632\n 10 0.17615173 0.7007040 1.7537869 0.5573491 -0.59737336\n 20 0.15680180 0.8158626 1.5168704 0.5362044 -0.50497523\n 50 0.15120048 0.8706103 1.1764833 0.4942313 -0.36858373\n100 0.13669747 0.7898063 0.9064651 0.4225302 -0.27224119\n200 0.10798351 0.6187171 0.6407444 0.3195920 -0.18719020\n500 0.06363418 0.3611011 0.3479026 0.1819390 -0.09983672\n\n\nSo we can pick out the coefficients associated to a particular lambda value by taking the corresponding row of this matrix. For example, \\(\\lambda = 5\\) is in the 4-th row of the matrix:\n\ncat(paste0(\"The 4-th lambda value is: \", lambda_vals[4]) );\n\nThe 4-th lambda value is: 5\n\ncoef( ridge_models )[4,]; # Pick out the 4-th row. these are coefs when lambda=5.\n\n                    cyl        disp          hp        drat          wt \n20.72198423 -0.30799694 -0.00362068 -0.01246065  1.00608409 -1.62199325 \n       qsec          vs          am        gear        carb \n 0.23220486  0.57490168  1.96220860  0.59330141 -0.65548632 \n\n\nNow, to get our prediction from these coefficients, we have to multiply each predictor by its coefficient and add the intercept term. Equivalently, we can think of adding an extra predictor that is just \\(1\\) for every observation. Something like \\[\n\\beta_0 + \\sum_{j=1}^p \\beta_j X_{i,j}\n= \\sum_{j=0}^p \\beta_j X_{i,j},\n\\]\nAs an aside, for those that have taken linear algebra, you should be looking at that and thinking “that’s just an inner product!” \\[\n\\beta^T X_i = \\sum_{j=0}^p \\beta_j X_{i,j}.\n\\]\nSo let’s modify the mtcars data to make that all easy.\n\n# The mpg column of mtcars needs to get removed (it is the outcome,\n# not a predictor), so we drop it-- it's the column numbered 1.\n# And we're using cbind to add a column of 1s with column name const.\nmtc_predictors &lt;- cbind(const=1,mtcars[,-c(1)]);\nhead(mtc_predictors);\n\n                  const cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4             1   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag         1   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710            1   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive        1   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout     1   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant               1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nNow, to make a prediction on, say, the Datsun 710 observation, we need to multiply each predictor (including the const column) by its coefficient, and sum up the total. Again, something like \\[\n\\sum_{j=0}^p \\beta_j X_{i,j},\n\\] where \\(X_{i,0}=1\\) is the extra constant term that we tacked on.\nSo to get the prediction for a particular observation (again, say, the Datsun 710 row in mtcars), we need to make this sum (i.e., inner product) between that row of the predictors matrix and the vector of coefficients.\n\nbeta5 &lt;- coef( ridge_models )[4,]; # 4th row was for lambda=5.\ndatsun &lt;- mtc_predictors['Datsun 710',]\nsum( beta5*datsun )\n\n[1] 26.62668\n\n\nAs a sanity check, let’s verify that the 1-th row, which is \\(\\lambda=0\\), agrees with our linear model’s prediction.\n\nbeta0 &lt;- coef( ridge_models )[1,]; # 1st row was for lambda=0, i.e., plain old LR.\ndatsun &lt;- mtc_predictors['Datsun 710',]\nsum( beta0*datsun );\n\n[1] 26.25064\n\n\nand compare with\n\npredict( lm_sanity_check, datsun )\n\nDatsun 710 \n  26.25064 \n\n\nOkay, but to compute the RSS of our model we want to compute predictions for all 32 of our observations in the mtcars data set. And we want to compute those predictions for each of our different choices of \\(\\lambda\\).\nWe’re going to get those predictions in a matrix. If you haven’t taken linear algebra, don’t let the word scare you. In this context, it’s enough to just think of a matrix as a big box of numbers.\nNow, we currently have two boxes of numbers. One is mtc_predictors. Each row is an observation (so there are 32 rows), and each row has 11 entries, corresponding to the intercept term and ten additional predictors.\n\ndim( mtc_predictors )\n\n[1] 32 11\n\n\nThe other box of numbers is our collection of coefficients. One row for each of the models we fit (i.e., \\(\\lambda\\) values), and one column for each predictor.\n\nbeta_matrix &lt;- coef( ridge_models )\ndim(beta_matrix);\n\n[1] 10 11\n\n\n\nbeta_matrix\n\n                    cyl          disp           hp      drat         wt\n  0 12.30337 -0.1114405  0.0133352399 -0.021482119 0.7871110 -3.7153039\n  1 16.53766 -0.1624028  0.0023330776 -0.014934856 0.9246313 -2.4611460\n  2 18.55460 -0.2212878 -0.0007347273 -0.013481440 0.9597173 -2.0619305\n  5 20.72198 -0.3079969 -0.0036206803 -0.012460649 1.0060841 -1.6219933\n 10 21.27391 -0.3563891 -0.0048700433 -0.011966312 1.0409643 -1.3618221\n 20 20.88322 -0.3792044 -0.0054324669 -0.011248991 1.0545238 -1.1389693\n 50 19.85752 -0.3614183 -0.0052401829 -0.009609992 0.9828882 -0.8673112\n100 19.37410 -0.3092845 -0.0044742424 -0.007814946 0.8353896 -0.6656981\n200 19.29829 -0.2337711 -0.0033702943 -0.005732613 0.6286426 -0.4706006\n500 19.54099 -0.1331434 -0.0019134597 -0.003201881 0.3567082 -0.2559721\n          qsec        vs        am      gear        carb\n  0 0.82104075 0.3177628 2.5202269 0.6554130 -0.19941925\n  1 0.49258752 0.3746517 2.3083758 0.6857159 -0.57579125\n  2 0.36772540 0.4389536 2.1835666 0.6545252 -0.64772938\n  5 0.23220486 0.5749017 1.9622086 0.5933014 -0.65548632\n 10 0.17615173 0.7007040 1.7537869 0.5573491 -0.59737336\n 20 0.15680180 0.8158626 1.5168704 0.5362044 -0.50497523\n 50 0.15120048 0.8706103 1.1764833 0.4942313 -0.36858373\n100 0.13669747 0.7898063 0.9064651 0.4225302 -0.27224119\n200 0.10798351 0.6187171 0.6407444 0.3195920 -0.18719020\n500 0.06363418 0.3611011 0.3479026 0.1819390 -0.09983672\n\n\nOnce again, each column corresponds to one of 11 predictors (the intercept term and ten non-trivial predictors), and the rows correspond to the different choice of \\(\\lambda\\).\nSo, for each value of \\(\\lambda\\) (i.e., each row of \\(\\beta\\)), and each row of mtc_predictors (i.e., each observation in the data set), we want to sum up the products of the coefficients with their corresponding predictors.\nWe are going to make a new matrix, whose rows correspond to the 32 data observations and whose columns correspond to different choices of \\(\\lambda\\). We need to use some basic matrix algebra to construct that. Let’s do the computation, then unpack it.\n\nmtc_mx &lt;- as.matrix( mtc_predictors );\ncat('Dimensions of the predictors matrix: ');\n\nDimensions of the predictors matrix: \n\ncat(dim(mtc_mx)) \n\n32 11\n\nbeta_mx &lt;- coef( ridge_models );\ncat('Dimensions of the coefficients matrix: ');\n\nDimensions of the coefficients matrix: \n\ncat( dim(beta_mx) );\n\n10 11\n\n# Now compute the appropriate matrix product.\n# We want to rows indexed by observations\n# and the columns indexed by lambdas.\n# That requires transposing the coefficients matrix, whose original form\n# has rows indexed by lambda and columns indexed by the predictors.\n# We transpose a matrix in R with t( ).\nobs_by_lambda_predictions &lt;- as.matrix( mtc_mx ) %*% t( beta_mx );\nobs_by_lambda_predictions\n\n                           0        1        2        5       10       20\nMazda RX4           22.59951 22.30763 22.23051 22.13369 22.02501 21.85270\nMazda RX4 Wag       22.11189 21.95588 21.91065 21.85012 21.77639 21.65007\nDatsun 710          26.25064 26.61821 26.68382 26.62668 26.42251 26.04170\nHornet 4 Drive      21.23740 20.88953 20.78456 20.66661 20.59049 20.52290\nHornet Sportabout   17.69343 17.20040 17.01742 16.78452 16.64525 16.59645\nValiant             20.38304 20.37257 20.35076 20.31018 20.28168 20.26423\nDuster 360          14.38626 14.15765 14.13388 14.17680 14.29042 14.52925\nMerc 240D           22.49601 22.68287 22.80572 23.00581 23.14734 23.21667\nMerc 230            24.41909 23.91587 23.73479 23.58564 23.58592 23.62033\nMerc 280            18.69903 19.10420 19.31008 19.67422 20.00760 20.33982\nMerc 280C           19.19165 19.39975 19.53072 19.81354 20.11329 20.43390\nMerc 450SE          14.17216 14.91618 15.12809 15.35748 15.52382 15.75031\nMerc 450SL          15.59957 15.85149 15.90269 15.95540 16.02207 16.16892\nMerc 450SLC         15.74222 15.92546 15.94668 15.96718 16.02444 16.17470\nCadillac Fleetwood  12.03401 11.67687 11.64501 11.75998 12.02127 12.49759\nLincoln Continental 10.93644 11.05719 11.16858 11.42987 11.76777 12.30084\nChrysler Imperial   10.49363 10.99657 11.21759 11.58203 11.96222 12.51055\nFiat 128            27.77291 27.88472 27.85376 27.69494 27.44262 27.01867\nHonda Civic         29.89674 29.26876 29.06962 28.80821 28.54153 28.10115\nToyota Corolla      29.51237 29.12150 28.91791 28.56765 28.21014 27.70198\nToyota Corona       23.64310 23.78667 23.85479 23.91651 23.89758 23.77409\nDodge Challenger    16.94305 16.84439 16.79091 16.69114 16.60761 16.57993\nAMC Javelin         17.73218 17.59335 17.50887 17.37192 17.27326 17.23149\nCamaro Z28          13.30602 13.73881 13.92543 14.19839 14.43699 14.75698\nPontiac Firebird    16.69168 16.24701 16.09680 15.91932 15.83133 15.84875\nFiat X1-9           28.29347 28.25684 28.19035 27.99133 27.70163 27.22949\nPorsche 914-2       26.15295 26.45050 26.49502 26.40196 26.15825 25.72990\nLotus Europa        27.63627 27.46919 27.38886 27.19338 26.92047 26.48469\nFord Pantera L      18.87004 18.79097 18.67829 18.47524 18.33544 18.26423\nFerrari Dino        19.69383 19.73505 19.79328 19.91247 20.01800 20.11261\nMaserati Bora       13.94112 13.74682 13.72632 13.83990 14.10487 14.56029\nVolvo 142E          24.36827 24.93714 25.10820 25.23790 25.21281 25.03481\n                          50      100      200      500\nMazda RX4           21.52222 21.21442 20.89232 20.52801\nMazda RX4 Wag       21.38573 21.12122 20.83279 20.49838\nDatsun 710          25.19341 24.25687 23.15477 21.80461\nHornet 4 Drive      20.44289 20.37973 20.30828 20.21609\nHornet Sportabout   16.82946 17.31265 17.99524 18.89895\nValiant             20.25478 20.24265 20.21593 20.16802\nDuster 360          15.18740 16.02340 17.06870 18.38818\nMerc 240D           23.04180 22.62447 22.01421 21.19076\nMerc 230            23.45483 23.00816 22.32148 21.37321\nMerc 280            20.63826 20.68452 20.59553 20.39937\nMerc 280C           20.72898 20.76654 20.66032 20.43755\nMerc 450SE          16.28647 16.94379 17.75743 18.77860\nMerc 450SL          16.61160 17.19747 17.93903 18.87836\nMerc 450SLC         16.62871 17.21886 17.95869 18.89101\nCadillac Fleetwood  13.57618 14.77514 16.18498 17.90822\nLincoln Continental 13.43666 14.67145 16.11294 17.86941\nChrysler Imperial   13.63141 14.83376 16.23293 17.93646\nFiat 128            26.06659 24.98855 23.70223 22.11461\nHonda Civic         27.04746 25.80880 24.31247 22.45784\nToyota Corolla      26.63521 25.44909 24.03979 22.30309\nToyota Corona       23.35933 22.81202 22.11937 21.23728\nDodge Challenger    16.81441 17.29638 17.98109 18.89022\nAMC Javelin         17.40983 17.80019 18.35988 19.10525\nCamaro Z28          15.45171 16.26403 17.25580 18.49633\nPontiac Firebird    16.20433 16.80970 17.62907 18.69568\nFiat X1-9           26.20867 25.08570 23.76438 22.14560\nPorsche 914-2       24.84061 23.92121 22.88154 21.63991\nLotus Europa        25.55719 24.54521 23.36083 21.91658\nFord Pantera L      18.35395 18.59356 18.94851 19.43374\nFerrari Dino        20.18278 20.18662 20.16519 20.13150\nMaserati Bora       15.45915 16.35277 17.35633 18.56415\nVolvo 142E          24.45802 23.71104 22.77794 21.60303\n\n\nSo this matrix has rows indexed by observations (i.e., cars) and columns indexed by choices of \\(\\lambda\\). So the \\((i,j)\\) entry of this matrix is the prediction made for the \\(i\\)-th car by the model with the \\(j\\)-th lambda value.\nWe are now ready (finally!) to compute the mean squared residuals for these different choices of \\(\\lambda\\). We just need to\n\nCompute the errors between these predictions and the true mpg values for the cars\nSquare those errors.\nSum along the columns (because each column corresponds to a different choice of \\(\\lambda\\), and hence a different fitted model).\n\n\nerrors &lt;- mtcars$mpg - obs_by_lambda_predictions;\n# Just to check, each column of errors should be length-32, because we have\n# 32 data points in the mtcars data set.\n# And there should be 32 columns, one for each of our ten lambda values.\ndim( errors );\n\n[1] 32 10\n\n\nSo we’re going to square those errors and take a mean along each column\n\n# We're going to squares the entries of errors,\n# then take a mean along the columns (that's the 2 argument to apply)\nRSS_by_model &lt;- apply( errors^2, 2, FUN=mean);\nRSS_by_model\n\n        0         1         2         5        10        20        50       100 \n 4.609201  4.724319  4.816983  4.986377  5.192462  5.590303  6.937583  9.407707 \n      200       500 \n13.821594 21.567053 \n\n\nThis is easier to see in a plot– and we’ll put the \\(\\lambda\\) values on a log-scale, because the lambda_vals vector spans multiple orders of magnitude.\n\nplot( log(lambda_vals), RSS_by_model, type='b', lwd=2)\n\n\n\n\nLet’s unpack this. We have the smallest RSS when \\(\\lambda = 0\\), and RSS increases as \\(\\lambda\\) increases. This is exactly what we expect. Recall that our loss function is \\[\n\\sum_{i=1}^n \\left( Y_i - \\sum_{j=0}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\beta_j^2\n\\] As \\(\\lambda\\) gets bigger, we pay a bigger and bigger penalty for making coefficients non-zero. Thus, as \\(\\lambda\\) get bigger, it becomes “harder” or “more expensive” to make the coefficients take the values that would make the RSS term smaller. As a result, for larger values of \\(\\lambda\\), the RSS of our solution is larger.\n\n\n32.13.1.3 Why is ridge regression helpful?\nWell, the short answer is that ridge regression (and other shrinkage methods) prevents over-fitting. \\(\\lambda\\) makes it more expensive to simply choose whatever coefficients we please, which in turn prevents us from over-fitting to the data.\nIn essence, this is the bias-variance tradeoff again! As \\(\\lambda\\) increases, our freedom to choose the coefficients becomes more constrained, and the variance decreases (and the bias increases).\nHere’s an example from ISLR.\n\n\n\nMSE (pink), squared bias (black) and variance (green), estimated from performance on previously unseen data, as a function of \\(\\lambda\\) (ISLR fig. 6.5)\n\n\nNotice that the variance decreases as \\(\\lambda\\) increases, while squares bias increases, but there is a “sweet spot” that minimizes the MSE. The whole point of model selection (CV, AIC, ridge regression, etc.) is to find this sweet spot (or spot close to it).\n\n\n\n32.13.2 The LASSO\nNow, there’s one issue with ridge regression, which becomes evident when we compare it with subset selection methods. Except when \\(\\lambda\\) is truly huge (i.e., infinite), ridge regression fits a model that still has all of the coefficients in it (i.e., all of the coefficients are nonzero, though perhaps small). Said another way, we haven’t simplified the model in the sense of reducing the number of predictors or only keeping the “useful” predictors around.\nThis isn’t a problem for prediction. After all, more predictors often make prediction better, especially when we have regularization to prevent us from over-fitting.\nBut this is a problem if our goal is to simplify our model by selecting only some predictors to include in our model. One way to do this would be to make it so that coefficients that aren’t “pulling their weight” in the sense of helping our prediction error will be set to zero. This is precisely the goal of the LASSO.\nThe LASSO looks a lot like ridge regression, except that the penalty is slightly different: \\[\n\\sum_{i=1}^n \\left( Y_i - \\sum_{j=0}^p \\beta_j X_{i,j} \\right)^2\n+ \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right|\n= \\operatorname{RSS} + \\lambda \\sum_{j=1}^p \\left| \\beta_j \\right|\n\\]\nThe penalty term now involves a sum of absolute values of the coefficients instead of a sum of squares.\nThe interesting thing is that this small change has a big effect on what our estimated coefficients look like. The LASSO penalty encourages coefficients to be set precisely equal to zero if they aren’t useful predictors (i.e., if they do not help to decrease the RSS).\nThere is an interesting geometric reason for this, though it is outside the scope of the course. See the end of Section 6.6.2 in ISLR.\nThe important point is that the LASSO performs variable selection for us by setting many coefficients to zero.\nThe glmnet package has a very good LASSO implementation. This is generally a very useful package for doing all kinds of different penalized regression, and you’ll likely use it extensively in your regression course(s). You’ll get a bit of practice with it in discussion section.\n\n\n32.13.3 How to choose \\(\\lambda\\)? CV to the rescue!\nA natural question in both ridge and the LASSO concerns how we should choose the term \\(\\lambda\\) that controls the “strength” of the penalty term.\nWe said a few lectures ago that CV was useful beyond just variable selection, and here’s the payoff.\nCV is also a great way to choose \\(\\lambda\\) in these kinds of penalized problems. We choose different values of \\(\\lambda\\), fit the corresponding models, and use CV to select among them!\nSection 6.2.3 has a more detailed discussion of this, using \\(10\\)-fold CV to compare different choices of \\(\\lambda\\)."
  },
  {
    "objectID": "cv.html#review",
    "href": "cv.html#review",
    "title": "32  Model Selection and Cross Validation",
    "section": "32.14 Review",
    "text": "32.14 Review\nIn these notes we covered\n\nThe concept of over-fitting to training data\nTraining / Validation data splits\nSingle Split Validation\nLeave One Out Cross Validation\nK-Fold CV\nBias - Variance Tradeoff\nBias & Variance of CV Methods\nBias & Variance of Model Error by Model Complexity\nModel Selection - Best Subset, Forward Stepwise, Backwards Stepwise\nModel Comparison Statistics (Adjusted \\(R^2\\), AIC, BIC)\nRidge Regression - shrinkage of coefficients\nLASSO regression - shrinkage and variable selection"
  },
  {
    "objectID": "bootstrap.html#learning-objectives",
    "href": "bootstrap.html#learning-objectives",
    "title": "34  Bootstrapping",
    "section": "34.1 Learning objectives",
    "text": "34.1 Learning objectives\nAfter this lecture, you will be able to\n\nExplain the bootstrap and its applicability.\nImplement and apply the bootstrap to estimate variance in simple models."
  },
  {
    "objectID": "bootstrap.html#estimating-variance",
    "href": "bootstrap.html#estimating-variance",
    "title": "34  Bootstrapping",
    "section": "34.2 Estimating Variance",
    "text": "34.2 Estimating Variance\nIn a wide range of applications, we need to estimate the variance of our data.\nExample: confidence intervals\nGiven data \\(X_1,X_2,\\dots,X_n\\) drawn from some distribution, we are often interested in constructing a confidence interval for some parameter \\(\\theta\\). For example, \\(\\theta\\) might be the mean of our distribution. If an estimator \\(\\hat{\\theta}\\) is approximately normal about \\(\\theta\\), we could construct an approximate CI for \\(\\theta\\) if only we knew the standard deviation of \\(\\hat{\\theta}\\), \\[\n\\sigma_{\\hat{\\theta}} = \\sqrt{ \\operatorname{Var} \\hat{\\theta} }.\n\\] We have seen in our lectures on estimation and confidence intervals that there are ways to estimate this variance, but sometimes it is a lot more complicated (e.g., more computationally expensive or more mathematically complicated).\nExample: errors in linear regression\nRecall that in linear regression we observe \\((X_i,Y_i)\\) pairs where \\[\nY_i = \\beta_0 + \\beta_1 X_i + \\epsilon_i,~~~i=1,2,\\dots,n\n\\] where \\(\\epsilon_1,\\epsilon_2,\\dots,\\epsilon_n\\) are drawn iid according to a normal with mean zero and unknown variance \\(\\sigma^2 &gt; 0\\).\nDuring our discussions of linear regression in the last few weeks, we saw situations where we were interested in the sampling distribution of the estimated coefficients \\(\\hat{\\beta}_0, \\hat{\\beta}_1\\) (e.g., in adjusted \\(R^2\\) or in interpreting the p-values associated to different coefficients). Often, we need to know \\(\\sigma^2\\) or at least have a good estimate of it. How might we estimate \\(\\sigma^2\\)?\n\n34.2.1 Tricky variance computations\nIn situations like the above, we had central limit theorems or similar results that allowed us to estimate the variance of the quantities we cared about. Unfortunately, this isn’t always the case. Here’s a simple example, adapted from ISLR (beginning of Section 5.2):\nExample: allocating investments\nSuppose that we have two different stocks we can invest in, which yield returns \\(X\\) and \\(Y\\). So, we invest a single dollar (just to keep things simple!), and we need to split between these two different stocks. Let’s say we put \\(\\alpha\\) dollars into the first stock (with return \\(X\\)) and \\(1-\\alpha\\) (i.e., the rest) into the second stock (with return \\(Y\\)).\nThen we get back an amount \\(\\alpha X+ (1-\\alpha) Y\\).\nNow, this amount of money that we get back on our investment is random, because it is based on the (random) stock yields \\(X\\) and \\(Y\\). An obvious thing to do is to try and maximize this, but this can result in us making risky bets.\nMost investment strategies instead aim to minimize the “risk” (note: in statistics, risk means something very different from this– this is the “finance” meaning of the term “risk”): \\[\n\\begin{aligned}\n  \\operatorname{Var}\\left( \\alpha X + (1-\\alpha) Y \\right)\n  &= \\alpha^2 \\operatorname{Var} X\n      + (1-\\alpha)^2 \\operatorname{Var} Y\n      + 2\\alpha(1-\\alpha) \\operatorname{Cov}(X,Y) \\\\\n  &= \\alpha^2 \\sigma_X^2\n    + (1-\\alpha)^2 \\sigma_Y^2\n    + 2\\alpha(1-\\alpha) \\sigma_{XY}\n    \\end{aligned}\n\\] Where \\[\n\\begin{aligned}\n\\sigma_X^2 &= \\operatorname{Var} X \\\\\n\\sigma_Y^2 &= \\operatorname{Var} Y \\\\\n\\sigma_{XY} &= \\operatorname{Cov}( X, Y ).\n\\end{aligned}\n\\] One can prove that \\(\\operatorname{Var}\\left( \\alpha X + (1-\\alpha) Y \\right)\\) is minimized by taking \\[\n\\alpha = \\frac{ \\sigma_Y^2 - \\sigma_{XY} }{ \\sigma_X^2 + \\sigma_Y^2 -2\\sigma_{XY} }.\n\\] Unfortunately, we don’t know the variance and covariance terms (i.e., \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\), \\(\\sigma_{XY}\\))…\nBut suppose we have observations \\((X_i,Y_i)\\), \\(i=1,2,\\dots,n\\) of, say, the performance of these two stocks on prior days. We could use this data to estimate \\(\\sigma_X^2\\), \\(\\sigma_Y^2\\) and \\(\\sigma_{XY}\\).\nIf we denote the estimates \\(\\hat{\\sigma}_X^2\\), \\(\\hat{\\sigma}_Y^2\\) and \\(\\hat{\\sigma}_{XY}\\), then we could plug these into the above equation and obtain an estimator for \\(\\alpha\\), \\[\n\\hat{\\alpha}\n= \\frac{ \\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY} }\n        { \\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 -2\\hat{\\sigma}_{XY} }.\n\\]\nNow, here’s where things get tricky. What if we want to quantify our uncertainty about \\(\\alpha\\)?\nAfter all, \\(\\hat{\\alpha}\\) is a function of the (random) data, so it is itself a random variable, and it is reasonable to ask about, for example, its variance.\nBut \\(\\hat{\\alpha}\\) depends on the data \\(\\{ (X_i,Y_i) : i=1,2,\\dots,n \\}\\) in a fairly complicated way, so it’s not obvious what \\(\\operatorname{Var} \\hat{ \\alpha}\\) should actually be."
  },
  {
    "objectID": "bootstrap.html#refresher-simulation-based-methods",
    "href": "bootstrap.html#refresher-simulation-based-methods",
    "title": "34  Bootstrapping",
    "section": "34.3 Refresher: simulation-based methods",
    "text": "34.3 Refresher: simulation-based methods\nIn our discussion of estimation and confidence intervals earlier in the semester, we obtained simulation-based confidence intervals by\n\nAssuming a model for our data (e.g., assuming the data came from a Poisson distribution)\nEstimating the parameter(s) of that model from the data (e.g., estimating \\(\\lambda\\) under the Poisson)\nGenerating new random samples from the model with the estimated parameter(s) (e.g., drawing from \\(\\operatorname{Pois}(\\hat{\\lambda})\\))\nUsing these “fake” data samples to approximate the sampling distribution of our statistic of interest (e.g., \\(\\hat{\\lambda}(X_1,X_2,\\dots,X_n)\\)).\n\nIf we had a model for our \\((X_i,Y_i)\\) pairs, we could use the observed pairs to estimate the parameter(s) of that model and generate new samples \\((X'_i,Y'_i), i=1,2,\\dots,n\\) and compute \\[\n\\hat{\\alpha}' =\n\\hat{\\alpha}\\left( (X'_1,Y'_1),(X'_2,Y'_2),\\dots,(X'_n,Y'_n) \\right).\n\\] Doing this many times, we would get multiple random variables that approximate the distribution of \\(\\hat{\\alpha}\\) (i.e., the statistic computed on our original data)."
  },
  {
    "objectID": "bootstrap.html#what-if-we-dont-have-a-model",
    "href": "bootstrap.html#what-if-we-dont-have-a-model",
    "title": "34  Bootstrapping",
    "section": "34.4 What if we don’t have a model?",
    "text": "34.4 What if we don’t have a model?\nSimulation-based approaches like the one discussed above work because we have made a model assumption about our data. If we assumed our data came from a Poisson distribution, then we could just estimate the Poisson parameters and generate new samples.\nBut often we don’t want to make such assumptions about our data. Because, for example\n\nOur parameter(s) may be expensive to estimate\nThe distribution may be expensive to draw from\nWe don’t want to make model assumptions in the first place!\n\nThis last concern gives rise to what we call non-parametric statistics. That is, doing statistics while avoiding assumptions of the form “We assume that the data are generated according to a normal (or Poisson or Binomial or…)”.\nThe details of non-parametric statistics will have to wait for your later courses, but these concerns lead us to try and come up with a different way to “resample” copies of our statistic."
  },
  {
    "objectID": "bootstrap.html#introducing-the-bootstrap",
    "href": "bootstrap.html#introducing-the-bootstrap",
    "title": "34  Bootstrapping",
    "section": "34.5 Introducing the Bootstrap",
    "text": "34.5 Introducing the Bootstrap\nSo, let’s try something a little weird.\nWhat we want to do is to draw new samples from the same population (i.e., distribution) as our data came from. In the simulation-based approach, we estimate the parameter(s) to get an approximation to that true distribution.\nThe bootstrap takes a different tack. The data, \\(X_1,X_2,\\dots,X_n\\) is a sample from the actual population that we care about (i.e., not an approximation!). The bootstrap says, “let’s just sample from \\(X_1,X_2,\\dots,X_n\\).”\nSaid another way, in the bootstrap, we sample with replacement from the observed data \\[\nX_1, X_2, \\dots, X_n,\n\\] obtaining the sample \\[\nX^*_1, X^*_2, \\dots, X^*_n.\n\\] The \\(X^*_i\\) notation is convention in statistics– the asterisk (\\(*\\)) denotes that the variable is resampled from the original data.\nImportant note: \\(X^*_i\\) does not necessarily equal \\(X_i\\). It is just the \\(i\\)-th resampled data point. \\(X^*_i\\) is equal to some \\(X_j\\), \\(j=1,2,\\dots,n\\), with each of the \\(n\\) different data points being equally likely. Each \\(X_i^*\\) is a sample with replacement from the data \\(X_1,X_2,\\dots,X_n\\).\nNow, having resampled our data, we can compute our estimator on the bootstrap sample \\(X^*_1,X^*_2,\\dots,X^*_n\\), say, \\[\n\\hat{\\theta}\\left(X^*_1,X^*_2,\\dots,X^*_n \\right).\n\\] Repeating this many times (say, \\(B\\) times), we can use these resampled replicates of our original estimator, say \\[\n\\hat{\\theta}_1, \\hat{\\theta}_2, \\dots, \\hat{\\theta}_B.\n\\] The intuition is that these \\(B\\) random variables, each based on a sample with replacement from the original data, are a good approximation to the true distribution of our estimate \\(\\hat{\\theta}(X_1,X_2,\\dots,X_n)\\), and we can use them to do things like estimating the variance.\n\n34.5.1 Wait, how can this possibly work?!\nYes, this is just as crazy as it sounds, and yet, it works (for certain problems, anyway).\nThe intuition is something like this: when \\(n\\) is suitably large, the “point cloud” formed by \\(X_1,X_2,\\dots,X_n\\) looks a lot like the true population distribution.\nAs a result, resampling from the observed sample \\(X_1,X_2,\\dots,X_n\\) and resampling from the true population distribution are not actually so different!\nThe careful mathematical proof of this intuition is beyond the scope of this course. But I agree that on a first glance this should not work at all. And yet it does!\nI have been studying statistics for more than 15 years, and I am still pretty sure the bootstrap is magic…\n\n\n34.5.2 Example: estimating the Poisson rate parameter\nLet’s start by illustrating on a simple data example that we have seen multiple times before. Let’s suppose that \\(X_1,X_2,\\dots,X_n\\) are drawn iid from a Poisson distribution with rate parameter \\(\\lambda = 5\\).\nOur goal is to construct a confidence interval for the rate parameter \\(\\lambda\\).\nNow, we’ve already seen ways to do this– we could construct a CLT-based confidence interval or use a simulation-based approach. The point here is just to illustrate how the bootstrap applies in a setting that we are already familiar with. The claim is not that this the bootstrap is the best solution to this particular problem.\n\nn = 25\nlambdatrue = 5\ndata = rpois(n=n, lambda=5); # Generate a sample of 25 iid RVs\nlambdahat = mean(data); # Estimate lambda.\n\n# Now, let's do the bootstrap.\n# We'll repeatedly (B times)  resample n=25 observations from the data.\n# On each resample, compute lambdahat (i.e., take the mean).\nB = 200; # Number of bootstrap replicates.\nreplicates = rep(NA,B); # We'll store\nfor( i in 1:B ) {\n  # Sample WITH REPLACEMENT from the data sample itself.\n  resample = sample(data, n, replace=TRUE)\n  # Compute our statistic on the resample data.\n  # This is a *bootstrap replicate* of our statistic.\n  replicates[i] = mean(resample)\n}\n\nNow, replicates is a vector of (approximate) replicates of our estimate for \\(\\lambda\\). Let’s visualize the replicates– we’ll indicate the true \\(\\lambda\\) in blue and our estimate (lambdahat) in red.\n\nhist(replicates)\nabline(v=lambdahat, lwd=2, col='red')\nabline(v=lambdatrue, lwd=2, col='blue')\n\n\n\n\nNow, we’re going to use those bootstrap replicates to estimate the standard deviation of \\(\\hat{\\lambda} = n^{-1} \\sum_i X_i\\), and then we’ll use that to get a 95% CI for \\(\\lambda\\).\n\nsd_lambda = sd( replicates )\nCI = c( lambdahat-1.96*sd_lambda, lambdahat+1.96*sd_lambda)\n# And check if our CI contains lambda = 5.\n(CI[1] &lt; lambdatrue) & (lambdatrue &lt; CI[2])\n\n[1] TRUE\n\n\nWell, about 95% of the time, we should catch \\(\\lambda=5\\). Let’s run that same experiment a bunch of times, just to verify. That is, we are going to:\n\nGenerate data \\(X_1,X_2,\\dots,X_n\\) iid from Poisson with \\(\\lambda=5\\).\nCompute \\(\\hat{\\lambda} = \\bar{X}\\)\nRepeatedly resample from \\(X_1,X_2,\\dots,X_n\\) with replacement, compute the mean of each resample (i.e., compute our estimator on each resample)\nCompute the standard deviation of these resampled copies of \\(\\hat{\\lambda}\\).\nUse that SD to compute a 95% CI, under the assumption that \\(\\hat{\\lambda}\\) is approximately normal about its expectation \\(\\lambda\\).\n\nOkay, let’s do that in code. It will be useful to have a command that runs all of the bootstrap machinery for us.\n\nrun_pois_bootstrap_expt = function( lambdatrue, n, B ) {\n  # lambdatrue is the true value of lambda to use in generating our data.\n  # n is the sample size.\n  # B is the number of bootstrap replicates to use.\n  \n  # First things first, generate data.\n  data = rpois( n=n, lambda=lambdatrue )\n  lambdahat = mean( data ); # Our point estimate for lambda.\n  \n  # Generate B bootstrap replicates.\n  replicates = rep(NA,B)\n  # Each replicate draws n data points, with replacement, from data\n  # and computes its mean (i.e., estimates lambda from the resampled data)\n  for( i in 1:B ) {\n    # resample n elements of the data, with replacement.\n    resampled_data = sample( data, n, replace=TRUE )\n    replicates[i] = mean( resampled_data )\n  }\n  # Now use those replicates to estimate SD of and construct a 95% CI.\n  sd_boot = sd( replicates ); # estimate of the std dev of alphahat\n  CI = c(lambdahat-1.96*sd_boot, lambdahat+1.96*sd_boot)\n  # Finally, check if this CI caught lambda successfully.\n  # Return TRUE/FALSE accordingly.\n  \n  return( (CI[1] &lt; lambdatrue) & (lambdatrue &lt; CI[2]) )\n}\n\nNow, let’s repeat this experiment a few hundred times, and see how often our CI contains the true \\(\\lambda\\).\n\nN_expt = 500; # number of CIs to construct.\nsuccesses = rep(NA, N_expt); # Keep track of whether or not each CI caught alpha.\nfor( i in 1:N_expt ) {\n  # Repeat the experiment N_expt times.\n  # lambdatrue=5, B=200 bootstrap replicates, n=25 observations in the data sample.\n  successes[i] = run_pois_bootstrap_expt( 5, 200, 25 )\n}\nmean(successes)\n\n[1] 0.932\n\n\nThat number should be between 0.93 and 0.97 (of course, we are always subject to randomness in our experiments…).\n\n\n34.5.3 Recap: general recipe for the bootstrap\nJust to drive things home, let’s walk through the “recipe” for the bootstrap again, this time at a slightly more abstract level. Then we’ll come back and do a more complicated example.\nSuppose that we have data \\(X_1,X_2,\\dots,X_n\\) drawn iid from some unknown distribution. We wish to estimate a parameter \\(\\theta\\), and we have an estimator \\(\\hat{\\theta} = \\hat{\\theta}(X_1,X_2,\\dots,X_n)\\) for \\(\\theta\\).\nThe basic idea behind the bootstrap is to resample from the data \\(X_1,X_2,\\dots,X_n\\) with replacement, evaluate \\(\\hat{\\theta}\\) on each resample, and use those replicates of \\(\\hat{\\theta}(X_1,X_2,\\dots,X_n)\\) to approximate its true distribution (usually we are specifically interested in variance, but we’ll come back to this point).\nSo, given data \\(X_1,X_2,\\dots,X_n\\), we:\n\nRepeatedly (\\(B\\) times) sample \\(n\\) observations with replacement from \\(X_1,X_2,\\dots,X_n\\), to obtain samples \\(X_{b,1}^*,X_{b,2}^*,\\dots,X_{b,n}^*\\), for \\(b=1,2,\\dots,B\\). Note that putting the asterisk (*) on the resampled data points is a common notation in statistics to indicate bootstrap samples.\nFor each of these \\(B\\) resamples, compute the estimator on that sample, to obtain \\[\n\\hat{\\theta}_b = \\hat{\\theta}(X_{b,1}^*,X_{b,2}^*,\\dots,X_{b,n}^*) ~~~ \\text{ for } b=1,2,\\dots,B.\n\\]\nCompute the standard deviation of our bootstrap replicates, \\[\n\\operatorname{SE}_{\\text{boot}}\n= \\sqrt{ \\frac{1}{B-1} \\sum_{b=1}^B \\left( \\hat{\\theta}_b - \\frac{1}{B}\\sum_{r=1}^B \\hat{\\theta}_r \\right)^2 }.\n\\]\nUse this standard deviation estimate to construct an (approximate) confidence interval, under the assumption that \\(\\hat{\\theta}\\) is normally distributed about its mean \\(\\theta\\). \\[\n(\\hat{\\theta} - 1.96\\operatorname{SE}_{\\text{boot}},\n\\hat{\\theta} + 1.96\\operatorname{SE}_{\\text{boot}} )\n\\]"
  },
  {
    "objectID": "bootstrap.html#bootstrapping-for-more-complicated-statistics",
    "href": "bootstrap.html#bootstrapping-for-more-complicated-statistics",
    "title": "34  Bootstrapping",
    "section": "34.6 Bootstrapping for more complicated statistics",
    "text": "34.6 Bootstrapping for more complicated statistics\nNow, our example above is kind of silly– we already know multiple different ways to construct confidence intervals for the Poisson parameter!\nBut what about more complicated functions of our data? That is often where the bootstrap really shines.\nTo see this, let’s return to our example of financial returns \\((X_i,Y_i)\\)\nThe bootstrap says that we should sample with replacement from the observed data \\[\n(X_1,Y_1), (X_2,Y_2), \\dots, (X_n, Y_n),\n\\] obtaining the sample \\[\n(X^*_1,Y^*_1), (X^*_2,Y^*_2), \\dots, (X^*_n, Y^*_n).\n\\] Again, the \\(X^*_i\\) notation is the conventional way to show that the variable is resampled from the original data.\nHaving resampled from our data in this way, we could then compute \\[\n\\alpha^* = \\hat{\\alpha}\\left( (X^*_1,Y^*_1), (X^*_2,Y^*_2), \\dots, (X^*_n, Y^*_n)  \\right).\n\\] Repeating this, say, \\(B\\) times, we would obtain bootstrap replicates \\[\n\\alpha^*_1, \\alpha^*_2, \\dots, \\alpha^*_B,\n\\] from which we can estimate the variance of \\(\\hat{\\alpha}\\) as \\[\n\\hat{\\sigma}^2_{\\alpha}\n=\n\\frac{1}{B-1}\\sum_{r=1}^B \\left( \\alpha^*_r - \\frac{1}{B} \\sum_{b=1}^B \\alpha^*_b \\right)^2.\n\\]\nNotice that this looks like a variance, except that we are using \\(\\hat{\\alpha}\\), the statistic computed on the original data sample, as our estimate of the mean.\n\n34.6.1 Demo: applying the bootstrap to financial data\nThe following chunk contains code for generating \\((X_i,Y_i)\\) pairs like those discussed in our financial example above.\n\nrequire(MASS)\n\nLoading required package: MASS\n\n\nWarning: package 'MASS' was built under R version 4.2.3\n\ngenerate_pairs = function( n ) {\n  # Generate n pairs of financial returns.\n  muX = 2; muY = -1\n  CovMx = matrix( c(1,-.25,-.25,2), nrow = 2)\n  data = mvrnorm(n=100, mu=c(muX,muY), Sigma=CovMx)\n  return( data.frame( 'X'=data[,1], 'Y'=data[,2]) )\n}\n\n\nfin_pairs = generate_pairs( 100 ); # Generate 100 (X,Y) pairs.\nhead(fin_pairs)\n\n          X           Y\n1 -1.149327 -0.66960862\n2  1.151513 -0.17073492\n3 -0.717036 -1.30541169\n4  1.850979 -2.16893482\n5  2.029031 -2.32692980\n6  1.848073  0.07619855\n\n\nAlways look at your data first. Here is a scatter plot.\n\npp = ggplot(data=fin_pairs, aes(x=X, y=Y)) + geom_point()\npp\n\n\n\n\nNow, let’s compute \\(\\hat{\\alpha}\\) on this data. Remember, \\(\\hat{\\alpha}\\) is just a function of the (estimated) variances of \\(X\\) and \\(Y\\), along with their covariance: \\[\n\\hat{\\alpha}\n= \\frac{ \\hat{\\sigma}_Y^2 - \\hat{\\sigma}_{XY} }\n        { \\hat{\\sigma}_X^2 + \\hat{\\sigma}_Y^2 -2\\hat{\\sigma}_{XY} }.\n\\] So, let’s just compute those three quantities and plug them in. The cov function gets us the whole sample covariance matrix of our data:\n\nSigmahat = cov( fin_pairs ); #Sigma is the common symbol for a covariance matrix.\nSigmahat\n\n          X         Y\nX  1.436377 -0.481378\nY -0.481378  1.711926\n\n\nNow we can just pluck our three estimates out of there\n\nsigma2hatXX = Sigmahat[1,1]\nsigma2hatYY = Sigmahat[2,2]\nsigmahatXY = Sigmahat[1,2]\n\nand we can plug these into our formula for \\(\\hat{\\alpha}\\) above.\n\nalphahat = (sigma2hatYY - sigmahatXY)/(sigma2hatXX + sigma2hatYY -2*sigmahatXY)\nalphahat\n\n[1] 0.5335132\n\n\nNow, in truth, the covariances that generated our data are: \\[\n\\sigma^2_X = 1,~~~\\sigma^2_Y = 2, ~~~\\sigma_{XY} = -0.25,\n\\] so the true optimal choice of \\(\\alpha\\) is \\[\n\\alpha = \\frac{ \\sigma_Y^2 - \\sigma_{XY} }\n        { \\sigma_X^2 + \\sigma_Y^2 -2\\sigma_{XY} }\n        = \\frac{2 - (-0.25) }{ 1 + 2 + 2*0.25  }\n        = \\frac{ 2.25 }{3.5 } \\approx 0.64\n\\] Let’s store the true value of alpha while we’re thinking of it.\n\nsigma2XX = 1\nsigma2YY = 2\nsigmaXY = -0.25\nalpha_true =(sigma2YY - sigmaXY)/(sigma2XX + sigma2YY -2*sigmaXY)\nalpha_true\n\n[1] 0.6428571\n\n\nNow, again, we’re going to resample with replacement from our data, and compute our statistic \\(\\hat{\\alpha}\\) on each resample. The hope is that these resampled versions of the statistic will resemble the distribution of the statistic evaluated on the original data.\nIt will be convenient to just have a function to compute alphahat from a given data set.\n\ncompute_alphahat = function( data ) {\n  # We're assuming that data is a data frame with two columns.\n  Sigmahat = cov( data )\n  # Extract the variance and covariance estimates from the sample covariance\n  sigma2hatXX = Sigmahat[1,1]\n  sigma2hatYY = Sigmahat[2,2]\n  sigmahatXY = Sigmahat[1,2]\n  # plug these into the definition of alpha.\n  alphahat = (sigma2hatYY - sigmahatXY)/(sigma2hatXX + sigma2hatYY -2*sigmahatXY)\n  return(alphahat)\n}\n\nalphahat = compute_alphahat( fin_pairs )\n\nOkay, we’re ready to go. Let’s resample the data \\(B=200\\) times, evaluating \\(\\hat{\\alpha}\\) on each resample. Then, we’ll use those resampled values to estimate the variance.\n\nB = 200\nreplicates = rep(NA,B)\nn = nrow( fin_pairs ); # number of observations in our data set.\nfor( i in 1:B ) {\n  # To resample the data, we will sample indices, and then grab those rows.\n  resample_indices = sample( 1:n, n, replace=TRUE )\n  resampled_data = fin_pairs[resample_indices,]\n  replicates[i] = compute_alphahat( resampled_data )\n}\nhist( replicates )\nabline( v=alphahat, col='red', lwd=2); # alpha of true data.\nabline( v=0.64, col='blue', lwd=2); # True alpha\n\n\n\n\nSo the red line indicates the value of \\(\\alpha\\) estimated from the original data in the data frame fin_pairs, while the blue line indicates the true value of \\(\\alpha\\) computed from the true covariance structure of \\((X,Y)\\).\nClearly, our resampled data is centering about our estimate of \\(\\alpha\\), not its true value.\nBut that is okay! We just want to use our bootstrap replicates to estimate the variance, so that we can center a confidence interval at our estimate…\n\n# Estimate the variance of alphahat from our bootstrap replicates.\nsd_alphahat = sd( replicates ); # estimate of the std dev of alphahat\nCI = c(alphahat-1.96*sd_alphahat, alphahat+1.96*sd_alphahat)\nCI\n\n[1] 0.4609714 0.6060550\n\n\n…and we can verify that the CI contains the true value of \\(\\alpha\\) (about 95% of the time you run this code, anyway):\n\n(CI[1] &lt; alpha_true) & (alpha_true &lt; CI[2])\n\n[1] FALSE\n\n\nJust to verify, let’s rerun this whole machinery a few times, and check that our bootstrap-based CI catches \\(\\alpha\\) about 95% of the time.\nImportantly, that “95% of the time” means “if we generate new data, then 95% of the time, our bootstrap-based CI will catch the true value of \\(\\alpha\\).”\nTo do this experiment, we want a function to run the above procedure once for us on a given data frame and a given number of bootstrap replicates.\n\nrun_bootstrap_expt = function( B ) {\n  # B is the number of bootstrap replicates to use.\n  \n  # Generate new data: 1-- X-Y pairs.\n  data = generate_pairs( 100 )\n  # Generate B bootstrap replicates.\n  replicates = rep(NA,B)\n  # Each replicate draws n data points, with replacement, from fin_pairs.\n  n = nrow( data ); # number of observations in our data set.\n  for( i in 1:B ) {\n    # To resample the data, we will sample indices, and then grab those rows.\n    resample_indices = sample( 1:n, n, replace=TRUE )\n    resampled_data = data[resample_indices,]\n    replicates[i] = compute_alphahat( resampled_data )\n  }\n  # Now use those replicates to estimate SD of alphahat and construct a 95% CI.\n  alphahat = compute_alphahat( data )\n  sd_alphahat = sd( replicates ); # estimate of the std dev of alphahat\n  CI = c(alphahat-1.96*sd_alphahat, alphahat+1.96*sd_alphahat)\n  # Finally, check if this CI caught alpha successfully.\n  # Return TRUE/FALSE accordingly.\n  return( (CI[1] &lt; alpha_true) & (alpha_true &lt; CI[2]) )\n}\n\n\nN_expt = 200; # number of CIs to construct.\nsuccesses = rep(NA,N_expt); # Keep track of whether or not each CI caught alpha.\nfor( i in 1:N_expt ) {\n  successes[i] = run_bootstrap_expt( 200 ); # Using B=200 replicates.\n}\nmean(successes)\n\n[1] 0.925\n\n\nOnce again, this should be approximately 0.95, up to the randomness in our experiment."
  },
  {
    "objectID": "bootstrap.html#limitations-and-extensions-of-the-bootstrap",
    "href": "bootstrap.html#limitations-and-extensions-of-the-bootstrap",
    "title": "34  Bootstrapping",
    "section": "34.7 Limitations and extensions of the bootstrap",
    "text": "34.7 Limitations and extensions of the bootstrap\nNow, the bootstrap is magic, at least when it works.\nBut the set of problems for which the “classical” bootstrap (i.e., the version of the bootstrap outlined above) works, is pretty limited.\nHere’s a simple example of where the bootstrap can fail.\n\n34.7.1 Example: estimating the distribution of the maximum\nSuppose that we see data \\(X_1,X_2,\\dots,X_n\\) drawn iid from some distribution, and we are interested in the distribution of \\[\nM = \\max\\{ X_1, X_2, \\dots, X_n \\}.\n\\] In particular, suppose we want to estimate \\(\\mathbb{E} M\\).\nThe bootstrap would suggest that we sample \\(X_1^*,X_2,^*,\\dots,X_n^*\\) with replacement from \\(X_1,X_2,\\dots,X_n\\) and compute \\[\nM^* = \\max\\{ X_1^*, X_2^*, \\dots, X_n^* \\}.\n\\] Repeating this \\(B\\) times, we get replicates \\(M_1^*,M_2^*,\\dots,M_B^*\\), and we can compute their standard deviation to estimate the standard deviation of our variable of interest \\(M\\).\nNow let’s implement that and see how well it works.\nWe’ll draw our data from a standard normal. First, let’s generate a bunch of copies of \\(M\\), so we can see its distribution (and its variance, in particular).\n\nn_samp = 25; # Number of data samples to take\nn_rep = 2000; # Number of replicates of M to create.\nM_reps = rep( NA, n_rep )\nfor( i in 1:n_rep ) {\n  data = rnorm(n=n_samp)\n  M_reps[i] = max(data)\n}\nhist( M_reps )\n\n\n\n\nThe true value of \\(\\mathbb{E} M\\) is actually very hard to compute, but we can use Monte Carlo estimation to approximate it:\n\nExpecM = mean(M_reps)\nExpecM\n\n[1] 1.953449\n\n\nIn particular, the (sample) standard deviation is\n\nsd(M_reps)\n\n[1] 0.5014746\n\n\nNow, let’s generate a new sample and run the bootstrap on it.\n\ndata = rnorm(n=n_samp)\nB = 200\nMboot_reps = rep( NA, B )\nfor( b in 1:B ){\n  resamp = sample( data, n_samp, replace=TRUE)\n  Mboot_reps[b] = max(resamp)\n}\nhist(Mboot_reps)\n\n\n\n\nYikes! That looks very different from the true distribution of \\(M\\) we saw above…\nLet’s just check the variance? Maybe it’s close to the observed \\(\\sigma_M \\approx 0.483\\)?\n\nsd(Mboot_reps)\n\n[1] 0.4090479\n\n\nCompare with\n\nsd(M_reps)\n\n[1] 0.5014746\n\n\nAnd if we try to construct a confidence interval, we get a CI\n\nM = max(data)\nseMboot = sd(Mboot_reps)\nCI = c( M-1.96*seMboot, M+1.96*seMboot )\nCI\n\n[1] 1.219668 2.823136\n\n\n\n(CI[1] &lt; ExpecM) & (ExpecM &lt; CI[2])\n\n[1] TRUE\n\n\nOkay, but we really have to run the experiment multiple times, so let’s package it up in a function…\n\n# A function to repeat the same experiment easily without having to repeat ourselves.\nrun_max_boot_expt = function( n, B ) {\n  data = rnorm(n)\n  M = max(data)\n  Mboot_reps = rep(NA,B)\n  for(b in 1:B) {\n    resamp = sample( data, n, replace=TRUE)\n    Mboot_reps[b] = max(resamp)\n  }\n  seMboot = sd(Mboot_reps)\n  CI = c( M-1.96*seMboot, M+1.96*seMboot )\n  return( CI[1] &lt; ExpecM & ExpecM &lt; CI[2] )\n}\n\n…and, just like we’ve done above, we’ll generate data samples a bunch of times and see how often our bootstrap CI catches the (estimated!) true value of \\(\\mathbb{E}M\\).\n\nNexpt = 500\nsuccesses = rep(NA,Nexpt)\nfor(i in 1:Nexpt) {\n  successes[i] = run_max_boot_expt( n_samp, 200 )\n}\nmean(successes)\n\n[1] 0.73\n\n\nHmm… Not ideal.\nThe problem is that the bootstrap tends (usually! it’s possible that we got lucky when knitting, but unlikely) to vastly underestimate the variance of “challenging” functions like the maximum.\nGenerally speaking, the bootstrap only works well when the statistic we are trying to work with is approximately normal about its expectation. The maximum of a collection of variables does obey a central limit theorem, but its limiting distribution is not a normal!\nSee here if you’re curious to learn more.\n\n\n34.7.2 A cautionary tale\nThere are, in fact, ways to modify the bootstrap to handle issues like resampling “challenging” functions like the maximum, but those are going to have to wait for your more advanced courses.\nThe above example is just a cautionary tale of how important it is to know the limitations of the methods that you are using. The bootstrap works amazingly for the problems that it is “right” for, but it is also demonstrably bad for certain other problems.\nGenerally speaking, it’s important to know what tool is right for what job!"
  },
  {
    "objectID": "bootstrap.html#review",
    "href": "bootstrap.html#review",
    "title": "34  Bootstrapping",
    "section": "34.8 Review",
    "text": "34.8 Review\nIn these notes we covered\n\nSituations where Parametric Estimation is inadequate\nA General method for bootstrap estimation\nBootstrap estimation for univariate data\nBootstrap estimation for bivariate data\nLimitations of bootstrapping"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#simple-example---estimating-the-mean-of-a-weibull-distribution",
    "href": "R12_Bootstrap_Examples.html#simple-example---estimating-the-mean-of-a-weibull-distribution",
    "title": "41  R13_Bootstrap",
    "section": "41.1 Simple Example - Estimating the mean of a Weibull Distribution",
    "text": "41.1 Simple Example - Estimating the mean of a Weibull Distribution\nThe Weibull distribution is a flexible random variable distribution which takes a shape and scale parameter. The Expected value of the distribution is not a pretty expression, but it’s true that the expected value of the sample mean is the mean of the RV. Suppose we have some data which comes from a Weibull shaped population\n\nplot(x=seq(0.1,20,.1), y=dweibull(seq(0.1,20,.1), 4,10), type=\"l\", main=\"Population Shape\")\n\n\n\nmyData &lt;- rweibull(15, shape=4, scale=10)\nmyData\n\n [1] 11.237286 10.499885  7.166874 11.822287 10.819460 10.958329  4.240888\n [8] 12.249702  8.523259  8.787081 12.530388  8.155726 10.157383 13.770476\n[15] 11.722401\n\nhist(myData)\n\n\n\n\nWe wish to come up with a 95% confidence interval for the population mean. We’ll use 10000 bootstrapped resamples\n\nB &lt;- 10000\nbootstrap.mean &lt;- vector(\"numeric\")\nfor(i in 1:B){\n  b.sample &lt;- sample(myData, size=length(myData), replace=TRUE)\n  bootstrap.mean[i]&lt;-mean(b.sample)  \n}\n\nhist(bootstrap.mean)\n\n\n\nquantile(bootstrap.mean, c(.025, .975))\n\n     2.5%     97.5% \n 8.903909 11.298939 \n\n#True mean?\n10*gamma(1+1/4)\n\n[1] 9.064025\n\n\nWhat if we wanted to use a MC estimate? Well we’d need to parameterize the weibull distribution. Finding estimates for the shape and scale are… annoying to do by hand. You would have to use software to fit these parameters using some method. The MASS library has a function fitdistr that can be used.\n\nlibrary(MASS)\n\nWarning: package 'MASS' was built under R version 4.2.3\n\nparam.hat &lt;- fitdistr(myData,\"weibull\")    \nNMC &lt;- 10000\nshape.hat &lt;- param.hat$estimate[1]\nscale.hat &lt;- param.hat$estimate[2]\nmc.mean &lt;- replicate(NMC, mean(rweibull(length(myData), shape=shape.hat, scale=scale.hat)))\nhist(mc.mean)\n\n\n\nquantile(mc.mean, c(.025, .975))\n\n     2.5%     97.5% \n 9.078901 11.254496 \n\n\n\n41.1.1 A bootstrap estimate for the standard deviation of the population\nLet’s look at the standard deviation of the distribution\n\nbootstrap.sd &lt;- vector(\"numeric\")\nfor(i in 1:B){\n  b.sample &lt;- sample(myData, size=length(myData), replace=TRUE)\n  bootstrap.sd[i]&lt;-sd(b.sample)  \n}\n\nhist(bootstrap.sd)\n\n\n\nquantile(bootstrap.sd, c(.025, .975))\n\n    2.5%    97.5% \n1.371631 3.227524"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#another-bootstrap-estimation-of-the-standard-deviation",
    "href": "R12_Bootstrap_Examples.html#another-bootstrap-estimation-of-the-standard-deviation",
    "title": "41  R13_Bootstrap",
    "section": "41.2 Another Bootstrap estimation of the standard deviation",
    "text": "41.2 Another Bootstrap estimation of the standard deviation\nStart with some data. Where did it come from?\n\nmysteryData &lt;- c(0.26,10.87,5.05,0.25,4.03,19.66,1.64,4.85,6.64,4.01,0.76,4.82,9.53,4.11,2.05,0.47)\nhist(mysteryData)\n\n\n\n\nSuppose we want to estimate the population standard deviation. A good point estimate is of course the sample standard deviation\n\nsd(mysteryData)\n\n[1] 5.025347\n\n\nA 95% bootstrapped confidence interval can be produced easily because we don’t have to make any population assumptions.\n\nsd.boot &lt;- 0\nfor(i in 1:10000){\n  sd.boot[i] &lt;- sd(sample(mysteryData, replace=TRUE))\n}\nhist(sd.boot, breaks=50)\n\n\n\nquantile(sd.boot, c(0.025, 0.975))\n\n    2.5%    97.5% \n2.158309 7.166193"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#mc-vs-bootstrap-intervals",
    "href": "R12_Bootstrap_Examples.html#mc-vs-bootstrap-intervals",
    "title": "41  R13_Bootstrap",
    "section": "41.3 MC vs Bootstrap Intervals",
    "text": "41.3 MC vs Bootstrap Intervals\nLet’s compare MC estimation vs bootstrapping methods. The difference will become apparent when we make the correct decision about the model to use for the population.\n\n41.3.1 Correctly Identifying the Model\nSuppose the data is drawn from a normal distribution. We can actually compare 3 methods of estimating the variance of the population.\n\nThe classical statistical approach is to use a chi squared distribution for the sampling distribution of \\(S^2\\)\nThe Monte Carlo approach requires us to simulate new data from \\(N(\\hat{\\mu}, \\hat{\\sigma^2})\\)\nThe bootstrap approach has us re-sample from the initial data.\n\nWe’ll simulate using all three of these methods over and over to get coverage estimates as well as precision estimates (average width of the intervals)\n\nMC.coverage &lt;- FALSE; boot.coverage &lt;- FALSE; param.coverage &lt;- FALSE\nMC.width &lt;- 0; boot.width &lt;- 0; param.width &lt;- 0;\n\nfor(i in 1:500){\n  myData &lt;- rnorm(15, 37, 2) #The data is actually coming from a normal distr.\n  trueSigma2 &lt;- 2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- unname(quantile(MC.var, c(0.025, 0.975)))\n  Boot.ci &lt;- unname(quantile(Boot.var, c(0.025, 0.975)))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage    width\n1   Classic    0.960 7.970988\n2        MC    0.920 5.953105\n3 Bootstrap    0.836 4.865681\n\n\n\n\n41.3.2 Incorrectly identifying the model - a slight skew.\nWhat if the population is skewed. Take t population is skewed\n\nplot(density(rgamma(1000, 2, scale=2)), main=\"Population Shape\")\n\n\n\n\n\nMC.coverage &lt;- FALSE\nboot.coverage &lt;- FALSE\nparam.coverage &lt;- FALSE\n\nfor(i in 1:500){\n  myData &lt;- rgamma(15, 2, scale=2) #data is simulated from a gamma\n  trueSigma2 &lt;- 2*2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n  Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage    width\n1   Classic    0.856 15.96129\n2        MC    0.820 11.91340\n3 Bootstrap    0.750 11.75740\n\n\n\n\n41.3.3 Misidentifying the population - A very skewed population\n\nplot(density(rgamma(1000, .5, scale=2)), main=\"Population Shape - very skewed\")\n\n\n\n\n\nMC.coverage &lt;- FALSE\nboot.coverage &lt;- FALSE\nparam.coverage &lt;- FALSE\n\nfor(i in 1:200){\n  myData &lt;- rgamma(15, .5, scale=2)\n  trueSigma2 &lt;- .5*2^2\n  n &lt;- length(myData)\n  MC.var &lt;- 0\n  Boot.var &lt;- 0\n  xbar &lt;- mean(myData)\n  sd &lt;- sd(myData)\n  for(j in 1:1000){\n    #Assuming the data was drawn from a normal population\n    MC.var[j] &lt;- var(rnorm(n, xbar, sd))\n    Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n  }\n  MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n  Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n  param.ci &lt;- (n-1)*var(myData)/qchisq(c(0.975, 0.025), n-1)\n  MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n  boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n  param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  MC.width[i] &lt;- diff(MC.ci)\n  boot.width[i] &lt;- diff(Boot.ci)\n  param.width[i] &lt;- diff(param.ci)\n}\nresults &lt;- data.frame('method' = c(\"Classic\",\"MC\",\"Bootstrap\"),\n                      'coverage'=c(mean(param.coverage),mean(MC.coverage),mean(boot.coverage)),\n                      'width' = c(mean(param.width),mean(MC.width), mean(boot.width)))\nresults\n\n     method coverage     width\n1   Classic     0.58 11.024475\n2        MC     0.57  8.206736\n3 Bootstrap     0.62  8.402749\n\n\nBottom line - if we misidentify the model MC methods can underperform bootstrap. Why do all three of these do so bad though? Small sample size is the answer. Let’s dig further into the effect of sample size on the performance of bootstrap estimation."
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#sensitivity-to-sample-size",
    "href": "R12_Bootstrap_Examples.html#sensitivity-to-sample-size",
    "title": "41  R13_Bootstrap",
    "section": "41.4 Sensitivity to sample size",
    "text": "41.4 Sensitivity to sample size\nWe will look at samples of size 10, 20, 30,…, 100 and how the three methods perform on this same population.\n\nn &lt;- seq(10, 100, 10)\nMC.rate &lt;- 0\nboot.rate &lt;- 0\nparam.rate &lt;- 0\nresults &lt;- data.frame(n, MC.rate, boot.rate, param.rate)\n\nfor(samplesize in n){\n  MC.coverage &lt;- FALSE\n  boot.coverage &lt;- FALSE\n  param.coverage &lt;- FALSE\n  \n  for(i in 1:200){\n    myData &lt;- rgamma(samplesize, .5, scale=2)\n    trueSigma2 &lt;- .5*2^2\n    MC.var &lt;- 0\n    Boot.var &lt;- 0\n    xbar &lt;- mean(myData)\n    sd &lt;- sd(myData)\n    for(j in 1:1000){\n      #Assuming the data was drawn from a normal population\n      MC.var[j] &lt;- var(rnorm(samplesize, xbar, sd))\n      Boot.var[j] &lt;- var(sample(myData, replace=TRUE))\n    }\n    MC.ci &lt;- quantile(MC.var, c(0.025, 0.975))\n    Boot.ci &lt;- quantile(Boot.var, c(0.025, 0.975))\n    param.ci &lt;- (samplesize-1)*var(myData)/qchisq(c(0.975, 0.025), samplesize-1)\n    MC.coverage[i] &lt;- MC.ci[1] &lt;= trueSigma2 & MC.ci[2] &gt;= trueSigma2\n    boot.coverage[i] &lt;- Boot.ci[1] &lt;= trueSigma2 & Boot.ci[2] &gt;= trueSigma2\n    param.coverage[i] &lt;- param.ci[1] &lt;= trueSigma2 & param.ci[2] &gt;= trueSigma2\n  }\n  results[results$n==samplesize, 2:4]=c(mean(MC.coverage),\n                                        mean(boot.coverage),\n                                        mean(param.coverage))\n    \n}\n\nplot(x=results$n, y=results$MC.rate, type=\"l\", ylim=c(0,1), main=\"'95%' CI Coverage Rate vs Sample Size (skewed population)\",\n     xlab=\"Sample Size\", ylab=\"Coverage Rate\")\nabline(h=.95, lty=2)\nlines(x=results$n, y=results$boot.rate, col=\"blue\")\nlines(x=results$n, y=results$param.rate, col=\"red\")\nlegend(x=10, y=.4, legend=c(\"MC\",\"Boot\",\"Parametric\"), col=c(\"black\",\"blue\",\"red\"), lwd=2)\n\n\n\n\nThe larger sample size allows bootstrap confidence intervals to improve and improve in terms of coverage rate, approaching 95%. This improvement is not seen in the MC or parametric (classical) method. They are both suffering from mis-identifying the population shape, thinking the population is a normal distribution. The bootstrap does not make any population assumptions, and the sample better represents the population as the sample size grows so naturally resampling will provide more representative simulated samples as \\(n\\) increases."
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#confidence-interval-for-the-difference-of-two-population-means",
    "href": "R12_Bootstrap_Examples.html#confidence-interval-for-the-difference-of-two-population-means",
    "title": "41  R13_Bootstrap",
    "section": "41.5 Confidence Interval for the difference of two population means",
    "text": "41.5 Confidence Interval for the difference of two population means\nExample: A mean experiment on crickets\nIn this experiment the treatment group of crickets were starved and we observed the time it took for the starved females to mate with males (they get to eat them after mating - do hungry females mate faster on average?)\n\nStarved&lt;-c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)\nFed&lt;-c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)\nboxplot(Starved, Fed)\n\n\n\n\nWe want to estimate \\(\\mu_{starved}-\\mu_{fed}\\) using bootstrapping\n\nB &lt;- 1000\ndiff.mean.boot &lt;- 0 #empty vector to store the estimated difference of means\nfor(i in 1:B){\n  #bootstrap both samples at the same time\n  boot.fed &lt;- sample(Fed, replace=TRUE)\n  boot.starved &lt;- sample(Starved, replace=TRUE)\n  #calculate the difference of means\n  diff.mean.boot[i] &lt;- mean(boot.starved) - mean(boot.fed)\n}\nhist(diff.mean.boot, breaks=50)\nabline(v=mean(Starved)-mean(Fed), col=\"red\")\n\n\n\nquantile(diff.mean.boot, c(0.025,.975))\n\n     2.5%     97.5% \n-39.34757   3.34257"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-estimation-of-correlation",
    "href": "R12_Bootstrap_Examples.html#bootstrap-estimation-of-correlation",
    "title": "41  R13_Bootstrap",
    "section": "41.6 Bootstrap estimation of Correlation",
    "text": "41.6 Bootstrap estimation of Correlation\nGenerate some data with a certain correlation matrix The variables each have a standard deviation of 1, and a correlation of .6 Thus covariance = \\(1\\times 1 \\times .6\\)\n\nlibrary(MASS)\nSigma &lt;- matrix(c(1, .6,\n                  .6, 1), byrow=TRUE, nrow=2)\ndf&lt;-as.data.frame(mvrnorm(n=15, mu=c(5,10), Sigma=Sigma))\nplot(df)\n\n\n\n\nGet a 95% confidence interval estimate of the correlation\n\nboot.correlations &lt;- 0\nB &lt;- 10000\n\nfor(i in 1:B){\n  #A bootstrapped sample\n  boot.df &lt;- df[sample(1:nrow(df), replace=TRUE),]\n  boot.correlations[i] = cor(boot.df$V1, boot.df$V2)\n}\nhist(boot.correlations)\n\n\n\n\nWe can use two methods to construct a 95% confidence interval.\n\nWe can take the 2.5th and 97.5th quantiles from the simulated correlations\nWe could find the standard deviation and take \\(\\hat\\rho \\pm 1.96 sd(\\hat{\\rho})\\)\n\n\nquantile(boot.correlations, c(.025, .975), names=FALSE)\n\n[1] 0.1868609 0.8810936\n\n#alternative\ncor(df$V1, df$V2) + c(-1,1)*1.96*sd(boot.correlations)\n\n[1] 0.3029538 1.0223900\n\n\nWhich method do we prefer? Let’s repeat the experiment to see how good each method’s coverage rate is. Our aim is 95%.\n\na.coverage &lt;- FALSE\nb.coverage &lt;- FALSE\nboot.correlations &lt;- 0\nB &lt;- 1000 #lowering to 1000\n\nfor(j in 1:1000){\n  Sigma &lt;- matrix(c(1,.6,.6,1), byrow=TRUE, nrow=2)\n  df&lt;-as.data.frame(mvrnorm(n=15, mu=c(5,10), Sigma=Sigma))\n  for(i in 1:B){\n    #A bootstrapped sample\n    boot.df &lt;- df[sample(1:nrow(df), replace=TRUE),]\n    boot.correlations[i] = cor(boot.df$V1, boot.df$V2)\n  }\n  a.CI &lt;- quantile(boot.correlations, c(.025, .975))\n  #alternative\n  b.CI &lt;- cor(df$V1, df$V2) + c(-1,1)*1.96*sd(boot.correlations)\n  \n  a.coverage[j] &lt;- a.CI[1] &lt;= .6 & a.CI[2] &gt;= .6\n  b.coverage[j] &lt;- b.CI[1] &lt;= .6 & b.CI[2] &gt;= .6\n}\nmean(a.coverage)\n\n[1] 0.909\n\nmean(b.coverage)\n\n[1] 0.885\n\n\nThey both under-cover, but the second method’s under-coverage is worse."
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-linear-regression",
    "href": "R12_Bootstrap_Examples.html#bootstrap-linear-regression",
    "title": "41  R13_Bootstrap",
    "section": "41.7 Bootstrap Linear Regression",
    "text": "41.7 Bootstrap Linear Regression\nNow let’s bootstrap a linear regression model. Suppose the population has the relationship \\[Y = 10 + 4X + \\epsilon\\] Where \\(\\epsilon \\sim exp(.1)\\). We can generate a small sample from such a population.\n\n#Generate data from a model: y = 10 + 4x + e\nset.seed(2024)\nx &lt;- runif(25, 0, 10)\ne &lt;- rexp(25, .1)\ny &lt;- 10 + 4*x + e\n\nAnd look at the scatter plot:\n\nplot(x,y)\n\n\n\n\nNote here that the assumptions of the linear model are not true; the errors are not normally distributed but actually exponentially distributed. This becomes a little clearer when we look at the residual QQ plot:\n\nplot(lm(y~x), which=2)\n\n\n\n\nNow we will pretend we don’t know the true parameter values. What can we do with bootstrapping to help us with our estimate of the model? \\[ y = \\beta_0 + \\beta_1 X + \\epsilon\\] Fit a linear model first - save those coefficients. Fitting the model will be done with least squares estimation. Bootstrapping will be used\n\nas.vector(lm(y ~ 1 + x)$coeff)\n\n[1] 20.926612  4.256114\n\n\nNow let’s bootstrap the data, fit the coefficients and repeat, keeping track of all of them\n\nnB &lt;- 1000\nslopes &lt;- rep(0, nB)\nintercepts &lt;- rep(0,nB)\nn &lt;- length(x)\n\nfor(i in 1:nB){\n  boot.index &lt;- sample(1:n, n, replace=TRUE)\n  boot.x &lt;- x[boot.index]\n  boot.y &lt;- y[boot.index]\n  boot.coeffs &lt;- as.vector(lm(boot.y~1+boot.x)$coeff)\n  intercepts[i] &lt;- boot.coeffs[1]\n  slopes[i] &lt;- boot.coeffs[2]\n}\n\nPlot the data, and all of the bootstrap lines\n\nplot(x,y)\nfor(i in 1:nB){\n  abline(a=intercepts[i], b=slopes[i], col=rgb(.5,.5,.5,.05))\n}\npoints(x,y)\n\n\n\n\naverage the bootstrapped intercepts and slopes - let this be the bootstrapped linear model.\n\nmean(intercepts)\n\n[1] 20.73885\n\nmean(slopes)\n\n[1] 4.307377\n\nas.vector(lm(y ~ 1 + x)$coeff)\n\n[1] 20.926612  4.256114\n\n\nHow well does it match the original linear model? How well does it match the population model? Plot it all on the same plot.\n\nplot(x,y, col=\"gray\")\nabline(a=10+10, b=4, col=\"black\") #E(e)=10 so\nabline(a=mean(intercepts), b=mean(slopes), col=\"red\")\nabline(lm(y ~ 1 + x), col=\"blue\")\n\n\n\n\nNow construct a bootstrapped confidence interval for the intercept, and a bootstrap interval for the slope.\n\nquantile(intercepts, c(0.025, 0.975))\n\n    2.5%    97.5% \n13.54361 28.33806 \n\nquantile(slopes, c(0.025, 0.975))\n\n    2.5%    97.5% \n2.961448 6.113239 \n\n#compared to the parametric method\nconfint(lm(y ~ 1 + x))\n\n               2.5 %    97.5 %\n(Intercept) 6.577624 35.275601\nx           1.939825  6.572403\n\n\nCome up with a bootstrap confidence interval for E(Y|x=5.8)\n\n# This is the parametric model fit\nxy.fit &lt;- lm(y ~ 1 + x)\npredict(xy.fit, newdata = data.frame(x=5.8))\n\n       1 \n45.61207 \n\n#Using the bootstrapped linear model\nb.intercept &lt;- mean(intercepts)\nb.slope &lt;- mean(slopes)\n\n#point estimate\nb.intercept + b.slope * 5.8\n\n[1] 45.72164\n\n#Let's estimate y-hat for every single bootstrapped model fit and use that as a distribution of means / expected values of y\nb.y.hats &lt;- intercepts + slopes * 5.8\n\nquantile(b.y.hats, c(0.025, .975))\n\n    2.5%    97.5% \n40.08676 53.50911 \n\n#compared to a parametric estimate\npredict(xy.fit, newdata = data.frame(x=5.8), interval=\"confidence\")\n\n       fit      lwr      upr\n1 45.61207 38.81064 52.41351\n\n\nCome up with a bootstrap prediction interval for Y|x=5.8\n\n# NOW BY BOOTSTRAP 95%-PREDICTION INTERVAL\nB &lt;- 1000\npred &lt;- numeric(B)\ndata &lt;- data.frame(x,y)\nfor (i in 1:B) {\n  boot &lt;- sample(n, n, replace = TRUE)\n  fit.b &lt;- lm(y ~ x, data = data[boot,])\n  pred[i] &lt;- predict(fit.b, list(x = 5.8)) + sample(resid(fit.b), size = 1)\n}\nquantile(pred, c(0.025, 0.975))\n\n     2.5%     97.5% \n 32.55693 102.30715 \n\n#compared to a parametric estimate\npredict(xy.fit, newdata = data.frame(x=5.8), interval=\"prediction\")\n\n       fit      lwr      upr\n1 45.61207 11.14919 80.07496\n\n#What is the true interval for 95% likelihood?\n10+4*5.8 + qexp(c(.025, .975), .1)\n\n[1] 33.45318 70.08879"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-interval-for-sigma-in-linear-model",
    "href": "R12_Bootstrap_Examples.html#bootstrap-interval-for-sigma-in-linear-model",
    "title": "41  R13_Bootstrap",
    "section": "41.8 Bootstrap Interval for sigma in linear model",
    "text": "41.8 Bootstrap Interval for sigma in linear model\nWe can also use bootstrapping to estimate the sigma. Here however I will go back to a linear model with normal errors. Let’s suppose our population can be described by \\[Y = 10 + 4X + e\\] Where \\(\\epsilon \\sim N(0, 10^2)\\)\n\n#Generate data from a model: y = 10 + 4x + e\nset.seed(2024)\nx &lt;- runif(25, 0, 10)\ne &lt;- rnorm(25, 10)\ny &lt;- 10 + 4*x + e\n\nA point estimate is obtained from the residual standard error. The summary output from linear regression calls it sigma\n\nxy.fit &lt;- lm(y~x)\nsummary(xy.fit)$sigma   \n\n[1] 0.8869751\n\n\nWe can create bootstrap samples from our residuals to come up with a bootstrap distribution for the residual standard error\n\nresid.se &lt;- 0\nfor(i in 1:1000){\n  resid.se[i] &lt;- sqrt(sum(sample(resid(xy.fit), replace=TRUE)^2)/summary(xy.fit)$df[2])\n}\nquantile(resid.se, c(0.025, 0.975))\n\n     2.5%     97.5% \n0.6116832 1.1408085"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#draft-data",
    "href": "R12_Bootstrap_Examples.html#draft-data",
    "title": "41  R13_Bootstrap",
    "section": "41.9 Draft Data",
    "text": "41.9 Draft Data\n\ndraft &lt;- read.csv(\"data/Celtics_Heat_Game_6_Actual.csv\")\n\nThis dataset is the last post-season game between two NBA teams – Celtics and Heat – at DraftKings. The dataset contains four variables: Player, Roster Position, %Drafted, and FPTS.\n\nPlayer: NBA players’ names\nRoster Position: the position the player held in the DraftKings lineups, whether as Captain (CPT) or Utility (UTIL). DraftKings has a 1.5 multiplication power over the captain.\n%Drafted: the percentage of people drafted this player\nFPTS: Fantasy Points\n\nThis project focuses on the last two variables – %Drafted and FPTS – and tries to understand how strongly they correlate with repeated samplings.\nNow let’s repeat this on the draft model; come up with a bootstrapped linear regression model to predict fantasy points (FPTS) from % drafted (X.Drafted). Compare it to the parametric model\n\ndraft.intercepts &lt;- 0\ndraft.slopes &lt;- 0\n\nfor(i in 1:1000){\n  draft.boot &lt;- draft[sample(nrow(draft), replace=TRUE),]\n  draft.boot.fit &lt;- lm(FPTS ~ X.Drafted, data=draft.boot)\n  draft.intercepts[i] &lt;- coef(draft.boot.fit)[1]\n  draft.slopes[i] &lt;- coef(draft.boot.fit)[2]\n}\n#Bootstrap linear model\nmean(draft.intercepts)\n\n[1] -0.5601898\n\nmean(draft.slopes)\n\n[1] 89.73204\n\n#Parametric linear model\nlm(FPTS ~ X.Drafted, data=draft)\n\n\nCall:\nlm(formula = FPTS ~ X.Drafted, data = draft)\n\nCoefficients:\n(Intercept)    X.Drafted  \n    -0.6888      90.8991"
  },
  {
    "objectID": "R12_Bootstrap_Examples.html#bootstrap-hypothesis-test",
    "href": "R12_Bootstrap_Examples.html#bootstrap-hypothesis-test",
    "title": "41  R13_Bootstrap",
    "section": "41.10 Bootstrap Hypothesis Test",
    "text": "41.10 Bootstrap Hypothesis Test\nBootstrapped One Sample Test function: What it does is comes up with a bootstraped estimate of the sampling distribution of a test statistic without relying on the central limit theorem or a population model. The t.hat returned is the bootstraped test statistic distribution.\n\nbootstrap=function(x,n.boot){\n  n&lt;-length(x)\n  x.bar&lt;-mean(x)\n  t.hat&lt;-rep(0, n.boot) \n  #create vector that we will fill with \"t\" values\n  for (i in 1:n.boot){\n    x.star&lt;-sample(x, size=n, replace=TRUE)\n    x.bar.star&lt;-mean(x.star)\n    s.star&lt;-sd(x.star)\n    t.hat[i]&lt;-(x.bar.star-x.bar)/(s.star/sqrt(n))\n  }\n  return(t.hat)\n}\n\n\n41.10.1 Example: Steel Conduits\nSteel conduits are buried for 2 years and the maximum depth of corrosion is measured on each conduit. If the average max penetration exceeds 50 micrometers then the conduits have to be re-engineered.\nIs there evidence that the mean maxPen &gt; 50?\n\\(H_0: \\mu \\leq 50\\) \\(H_1: \\mu &gt; 50\\)\n\nmaxPen&lt;-c(53, 61.1, 49.6, 59.3, 57.2, 57.4, 46.2, 55.1, 63, 54, 61.9, 62.2, 52.4, 45.4, 57.7, 45.1)\n\nx.bar&lt;-mean(maxPen)\nsd.maxPen&lt;-sd(maxPen)\nt.obs = (x.bar - 50)/(sd.maxPen/sqrt(length(maxPen)))\nt.obs\n\n[1] 3.341184\n\nset.seed(1)\nn.boot&lt;-10000\nMaxPen.boot&lt;-bootstrap(x=maxPen, n.boot)\nhist(MaxPen.boot, breaks=50)\nabline(v=t.obs)\n\n\n\n#pvalue\nsum(MaxPen.boot&gt;=t.obs) \n\n[1] 59\n\nsum(MaxPen.boot&lt;=t.obs) \n\n[1] 9941\n\n#Right-tailed test p-value - mean calculates the proportion for me\nmean(MaxPen.boot&gt;=t.obs) \n\n[1] 0.0059\n\n#if it were a 2 tailed test\np_upper = mean(MaxPen.boot &gt;= t.obs)\np_lower = mean(MaxPen.boot &lt;= t.obs)\n2*min(p_upper, p_lower)\n\n[1] 0.0118\n\n\n\n\n41.10.2 Two Sample Bootstrap Test\nYou can use bootstrapping as opposed to permutation testing. The nice thing about this method is the null hypothesis does not have to assume that the populations are equal to each others - we can directly test whether the population means are unequal. We calculate the bootstrapped test statistic distribution by resampling from each sample with the appropriate sample size. This function calculates a t-type statistic, but you can modify it to use any 2 sample test statistic that is useful for quantifying evidence for the alternative hypothesis.\nBootstrap Function from 2 independent samples\n\nboottwo = function(dat1, dat2, nboot) {\n  bootstat = numeric(nboot)     #Make Empty Vector for t* to fill\n  obsdiff = mean(dat1) - mean(dat2)\n  n1 = length(dat1)\n  n2 = length(dat2)\n  for(i in 1:nboot) {\n    samp1 = sample(dat1, size = n1, replace = T)    #Sample From Sample Data\n    samp2 = sample(dat2, size = n2, replace = T)\n    bootmean1 = mean(samp1)\n    bootmean2 = mean(samp2)\n    bootvar1 = var(samp1)\n    bootvar2 = var(samp2)\n    bootstat[i] = ((bootmean1 - bootmean2) - obsdiff)/sqrt((bootvar1/n1) + (bootvar2/n2))         \n    #Compute and Save “bootstrap t” value\n  }\n  return(bootstat)\n}\n\n\n\n41.10.3 Example: A mean experiment on crickets\nIn this experiment the treatmetn group of crickets were starved and we observed the time it took for the starved females to mate with males (they get to eat them after mating - do hungry females mate faster on average?)\n\nStarved&lt;-c(1.9, 2.1, 3.8, 9.0, 9.6, 13.0, 14.7, 17.9, 21.7, 29.0, 72.3)\nFed&lt;-c(1.5, 1.7, 2.4, 3.6, 5.7, 22.6, 22.8, 39.0, 54.4, 72.1, 73.6, 79.5, 88.9)\nboxplot(Starved, Fed)\n\n\n\n\nA permutation test would make the null hypothesis assumption that the two populations are identically shaped, and we can see that really is not the case. No need to test that.\n\nt.Obs &lt;- (mean(Starved)-mean(Fed))/sqrt(var(Starved)/length(Starved)+var(Fed)/length(Fed))\n\nboot.t &lt;- boottwo(Starved, Fed, 10000)\nhist(boot.t)\nabline(v=t.Obs)\n\n\n\n#2-tailed p-value\nmean(boot.t&lt;t.Obs)*2\n\n[1] 0.1194"
  },
  {
    "objectID": "testing1.html",
    "href": "testing1.html",
    "title": "13  Introduction to Statistical Testing",
    "section": "",
    "text": "13.1 Learning objectives\nAfter this lecture, you will be able to",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#learning-objectives",
    "href": "testing1.html#learning-objectives",
    "title": "13  Introduction to Statistical Testing",
    "section": "",
    "text": "Explain the basic setup of hypothesis testing, what a null hypothesis is, and what a test statistic is.\nExplain the interpretation of p-values as a measure of how “surprising” data is under a given null hypothesis.\nExplain the difference between parametric and non-parametric tests.\nExplain the permutation test and use it to test appropriate null hypotheses.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#motivating-example-the-lady-tasting-tea",
    "href": "testing1.html#motivating-example-the-lady-tasting-tea",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.2 Motivating example: the lady tasting tea",
    "text": "13.2 Motivating example: the lady tasting tea\nThere is a story from the early days of modern statistics about a British woman who claimed that she could tell how her tea was prepared. Specifically, she claimed that she could tell whether milk was added to her tea cup before or after the tea itself was brewed.\nThis British woman was Muriel Bristol, who was working as a researcher at the Rothamsted Experimental Station in Great Britain. It so happened that among the other researchers working at the research station was R. A. Fisher, one of the founders of modern statistics.\nFisher and others at the research station were interested in finding a way to test whether Bristol’s claims were true. They devised a test wherein Bristol would be given eight cups of tea, four prepared with milk first, and four with milk added after the tea. Bristol would then be permitted to taste the eight cups of tea and would have to identify which four cups of tea she thought were prepared with milk first.\nWhen they actually conducted this experiment, Bristol successfully identified the four “milk-first” cups of tea.\nThe question is: was Bristol actually able to taste the difference in tea or was she just lucky?\nSaid another way: how well does Bristol have to do on this task to convince us that she isn’t just a lucky guesser?",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#better-than-chance",
    "href": "testing1.html#better-than-chance",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.3 Better than chance?",
    "text": "13.3 Better than chance?\nThe natural place to start here is to think about what we would expect to happen if Bristol were just guessing at random.\nLet’s write code to simulate the experiment and see what happens.\nFirst, we need to generate cups of tea. We’ll represent a milk-first cup as a 1, and a “milk-second” cup as a 0.\n\ngenerate_cups &lt;- function(n,k) {\n  # n cups of tea; choose k of them to be \"milk-first\".\n  # We'll represent a milk-first cup as a 1, and a \"milk-second\" cup as a 0.\n  cups &lt;- c( rep(1,k), rep(0,n-k) ); # k milk-first, n-k milk-second.\n  # Now, randomize the order of the cups and return it.\n  # Randomize by sampling all n cups, WITHOUT replacement.\n  return( sample( cups, size=n, replace=FALSE ) );\n}\n\ngenerate_cups( 8, 4); # Generate eight cups with four \"milk-first\"\n\n[1] 1 0 0 0 1 1 1 0\n\n\nNow, we need to generate Muriel Bristol’s guesses, in the (imaginary/hypothetical) situation where she guesses at random.\n\ngenerate_guesses &lt;- function( n,k ) {\n  # Generate Muriel Bristol's guesses.\n  # Conveniently, randomly guessing cups is the same as \"generating\" cups!\n  return( generate_cups(n,k) )\n}  \n\nOkay, now, the last function we need: given the true cups and given Bristol’s guesses, we need to count how many she got right.\n\ncount_correct &lt;- function( true_cups, guess_cups ) {\n  # NOTE: We're assuming both vectors are binary and have the same length.\n  # In \"real-world\" or \"production\" code, we would include code to check\n  # that these assumptions are true and raise an error or warning to tell the\n  # user if these assumptions fail,\n  # but that's a matter for your programming courses.\n  return( sum( true_cups==guess_cups) )\n}\n\n# Example\ntruecups &lt;- generate_cups( 8, 4 )\nguesscups &lt;- generate_guesses( 8,4 )\ncount_correct( truecups, guesscups )\n\n[1] 6\n\n\nNow, we’re ready. Let’s generate lots of random instances of our experiment, and see how often a randomly-guessing Muriel Bristol would get all eight cups right.\n\nNMC &lt;- 2000;\n\nncorrect &lt;- rep(0, NMC); # We'll store results in this vector\nfor (i in 1:NMC ) {\n  truecups &lt;- generate_cups( 8, 4 );\n  guesscups &lt;- generate_cups( 8,4 );\n  ncorrect[i] &lt;- count_correct( truecups, guesscups );\n}\n\nhist( ncorrect )\n\n\n\n\n\n\n\n\nOkay, we could certainly make that histogram look a little nicer, but that’s not the point right now. The vast majority of the time, a randomly-guessing Muriel Bristol does not get all eight cups correct– she usually gets between two and six cups correct, and most of the time she gets four correct. This isn’t shocking. We expect that on average, random guessing should get half of the cups right.\nNow let’s recall that the real-life Muriel Bristol correctly guessed all eight of the cups correctly. In light of the above histogram, do we think it is likely that Muriel was guessing randomly? According to our Monte Carlo simulation, the probability of getting all eight cups correct when guessing randomly is\n\nsum( ncorrect==8 )/NMC\n\n[1] 0.014\n\n\nSo, if Muriel was guessing completely randomly, she must have gotten awfully lucky!",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#wait-that-was-hypothesis-testing",
    "href": "testing1.html#wait-that-was-hypothesis-testing",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.4 Wait, that was hypothesis testing!",
    "text": "13.4 Wait, that was hypothesis testing!\nWe didn’t make a big deal of it, but what we just did was an example of hypothesis testing, and that probability there at the end was actually a p-value!\nLet’s think about what we did:\n\nwe imagined that Muriel Bristol was just guessing at random\nwe simulated what would have happened if Bristol had indeed been guessing at random\nwe compared what actually happened (i.e., our real data– Bristol guessing all eight cups correctly) to the simulation\n\nIn this case, we found that Bristol’s eight correct cups are fairly unlikely if she had been guessing at random. This is evidence against the hypothesis that she was guessing completely randomly.\nLet’s think through that last point carefully–\n\nif Bristol guesses at random, we expect (based on our simulations, but also just based on common sense) that she is very unlikely to get all eight cups correct.\nWe then observed that Bristol got all eight cups correct.\nBut this is very unlikely if Bristol was guessing randomly.\nSo we conclude that Bristol was probably not guessing randomly.\n\nThis is the basic idea behind hypothesis testing!\n\nWe start with a null hypothesis, and we determine what kinds of observations we would expect to see if that null hypothesis were true.\nWe collect our data.\nWe compare our observed data to what we would expect to be true under the null.\nIf our observed data is “unlikely” or “unusual” under the null, then this is evidence against the null hypothesis.\n\nNow, of course, the hard/interesting part of hypothesis testing is that we can actually assign probabilities to these various notions of “unlikely” and such, but at a conceptual level, that’s all there is to it!",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#formalizing-slowly",
    "href": "testing1.html#formalizing-slowly",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.5 Formalizing (slowly)",
    "text": "13.5 Formalizing (slowly)\nSo let’s try and formalize what we did a little bit.\nWe started by assuming a null hypothesis in the form of a model that describes one way that the data could have come to be. We often write this null hypothesis \\(H_0\\) (“H null” or “H nought”).\nWe then try to assess how likely it would be, under this null hypothesis \\(H_0\\), to see the data that we actually saw. We measure this according to a p-value. A p-value is a probability, measured under the null hypothesis of seeing the data “at least as extreme” or “at least as unusual” as the data we observed. If this probability is suitably small, we declare that we reject the null hypothesis.\nSo, in our example above, our null hypothesis was that Muriel Bristol was guessing completely at random. We used Monte Carlo to estimate the probability that a randomly-guessing Muriel Bristol would correctly identify all of the “milk-first” cups of tea. We estimated the probability to be about 0.0143 (the exact value in the code above will change each time we rerun it, of course, so don’t fret if this number isn’t exactly what you saw above).\nThis probability is our p-value. It captures how likely the observed data would be under our model. Since this probability is fairly small, this constitutes evidence against the null hypothesis. Depending on other considerations, which we will return to soon, we might decide to reject the null hypothesis in light of this evidence. Otherwise, we would conclude that there was insufficient evidence to reject the null hypothesis and decide to accept the null hypothesis.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#another-example-vaccine-efficacy",
    "href": "testing1.html#another-example-vaccine-efficacy",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.6 Another example: vaccine efficacy",
    "text": "13.6 Another example: vaccine efficacy\nLet’s go back to Moderna’s vaccine trial from the Fall of 2020.\nThere were approximately 30,000 people in the trial, split (approximately) evenly between treatment (got the vaccine) and control (got a placebo).\n\nIn total, there were 95 cases of COVID-19 among the participants; 90 among the placebo group and 5 among the treated group.\nOf the 95 cases, 11 were severe cases, all in the placebo group.\n\nSuppose we have been asked to assess, using this data, whether or not the Moderna vaccine “works”, in the sense that vaccinated individuals are less likely to become ill with COVID-19.\nFor simplicity, let’s assume that the treatment and control groups were of the exact same size (I promise that this simplifying assumption will have no serious bearing on our conclusion).\nNow, if the vaccine did nothing, then we would expect that any particular case of COVID would be equally likely to occur in either the treatment or control group. That is, of the 95 cases that occurred, each of the 95 are equally likely to have come from the treatment or control group.\nSaid another way, if the vaccine did nothing at all, then infecting 95 people is just like picking 95 people at random from the whole study– the control and treatment groups put together.\nUnder that assumption, our null hypothesis can be encoded in a simulation (which is really just a model!) as follows:\n\ngenerate_data &lt;- function( ncontrol, ntreatment, ncases ) {\n  # Simulate data.\n  # Generate cases in a collection of ncontrol+ntreatment study participants.\n  # We will encode control participants as 1 and treatment as 0.\n  participants &lt;- c( rep(1,ncontrol), rep(0,ntreatment));\n  # Now, choose ncases participants at random.\n  # These are the participants who are randomly infected with COVID. Bummer!\n  # Note that under our null model, treatment and control participants are all\n  # equally likely to be infected.\n  # Draw WITHOUT replacement: we want to choose ncases participants total.\n  infected &lt;- sample( participants, size=ncases, replace=FALSE )\n  # We'll just return the number of control subjects who got infected.\n  # returning sum(infected) works because the controls are\n  # encoded as 1s in the vector `participants`.\n  return( sum(infected) );\n}\n\n# Generate for the case of 15K controls, 15K treatment, 95 cases,\n# like in the Moderna data.\ngenerate_data( 15000, 15000, 95 )\n\n[1] 47\n\n\nLet’s generate data a bunch of times and create a histogram, similar to with our tea-tasting example.\n\nNMC &lt;- 2000; # 2000 Monte Carlo replicates of our experiment.\n\ncase_counts &lt;- rep( 0, NMC );\nfor( i in 1:NMC ) {\n  case_counts[i] &lt;- generate_data( 15000, 15000, 95 );\n}\n\nhist( case_counts )\n\n\n\n\n\n\n\n\nNow, in the actual trial data, there were 90 cases in the placebo group. Let’s add that point on the histogram.\n\nhist( case_counts, xlim=c(min(case_counts), 100) )\nabline( v=90, lw=3, col='red' )\n\n\n\n\n\n\n\n\nHmm… so our null model generated control case counts that are usually between 30 and 65. That is, typically, between 30 and 65 of the 95 COVID cases are in the control group when infections are completely random (i.e., treatment and control are equally likely to be infected).\nOur actually observed data had 90 of the 95 cases in the control group, which is way outside the norm for our null model, according to our simulations.\nWhat do we think about the null hypothesis, now? Is our data likely under the null hypothesis that cases are equally likely in the control and treatment groups?\nWell, it certainly seems like our observed data is unlikely under our null hypothesis that the Moderna vaccine has no effect. But just how unlikely?\nWe could do a bunch of algebra to figure out just how likely our observed data is under the null, but let’s leave that to your probability classes. Instead, let’s try and estimate it using Monte Carlo. Let’s generate more data, just like our simulation above, and count how often we have 90 or more control subjects in our group of 95 sick patients.\nJust a heads up: running this code may take some time– we’re generating a LOT of MC replicates. You might want to decrease the 1e5 down to more like \\(5000\\) before running this for yourself, or else be prepared to sit for a few minutes while it runs.\n\nNMC &lt;- 1e5; # 100K MC replicates\n\natleast90 &lt;- 0; # We'll keep track of how often we get 90 or ore infected.\nfor( i in 1:NMC ) {\n  if( generate_data( 15000, 15000, 95 ) &gt;= 90 ) {\n    atleast90 &lt;- atleast90 + 1;\n  }\n}\n\n# To estimate our probability, what fraction of our trials \"succeeded\"?\ncat(atleast90/NMC)\n\n0\n\n\nWell, unless something rather surprising happened when knitting this document, our estimated probability there should be… 0.0.\nNow, is the true probability actually zero? No, of course not.\nBut the true probability is really tiny.\nThe true probability is given by (this calculation can wait for your probability classes; just take it as a given for now; look up the “hypergeometric distribution” if you’re curious)\n\n# This is computing the probability that we see 90,91,92,93,94 or 95\n# of the 95 infections in the placebo group, under the setting where\n# the 95 infections are assigned randomly to the treatment and control groups.\nsum( dhyper(c(90,91,92,93,94,95), 15000,15000, 95) )\n\n[1] 1.372347e-21\n\n\nDang, that’s a tiny number!\nSuffice it to say, then, that the probability of seeing the data that we did (or data even more “unusual”) under our null hypothesis is tiny. Something like \\(p = 1.37 \\times 10^{-21}\\). That’s a p-value– again, it encodes the probability, under the null hypothesis of seeing data at least as “extreme” or “unusual” (here “extreme” can be interpreted to mean “unlikely”).\nThis is (strong!) evidence against the null hypothesis. If the null hypothesis were true, we would be very unlikely to see the data that we did, hence we conclude that the null hypothesis is probably not true. After all, if it were true, we would (probably) not have seen the data that we did.\nIt is pretty reasonable to, in light of this evidence, reject the null hypothesis that the Moderna vaccine does nothing to change susceptibility to COVID-19.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#formalizing-for-real-this-time",
    "href": "testing1.html#formalizing-for-real-this-time",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.7 Formalizing (for real, this time)",
    "text": "13.7 Formalizing (for real, this time)\nOkay, let’s try and turn our ideas above into a bit more of a proper recipe or algorithm for performing hypothesis testing. This follows a framework called Neyman-Pearson hypothesis testing.\nWe begin by specifying the null hypothesis. Usually, this is a model that describes how things would look if there were “no effect” or if there were “no difference” between groups that we are comparing.\n\nIn the case of the tea tasting example, the null hypothesis was that the taster was guessing completely at random.\nIn the case of the vaccine trial, the null hypothesis was that subjects in the treatment and control groups were equally likely to get infected with COVID.\n\nHaving specified our null hypothesis, we come up with a measure of how “usual” or “unusual” data looks under the null hypothesis.\n\nIn the case of our tea tasting example, this was just the number of cups that the taster guessed correctly– if this number was especially large, that would be evidence that the taster was doing much better than chance would dictate (and, I suppose, if this number is especially small, this would be evidence that the taster is especially bad at guessing).\nIn the case of our vaccine trial, this was just the number of infected subjects in the control group. If this number were especially far from about 47 (\\(\\approx 95/2\\)), that would be an indication that something was different between the two groups.\n\nNow, having come up with a measure of how “weird” a particular set of data might be, we want to just look at our observed data and check– was our data “weird” under the null hypothesis? But how “weird” is weird? To assess this in our examples above, we used Monte Carlo simulation to generate lots of “fake” data sets from the model. These Monte Carlo replicates provide a perfect baseline for what data should look like under the null hypothesis– after all, we are generating it from the null model ourselves!\nLater on, we’ll see different ways of assessing how “usual” or “unusual” our data is (and if you think back to STAT240, you saw another way, in the context of the t-test, which we’ll revisit soon), but for now, Monte Carlo simulation is the only tool in our toolkit.\nSo, we can generate lots of copies of what the data would look like if the null hypothesis were true, and we can apply our measure of “weirdness” to each of these fake data sets\n\nIn our tea tasting example, we generated random orderings and guesses of the eight cups of tea and counted how many cups were guessed correctly.\nIn our vaccine example, we generated lots of random choices of which 95 of the 30K participants got sick with COVID.\n\nSometimes, having performed the above steps up to here, it will be quite clear that the null hypothesis can’t really be correct– think about our vaccine data, where 90 of 95 COVID cases being in the control group was just so far outside the norm. More often, though, it will not be quite so obvious, and we will want to be able to quantify how usual or unusual our observed data is under the null model. This is the role of p-values.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#introducing-p-values",
    "href": "testing1.html#introducing-p-values",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.8 Introducing p-values",
    "text": "13.8 Introducing p-values\nWe’ve already seen p-values in our examples above, and you’ve hopefully seen them before in STAT240 or other introductory courses, but just in case you haven’t seen them (or if you just don’t remember), let’s start from the very beginning.\nWe want a way of capturing how “unusual” our observed data is under the null hypothesis. A p-value does precisely this– the p-value is the probability under the null hypothesis of seeing data at least as extreme as our observed data (if you prefer, substitute “unusual” for “extreme”, there).\nIn symbols, suppose that we have a function \\(T\\) that takes our data and spits out a number describing how “unusual” the data is. So, if \\(D\\) is our data (\\(D\\) could be a sample of observations, but it could be lots of other things too, so let’s keep it purposefully vague for now), then \\(T(D) \\in \\mathbb{R}\\) is a number that describes how “unusual” or “extreme” the data is.\nThis \\(T\\) function is called our test statistic, because it is a summary of our data (i.e., a statistic) that we use for… our test.\nWe’ll just arbitrarily decide for now that larger values of \\(T(D)\\) mean that the data was “more unusual”, though in some situations it might be reversed– it depends!\nRemember, the null hypothesis is really just a statistical model that describes how the data got generated. The p-value is the probability under that null hypothesis of seeing data at least as extreme/unusual as the data we actually saw.\nSo, if the actually observed data is \\(d\\) with test statistic \\(T(d)\\), and \\(D_0\\) is another random copy of the data generated under \\(H_0\\), the p-value associated to our observed data under the null hypothesis \\(H_0\\) is \\[\n\\Pr[ T(D_0) \\ge T(d); H_0 ],\n\\] where the semicolon notation is just to stress that this is the probability under the null model.\nReading out that probability is exactly the definition of a p-value that we laid out just a moment ago– the probability under the null hypothesis, that we observe data (i.e., \\(D_0\\), generated from the null model) that is at least as extreme (hence the \\(\\ge\\) symbol) as the data that we actually observed (\\(d\\)).\nNow, sometimes it is possible to compute this p-value (i.e., the probability above) exactly, but often it isn’t. Lucky for us, we have spent a lot of time so far this semester talking about a set of tools for approximating probabilities like the one above– Monte Carlo simulation!\nEven if we can’t compute the p-value exactly, we can simulate data from the null model lots of times and just count how often the data is at least as extreme as the data we actually saw. That’s exactly what we did in our two examples above– we generated data from the null model, and counted how often the data that was generated was as unusual as our observed data.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#example-drug-trials",
    "href": "testing1.html#example-drug-trials",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.9 Example: drug trials",
    "text": "13.9 Example: drug trials\nHere is some (fictionalized) data from a medical trial. 40 subjects were recruited to the study and assigned randomly to either control or treatment (20 subjects in each group). Subjects were measured according to how well they responded to treatment. Let’s say that larger numbers are “better” for the sake of example.\nHere is the data:\n\ncontrol &lt;- c(-0.10, -0.55, 1.24, -0.97, -0.76,  0.21,-0.27, -1.02, 0.58, 1.67, -1.07, 0.17, 1.45, 0.34, 1.15, 0.18, -0.97, 0.43, -1.39, -2.76 );\n\ntreatment &lt;- c( 0.54, 0.36, 0.59, -0.57, 0.53, -0.78, -0.44, -0.98, 1.31, 0.50, 0.57, 0.49, -0.96, 2.41, 0.85, 1.93, 0.95, 1.45, 1.61, -1.16 );\n\nHow might we go about testing the hypothesis \\[\nH_0 : \\text{ treatment has no effect. }\n\\]\nQuestion: let’s pause and discuss what this null hypothesis actually means.\nThat is, “in math”, or in terms of our model of how the data came to be, what do we mean when we say that treatment has no effect?\nDo we mean that the control and treatment groups have exactly the same distribution?\nOr do we mean just that they have the same mean? or the same variance, or the same kurtosis or…?\nDifferent answers to this question might (and often do!) lead us to use different testing procedures.\n\n13.9.1 Reminder: parametric testing\nIn STAT240, you may have seen the simple setting where we assume that the two groups are both normally distributed with the same known variance but (possibly) different means.\nAlternatively (again, depending on when you took STAT240), you might have seen (unpaired) two-sample t-tests, where we assumed that the data came from a normal distribution, but with an unknown variance (Student’s t-test) or two normals with the same mean but possibly different variances (Welch’s t-test).\nIn either of these cases, we might write \\[\nH_0 : \\mu_{\\text{control}} = \\mu_{\\text{treatment}},\n\\]\neven though the model assumptions that we make are different for these two different null hypotheses.\nThe important point here is to be careful about exactly what assumptions we are making about the data when we conduct a test.\nIn the case of the testing examples that you saw in STAT240, we always assumed that the data came from a specific model, then we computed a quantity that is motivated by the parameters of that model (i.e., we are secretly trying to estimate certain parameters of the model, a topic we’ll discuss more soon). So here we might do something like the following to conduct an unpaired two-sample t-test (in particular, here we’re assuming that under the null, the two groups came from normals with the same mean and the same variances):\n\ns_control &lt;- sd( control );\ns_treatment &lt;- sd( treatment );\ns_pooled &lt;- sqrt( (s_control^2 + s_treatment^2)/2 );\n# Form the t-statistic. See, e.g.,\n# https://en.wikipedia.org/wiki/Student's_t-test#Unpaired_and_paired_two-sample_t-tests\nn &lt;- length(control); # n = n_1 = n_2, i.e., same sample sizes\nt &lt;- (mean(treatment)-mean(control))/(s_pooled*sqrt(2/n));\nt\n\n[1] 1.743728\n\n\nAnd we could compute the degrees of freedom of this t-statistic (\\(n_1 + n_2-2 = 78\\), in this case), and use a t-table to determine a p-value:\n\n1 - pt( t, df=length(control)+length(treatment)-2)\n\n[1] 0.04464634\n\n\nThat’s a reasonably small number, indicating that our observed data is not very likely under the null hypothesis that the means are the same.\nNow, the nitty-gritty details of the above (e.g., whether to use Welch’s t-test vs Student’s t-test) are not important, here (Note: that isn’t to say that it isn’t important! Just that it isn’t the point right now), but this general procedure should be at least broadly familiar to you from STAT240 and/or other intro courses.\nThis is an approach called parametric testing. We assume a parametric model for the data. That is, a model where the data is generated according to random variables whose distributions are controlled by parameters (e.g., the normal, binomial, etc.). The process of statistical testing then amounts to fitting a parametric model, by estimating those parameters from the data. That is why, for example, the sample means and standard deviations show up in the t-statistic.\nHaving fit the parameters from the data, we have a model– the model that “best fits” the data (again, we’ll have lots to say soon about exactly what that means, but let’s leave it vague for now). We can then use that model to assess how likely our actually observed data is.\nThis is a perfectly fine approach, especially when we have good reason to believe that our data came from a specific distribution (or is well-approximated by such a distribution– all models are wrong!).\nBut what if those assumptions are unrealistic, or if we just don’t want to make those assumptions?\nThe examples we’ve seen so far in this lecture (well, before our t-statistic example just now) rely on a different approach, called non-parametric testing, so-called because we are not making any parametric assumptions about where our data came from. That is, we are not assuming anything about our data coming from a normal, or Bernoulli or Poisson or etc.\nLet’s look at a more general non-parametric test to get a better idea of what we mean.",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing1.html#permutation-tests",
    "href": "testing1.html#permutation-tests",
    "title": "13  Introduction to Statistical Testing",
    "section": "13.10 Permutation tests",
    "text": "13.10 Permutation tests\nWithout a doubt the most common and widely used non-parametric test is the permutation test.\nTo illustrate it, let’s recall our (fictional) drug trial data.\n\ncontrol &lt;- c(-0.10, -0.55, 1.24, -0.97, -0.76,  0.21,-0.27, -1.02, 0.58, 1.67, -1.07, 0.17, 1.45, 0.34, 1.15, 0.18, -0.97, 0.43, -1.39, -2.76 );\n\ntreatment &lt;- c( 0.54, 0.36, 0.59, -0.57, 0.53, -0.78, -0.44, -0.98, 1.31, 0.50, 0.57, 0.49, -0.96, 2.41, 0.85, 1.93, 0.95, 1.45, 1.61, -1.16 );\n\nWe want to test the null hypothesis that the drug has no effect, but we want to do so without making any assumptions about the specific distribution (e.g., normal, exponential, Poisson, etc.) that gave rise to the data. That is, we want to conduct a non-parametric test of the hypothesis \\[\nH_0 : F_{\\text{control}} = F_{\\text{treatment}},\n\\]\nwhere \\(F_{\\text{control}}\\) is the distribution of the control subjects and \\(F_{\\text{treatment}}\\) is the distribution of the treatment subjects.\nWe need to start with a test statistic. Our test statistic should capture how different or alike our two samples are. There are lots of possible choices, here, but what better choice than the difference in the means?\nLet’s start by computing the means of the two groups:\n\nXbar_ctl &lt;- mean(control);\nXbar_trt &lt;- mean(treatment);\nc( Xbar_ctl, Xbar_trt )\n\n[1] -0.122  0.460\n\n\nThe difference between these two means is\n\nXbar_trt - Xbar_ctl\n\n[1] 0.582\n\n\nWell, is that a big difference or not? As we’ve seen, the whole point of statistical testing is to assess whether or not this is statistically big (and, okay, we already looked at this with the t-test above, but let’s press on…).\nThe permutation test is a very general tool for testing this.\nLet’s recall that our null hypothesis is that the control and treatment patients have the same distribution.\nThe basic recipe for statistical hypothesis testing is to assume this null is true and then generate data as it would look if that null were true.\nSo, the permutation test assumes that the two groups, control and treatment, came from the same distribution (\\(H_0: F_{\\text{control}} = F_{\\text{treatment}}\\)), and generates copies of what the data could have looked like if the null were true.\nMillion dollar question: How do we do that, if we don’t make any assumptions about what that distribution looks like (i.e., normal, binomial, etc)?\nWell, if the data all came from the same distribution (again, \\(H_0: F_{\\text{control}} = F_{\\text{treatment}}\\)), then any reshuffling of the data was equally likely– any observation in the control group was just as likely to be seen in the treatment group, and vice versa.\nOkay, but how does that give us a distribution? Well, it doesn’t.\nTo estimate the distribution of the test statistic under the null distribution, the permutation test repeatedly permutes the data, reassigning the observations to groups randomly, and compares the means again.\nSaid another way, we repeatedly reshuffle the data, reassigning the observations randomly to the treatment and control groups, and recomputing the test statistic.\nUnder the null hypothesis, all of these “reshufflings” of the data were equally likely, so all of these test statistics are drawn from the null distribution!\nImportant point: the permutation test is a Monte Carlo method! To estimate the behavior of the test statistic under the null, we are going to repeatedly reshuffle the data and recompute the difference of the means!\nOkay, let’s go ahead an implement that. We want a function in R that\n\nReshuffles the control and treatment data randomly\nAssigns the shuffled data to control and treatment groups of the same sizes as the original ones\nComputes our test statistic on the reshuffled data (in this case, we’re taking the test statistic to be the difference of means, but we could make a different choice if we really wanted).\n\n\npermute_and_compute &lt;- function( ctrl_data, trmt_data ) {\n  # ctrl_data and trmt_data are vectors storing our control and treatment data\n  # We are going to pretend that these two data sets came from the same\n  # distribution. That means\n  # 1) pooling them and randomly reassigning them\n  #     to the treatment and control groups.\n  # and then\n  # 2) Computing our test statistic (the difference in means) on that\n  #     new \"version\" of the data.\n  \n  # Pool the data\n  pooled_data &lt;- c( ctrl_data, trmt_data );\n  # Randomly shuffle the data and assign it to control and treatment groups.\n  n_ctrl &lt;- length( ctrl_data );\n  n_trmt &lt;- length( trmt_data );\n  n_total &lt;- n_ctrl + n_trmt;\n  # Now, let's shuffle the data, and assign the first n_ctrl elements\n  # to the control group, and the rest to the treatment group.\n  # We're going to do this using the sample() function.\n  # To randomly shuffle the data, it's enough to sample from the\n  # original data (i.e., the pooled data) WITHOUT replacement.\n  # The result is that shuffled_data contains the same elements as\n  # pooled_data, just in a (random) different order.\n  shuffled_data &lt;- sample( pooled_data, size=n_total, replace=FALSE );\n  # Now, the first n_ctrl of these data points are our new control group\n  # and the remaining elements are assigned to our treatment group.\n  shuffled_ctrl &lt;- shuffled_data[1:n_ctrl];\n  shuffled_trmt &lt;- shuffled_data[(n_ctrl+1):n_total];\n  # Okay, last step: compute the difference in means of our two samples.\n  return( mean(shuffled_trmt)-mean(shuffled_ctrl) );\n}\n\nNow, the permutation test just says we should do that shuffling operation that we just implemented lots of times. That will give us a bunch of Monte Carlo replicates of the behavior of our test statistic under the null, and we can compare the test statistic that we actually observed to that null distribution.\nHopefully this recipe is familiar by now– we generate lots of data under the null hypothesis, compute the test statistic on each of those “fake” data sets, and compare our actually observed test statistic to the distribution of our “fake” data.\nSo let’s try that.\n\nNMC &lt;- 1e4; # Might want to increase this to more like 1e5 for better accuracy\ntest_statistics &lt;- rep( 0, NMC ); # Vector to store our \"fake\" test statistics\n\n# Now, NMC times, shuffle the data, recompute the test statistic, and record.\nfor(i in 1:NMC ) {\n  test_statistics[i] &lt;- permute_and_compute( control, treatment );\n}\n\n# Now, let's make a histogram of those permuted test statistics.\nhist( test_statistics )\n\n\n\n\n\n\n\n\nAnd now let’s put our actually observed test statistic in that plot.\n\nhist( test_statistics )\nabline( v=mean(treatment)-mean(control), lw=3, col='red' )\n\n\n\n\n\n\n\n\nWell, that looks at least reasonably extreme.\nCan we associate this with a p-value? Of course! At least, an estimated p-value (why? because that histogram is just simulated data! not the exact distribution!).\nRemember, we just need to estimate the probability that test statistic under the null is at least as big as our observed test statistic.\nThat is, we want to estimate \\(\\Pr[ T(D) \\ge t]\\), where \\(T(D)\\) is the test statistic under the null and \\(t\\) is the actually observed test statistic.\nWe estimate that probability with Monte Carlo just like we saw last week. We just count how often that event happened in our simulation:\n\nTobsd &lt;- mean(treatment) - mean(control); # Actually observed test statistic\n\n# Monte Carlo estimate: how often in our simulation did the \"fake\" data\n# have a difference in means greater than or equal to Tobsd?\nsum(test_statistics &gt;= Tobsd)/NMC;\n\n[1] 0.0443\n\n\nSo that’s a (approximate! don’t forget that this is estimated using Monte Carlo!) p-value of \\(0.0434\\). That’s a reasonably small probability that the difference in distributions is merely due to chance.\nOnce again, this is evidence against the null hypothesis that the control and treatment groups had the same distribution.\nIf they did have the same distribution, we would expect our observed difference in means to be quite a bit smaller– as things stand, our observed test statistic appears fairly unlikely under the null!\nNext week’s lectures will focus on how we decide how unlikely is unlikely enough for us to “reject” the null hypothesis.\n\n13.10.1 Review:\nIn these notes we covered:\n\nFormulating a decision into a null and alternative hypothesis\nRandomization tests (model-based)\nPermutation tests (two-sample)\nSimulating a distribution of test statistics\nCalculation of one or two-tailed \\(p\\)-values from simulated distribution of test statistics\nInterpretation of a \\(p\\)-value",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Introduction to Statistical Testing</span>"
    ]
  },
  {
    "objectID": "testing2.html",
    "href": "testing2.html",
    "title": "16  Statistical Testing, Continued",
    "section": "",
    "text": "16.1 Learning objectives\nAfter this lesson, you will be able to",
    "crumbs": [
      "Testing",
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Statistical Testing, Continued</span>"
    ]
  },
  {
    "objectID": "estimation1.html",
    "href": "estimation1.html",
    "title": "19  Estimation Part 1",
    "section": "",
    "text": "19.1 Learning objectives\nAfter this lesson, you will be able to",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estimation Part 1</span>"
    ]
  },
  {
    "objectID": "estimation2.html",
    "href": "estimation2.html",
    "title": "22  Estimation Part 2",
    "section": "",
    "text": "22.1 Learning objectives\nAfter this lesson, you will be able to",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#learning-objectives",
    "href": "estimation2.html#learning-objectives",
    "title": "22  Estimation Part 2",
    "section": "",
    "text": "Explain the concept of confidence intervals\nUse Monte Carlo methods to construct a confidence interval for the parameter of a distribution.\nExplain the central limit theorem and use it to construct a confidence interval for the parameter of a distribution\nExplain the connection between confidence intervals and hypothesis testing",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#recap-estimation-so-far",
    "href": "estimation2.html#recap-estimation-so-far",
    "title": "22  Estimation Part 2",
    "section": "22.2 Recap: estimation so far",
    "text": "22.2 Recap: estimation so far\nLast week, we began discussing the problem of estimation, where we are interested in determining the value of some quantity out there in the world (most commonly the parameter of a distribution). We focused in our previous lectures on point estimation, where we are concerned with simply giving a single number as our “best guess” of the true value of the parameter of interest. Still, we mentioned several times in lecture the fact that our “best guess”, i.e., our estimate, would itself be random because the data is random. The result of this randomness is that for any estimate that we might produce, we have level of “uncertainty” associated to that estimate.\n\n22.2.1 Example: gone fishing\nSuppose we are working for the Department of Natural Resources (DNR), tasked with estimating how many fish live in Lake Mendota. Our colleagues in ecology go out in the field and come up with an estimate of 10,848 fish in Lake Mendota (note: this number is completely fictional; indeed, I am not even sure that it is of the correct order of magnitude).\nPresumably, the exact number of fish in Lake Mendota changes from day to day, week to week, month to month, etc. So perhaps we would not object much if someone came along and revised this number from, say, 10,848 to 10,306. Perhaps we would not even object much if this estimate were revised by a thousand fish in one direction or the other. Still, there are a range of values around 10,848 that seem like “reasonable” guesses, and there are other values that seem pretty clearly “unreasonable”. For example, revising our estimate from approximately 10,000 fish down to 1,000 fish seems extreme.\nWhat we would like, ideally, is to provide a range of values for our estimate that capture our level of uncertainty (or, conversely, our level of certainty, i.e., … confidence) about this number. Specifically, we would like to produce an interval that is somehow “usually” correct.\nBut what does it mean for an interval to be “usually right”?",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#enter-confidence-intervals",
    "href": "estimation2.html#enter-confidence-intervals",
    "title": "22  Estimation Part 2",
    "section": "22.3 Enter: Confidence intervals",
    "text": "22.3 Enter: Confidence intervals\nSuppose we observe a particular value for our statistic \\(S\\). Remember, \\(S = S(X_1,X_2,\\dots,X_n)\\) is a function of our data and estimates our parameter of interest, say, \\(\\theta\\). We’ll assume further, for simplicity, that \\(\\mathbb{E} S = \\theta\\). That is, our estimator is unbiased for \\(\\theta\\). Note that this isn’t always the case, but further discussion of the case where \\(S\\) is biased will have to wait (and there’s no need to make things more complicated than they already are!).\nWe would like to be able to give an interval of values around our observed value of \\(S\\) such that \\(\\theta\\) is (with some level of certainty, anyway), in that interval.\nThat is, roughly speaking, the definition of a confidence interval. We’ll put that definition on firmer ground soon, but let’s start by building some intuition and recalling some ideas from earlier in the semester.\nFirst things first: \\(S = S(X_1,X_2,\\dots,X_n)\\) is a random variable, thus it has a distribution.\n\nIf we knew that distribution, we could compute \\(\\mathbb{E} S = \\theta\\) exactly\nOf course, in practice, we don’t know that distribution\n\nBut let’s imagine for now that we knew the true distribution of \\(S\\).\nExample: for the sake of easy math, let’s say that \\(S\\) is the sample mean of \\(n=25\\) independent draws from \\(\\operatorname{Normal}(\\mu=1, \\sigma=1)\\).\nThe true value of \\(\\mathbb{E} S\\) in this case is \\[\n\\newcommand{\\E}{\\mathbb{E}}\n\\E S = \\E \\frac{1}{n} \\sum_{i=1}^n X_i\n= \\frac{1}{n} \\sum_{i=1}^n \\E X_i\n= \\frac{1}{n} \\sum_{i=1}^n \\mu\n= \\mu = 1,\n\\]\nWhere we have used the facts that\n\n\\(S\\) is the sample mean, i.e., \\(S = \\sum_{i=1}^n X_i / n\\)\nThe expectation operator is linear: \\(\\mathbb{E} aX = a \\mathbb{E} X\\) and expectation of a sum is the sum of expectations\n\\(\\mathbb{E} X_i = \\mu\\) (because \\(X_i \\sim \\operatorname{Normal}(1,1)\\) for all \\(i\\))\n\nIn fact, in this case, we know the exact distribution of \\(S\\).\n\n\\(S\\) is the mean of \\(n=25\\) independent \\(\\operatorname{Normal}(1,1)\\) RVs\n\\(S \\sim \\operatorname{Normal}(\\mu=1,\\sigma=1/\\sqrt{n})\\) (you’ll prove this for yourself in a theory class in the future, for now refer to Wikipedia or any probability textbook if you want to see why this is true)\nPlugging in \\(n=25\\), \\(S \\sim \\operatorname{Normal}(\\mu=1,\\sigma=1/5)\\)\n\nLet’s use R to compute the exact quantiles of this distribution.\nReminder: We do that with the function qnorm. Analogously to rnorm, pnorm and dnorm, think “q for quantiles”\n\n# The first argument to qnorm is a vector of the quantiles we want (given as probabilities).\n# Asking for the 2.5% and 97.5% quantiles.\nqnorm( c(0.025, 0.975), mean=1, sd=1/5)\n\n[1] 0.6080072 1.3919928\n\n\nWhat this says is that if \\(Z \\sim \\operatorname{Normal}(\\mu=1, \\sigma=1/5)\\), then 2.5% of the time \\(Z\\) will be less than \\(\\approx 0.608\\), and 97.5% of the time \\(Z\\) will be less than \\(\\approx 1.392\\).\nPutting those facts together, we conclude that \\[\n\\Pr\\left[ 0.608 \\le S \\le 1.392 \\right] = 0.95.\n\\]\n95% of the time, \\(S\\) is between \\(0.608\\) and \\(1.392\\).",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#aside-wait-that-wasnt-a-confidence-interval",
    "href": "estimation2.html#aside-wait-that-wasnt-a-confidence-interval",
    "title": "22  Estimation Part 2",
    "section": "22.4 Aside: wait, that wasn’t a confidence interval!",
    "text": "22.4 Aside: wait, that wasn’t a confidence interval!\nNow, if you’ve seen confidence intervals before, you might see a small problem with this.\nThe definition of a \\((1-\\alpha)\\)-confidence interval for the parameter \\(\\theta\\) (for simplicity, just think of \\(\\theta\\) as the mean, \\(\\mu\\), here, or the probability \\(p\\) from our widgets example, but it could be any quantity of interest) is\n\na data-dependent interval \\(C(X_1,X_2,\\dots,X_n) = (L,U)\\) (\\(L,U\\) for “lower” and “upper”; they are functions of the data!),\nwith the property that when the data are drawn under the distribution with parameter \\(\\theta\\) (note: we write \\(\\Pr[ \\dots ; \\theta]\\) to drive home the fact that this is the probability under the model where \\(\\theta\\) is the true parameter),\n\n\\[\n\\Pr\\left[ \\theta \\in C(X_1,X_2,\\dots,X_n) ; \\theta \\right] = 1-\\alpha.\n\\]\nDo you see the issue? In our statements about \\(S\\) above, all the randomness was in \\(S\\), and our interval \\((0.608,1.392)\\) was not random at all. Here, on the other hand, all the randomness is in the interval \\(C\\).\nWhy this discrepancy?\nWell, the short answer is that when classical statistics (think Z-scores, confidence intervals, t-tests, etc) was invented, we didn’t have fast, cheap computers.\nWhat we are aiming to teach you in this course is what people like R. A. Fisher, Karl Pearson and W. S. Gossett (a.k.a. T. Student) would have done (I think, anyway) if they had laptops and access to cloud computing.\nOur goal here is to compute an interval that captures our level of uncertainty about our estimate. Roughly speaking, wider intervals will correspond to higher levels of uncertainty.\nThere are many different ways to construct such intervals, each with its own interpretation.\nThere are many approaches to this problem beyond the simulation-based approach we’re taking today. If you’re impatient to read more, this Wikipedia article is a good place to start. Any introductory statistics textbook will also have a good treatment of confidence intervals in particular, though I especially like John Rice’s book, Mathematical Statistics and Data Analysis.\nIf you are already familiar with confidence intervals, you should be able to convince yourself (after meditating on this and the next few sections) that this “simulation-based” approach and the classical confidence interval are, in a sense, two sides of the same coin.\nUnfortunately, the details of that are beyond the scope of our course, but if you’re taking a theoretical statistics course now or in the near future, keep it in mind!",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#capturing-uncertainty-in-our-estimate",
    "href": "estimation2.html#capturing-uncertainty-in-our-estimate",
    "title": "22  Estimation Part 2",
    "section": "22.5 Capturing uncertainty in our estimate",
    "text": "22.5 Capturing uncertainty in our estimate\nSo let’s return to our illustrative example above. Recall that\n\n\\(S\\) is the sample mean of \\(n=25\\) independent draws from \\(\\operatorname{Normal}(\\mu=1, \\sigma=1)\\).\n\\(\\mathbb{E} S = \\mu = 1\\).\n\\(S \\sim \\operatorname{Normal}(\\mu=1,\\sigma=1/5)\\)\n\nIn our experiment above, we computed an exact interval, \\(C = (0.6080072, 1.3919928)\\), such that with probability \\(0.95\\), \\(S \\in C\\).\nHow does that help us to express (un)certainty?\nWell, the standard deviation of \\(S\\) is \\(\\sigma/\\sqrt{n}\\). If we increase \\(n\\), the standard deviation of \\(S\\) decreases, and our resulting confidence interval gets narrower.\nFor example, if \\(n=100\\), then \\(S \\sim \\operatorname{Normal}(\\mu=1,\\sigma=1/10)\\), and our quantiles become\n\nqnorm( c(0.025, 0.975), mean=1, sd=1/sqrt(100) )\n\n[1] 0.8040036 1.1959964\n\n\nCompare that with our interval \\((0.6080072, 1.3919928)\\) when \\(n=25\\).\nThis is in keeping with the fact that as we collect more data (i.e., observe more samples), we get more confident about our predictions. Again, a narrower interval corresponds to a higher level of confidence.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#okay-but-we-dont-know-mathbbe-s",
    "href": "estimation2.html#okay-but-we-dont-know-mathbbe-s",
    "title": "22  Estimation Part 2",
    "section": "22.6 Okay, but we don’t know \\(\\mathbb{E} S\\)",
    "text": "22.6 Okay, but we don’t know \\(\\mathbb{E} S\\)\nWe have been assuming in our example above that we know the expectation of \\(S\\), which is… well, never true in practice.\nIndeed, we are assuming that we know the full distribution of \\(S\\), which is true even less often.\nIf we knew the true parameter (i.e., \\(\\theta = \\mathbb{E} S\\)), we could generate data from the “true” distribution like above.\nIn particular, we could see what data and/or what values of \\(S\\) are “typical” for that value of the parameter \\(\\theta\\).\nThis suggests a solution: let’s estimate our model (i.e., estimate the parameter \\(\\theta\\), presumably using \\(\\hat{\\theta} = S(X_1,X_2,\\dots,X_n)\\) as our estimate). Then, let’s generate data from our model, pretending that \\(S(X_1,X_2,\\dots,X_n)\\) is the true parameter, and see what values of our estimator are “typical”, by computing our statistic on the generated data. Said another way, we pretend to believe that our estimated model is the truth and generate data from it many times to get a sense of the “typical” behavior of \\(S\\).\nIn essence, we are doing the following:\n\nPretend for now that our estimated parameter is the truth\nGenerate data from our model with that parameter\nCompute the statistic \\(S\\) on that data.\n\nThis gives us multiple replicates of our data under the setting where our observed value of \\(S\\) is the truth.\nIn a sense, these samples are values of our statistic that would be “reasonable” if the true parameter were equal to \\(s= S(x_1,x_2,\\dots,x_n)\\), where \\(x_1,x_2,\\dots,x_n\\) are our observed data.\nAside: As we’ve mentioned before, this is a convention in statistics that upper-case letters denote random variables, while their lower-case equivalents denote the observed value of that random variable. So in this case, \\(X_1,X_2,\\dots,X_n\\) are the random variables observed in our sample, while \\(x_1,x_2,\\dots,x_n\\) would denote the specific values that they take on some particular draw of those variables.\nThis is a tricky distinction at first, so don’t worry if it takes some time to get used to!\n\n22.6.1 Example: Intervals for the Poisson rate parameter\nSuppose that we have data \\(X_1,X_2,\\dots,X_n\\) drawn i.i.d.~from a Poisson with unknown rate parameter \\(\\lambda\\), and our goal is to estimate \\(\\lambda\\).\nSince \\(\\mathbb{E} X_i = \\lambda\\) for all \\(i=1,2,\\dots,n\\), a reasonable choice of estimator is the sample mean \\(\\bar{X} = n^{-1} \\sum_i X_i\\). Indeed, the law of large numbers guarantees that \\(\\bar{X}\\) is close to \\(\\lambda = \\mathbb{E} X_1\\) with high probability.\nNow, let’s generate data from this model, this time imagining that we don’t know the parameter \\(\\lambda\\).\nInstead, we’ll estimate \\(\\lambda\\) from our data, and to get a “confidence interval” for \\(\\lambda\\), we will pretend that our estimate \\(\\bar{X}\\) is the truth and generate new copies of the data using \\(\\lambda = \\bar{X}\\).\n\nn &lt;- 80; # Sample size of our data\nlambda_true &lt;- 2.718; # True (but unknown!) Poisson parameter\n\ndata &lt;- rpois(n=n, lambda=lambda_true);\n\nhist(data, breaks=seq(-.5,max(data)+.5,1)); # Always good to look at your data!\n\n\n\n\n\n\n\n\nNow, let’s get our estimate (just the sample mean!), pretend it’s the true value of \\(\\lambda\\), and generate data.\n\n# Reminder: we say lambdahat to remind ourselves this is\n# an estimate of lambda.\nlambda_hat &lt;- mean(data); # Sample mean is our estimate of lambda.\n\n# Generate data under the distribution where lambda_hat is the rate parameter.\n# To reiterate, we're pretending that our estimate is the truth and seeing what our data would look like.\nfake_data &lt;- rpois(n=n, lambda=lambda_hat);\n\nhist(fake_data, breaks=seq(-.5,max(fake_data)+.5,1))\n\n\n\n\n\n\n\n\nIn our previous example, we worked out exactly what the distribution of our statistic was, because the math was easy.\nThe math is a bit less easy now. We could, if we wanted to, work out the exact sampling distribution of the sample mean under the Poisson.\nInstead of doing all that annoying math, though, let’s just use the computer.\nAgain, here’s the recipe:\n\nEstimate our distribution’s parameter from the data (we’ve done that with our sample mean lambda_hat)\nGenerate data from the distribution with parameter equal to our estimate (we just did that above)\nRepeat the previous step many times, computing a new estimate on each new data sample. Each repetition gives us a replicate of our statistic, under the distribution where \\(\\lambda = \\texttt{lambda_hat}\\).\nWe can use that to construct a confidence interval for \\(\\lambda\\), by looking at the quantiles of our sample.\n\nLet’s implement that:\n\n# Reminder: sample size and true parameter\nn&lt;- 80; lambda_true &lt;- 2.718;\n# Generate data from the true distribution\ndata &lt;- rpois( n=n, lambda=lambda_true);\nlambda_hat &lt;- mean(data); # Just estimating this again to remind you\n\nNrep &lt;- 2000; # Number of repetitions (i.e., replicates)\nreplicates &lt;- rep(NA,Nrep); # We will store replicates of lambdahat here.\nfor ( i in 1:Nrep) {\n  fake_data &lt;- rpois(n=n, lambda=lambda_hat);\n  replicates[i] &lt;- mean( fake_data );\n}\n\n# Now construct the confidence interval\nCI &lt;- quantile( replicates, probs=c(0.025, 0.975) );\ncat(CI)\n\n2.162187 2.8625\n\n\nMost of the time, when we run the code above, we should find that \\(\\lambda=2.718\\) is inside of that interval.\nSometimes, though, due to randomness in our data, the interval will not contain the true value of \\(\\lambda\\).\nNow, how often does that happen? Well, let’s try creating lots of intervals like the one above and count how often our interval contains the true value of \\(\\lambda\\).\nFirst, we’ll implement a function to run one instance of the experiment we just ran above.\n\nrun_trial &lt;- function(lambdatrue, n, Nrep) {\n  # Run one instance of our experiment above, wherein we\n  # 1) Generate data from Pois(lambda_true)\n  # 2) Estimate lambdahat from that data.\n  # 3) Repeatedly generate data from Pois(lambdahat)\n  # 4) Use those repetitions to get an interval.\n  # lambdatrue : the true lambda parameter for Poisson\n  # n : sample size for the data\n  # Nrep : repetitions to use when computing the interval\n  data &lt;- rpois(n=n, lambda=lambdatrue);\n  lambdahat &lt;- mean(data);\n  replicates &lt;- rep(NA,Nrep); # We will store replicates of lambdahat here.\n  for ( i in 1:Nrep) {\n    fake_data &lt;- rpois(n=n, lambda=lambdahat);\n    replicates[i] &lt;- mean( fake_data );\n  }\n\n  # Now construct the confidence interval\n  # the names=FALSE tells R to return the quantiles with no header\n  CI &lt;- quantile( replicates, probs=c(0.025, 0.975), names=FALSE );\n  return( (CI[1] &lt; lambdatrue) & (lambdatrue &lt; CI[2]) );\n}\n\nNow, let’s try repeating the experiment many times by calling run_trial(lambdatrue=2.718, n=80, Nrep=1000 ) a bunch.\nImportant note: This section of code might take a minute or two to run. Each time we call run_trial, we are generating n Poisson variables Nrep times. And here we are about to call run_trial many times (the variable is Nexpt below). So, in total, we’re generating n*Nrep*Nexpt random variables, and random variables aren’t cheap (think back to our discussion of randomness in our Monte Carlo lectures)!\n\n# NOTE: This section of code might take a minute or two to run!\nNexpt &lt;- 500; # Number of times to run the experiment\nexpt_results &lt;- rep(NA,Nexpt); # Vector to store results\nfor (i in 1:Nexpt) {\n  expt_results[i] &lt;- run_trial(lambdatrue=2.718, n=80, Nrep=1000)\n}\n# Count what fraction of the time we were correct.\nsum(expt_results)/length(expt_results)\n\n[1] 0.954\n\n\nThe result should be about .95.\nThis is not a coincidence– remember we chose the quantiles (0.025, 0.975) to mimic a 95% confidence interval. Our experiment shows that the intervals that we constructed above based on simulations are (approximately) 95% confidence intervals!\nLet’s do another worked example, revisiting the exponential distribution.\n\n\n22.6.2 Example: estimating the rate in the exponential distribution\nSuppose that we observe data \\(X_1,X_2,\\dots,X_n\\) i.i.d. from an exponential distribution with rate parameter \\(\\lambda\\), so that \\(\\mathbb{E} X_1 = 1/\\lambda\\), and our goal is to estimate \\(\\lambda\\).\nThe plug-in principal and/or the method of moments suggests that we use \\(1/\\bar{X}\\) as our estimate of \\(\\lambda\\).\nApplying our simulation-based CI recipe, we should:\n\nEstimate \\(\\lambda\\) as \\(\\hat{\\lambda} = 1/\\bar{X}\\).\nRepeatedly generate data as though \\(\\hat{\\lambda}\\) were the true value of the parameter, and “re-estimate” \\(\\lambda\\) on each such data set.\nUse the histogram of the resulting “fake” estimates to estimate our interval.\n\n\ntrue_rate &lt;- 5;\nnsamp &lt;- 25; #sample size in the data that we generate.\nobsd_data &lt;- rexp(n=nsamp, rate=true_rate);\n# Estimate the rate parameter lambda from the data.\n# Recall that the mean is 1/rate, so we can estimate the\n# rate by taking the sample mean Xbar, which estimates 1/rate,\n# and then using 1/Xbar as our estimate of the rate lambda.\nestd_rate &lt;- 1/mean(obsd_data);\n\nNMC &lt;- 2000;\nreplicates &lt;- rep(NA, NMC);\nfor ( i in 1:NMC) {\n  fake_data &lt;- rexp(nsamp, rate=estd_rate);\n  # We want to estimate the sampling distribution of the estimated\n  # rate parameter from the data.\n  replicates[i] &lt;- 1/mean(fake_data);\n}\n\n# Now construct the confidence interval\n# the names=FALSE tells R to return the quantiles with no header\nCI &lt;- quantile( replicates, probs=c(0.025, 0.975), names=FALSE );\nCI\n\n[1] 3.815043 8.516964\n\n(CI[1] &lt; true_rate) & (true_rate &lt; CI[2]) ;\n\n[1] TRUE\n\n\nOkay, let’s package that up in a function and run it a bunch of times to check whether or not our CI is achieve its stated rate of \\(0.95\\).\n\nrun_exprate_expt &lt;- function( nsamp, true_rate ) {\n  obsd_data &lt;- rexp(n=nsamp, rate=true_rate);\n  # Estimate the rate parameter lambda from the data.\n  # Recall that the mean is 1/rate, so we can estimate the\n  # rate by taking the sample mean Xbar, which estimates 1/rate,\n  # and then using 1/Xbar as our estimate of the rate lambda.\n  estd_rate &lt;- 1/mean(obsd_data);\n  \n  # Now generate \"fake\" data sets and estimate lambda on each.\n  NMC &lt;- 2000;\n  replicates &lt;- rep(NA, NMC);\n  for ( i in 1:NMC) {\n    fake_data &lt;- rexp(nsamp, rate=estd_rate);\n    # We want to estimate the sampling distribution of the estimated\n    # rate parameter from the data.\n    replicates[i] &lt;- 1/mean(fake_data);\n  }\n  \n  # Now construct the confidence interval\n  # the names=FALSE tells R to return the quantiles with no header\n  CI &lt;- quantile( replicates, probs=c(0.025, 0.975), names=FALSE );\n  return( (CI[1] &lt; true_rate) & (true_rate &lt; CI[2]) );\n}\n\n# Now let's run the above experiment a few thousand times\n# and see how often we \"catch\" the true value of lambda.\nlambda_true &lt;- 2;\nnsamp &lt;- 100;\nM &lt;- 2000;\nreps &lt;- rep(NA, M);\nfor (i in 1:M) {\n  reps[i] &lt;- run_exprate_expt( nsamp, lambda_true )\n}\n\nsum(reps)/M\n\n[1] 0.947\n\n\nOne important thing to bear in mind is that while the simulation-based recipe demonstrated above is a fairly general approach, it only produces an approximately correct confidence interval, which relies on how well we can estimate the model parameter to begin with.\nExercise: try playing around with nsamp and lambda_true in the code above. Consider what happens if nsamp is small (e.g., 20 or 25): \\(\\bar{X}\\) will not necessarily be close to \\(\\mathbb{E} \\bar{X} = 1/\\lambda\\), and thus our estimate of \\(\\lambda\\), \\(1/\\bar{X}\\) will likely be far from \\(\\lambda\\), and thus our “fake” data will not actually look very much like it should. In another direction, try playing around with the parameter \\(\\lambda &gt; 0\\). What happens if you make \\(\\lambda\\) really big or really close to zero?\nIn your discussion section and homework, you’ll get more practice with this framework. The important takeaway is this: our goal in producing lots of replicates is to get a sense of what “reasonable” other values of our statistic might have been.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#confidence-intervals-another-way-the-clt",
    "href": "estimation2.html#confidence-intervals-another-way-the-clt",
    "title": "22  Estimation Part 2",
    "section": "22.7 Confidence intervals another way: the CLT",
    "text": "22.7 Confidence intervals another way: the CLT\nNow, how do we relate the above framework to the version of confidence intervals that you might have learned in your intro courses (if you saw CIs previously; if you didn’t, no worries)?\nThe central limit theorem states that if \\(X_1,X_2,\\dots\\) are iid random variables with shared mean \\(\\mu = \\mathbb{E} X_1\\) and variance \\(\\sigma^2 = \\operatorname{Var} X_1\\), then as \\(n \\rightarrow \\infty\\), the recentered, rescaled random variable \\[\n\\frac{ \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu }{ \\sqrt{ \\sigma^2 / n } }\n\\] is well approximated by a standard normal.\nIn your discussion section and your homework, you’ll run some experiments to explore this phenomenon further.\nSpecifically, what we mean by “well approximated” is that, letting \\(Z \\sim \\operatorname{Normal}(0,1)\\), for all \\(t \\in \\mathbb{R}\\),\n\\[\n\\lim_{n \\rightarrow \\infty} \\Pr\\left[ \\frac{ \\frac{1}{n} \\sum_{i=1}^n X_i - \\mu }{ \\sqrt{\\sigma^2/n} } \\le t \\right]\n= \\Pr[ Z \\le t ].\n\\]\nAside: if you’ve seen convergence in distribution before, this should look familiar– this says that the sample mean (after centering and scaling) converges in distribution to a standard normal. If you haven’t seen convergence in distribution before, not to worry. It’s just the technical name for the limit property in the equation above, and it won’t be on an exam this semester.\nSaid another way, the central limit theorem says that for large sample size, the distribution of the sample mean \\(\\bar{X}\\) is well approximated by a normal random variable with mean \\(\\mu = \\mathbb{E} X_1\\) and variance \\(\\sigma^2/n = n^{-1} \\operatorname{Var} X_1\\).\n\n22.7.1 The CLT in action\nLet’s quickly do an experiment that you’ve probably seen before: watching the CLT in action.\nAgain, the CLT says that if we take a sample of i.i.d. random variables, take their mean, center it about their expectation, and rescale them by their standard deviation, they should be approximately normal.\nLet’s have a look.\n\nNMC &lt;- 1e3;\n\n# Code to draw n iid RVs from exp(1) and compute their sample mean.\ngenerate_sample_mean &lt;- function( n ) {\n  X &lt;- rexp(n=n, rate=1);\n  # X has mean 1, variance 1\n  # See https://en.wikipedia.org/wiki/Exponential_distribution\n  return( mean(X) )\n}\n\n# Generate a bunch of such sample means, each on n=10 data points\nXbar_sample_means &lt;- rep( NA, NMC);\nfor( i in 1:NMC ) {\n  Xbar_sample_means[i] &lt;- generate_sample_mean(10);\n}\n\n# We center about the mean 1,\n# and rescale by the standard deviation of the sample mean,\n# sqrt( ( Var X_1 )/10 ) = sqrt(1/10)\nhist( (Xbar_sample_means-1)/sqrt(1/10) )\n\n\n\n\n\n\n\n\n\n# Now let's try again, but with a larger sample size.\n# Changing n=10 to n=15.\n\nXbar_sample_means &lt;- rep( NA, NMC);\nfor( i in 1:NMC ) {\n  Xbar_sample_means[i] &lt;- generate_sample_mean(15);\n}\n\nhist( (Xbar_sample_means-1)/(1/sqrt(15)) )\n\n\n\n\n\n\n\n\nLet’s try one more, this time with \\(n=20\\) data points.\n\nXbar_sample_means &lt;- rep( NA, NMC);\nfor( i in 1:NMC ) {\n  Xbar_sample_means[i] &lt;- generate_sample_mean(20);\n}\n\nhist( (Xbar_sample_means-1)/(1/sqrt(20)) )\n\n\n\n\n\n\n\n\nNow let’s crank it all the way up to \\(n=30\\).\n\nXbar_sample_means &lt;- rep( NA, NMC);\nfor( i in 1:NMC ) {\n  Xbar_sample_means[i] &lt;- generate_sample_mean(30);\n}\n\nhist( (Xbar_sample_means-1)/(1/sqrt(30)) )\n\n\n\n\n\n\n\n\nWith just \\(20\\) to \\(30\\) samples, the CLT is really kicking in– that last histogram looks pretty undeniably normal to me!\nJust to reiterate, the underlying data here look nothing like normals:\n\nX &lt;- rexp(n=40, rate=1);\n# X has mean 1, variance 1\n# See https://en.wikipedia.org/wiki/Exponential_distribution\n\nhist( X-1 )\n\n\n\n\n\n\n\n\nNow, the variance of our data above is \\(1\\), so there is no need to worry about the rescaling part of the CLT. Let’s look at a different example where the variance is not \\(1\\) to see what that means.\n\nNMC &lt;- 1e3;\n\ngenerate_sample_mean &lt;- function( n ) {\n  X &lt;- rnorm(n=10, mean=1, sd=2);\n  return( mean(X) )\n}\n\nXbar_sample_means &lt;- rep( NA, NMC);\nfor( i in 1:NMC ) {\n  Xbar_sample_means[i] &lt;- generate_sample_mean(n);\n}\n\nhist( Xbar_sample_means-1 );\n\n\n\n\n\n\n\n\nExercise: repeat the above experiment with a different data distribution and see how large \\(n\\) has to be for the CLT to look like a reasonably good distribution. If you play around enough with different distributions and different choices of parameter values, you should be able to find two different data distributions that have decidedly different behavior in terms of when the centered, rescaled sample mean starts to “look” normal.\n\n\n22.7.2 Using the CLT to build a CI\nSo the CLT says that the (recentered, rescaled) sample mean “looks like” a standard normal once \\(n\\) is large enough.\nNote: like the LLN, the CLT doesn’t tell us how large is “large enough”, but let’s assume we’re good and press on.\nWell, looking like the standard normal is good– we know how to compute quantiles for the standard normal (cue flashback to Z-scores from your intro class…)!\nAnd that means we can compute (approximate) quantiles for the (recentered, rescaled) sample mean, \\[\n\\frac{ \\bar{X} - \\mu }{ \\sqrt{ \\sigma^2 / n } },\n\\]\nThe basic idea is that we know our quantiles for the standard normal.\nFor example, if \\(Z \\sim \\operatorname{Normal}(0,1)\\), then \\[\n0.95 = \\Pr[ -1.96 \\le Z \\le 1.96 ]\n\\approx \\Pr\\left[ -1.96 \\le \\frac{ \\bar{X} - \\mu }{ \\sqrt{ \\sigma^2 / n } } \\le 1.96 \\right].\n\\]\nRearranging terms inside that second probability, \\[\n0.95\n\\approx \\Pr\\left[ \\bar{X} - 1.96 \\sqrt{\\sigma^2/n} \\le \\mu \\le \\bar{X} + 1.96 \\sqrt{\\sigma^2/n} \\right]\n\\] so \\(\\bar{X} \\pm 1.96 \\sqrt{ \\sigma^2/n}\\) would be an (approximate) 95% CI for \\(\\mu\\) if only we knew \\(\\sigma^2\\).\nThat’s a pesky issue that we will ignore here, but you’ll get to play around with estimating \\(\\sigma^2\\) in your homework (see below for an example of the case where we estimate the variance).\nThis confidence interval is only approximate, because it relies on the CLT, in much the same way as our simulation-based procedure above was only approximate.\n\n\n22.7.3 Example: confidence interval for the mean using CLT\nLet’s consider the problem of estimating the mean of a geometric random variable. For a geometric random variable with parameter \\(p\\), the mean is \\((1-p)/p\\) (see ?rgeom).\nSuppose that we observe data \\(X_1,X_2,\\dots,X_n\\) drawn i.i.d. according to a geometric distribution with parameter \\(p\\). To obtain a confidence interval for the mean, we begin by noting that the expectation of a geometric random variable \\[\n\\mathbb{E} \\bar{X} = \\frac{1-p}{p},\n\\]\nand the variance is \\((1-p)/(n p^2)\\) (the variance of a single geometric RV is \\((1-p)/p^2\\), so the variance of a mean of \\(n\\) of them is this quantity divided by \\(n\\)). You can check both of these facts on the Wikipedia page linked above. Bear in mind that in R, the geometric RV counts the number of flips before the first heads, so it can take the value \\(0\\).\nThe central limit theorem says that the quantity \\[\n\\frac{ \\bar{X} - (1-p)/p }{ \\sqrt{(1-p)/n}/p }\n\\]\nshould be (approximately) normal with mean zero and variance one.\nLet’s first check that this is the case.\n\nptrue &lt;- 0.1;\nnsamp &lt;- 30;\nM &lt;- 2000;\nreplicates &lt;- rep(NA, M);\nfor( i in 1:M) {\n  data &lt;- rgeom(n=nsamp, prob=ptrue);\n  replicates[i] &lt;- mean( data ); # = Xbar.\n}\n\ntrue_mean &lt;- (1-ptrue)/ptrue;\ntrue_var &lt;- (1-ptrue)/ptrue^2;\nhist( (replicates-true_mean)/sqrt(true_var/nsamp) );\n\n\n\n\n\n\n\n\nThe approximation isn’t perfect– the fact that geometric RVs are non-negative means that the sampling distribution of the sample mean is skewed to the right. Further, the geometric distribution is discrete, which complicates things further. Still, let’s press on and see if this is a good enough approximation.\nNote: you may want to look at what happens if you decrease the number of samples (e.g., down to 25) or increase them (e.g., up to 35 or 40).\nNow, let’s use this fact to construct a CLT-based confidence interval for the mean \\(1/p\\) under the geometric distribution.\nSuppose that we observe the following data:\n\ndata &lt;- c( 2, 0, 1, 1, 0, 3, 2, 2, 0, 0, 1, 6, 0, 1, 0, 0, 0, 11, 0, 1, 0, 1, 0, 2, 0, 2, 1, 2, 1, 0 );\n\nOur sample mean is\n\nxbar &lt;- mean(data);\nxbar\n\n[1] 1.333333\n\n\nNow, to construct our confidence interval for the mean \\((1-p)/p\\), we need to have an estimate for the variance. Remember, \\[\n\\frac{ \\bar{X} - (1-p)/p }{ \\sqrt{(1-p)/n}/p }\n\\] will be approximately normal.\nOne option for estimating the variance would be to estimate \\(p\\) and then plug that into the expression for the variance. Exercise: try that!\nHere, though, let’s estimate the variance the easy way– computing the sample variance of the data.\n\nvarhat &lt;- var( data );\nvarhat\n\n[1] 4.988506\n\n\nPlugging this into the CLT, a \\(0.95\\) confidence interval is\n\nn &lt;- length( data )\nc( xbar - 1.96*sqrt( varhat/n ), xbar + 1.96*sqrt( varhat/n ))\n\n[1] 0.5340869 2.1325797\n\n\nThe true parameter was \\(p = 0.33\\), so the true mean is\n\ntruep &lt;- 0.33;\ntruemean &lt;- (1-truep)/truep\ntruemean\n\n[1] 2.030303\n\n\nOur CI caught the mean! Great!\nExercise: repeat our experiment from earlier to estimate how often the above CLT-based confidence interval contains the true mean. You’ll want to repeatedly\n\nGenerate \\(n=30\\) random geometric RVs with success probability \\(p=0.33\\).\nCompute the sample mean and sample variance.\nUse these to build the confidence interval\nCheck whether or not the CI contains the true mean \\((1-p)/p = 2.03030\\).\n\nRepeat the above a couple of thousand times and check how often the CI contains the true mean. It should be about \\(0.95\\), though of course it won’t be exact, both because of randomness and because the CLT is only an approximation.",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#which-confidence-interval",
    "href": "estimation2.html#which-confidence-interval",
    "title": "22  Estimation Part 2",
    "section": "22.8 Which confidence interval?",
    "text": "22.8 Which confidence interval?\nNow you know two completely different ways to construct confidence intervals! Unfortunately, which one is the “right one” is a tricky question.\nSometimes it’s a question of which one is easier (i.e., requires less computer time and/or thinking time).\nSometimes there are obvious reasons why one or the other will be more accurate than the other (e.g., because you know that the CLT is a good approximation even for small \\(n\\) for the data distribution).",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#duality-of-testing-and-confidence-intervals",
    "href": "estimation2.html#duality-of-testing-and-confidence-intervals",
    "title": "22  Estimation Part 2",
    "section": "22.9 Duality of testing and confidence intervals",
    "text": "22.9 Duality of testing and confidence intervals\nOne important observation that we should make (but not one that you need to know for an exam!), is that there is a connection between confidence intervals, which we have been discussing, and the problem of testing that we discussed in previous lectures.\nLet’s recall that the idea behind a confidence interval is that it is a random interval, and a confidence interval with confidence level \\(1-\\alpha\\) should contain our parameter of interest with probability \\(1-\\alpha\\).\nLet’s call our random interval \\(C = (L,U)\\), bearing in mind that \\(C\\) is a random interval, which depends on our observed data. In particular, letting \\(\\theta\\) denote the true value of our parameter of interest, our confidence interval \\(C\\) obeys the property that \\[\n\\Pr[ \\theta \\in C ] = \\Pr[ L \\le \\theta \\le U ] = 1 - \\alpha.\n\\]\nSuppose that we wish to test the hypothesis \\[\nH_0 : \\theta = \\theta_0,\n\\]\nwhere \\(\\theta_0\\) is some particular value. For example, perhaps the parameter \\(\\theta\\) describes the size of the difference between two groups, and \\(\\theta_0\\) corresponds to that difference being zero.\nConsider the following test of \\(H_0\\): given our data, compute the confidence interval \\(C\\). If \\(\\theta_0 \\in C\\), accept \\(H_0\\). If \\(\\theta_0 \\not \\in C\\), reject \\(H_0\\).\nIt turns out that if \\(C\\) is a \\(100(1-\\alpha)\\) confidence interval, then this test has level \\(\\alpha\\). Let’s see why this is.\nBy the definition of our test, \\[\n\\Pr[ \\text{ reject } ; H_0 ]\n=\n\\Pr[ \\theta_0 \\not \\in C; H_0 ]\n=\n1-\\Pr[ \\theta_0 \\in C; H_0 ].\n\\]\nNow, because \\(C\\) is a \\(100(1-\\alpha)\\)-level confidence interval, \\(\\Pr[ \\theta_0 \\in C; H_0 ] = 1-\\alpha\\). Therefore, \\[\n\\Pr[ \\text{ reject } ; H_0 ] = \\alpha.\n\\]\nThat is, our test has level \\(\\alpha\\).\nIt turns out that this argument runs the other way, too: given any test with level \\(\\alpha\\), we can construct a \\(100(1-\\alpha)\\) confidence interval.\nChallenge: sketch how we might do this. Imagine that we have a procedure such that for any value of \\(\\theta_0\\), we can conduct a level-\\(\\alpha\\) test of the hypothesis \\[\nH_0 : \\theta = \\theta_0.\n\\] Explain how to turn this into a \\(100(1-\\alpha)\\) confidence interval for \\(\\theta\\).",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "estimation2.html#review",
    "href": "estimation2.html#review",
    "title": "22  Estimation Part 2",
    "section": "22.10 Review",
    "text": "22.10 Review\n\nUncertainty in point estimates\nProbability intervals for estimator values\nMC Confidence interval for parameter value\nThe Central Limit Theorem\nParametric Confidence intervals (CLT)\nComparison of MC vs CLT estimation assumptions\nDuality of testing & estimation",
    "crumbs": [
      "Estimation",
      "<span class='chapter-number'>22</span>  <span class='chapter-title'>Estimation Part 2</span>"
    ]
  },
  {
    "objectID": "prediction1.html",
    "href": "prediction1.html",
    "title": "25  Prediction (Simple Linear Regression",
    "section": "",
    "text": "25.1 Learning objectives\nAfter this lesson, you will be able to",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>25</span>  <span class='chapter-title'>Prediction (Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "prediction2.html",
    "href": "prediction2.html",
    "title": "28  Prediction (Multiple Linear Regression)",
    "section": "",
    "text": "28.1 Multiple regression\nEverything so far is likely (mostly) familiar to you from STAT240: regressing one variable against another. What happens, however, when we want to incorporate multiple different predictors in our model?\nExample: salary and education Recall from last week’s lecture our data set relating salaries to years of education. We noted that there were many other factors that might predict salary in addition to simply years of education, and that their inclusion in our model would improve our predictive accuracy. For example, we might add additional information concerning college major, demographic information, parents’ level of education, etc.\nExample: housing prices Suppose that we are looking to invest in real estate. To do that, it would be quite useful if we could predict, based on what we know about a house, how much that house is likely to sell for. One approach would be to predict housing price based on a collection of information such as square footage, age of the house, number of bedrooms, proximity to parks, etc.",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>28</span>  <span class='chapter-title'>Prediction (Multiple Linear Regression)</span>"
    ]
  },
  {
    "objectID": "logistic.html",
    "href": "logistic.html",
    "title": "31  Logistic Regression",
    "section": "",
    "text": "31.1 Learning objectives\nAfter this lecture, you will be able to",
    "crumbs": [
      "Prediction",
      "<span class='chapter-number'>31</span>  <span class='chapter-title'>Logistic Regression</span>"
    ]
  }
]